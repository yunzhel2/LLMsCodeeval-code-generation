{"diff_hunk":"@@ -163,6 +163,9 @@ int\n main(int argc, char **argv)\n {\n     test_query(SIGTERM);\n+#if !defined(MACOS)\n+    test_rt_sigprocmask();\n+#endif\n #if !defined(MACOS) && !defined(X64)\n     test_non_rt_sigaction(SIGPIPE);\n #endif","old_code":"\/* **********************************************************\n * Copyright (c) 2015-2016 Google, Inc.  All rights reserved.\n * **********************************************************\/\n\n\/*\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * * Redistributions of source code must retain the above copyright notice,\n *   this list of conditions and the following disclaimer.\n *\n * * Redistributions in binary form must reproduce the above copyright notice,\n *   this list of conditions and the following disclaimer in the documentation\n *   and\/or other materials provided with the distribution.\n *\n * * Neither the name of Google, Inc. nor the names of its contributors may be\n *   used to endorse or promote products derived from this software without\n *   specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL GOOGLE, INC. OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n *\/\n\n\/*\n * test of sigaction\n *\/\n#include \"tools.h\"\n\n#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <signal.h>\n#include <sys\/types.h>\n#include <unistd.h>\n#include <sys\/syscall.h>\n\n#define SENTINEL 0x12345678UL\n\n#if !defined(MACOS) && !defined(X64)\ntypedef struct old_sigaction_t {\n    void (*handler)(int, siginfo_t *, void *);\n    unsigned int sa_mask;\n    unsigned long sa_flags;\n    void (*sa_restorer)(void);\n} old_sigaction_t;\n#endif\n\nstatic void\ntest_query(int sig)\n{\n    \/* i#1984: test that the prior action is returned *\/\n    int rc;\n    struct sigaction first_act;\n    struct sigaction new_act;\n    struct sigaction old_act;\n    memset((void *)&first_act, 0, sizeof(first_act));\n    first_act.sa_sigaction = (void (*)(int, siginfo_t *, void *))SENTINEL;\n    sigemptyset(&first_act.sa_mask);\n    sigaddset(&first_act.sa_mask, SIGUSR1);\n    sigaddset(&first_act.sa_mask, SIGUSR2);\n    rc = sigaction(sig, &first_act, NULL);\n    assert(rc == 0);\n\n    \/* Test with nothing. *\/\n    rc = sigaction(sig, NULL, NULL);\n    assert(rc == 0);\n\n    \/* Test without a new action. *\/\n    memset((void *)&old_act, 0xff, sizeof(old_act));\n    rc = sigaction(sig, NULL, &old_act);\n    assert(rc == 0 && old_act.sa_sigaction == first_act.sa_sigaction &&\n           \/* The flags do not match due to SA_RESTORER. *\/\n           \/* The rest of mask is uninit stack values from the libc wrapper. *\/\n           *(long *)&old_act.sa_mask == *(long *)&first_act.sa_mask);\n\n    \/* Test with a new action. *\/\n    memset((void *)&old_act, 0xff, sizeof(old_act));\n    memset((void *)&new_act, 0, sizeof(new_act));\n    new_act.sa_sigaction = (void (*)(int, siginfo_t *, void *))SIG_IGN;\n    sigemptyset(&new_act.sa_mask);\n    rc = sigaction(sig, &new_act, &old_act);\n    assert(rc == 0 && old_act.sa_sigaction == first_act.sa_sigaction &&\n           \/* The flags do not match due to SA_RESTORER. *\/\n           \/* The rest of mask is uninit stack values from the libc wrapper. *\/\n           *(long *)&old_act.sa_mask == *(long *)&first_act.sa_mask);\n\n    \/* Test pattern from i#1984 issue and ensure no assert. *\/\n    memset(&new_act, 0, sizeof(new_act));\n    memset(&old_act, 0, sizeof(old_act));\n    new_act.sa_sigaction = (void (*)(int, siginfo_t *, void *))SENTINEL;\n    sigaction(SIGINT, &new_act, 0);\n    sigaction(SIGINT, &new_act, &old_act);\n    new_act.sa_handler = SIG_IGN;\n    sigaction(SIGTSTP, &new_act, &old_act);\n}\n\nstatic void\nset_sigaction_handler(int sig, void *action)\n{\n    int rc;\n    struct sigaction act;\n    memset((void *)&act, 0, sizeof(act));\n    act.sa_sigaction = (void (*)(int, siginfo_t *, void *))action;\n    \/* Arm the signal. *\/\n    rc = sigaction(sig, &act, NULL);\n    assert(rc == 0);\n}\n\n#if !defined(MACOS) && !defined(X64)\nstatic void\ntest_non_rt_sigaction(int sig)\n{\n    int rc;\n    old_sigaction_t first_act;\n    old_sigaction_t new_act;\n    old_sigaction_t old_act;\n    memset((void *)&first_act, 0, sizeof(first_act));\n    first_act.handler = (void (*)(int, siginfo_t *, void *))SENTINEL;\n    first_act.sa_mask |= (1 << SIGUSR1);\n    first_act.sa_mask |= (1 << SIGUSR2);\n    rc = dynamorio_syscall(SYS_sigaction, 3, sig, &first_act, NULL);\n    assert(rc == 0);\n\n    \/* Test with nothing. *\/\n    rc = dynamorio_syscall(SYS_sigaction, 3, sig, NULL, NULL);\n    assert(rc == 0);\n\n    \/* Test passing NULL to non-rt sigaction, which is used on Android (i#1822) *\/\n    memset((void *)&old_act, 0xff, sizeof(old_act));\n    rc = dynamorio_syscall(SYS_sigaction, 3, sig, NULL, &old_act);\n    assert(rc == 0 && old_act.handler == first_act.handler &&\n           \/* The flags do not match due to SA_RESTORER. *\/\n           \/* The rest of mask is uninit stack values from the libc wrapper. *\/\n           *(long *)&old_act.sa_mask == *(long *)&first_act.sa_mask);\n\n    \/* Test with a new action. *\/\n    memset((void *)&old_act, 0xff, sizeof(old_act));\n    memset((void *)&new_act, 0, sizeof(new_act));\n    new_act.handler = (void (*)(int, siginfo_t *, void *))SIG_IGN;\n    rc = dynamorio_syscall(SYS_sigaction, 3, sig, &new_act, &old_act);\n    assert(rc == 0 && old_act.handler == first_act.handler &&\n           \/* The flags do not match due to SA_RESTORER. *\/\n           \/* The rest of mask is uninit stack values from the libc wrapper. *\/\n           *(long *)&old_act.sa_mask == *(long *)&first_act.sa_mask);\n\n    \/* Clear handler *\/\n    memset((void *)&new_act, 0, sizeof(new_act));\n    rc = dynamorio_syscall(SYS_sigaction, 3, sig, &new_act, NULL);\n    assert(rc == 0);\n}\n#endif\n\nint\nmain(int argc, char **argv)\n{\n    test_query(SIGTERM);\n#if !defined(MACOS) && !defined(X64)\n    test_non_rt_sigaction(SIGPIPE);\n#endif\n    set_sigaction_handler(SIGTERM, (void *)SIG_IGN);\n    print(\"Sending SIGTERM first time\\n\");\n    kill(getpid(), SIGTERM);\n    set_sigaction_handler(SIGTERM, (void *)SIG_DFL);\n    print(\"Sending SIGTERM second time\\n\");\n    kill(getpid(), SIGTERM);\n    print(\"Should not be reached\\n\");\n}\n","lang_cluster":"C","length":176,"code_uid":"6594c78e0599474284d91259e5a4b6df"}
{"diff_hunk":"@@ -147,17 +147,12 @@ class TestCtu(unittest.TestCase):\n         for arch in glob.glob(os.path.join(ctu_dir, '*')):\n             fn_map_file = os.path.join(ctu_dir, arch, 'externalFnMap.txt')\n             self.assertTrue(os.path.isfile(fn_map_file))\n-            if not reparse:\n-                ast_dir = os.path.join(ctu_dir, arch, 'ast')\n-                self.assertTrue(os.path.isdir(ast_dir))\n \n-    def __do_ctu_analyze(self, reparse):\n+    def __do_ctu_analyze(self):\n         \"\"\" Execute CTU analyze phase. \"\"\"\n \n         cmd = [self._codechecker_cmd, 'analyze', '-o', self.report_dir,\n                '--analyzers', 'clangsa', '--ctu-analyze']\n-        if reparse:\n-            cmd.append('--ctu-on-the-fly')\n         cmd.append(self.buildlog)\n         out, _ = call_command(cmd, cwd=self.test_dir, env=self.env)\n         return out","old_code":"#\n# -----------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -----------------------------------------------------------------------------\n\"\"\" CTU function test.\"\"\"\n\nimport glob\nimport json\nimport os\nimport shutil\nimport unittest\n\nfrom libtest import env\nfrom libtest.codechecker import call_command\n\nNO_CTU_MESSAGE = \"CTU is not supported\"\n\n\nclass TestCtu(unittest.TestCase):\n    \"\"\" Test CTU functionality. \"\"\"\n\n    def setUp(self):\n        \"\"\" Set up workspace.\"\"\"\n\n        # TEST_WORKSPACE is automatically set by test package __init__.py .\n        self.test_workspace = os.environ['TEST_WORKSPACE']\n\n        test_class = self.__class__.__name__\n        print('Running ' + test_class + ' tests in ' + self.test_workspace)\n\n        # Get the CodeChecker cmd if needed for the tests.\n        self._codechecker_cmd = env.codechecker_cmd()\n        self.env = env.codechecker_env()\n        self.report_dir = os.path.join(self.test_workspace, 'reports')\n        os.makedirs(self.report_dir)\n        self.test_dir = os.path.join(os.path.dirname(__file__), 'test_files')\n\n        # Get if clang is CTU-capable or not.\n        cmd = [self._codechecker_cmd, 'analyze', '-h']\n        output, _ = call_command(cmd, cwd=self.test_dir, env=self.env)\n        self.ctu_capable = '--ctu-' in output\n        print(\"'analyze' reported CTU-compatibility? \" + str(self.ctu_capable))\n\n        # Fix the \"template\" build JSONs to contain a proper directory\n        # so the tests work.\n        raw_buildlog = os.path.join(self.test_dir, 'buildlog.json')\n        with open(raw_buildlog) as log_file:\n            build_json = json.load(log_file)\n            for command in build_json:\n                command['directory'] = self.test_dir\n\n        self.__old_pwd = os.getcwd()\n        os.chdir(self.test_workspace)\n        self.buildlog = os.path.join(self.test_workspace, 'buildlog.json')\n        with open(self.buildlog, 'w') as log_file:\n            json.dump(build_json, log_file)\n\n    def tearDown(self):\n        \"\"\" Tear down workspace.\"\"\"\n\n        shutil.rmtree(self.report_dir, ignore_errors=True)\n        os.chdir(self.__old_pwd)\n\n    def test_ctu_all_no_reparse(self):\n        \"\"\" Test full CTU without reparse. \"\"\"\n\n        self.__test_ctu_all(False)\n\n    def test_ctu_collect_no_reparse(self):\n        \"\"\" Test CTU collect phase without reparse. \"\"\"\n\n        self.__test_ctu_collect(False)\n\n    def test_ctu_analyze_no_reparse(self):\n        \"\"\" Test CTU analyze phase without reparse. \"\"\"\n\n        self.__test_ctu_analyze(False)\n\n    def test_ctu_all_reparse(self):\n        \"\"\" Test full CTU with reparse. \"\"\"\n\n        self.__test_ctu_all(True)\n\n    def test_ctu_collect_reparse(self):\n        \"\"\" Test CTU collect phase with reparse. \"\"\"\n\n        self.__test_ctu_collect(True)\n\n    def test_ctu_analyze_reparse(self):\n        \"\"\" Test CTU analyze phase with reparse. \"\"\"\n\n        self.__test_ctu_analyze(True)\n\n    def __test_ctu_all(self, reparse):\n        \"\"\" Test full CTU. \"\"\"\n\n        if not self.ctu_capable:\n            self.skipTest(NO_CTU_MESSAGE)\n        output = self.__do_ctu_all(reparse)\n        self.__check_ctu_analyze(output)\n\n    def __test_ctu_collect(self, reparse):\n        \"\"\" Test CTU collect phase. \"\"\"\n\n        if not self.ctu_capable:\n            self.skipTest(NO_CTU_MESSAGE)\n        self.__do_ctu_collect(reparse)\n        self.__check_ctu_collect(reparse)\n\n    def __test_ctu_analyze(self, reparse):\n        \"\"\" Test CTU analyze phase. \"\"\"\n\n        if not self.ctu_capable:\n            self.skipTest(NO_CTU_MESSAGE)\n        self.__do_ctu_collect(reparse)\n        output = self.__do_ctu_analyze(reparse)\n        self.__check_ctu_analyze(output)\n\n    def __do_ctu_all(self, reparse):\n        \"\"\" Execute a full CTU run. \"\"\"\n\n        cmd = [self._codechecker_cmd, 'analyze', '-o', self.report_dir,\n               '--analyzers', 'clangsa', '--ctu-all']\n        if reparse:\n            cmd.append('--ctu-on-the-fly')\n        cmd.append(self.buildlog)\n        out, _ = call_command(cmd, cwd=self.test_dir, env=self.env)\n        return out\n\n    def __do_ctu_collect(self, reparse):\n        \"\"\" Execute CTU collect phase. \"\"\"\n\n        cmd = [self._codechecker_cmd, 'analyze', '-o', self.report_dir,\n               '--analyzers', 'clangsa', '--ctu-collect']\n        if reparse:\n            cmd.append('--ctu-on-the-fly')\n        cmd.append(self.buildlog)\n        call_command(cmd, cwd=self.test_dir, env=self.env)\n\n    def __check_ctu_collect(self, reparse):\n        \"\"\" Check artifacts of CTU collect phase. \"\"\"\n\n        ctu_dir = os.path.join(self.report_dir, 'ctu-dir')\n        self.assertTrue(os.path.isdir(ctu_dir))\n        for arch in glob.glob(os.path.join(ctu_dir, '*')):\n            fn_map_file = os.path.join(ctu_dir, arch, 'externalFnMap.txt')\n            self.assertTrue(os.path.isfile(fn_map_file))\n            if not reparse:\n                ast_dir = os.path.join(ctu_dir, arch, 'ast')\n                self.assertTrue(os.path.isdir(ast_dir))\n\n    def __do_ctu_analyze(self, reparse):\n        \"\"\" Execute CTU analyze phase. \"\"\"\n\n        cmd = [self._codechecker_cmd, 'analyze', '-o', self.report_dir,\n               '--analyzers', 'clangsa', '--ctu-analyze']\n        if reparse:\n            cmd.append('--ctu-on-the-fly')\n        cmd.append(self.buildlog)\n        out, _ = call_command(cmd, cwd=self.test_dir, env=self.env)\n        return out\n\n    def __check_ctu_analyze(self, output):\n        \"\"\" Check artifacts of CTU analyze phase. \"\"\"\n\n        self.assertNotIn(\"Failed to analyze\", output)\n        self.assertIn(\"analyzed lib.c successfully\", output)\n        self.assertIn(\"analyzed main.c successfully\", output)\n\n        cmd = [self._codechecker_cmd, 'parse', self.report_dir]\n        output, _ = call_command(cmd, cwd=self.test_dir, env=self.env)\n        self.assertIn(\"no defects while analyzing lib.c\", output)\n        self.assertIn(\"defect(s) while analyzing main.c\", output)\n        self.assertIn(\"lib.c:3:\", output)\n        self.assertIn(\"[core.NullDereference]\", output)\n","lang_cluster":"C","length":177,"code_uid":"d5c076c392d3470d92cda7f1ae46a189"}
{"diff_hunk":"@@ -4,6 +4,7 @@\n #include \"..\/mem\/pool.h\"\n #include \"ponyassert.h\"\n #include <string.h>\n+#include <dtrace.h>\n \n #ifdef USE_VALGRIND\n #include <valgrind\/helgrind.h>","old_code":"#define PONY_WANT_ATOMIC_DEFS\n\n#include \"messageq.h\"\n#include \"..\/mem\/pool.h\"\n#include \"ponyassert.h\"\n#include <string.h>\n\n#ifdef USE_VALGRIND\n#include <valgrind\/helgrind.h>\n#endif\n\n#ifndef NDEBUG\n\nstatic size_t messageq_size_debug(messageq_t* q)\n{\n  pony_msg_t* tail = q->tail;\n  size_t count = 0;\n\n  while(atomic_load_explicit(&tail->next, memory_order_relaxed) != NULL)\n  {\n    count++;\n    tail = atomic_load_explicit(&tail->next, memory_order_relaxed);\n  }\n\n  return count;\n}\n\n#endif\n\nvoid ponyint_messageq_init(messageq_t* q)\n{\n  pony_msg_t* stub = POOL_ALLOC(pony_msg_t);\n  stub->index = POOL_INDEX(sizeof(pony_msg_t));\n  atomic_store_explicit(&stub->next, NULL, memory_order_relaxed);\n\n  atomic_store_explicit(&q->head, (pony_msg_t*)((uintptr_t)stub | 1),\n    memory_order_relaxed);\n  q->tail = stub;\n\n#ifndef NDEBUG\n  messageq_size_debug(q);\n#endif\n}\n\nvoid ponyint_messageq_destroy(messageq_t* q)\n{\n  pony_msg_t* tail = q->tail;\n  pony_assert((((uintptr_t)atomic_load_explicit(&q->head, memory_order_relaxed) &\n    ~(uintptr_t)1)) == (uintptr_t)tail);\n#ifdef USE_VALGRIND\n  ANNOTATE_HAPPENS_BEFORE_FORGET_ALL(tail);\n#endif\n\n  ponyint_pool_free(tail->index, tail);\n  atomic_store_explicit(&q->head, NULL, memory_order_relaxed);\n  q->tail = NULL;\n}\n\nbool ponyint_messageq_push(messageq_t* q, pony_msg_t* first, pony_msg_t* last)\n{\n  atomic_store_explicit(&last->next, NULL, memory_order_relaxed);\n\n  \/\/ Without that fence, the store to last->next above could be reordered after\n  \/\/ the exchange on the head and after the store to prev->next done by the\n  \/\/ next push, which would result in the pop incorrectly seeing the queue as\n  \/\/ empty.\n  \/\/ Also synchronise with the pop on prev->next.\n  atomic_thread_fence(memory_order_release);\n\n  pony_msg_t* prev = atomic_exchange_explicit(&q->head, last,\n    memory_order_relaxed);\n\n  bool was_empty = ((uintptr_t)prev & 1) != 0;\n  prev = (pony_msg_t*)((uintptr_t)prev & ~(uintptr_t)1);\n\n#ifdef USE_VALGRIND\n  \/\/ Double fence with Valgrind since we need to have prev in scope for the\n  \/\/ synchronisation annotation.\n  ANNOTATE_HAPPENS_BEFORE(&prev->next);\n  atomic_thread_fence(memory_order_release);\n#endif\n  atomic_store_explicit(&prev->next, first, memory_order_relaxed);\n\n  return was_empty;\n}\n\nbool ponyint_messageq_push_single(messageq_t* q, pony_msg_t* first,\n  pony_msg_t* last)\n{\n  atomic_store_explicit(&last->next, NULL, memory_order_relaxed);\n\n  \/\/ If we have a single producer, the swap of the head need not be atomic RMW.\n  pony_msg_t* prev = atomic_load_explicit(&q->head, memory_order_relaxed);\n  atomic_store_explicit(&q->head, last, memory_order_relaxed);\n\n  bool was_empty = ((uintptr_t)prev & 1) != 0;\n  prev = (pony_msg_t*)((uintptr_t)prev & ~(uintptr_t)1);\n\n  \/\/ If we have a single producer, the fence can be replaced with a store\n  \/\/ release on prev->next.\n#ifdef USE_VALGRIND\n  ANNOTATE_HAPPENS_BEFORE(&prev->next);\n#endif\n  atomic_store_explicit(&prev->next, first, memory_order_release);\n\n  return was_empty;\n}\n\npony_msg_t* ponyint_messageq_pop(messageq_t* q)\n{\n  pony_msg_t* tail = q->tail;\n  pony_msg_t* next = atomic_load_explicit(&tail->next, memory_order_relaxed);\n\n  if(next != NULL)\n  {\n    q->tail = next;\n    atomic_thread_fence(memory_order_acquire);\n#ifdef USE_VALGRIND\n    ANNOTATE_HAPPENS_AFTER(&tail->next);\n    ANNOTATE_HAPPENS_BEFORE_FORGET_ALL(tail);\n#endif\n    ponyint_pool_free(tail->index, tail);\n  }\n\n  return next;\n}\n\nbool ponyint_messageq_markempty(messageq_t* q)\n{\n  pony_msg_t* tail = q->tail;\n  pony_msg_t* head = atomic_load_explicit(&q->head, memory_order_relaxed);\n\n  if(((uintptr_t)head & 1) != 0)\n    return true;\n\n  if(head != tail)\n    return false;\n\n  head = (pony_msg_t*)((uintptr_t)head | 1);\n\n#ifdef USE_VALGRIND\n  ANNOTATE_HAPPENS_BEFORE(&q->head);\n#endif\n  return atomic_compare_exchange_strong_explicit(&q->head, &tail, head,\n    memory_order_release, memory_order_relaxed);\n}\n","lang_cluster":"C","length":146,"code_uid":"980ba026b7cb490fad27b4ab4b8ddf88"}
{"diff_hunk":"@@ -16,6 +16,7 @@\n  *  limitations under the License.\n  *\/\n #include \"stackdriver.h\"\n+#include \"stackdriver_helper.h\"\n #include \"stackdriver_operation.h\"\n \n typedef enum {","old_code":"\/*  Fluent Bit\n *  ==========\n *  Copyright (C) 2019-2020 The Fluent Bit Authors\n *  Copyright (C) 2015-2018 Treasure Data Inc.\n *\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n *\/\n#include \"stackdriver.h\"\n#include \"stackdriver_operation.h\"\n\ntypedef enum {\n    NO_OPERATION = 1,\n    OPERATION_EXISTED = 2\n} operation_status;\n\n\nvoid add_operation_field(flb_sds_t *operation_id, flb_sds_t *operation_producer, \n                         int *operation_first, int *operation_last, \n                         msgpack_packer *mp_pck)\n{    \n    msgpack_pack_str(mp_pck, 9);\n    msgpack_pack_str_body(mp_pck, \"operation\", 9);\n    msgpack_pack_map(mp_pck, 4);\n    msgpack_pack_str(mp_pck, 2);\n    msgpack_pack_str_body(mp_pck, \"id\", 2);\n    msgpack_pack_str(mp_pck, flb_sds_len(*operation_id));\n    msgpack_pack_str_body(mp_pck, *operation_id, flb_sds_len(*operation_id));\n    msgpack_pack_str(mp_pck, 8);\n    msgpack_pack_str_body(mp_pck, \"producer\", 8);\n    msgpack_pack_str(mp_pck, flb_sds_len(*operation_producer));\n    msgpack_pack_str_body(mp_pck, *operation_producer, flb_sds_len(*operation_producer));\n    msgpack_pack_str(mp_pck, 5);\n    msgpack_pack_str_body(mp_pck, \"first\", 5);\n    if (*operation_first == FLB_TRUE) {\n        msgpack_pack_true(mp_pck);\n    }\n    else {\n        msgpack_pack_false(mp_pck);\n    }\n    \n    msgpack_pack_str(mp_pck, 4);\n    msgpack_pack_str_body(mp_pck, \"last\", 4);\n    if (*operation_last == FLB_TRUE) {\n        msgpack_pack_true(mp_pck);\n    }\n    else {\n        msgpack_pack_false(mp_pck);\n    }\n}\n\n\/* Return true if operation extracted *\/\nint extract_operation(flb_sds_t *operation_id, flb_sds_t *operation_producer, \n                      int *operation_first, int *operation_last, \n                      msgpack_object *obj, int *extra_subfields)\n{\n    operation_status op_status = NO_OPERATION;\n\n    if (obj->via.map.size != 0) {    \t\n        msgpack_object_kv *p = obj->via.map.ptr;\n        msgpack_object_kv *const pend = obj->via.map.ptr + obj->via.map.size;\n\n        for (; p < pend && op_status == NO_OPERATION; ++p) {\n            if (p->val.type == MSGPACK_OBJECT_MAP && p->key.type == MSGPACK_OBJECT_STR\n                && strncmp(OPERATION_FIELD_IN_JSON, p->key.via.str.ptr, p->key.via.str.size) == 0) {\n                \n                op_status = OPERATION_EXISTED;\n                msgpack_object sub_field = p->val;\n                \n                msgpack_object_kv *tmp_p = sub_field.via.map.ptr;\n                msgpack_object_kv *const tmp_pend = sub_field.via.map.ptr + sub_field.via.map.size;\n\n                \/* Validate the subfields of operation *\/\n                for (; tmp_p < tmp_pend; ++tmp_p) {\n                    if (tmp_p->key.type != MSGPACK_OBJECT_STR) {\n                        continue;\n                    }\n                    if (strncmp(\"id\", tmp_p->key.via.str.ptr, tmp_p->key.via.str.size) == 0) {\n                        if (tmp_p->val.type != MSGPACK_OBJECT_STR) {\n                            continue;\n                        }\n                        *operation_id = flb_sds_copy(*operation_id, tmp_p->val.via.str.ptr, tmp_p->val.via.str.size);\n                    }\n                    else if (strncmp(\"producer\", tmp_p->key.via.str.ptr, tmp_p->key.via.str.size) == 0) {\n                        if (tmp_p->val.type != MSGPACK_OBJECT_STR) {\n                            continue;\n                        }\n                        *operation_producer = flb_sds_copy(*operation_producer, tmp_p->val.via.str.ptr, tmp_p->val.via.str.size);\n                    }\n                    else if (strncmp(\"first\", tmp_p->key.via.str.ptr, tmp_p->key.via.str.size) == 0) {\n                        if (tmp_p->val.type != MSGPACK_OBJECT_BOOLEAN) {\n                            continue;\n                        }\n                        if (tmp_p->val.via.boolean) {\n                            *operation_first = FLB_TRUE;\n                        }\n                    }\n                    else if (strncmp(\"last\", tmp_p->key.via.str.ptr, tmp_p->key.via.str.size) == 0) {\n                        if (tmp_p->val.type != MSGPACK_OBJECT_BOOLEAN) {\n                            continue;\n                        }\n                        if (tmp_p->val.via.boolean) {\n                            *operation_last = FLB_TRUE;\n                        }\n                    }\n                    else {\n                        \/* extra sub-fields *\/ \n                        *extra_subfields += 1;\n                    }\n\n                }\n            }\n        }\n    }\n    \n    return op_status == OPERATION_EXISTED;\n}\n\nvoid pack_extra_operation_subfields(msgpack_packer *mp_pck, msgpack_object *operation, int extra_subfields) {\n    msgpack_object_kv *p = operation->via.map.ptr;\n    msgpack_object_kv *const pend = operation->via.map.ptr + operation->via.map.size;\n\n    msgpack_pack_map(mp_pck, extra_subfields);\n\n    for (; p < pend; ++p) {\n        if (strncmp(\"id\", p->key.via.str.ptr, p->key.via.str.size) != 0 \n            && strncmp(\"producer\", p->key.via.str.ptr, p->key.via.str.size) != 0\n            && strncmp(\"first\", p->key.via.str.ptr, p->key.via.str.size) != 0\n            && strncmp(\"last\", p->key.via.str.ptr, p->key.via.str.size) != 0) {\n            msgpack_pack_object(mp_pck, p->key);\n            msgpack_pack_object(mp_pck, p->val);\n        }\n    }\n\n}\n","lang_cluster":"C","length":144,"code_uid":"a08d342c852a495c8dd0d7979d3870f8"}
{"diff_hunk":"@@ -23,15 +23,20 @@ import (\n \t\"github.com\/projectcalico\/libcalico-go\/lib\/net\"\n )\n \n-\/\/ DataplanePassthru simply passes through some datamodel updates to the dataplane layer.\n-\/\/ It maps OnUpdate() calls to dedicated method calls for consistency with the\n-\/\/ rest of the dataplane API.\n+\/\/ DataplanePassthru passes through some datamodel updates to the dataplane layer, removing some\n+\/\/ duplicates along the way.  It maps OnUpdate() calls to dedicated method calls for consistency\n+\/\/ with the rest of the dataplane API.\n type DataplanePassthru struct {\n \tcallbacks passthruCallbacks\n+\n+\thostIPs map[string]*net.IP\n }\n \n func NewDataplanePassthru(callbacks passthruCallbacks) *DataplanePassthru {\n-\treturn &DataplanePassthru{callbacks: callbacks}\n+\treturn &DataplanePassthru{\n+\t\tcallbacks: callbacks,\n+\t\thostIPs:   map[string]*net.IP{},\n+\t}\n }\n \n func (h *DataplanePassthru) RegisterWith(dispatcher *dispatcher.Dispatcher) {","old_code":"\/\/ Copyright (c) 2016-2017 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage calc\n\nimport (\n\tlog \"github.com\/Sirupsen\/logrus\"\n\n\t\"github.com\/projectcalico\/felix\/dispatcher\"\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/backend\/api\"\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/backend\/model\"\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/net\"\n)\n\n\/\/ DataplanePassthru simply passes through some datamodel updates to the dataplane layer.\n\/\/ It maps OnUpdate() calls to dedicated method calls for consistency with the\n\/\/ rest of the dataplane API.\ntype DataplanePassthru struct {\n\tcallbacks passthruCallbacks\n}\n\nfunc NewDataplanePassthru(callbacks passthruCallbacks) *DataplanePassthru {\n\treturn &DataplanePassthru{callbacks: callbacks}\n}\n\nfunc (h *DataplanePassthru) RegisterWith(dispatcher *dispatcher.Dispatcher) {\n\tdispatcher.Register(model.HostIPKey{}, h.OnUpdate)\n\tdispatcher.Register(model.IPPoolKey{}, h.OnUpdate)\n}\n\nfunc (h *DataplanePassthru) OnUpdate(update api.Update) (filterOut bool) {\n\tswitch key := update.Key.(type) {\n\tcase model.HostIPKey:\n\t\thostname := key.Hostname\n\t\tif update.Value == nil {\n\t\t\tlog.WithField(\"update\", update).Debug(\"Passing-through HostIP deletion\")\n\t\t\th.callbacks.OnHostIPRemove(hostname)\n\t\t} else {\n\t\t\tlog.WithField(\"update\", update).Debug(\"Passing-through HostIP update\")\n\t\t\tip := update.Value.(*net.IP)\n\t\t\th.callbacks.OnHostIPUpdate(hostname, ip)\n\t\t}\n\tcase model.IPPoolKey:\n\t\tif update.Value == nil {\n\t\t\tlog.WithField(\"update\", update).Debug(\"Passing-through IPPool deletion\")\n\t\t\th.callbacks.OnIPPoolRemove(key)\n\t\t} else {\n\t\t\tlog.WithField(\"update\", update).Debug(\"Passing-through IPPool update\")\n\t\t\tpool := update.Value.(*model.IPPool)\n\t\t\th.callbacks.OnIPPoolUpdate(key, pool)\n\t\t}\n\t}\n\n\treturn false\n}\n","lang_cluster":"C","length":66,"code_uid":"a4494635ccb74dad8feca358acad0c7c"}
{"diff_hunk":"@@ -19,7 +19,39 @@\n \n #include \"jpath.h\"\n \n-static int jpath_set_destructive (json_t *o, char *path, json_t *val)\n+static int update_object_recursive (json_t *orig, json_t *val)\n+{\n+    const char *key;\n+    json_t *value;\n+\n+    json_object_foreach (val, key, value) {\n+        json_t *orig_value = json_object_get (orig, key);\n+\n+        if (json_is_object (value)) {\n+            if (!json_is_object (orig_value)) {\n+                json_t *o = json_object ();\n+                if (!o || json_object_set_new (orig, key, o) < 0) {\n+                    errno = ENOMEM;\n+                    json_decref (o);\n+                    return -1;\n+                }\n+                orig_value = o;\n+            }\n+            if (update_object_recursive (orig_value, value) < 0)\n+                return -1;\n+        }\n+        else if (json_object_set (orig, key, value) < 0) {\n+            errno = ENOMEM;\n+            return -1;\n+        }\n+    }\n+    return 0;\n+}\n+\n+static int jpath_set_destructive (json_t *o,\n+                                  int replace,\n+                                  char *path,\n+                                  json_t *val)\n {\n     char *cp;\n     json_t *dir;","old_code":"\/************************************************************\\\n * Copyright 2021 Lawrence Livermore National Security, LLC\n * (c.f. AUTHORS, NOTICE.LLNS, COPYING)\n *\n * This file is part of the Flux resource manager framework.\n * For details, see https:\/\/github.com\/flux-framework.\n *\n * SPDX-License-Identifier: LGPL-3.0\n\\************************************************************\/\n\n#if HAVE_CONFIG_H\n#include \"config.h\"\n#endif\n#include <errno.h>\n#include <string.h>\n#include <jansson.h>\n\n#include \"src\/common\/libutil\/errno_safe.h\"\n\n#include \"jpath.h\"\n\nstatic int jpath_set_destructive (json_t *o, char *path, json_t *val)\n{\n    char *cp;\n    json_t *dir;\n\n    if ((cp = strchr (path, '.'))) {\n        *cp++ = '\\0';\n        if (strlen (path) == 0) {\n            errno = EINVAL;\n            return -1;\n        }\n        if (!(dir = json_object_get (o, path))) {\n            if (!(dir = json_object ()))\n                goto nomem;\n            if (json_object_set_new (o, path, dir) < 0) {\n                json_decref (dir);\n                goto nomem;\n            }\n        }\n        return jpath_set_destructive (dir, cp, val);\n    }\n\n    if (strlen (path) == 0) {\n        errno = EINVAL;\n        return -1;\n    }\n    if (json_object_set (o, path, val) < 0)\n        goto nomem;\n    return 0;\nnomem:\n    errno = ENOMEM;\n    return -1;\n}\n\nstatic int jpath_del_destructive (json_t *o, char *path)\n{\n    char *cp;\n    json_t *dir;\n\n    if ((cp = strchr (path, '.'))) {\n        *cp++ = '\\0';\n        if (strlen (path) == 0) {\n            errno = EINVAL;\n            return -1;\n        }\n        if (!(dir = json_object_get (o, path)))\n            return 0;\n        return jpath_del_destructive (dir, cp);\n    }\n\n    if (strlen (path) == 0) {\n        errno = EINVAL;\n        return -1;\n    }\n    (void)json_object_del (o, path);\n    return 0;\n}\n\nstatic json_t *jpath_get_destructive (json_t *o, char *path)\n{\n    char *cp;\n    json_t *dir;\n    json_t *val;\n\n    if ((cp = strchr (path, '.'))) {\n        *cp++ = '\\0';\n        if (strlen (path) == 0) {\n            errno = EINVAL;\n            return NULL;\n        }\n        if (!(dir = json_object_get (o, path))) {\n            errno = ENOENT;\n            return NULL;\n        }\n        return jpath_get_destructive (dir, cp);\n    }\n\n    if (strlen (path) == 0) {\n        errno = EINVAL;\n        return NULL;\n    }\n    if (!(val = json_object_get (o, path))) {\n        errno = ENOENT;\n        return NULL;\n    }\n    return val;\n}\n\nint jpath_set (json_t *o, const char *path, json_t *val)\n{\n    char *cpy;\n    int rc;\n\n    if (!o || !path || !val) {\n        errno = EINVAL;\n        return -1;\n    }\n    if (!(cpy = strdup (path)))\n        return -1;\n    rc = jpath_set_destructive (o, cpy, val);\n    ERRNO_SAFE_WRAP (free, cpy);\n    return rc;\n}\n\nint jpath_del (json_t *o, const char *path)\n{\n    char *cpy;\n    int rc;\n\n    if (!o || !path) {\n        errno = EINVAL;\n        return -1;\n    }\n    if (!(cpy = strdup (path)))\n        return -1;\n    rc = jpath_del_destructive (o, cpy);\n    ERRNO_SAFE_WRAP (free, cpy);\n    return rc;\n}\n\njson_t *jpath_get (json_t *o, const char *path)\n{\n    char *cpy;\n    json_t *ret;\n\n    if (!o || !path) {\n        errno = EINVAL;\n        return NULL;\n    }\n    if (!(cpy = strdup (path)))\n        return NULL;\n    ret = jpath_get_destructive (o, cpy);\n    ERRNO_SAFE_WRAP (free, cpy);\n    return ret;\n}\n\n\/\/ vi:ts=4 sw=4 expandtab\n","lang_cluster":"C","length":158,"code_uid":"b7545a3f91c24f849a7e1d753680cd1e"}
{"diff_hunk":"@@ -53,7 +53,13 @@ static void keyboard_binding_execute(struct roots_keyboard *keyboard,\n \t}\n }\n \n-static void keyboard_keysym_press(struct roots_keyboard *keyboard,\n+\/**\n+ * Process a keypress from the keyboard.\n+ *\n+ * Returns true if the keysym was handled by a binding and false if the event\n+ * should be propagated to clients.\n+ *\/\n+static bool keyboard_keysym_press(struct roots_keyboard *keyboard,\n \t\txkb_keysym_t keysym) {\n \tssize_t i = keyboard_pressed_keysym_index(keyboard, keysym);\n \tif (i < 0) {","old_code":"#include <assert.h>\n#include <stdint.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <wayland-server.h>\n#include <wlr\/types\/wlr_input_device.h>\n#include <wlr\/types\/wlr_pointer.h>\n#include <wlr\/backend\/multi.h>\n#include <wlr\/backend\/session.h>\n#include <wlr\/util\/log.h>\n#include <xkbcommon\/xkbcommon.h>\n#include \"rootston\/input.h\"\n\nstatic ssize_t keyboard_pressed_keysym_index(struct roots_keyboard *keyboard,\n\t\txkb_keysym_t keysym) {\n\tfor (size_t i = 0; i < ROOTS_KEYBOARD_PRESSED_KEYSYMS_CAP; i++) {\n\t\tif (keyboard->pressed_keysyms[i] == keysym) {\n\t\t\treturn i;\n\t\t}\n\t}\n\treturn -1;\n}\n\nstatic const char *exec_prefix = \"exec \";\n\nstatic void keyboard_binding_execute(struct roots_keyboard *keyboard,\n\t\tconst char *command) {\n\tstruct roots_server *server = keyboard->input->server;\n\tif (strcmp(command, \"exit\") == 0) {\n\t\twl_display_terminate(server->wl_display);\n\t} else if (strcmp(command, \"close\") == 0) {\n\t\tif (keyboard->input->last_active_view != NULL) {\n\t\t\tview_close(keyboard->input->last_active_view);\n\t\t}\n\t} else if (strcmp(command, \"next_window\") == 0) {\n\t\tif (server->desktop->views->length > 0) {\n\t\t\tstruct roots_view *view = server->desktop->views->items[0];\n\t\t\tset_view_focus(keyboard->input, server->desktop, view);\n\t\t\twlr_seat_keyboard_notify_enter(keyboard->input->wl_seat,\n\t\t\t\tview->wlr_surface);\n\t\t}\n\t} else if (strncmp(exec_prefix, command, strlen(exec_prefix)) == 0) {\n\t\tconst char *shell_cmd = command + strlen(exec_prefix);\n\t\tpid_t pid = fork();\n\t\tif (pid < 0) {\n\t\t\twlr_log(L_ERROR, \"cannot execute binding command: fork() failed\");\n\t\t\treturn;\n\t\t} else if (pid == 0) {\n\t\t\texecl(\"\/bin\/sh\", \"\/bin\/sh\", \"-c\", shell_cmd, (void *)NULL);\n\t\t}\n\t} else {\n\t\twlr_log(L_ERROR, \"unknown binding command: %s\", command);\n\t}\n}\n\nstatic void keyboard_keysym_press(struct roots_keyboard *keyboard,\n\t\txkb_keysym_t keysym) {\n\tssize_t i = keyboard_pressed_keysym_index(keyboard, keysym);\n\tif (i < 0) {\n\t\ti = keyboard_pressed_keysym_index(keyboard, XKB_KEY_NoSymbol);\n\t\tif (i >= 0) {\n\t\t\tkeyboard->pressed_keysyms[i] = keysym;\n\t\t}\n\t}\n\n\tif (keysym >= XKB_KEY_XF86Switch_VT_1 &&\n\t\t\tkeysym <= XKB_KEY_XF86Switch_VT_12) {\n\t\tstruct roots_server *server = keyboard->input->server;\n\t\tif (wlr_backend_is_multi(server->backend)) {\n\t\t\tstruct wlr_session *session =\n\t\t\t\twlr_multi_get_session(server->backend);\n\t\t\tif (session) {\n\t\t\t\tunsigned vt = keysym - XKB_KEY_XF86Switch_VT_1 + 1;\n\t\t\t\twlr_session_change_vt(session, vt);\n\t\t\t}\n\t\t}\n\t\treturn;\n\t}\n\n\tuint32_t modifiers = wlr_keyboard_get_modifiers(keyboard->device->keyboard);\n\tstruct wl_list *bindings = &keyboard->input->server->config->bindings;\n\tstruct binding_config *bc;\n\twl_list_for_each(bc, bindings, link) {\n\t\tif (modifiers ^ bc->modifiers) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tbool ok = true;\n\t\tfor (size_t i = 0; i < bc->keysyms_len; i++) {\n\t\t\tssize_t j = keyboard_pressed_keysym_index(keyboard, bc->keysyms[i]);\n\t\t\tif (j < 0) {\n\t\t\t\tok = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (ok) {\n\t\t\tkeyboard_binding_execute(keyboard, bc->command);\n\t\t}\n\t}\n}\n\nstatic void keyboard_keysym_release(struct roots_keyboard *keyboard,\n\t\txkb_keysym_t keysym) {\n\tssize_t i = keyboard_pressed_keysym_index(keyboard, keysym);\n\tif (i >= 0) {\n\t\tkeyboard->pressed_keysyms[i] = XKB_KEY_NoSymbol;\n\t}\n}\n\nstatic void keyboard_key_notify(struct wl_listener *listener, void *data) {\n\tstruct wlr_event_keyboard_key *event = data;\n\tstruct roots_keyboard *keyboard = wl_container_of(listener, keyboard, key);\n\n\tuint32_t keycode = event->keycode + 8;\n\tconst xkb_keysym_t *syms;\n\tint syms_len = xkb_state_key_get_syms(keyboard->device->keyboard->xkb_state,\n\t\tkeycode, &syms);\n\tfor (int i = 0; i < syms_len; i++) {\n\t\tif (event->state == WLR_KEY_PRESSED) {\n\t\t\tkeyboard_keysym_press(keyboard, syms[i]);\n\t\t} else { \/\/ WLR_KEY_RELEASED\n\t\t\tkeyboard_keysym_release(keyboard, syms[i]);\n\t\t}\n\t}\n}\n\nvoid keyboard_add(struct wlr_input_device *device, struct roots_input *input) {\n\tstruct roots_keyboard *keyboard = calloc(sizeof(struct roots_keyboard), 1);\n\tif (keyboard == NULL) {\n\t\treturn;\n\t}\n\tdevice->data = keyboard;\n\tkeyboard->device = device;\n\tkeyboard->input = input;\n\tkeyboard->key.notify = keyboard_key_notify;\n\twl_signal_add(&device->keyboard->events.key, &keyboard->key);\n\twl_list_insert(&input->keyboards, &keyboard->link);\n\n\tstruct xkb_rule_names rules;\n\tmemset(&rules, 0, sizeof(rules));\n\trules.rules = getenv(\"XKB_DEFAULT_RULES\");\n\trules.model = getenv(\"XKB_DEFAULT_MODEL\");\n\trules.layout = getenv(\"XKB_DEFAULT_LAYOUT\");\n\trules.variant = getenv(\"XKB_DEFAULT_VARIANT\");\n\trules.options = getenv(\"XKB_DEFAULT_OPTIONS\");\n\tstruct xkb_context *context = xkb_context_new(XKB_CONTEXT_NO_FLAGS);\n\tif (context == NULL) {\n\t\twlr_log(L_ERROR, \"Cannot create XKB context\");\n\t\treturn;\n\t}\n\twlr_keyboard_set_keymap(device->keyboard, xkb_map_new_from_names(context,\n\t\t&rules, XKB_KEYMAP_COMPILE_NO_FLAGS));\n\txkb_context_unref(context);\n\n\twlr_seat_attach_keyboard(input->wl_seat, device);\n}\n\nvoid keyboard_remove(struct wlr_input_device *device, struct roots_input *input) {\n\tstruct roots_keyboard *keyboard = device->data;\n\twlr_seat_detach_keyboard(input->wl_seat, device->keyboard);\n\twl_list_remove(&keyboard->key.link);\n\twl_list_remove(&keyboard->link);\n\tfree(keyboard);\n}\n","lang_cluster":"C","length":165,"code_uid":"187a2ba83a1f4f7898827edef6055679"}
{"diff_hunk":"@@ -111,5 +111,5 @@ def remove_from_suppress_file(supp_file, value, hash_type):\n \n     except Exception as ex:\n         LOG.error(str(ex))\n-        LOG.error(\"Failed to write: %s\" % (supp_file))\n+        LOG.error(\"Failed to write: %s\" % (suppress_file))\n         return False","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n''' suppress file format\n\n123324353456463442341242342343#1 || bug hash comment\n\/sdfsfs\/sdf\/ || some path to suppress\n\n'''\n\nimport os\nimport string\nimport codecs\nfrom codechecker_lib import logger\n\nLOG = logger.get_new_logger('SUPPRESS_FILE_HANDLER')\n\n\nCOMMENT_SEPARATOR = '||'\nHASH_TYPE_SEPARATOR = '#'\n\n\ndef get_hash_and_path(suppress_file):\n\n    paths, hashes = {}, {}\n\n    def is_bug_hash(line):\n        valid_chars = string.hexdigits + HASH_TYPE_SEPARATOR\n        return all(c in valid_chars for c in line) and len(line) == 34\n\n    LOG.debug('Processing suppress file: '+suppress_file)\n\n    if os.path.exists(suppress_file):\n        with codecs.open(suppress_file, 'r', 'UTF-8') as s_file:\n            for line in s_file:\n                if line == '':\n                    # skip empty lines\n                    continue\n                res = line.split(COMMENT_SEPARATOR)\n                if len(res) == 2:\n                    # there is a comment\n\n                    data = res[0].strip()\n                    comment = res[1].strip()\n                    if is_bug_hash(data):\n                        hashes[data] = comment\n                    else:\n                        paths[data] = comment\n                if len(res) == 1:\n                    data = res[0].strip()\n                    if is_bug_hash(data):\n                        hashes[data] = ''\n                    else:\n                        paths[data] = ''\n\n    LOG.debug(hashes)\n    LOG.debug(paths)\n\n    return hashes, paths\n\n\n# ---------------------------------------------------------------------------\ndef write_to_suppress_file(supp_file, value, hash_type, comment=''):\n\n    comment = comment.decode('UTF-8')\n\n    hashes, paths = get_hash_and_path(supp_file)\n\n    value = value+HASH_TYPE_SEPARATOR+str(hash_type)\n    try:\n        if not os.stat(supp_file)[6] == 0:\n            if value in hashes or value in paths:\n                LOG.debug(\"Already found in\\n %s\" % (supp_file))\n                return True\n\n        s_file = codecs.open(supp_file, 'a', 'UTF-8')\n\n        s_file.write(value+COMMENT_SEPARATOR+comment+'\\n')\n        s_file.close()\n\n        return True\n\n    except Exception as ex:\n        LOG.error(str(ex))\n        LOG.error(\"Failed to write: %s\" % (supp_file))\n        return False\n\n\ndef remove_from_suppress_file(supp_file, value, hash_type):\n\n    LOG.debug('Removing ' + value + ' from \\n' + supp_file)\n\n    try:\n        s_file = codecs.open(supp_file, 'r+', 'UTF-8')\n        lines = s_file.readlines()\n\n        lines = filter(lambda line: not line.startswith(value +\n                                                        HASH_TYPE_SEPARATOR +\n                                                        str(hash_type) +\n                                                        COMMENT_SEPARATOR),\n                       lines)\n\n        s_file.seek(0)\n        s_file.truncate()\n        s_file.writelines(lines)\n        s_file.close()\n\n        return True\n\n    except Exception as ex:\n        LOG.error(str(ex))\n        LOG.error(\"Failed to write: %s\" % (supp_file))\n        return False\n","lang_cluster":"C","length":115,"code_uid":"6af903f2e2de4c9f9e6eb500d9ea554e"}
{"diff_hunk":"@@ -34,8 +34,9 @@ type policyRenderer interface {\n \tProfileToIptablesChains(profileID *proto.ProfileID, policy *proto.Profile, ipVersion uint8) []*iptables.Chain\n }\n \n-func newPolicyManager(filterTable iptablesTable, ruleRenderer policyRenderer, ipVersion uint8) *policyManager {\n+func newPolicyManager(rawTable, filterTable iptablesTable, ruleRenderer policyRenderer, ipVersion uint8) *policyManager {\n \treturn &policyManager{\n+\t\trawTable:     rawTable,\n \t\tfilterTable:  filterTable,\n \t\truleRenderer: ruleRenderer,\n \t\tipVersion:    ipVersion,","old_code":"\/\/ Copyright (c) 2016-2017 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage intdataplane\n\nimport (\n\tlog \"github.com\/Sirupsen\/logrus\"\n\t\"github.com\/projectcalico\/felix\/go\/felix\/iptables\"\n\t\"github.com\/projectcalico\/felix\/go\/felix\/proto\"\n\t\"github.com\/projectcalico\/felix\/go\/felix\/rules\"\n)\n\n\/\/ policyManager simply renders policy\/profile updates into iptables.Chain objects and sends\n\/\/ them to the dataplane layer.\ntype policyManager struct {\n\tfilterTable  iptablesTable\n\truleRenderer policyRenderer\n\tipVersion    uint8\n}\n\ntype policyRenderer interface {\n\tPolicyToIptablesChains(policyID *proto.PolicyID, policy *proto.Policy, ipVersion uint8) []*iptables.Chain\n\tProfileToIptablesChains(profileID *proto.ProfileID, policy *proto.Profile, ipVersion uint8) []*iptables.Chain\n}\n\nfunc newPolicyManager(filterTable iptablesTable, ruleRenderer policyRenderer, ipVersion uint8) *policyManager {\n\treturn &policyManager{\n\t\tfilterTable:  filterTable,\n\t\truleRenderer: ruleRenderer,\n\t\tipVersion:    ipVersion,\n\t}\n}\n\nfunc (m *policyManager) OnUpdate(msg interface{}) {\n\tswitch msg := msg.(type) {\n\tcase *proto.ActivePolicyUpdate:\n\t\tlog.WithField(\"id\", msg.Id).Debug(\"Updating policy chains\")\n\t\tchains := m.ruleRenderer.PolicyToIptablesChains(msg.Id, msg.Policy, m.ipVersion)\n\t\tm.filterTable.UpdateChains(chains)\n\tcase *proto.ActivePolicyRemove:\n\t\tlog.WithField(\"id\", msg.Id).Debug(\"Removing policy chains\")\n\t\tinName := rules.PolicyChainName(rules.PolicyInboundPfx, msg.Id)\n\t\toutName := rules.PolicyChainName(rules.PolicyOutboundPfx, msg.Id)\n\t\tm.filterTable.RemoveChainByName(inName)\n\t\tm.filterTable.RemoveChainByName(outName)\n\tcase *proto.ActiveProfileUpdate:\n\t\tlog.WithField(\"id\", msg.Id).Debug(\"Updating profile chains\")\n\t\tchains := m.ruleRenderer.ProfileToIptablesChains(msg.Id, msg.Profile, m.ipVersion)\n\t\tm.filterTable.UpdateChains(chains)\n\tcase *proto.ActiveProfileRemove:\n\t\tlog.WithField(\"id\", msg.Id).Debug(\"Removing profile chains\")\n\t\tinName := rules.ProfileChainName(rules.PolicyInboundPfx, msg.Id)\n\t\toutName := rules.ProfileChainName(rules.PolicyOutboundPfx, msg.Id)\n\t\tm.filterTable.RemoveChainByName(inName)\n\t\tm.filterTable.RemoveChainByName(outName)\n\t}\n}\n\nfunc (m *policyManager) CompleteDeferredWork() error {\n\t\/\/ Nothing to do, we don't defer any work.\n\treturn nil\n}\n","lang_cluster":"C","length":73,"code_uid":"bf394f4c10384fa5a5f444b6e63e0bd0"}
{"diff_hunk":"@@ -121,7 +121,7 @@ def check(check_data):\n                 rh.postprocess_result()\n                 rh.handle_results()\n             else:\n-                # analisys failed\n+                # Analyses failed.\n                 LOG.error('Analyzing ' + source_file_name + ' failed.')\n                 if rh.analyzer_stdout != '':\n                     LOG.error(rh.analyzer_stdout)","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n'''\n'''\n\nimport os\nimport re\nimport sys\nimport signal\nimport multiprocessing\nimport ntpath\nimport traceback\nimport shutil\nfrom collections import defaultdict\n\nfrom codechecker_lib import logger\nfrom codechecker_lib import analyzer_env\n\nfrom codechecker_lib.analyzers import analyzer_types\n\nLOG = logger.get_new_logger('ANALISYS MANAGER')\n\n\ndef worker_result_handler(results):\n    \"\"\"\n    print the analisys summary\n    \"\"\"\n\n    successful_analysis = defaultdict(int)\n    failed_analisys = defaultdict(int)\n    skipped_num = 0\n\n    for res, skipped, analyzer_type in results:\n        if skipped:\n            skipped_num += 1\n        else:\n            if res == 0:\n                successful_analysis[analyzer_type] += 1\n            else:\n                failed_analisys[analyzer_type] += 1\n\n    LOG.info(\"----==== Summary ====----\")\n    LOG.info('Total compilation commands: ' + str(len(results)))\n    if successful_analysis:\n        LOG.info('Successfully analyzed')\n        for analyzer_type, res in successful_analysis.iteritems():\n            LOG.info('  ' + analyzer_type + ': ' + str(res))\n\n    if failed_analisys:\n        LOG.info(\"Failed to analyze\")\n        for analyzer_type, res in failed_analisys.iteritems():\n            LOG.info('  ' + analyzer_type + ': ' + str(res))\n\n    if skipped_num:\n        LOG.info('Skipped compilation commands: ' + str(skipped_num))\n    LOG.info(\"----=================----\")\n\ndef check(check_data):\n    \"\"\"\n    Invoke clang with an action which called by processes.\n    Different analyzer object belongs to for each build action\n\n    skiplist handler is None if no skip file was configured\n    \"\"\"\n    args, action, context, analyzer_config_map, skp_handler, \\\n        report_output_dir, use_db = check_data\n\n    skipped = False\n    try:\n        # if one analysis fails the check fails\n        return_codes = 0\n        skipped = False\n        for source in action.sources:\n\n            # if there is no skiplist handler there was no skip list file\n            # in the command line\n            # cpp file skipping is handled here\n            _, source_file_name = ntpath.split(source)\n\n            if skp_handler and skp_handler.should_skip(source):\n                LOG.debug_analyzer(source_file_name + ' is skipped')\n                skipped = True\n                continue\n\n            # construct analyzer env\n            analyzer_environment = analyzer_env.get_check_env(context.path_env_extra,\n                                                      context.ld_lib_path_extra)\n            run_id = context.run_id\n\n            rh = analyzer_types.construct_result_handler(args,\n                                                         action,\n                                                         run_id,\n                                                         report_output_dir,\n                                                         context.severity_map,\n                                                         skp_handler,\n                                                         use_db)\n\n            #LOG.info('Analysing ' + source_file_name)\n\n            # create a source analyzer\n            source_analyzer = analyzer_types.construct_analyzer(action,\n                                                                analyzer_config_map)\n\n            # source is the currently analyzed source file\n            # there can be more in one buildaction\n            source_analyzer.source_file = source\n\n            # fills up the result handler with the analyzer information\n            source_analyzer.analyze(rh, analyzer_environment)\n\n            if rh.analyzer_returncode == 0:\n                # analysis was successful\n                # processing results\n                if rh.analyzer_stdout != '':\n                    LOG.debug_analyzer('\\n' + rh.analyzer_stdout)\n                if rh.analyzer_stderr != '':\n                    LOG.debug_analyzer('\\n' + rh.analyzer_stderr)\n                rh.postprocess_result()\n                rh.handle_results()\n            else:\n                # analisys failed\n                LOG.error('Analyzing ' + source_file_name + ' failed.')\n                if rh.analyzer_stdout != '':\n                    LOG.error(rh.analyzer_stdout)\n                if rh.analyzer_stderr != '':\n                    LOG.error(rh.analyzer_stderr)\n                return_codes = rh.analyzer_returncode\n\n            if not args.keep_tmp:\n                rh.clean_results()\n\n        return (return_codes, skipped, action.analyzer_type)\n\n    except Exception as e:\n        LOG.debug_analyzer(str(e))\n        traceback.print_exc(file=sys.stdout)\n        return (1, skipped, action.analyzer_type)\n\ndef start_workers(args, actions, context, analyzer_config_map, skp_handler):\n    \"\"\"\n    start the workers in the process pool\n    for every buildaction there is worker which makes the analysis\n    \"\"\"\n\n    # Handle SIGINT to stop this script running\n    def signal_handler(*arg, **kwarg):\n        try:\n            pool.terminate()\n        finally:\n            sys.exit(1)\n\n    signal.signal(signal.SIGINT, signal_handler)\n\n    # Remove characters which could cause directory creation problems.\n    no_spec_char_name = re.sub(r'[^\\w\\-_\\. ]', '_', args.name)\n    report_output = os.path.join(context.codechecker_workspace,\n                                 no_spec_char_name + '_reports')\n\n    # create report output dir this will be used by the result handlers for each\n    # analyzer to store analyzer results or temporary files\n    # each analyzer instance does its own cleanup\n    if not os.path.exists(report_output):\n        os.mkdir(report_output)\n\n    # Start checking parallel\n    pool = multiprocessing.Pool(args.jobs)\n    # pool.map(check, actions, 1)\n\n    try:\n        # Workaround, equialent of map\n        # The main script does not get signal\n        # while map or map_async function is running\n        # It is a python bug, this does not happen if a timeout is specified;\n        # then receive the interrupt immediately\n\n        analyzed_actions = [(args,\n                             build_action,\n                             context,\n                             analyzer_config_map,\n                             skp_handler,\n                             report_output,\n                             True ) for build_action in actions]\n\n        pool.map_async(check,\n                       analyzed_actions,\n                       1,\n                       callback=worker_result_handler).get(float('inf'))\n\n        pool.close()\n    except Exception:\n        pool.terminate()\n        raise\n    finally:\n        pool.join()\n        if not args.keep_tmp:\n            LOG.debug('Removing temporary directory: ' + report_output)\n            shutil.rmtree(report_output)\n","lang_cluster":"C","length":200,"code_uid":"1393fb1ccb054332a719117f02345a15"}
{"diff_hunk":"@@ -147,6 +147,13 @@ void shell_svc_destroy (struct shell_svc *svc)\n {\n     if (svc) {\n         int saved_errno = errno;\n+        if (svc->registered) {\n+            flux_future_t *f = NULL;\n+            if (!(f = flux_service_unregister (svc->shell->h, svc->name))\n+                || (flux_future_get (f, NULL) < 0))\n+                fprintf (stderr, \"unregister %s\\n\", svc->name);\n+            flux_future_destroy (f);\n+        }\n         free (svc->rank_table);\n         free (svc);\n         errno = saved_errno;","old_code":"\/************************************************************\\\n * Copyright 2019 Lawrence Livermore National Security, LLC\n * (c.f. AUTHORS, NOTICE.LLNS, COPYING)\n *\n * This file is part of the Flux resource manager framework.\n * For details, see https:\/\/github.com\/flux-framework.\n *\n * SPDX-License-Identifier: LGPL-3.0\n\\************************************************************\/\n\n\/* Register a service named \"shell-<jobid>\" on each shell and provide\n * helpers for registering request handlers for different \"methods\".\n *\n * Notes:\n * - Message handlers are not exposed.  They are automatically set up to\n *   allow FLUX_ROLE_USER access, started, and tied to flux_t for destruction.\n *\n * - Since request handlers can receive messages from any user, handlers\n *   should call shell_svc_allowed() to verify that sender is instance owner,\n *   or the shell user (job owner).\n *\n * - shell_svc_create () makes a synchronous RPC to register the service with\n *   the broker.\n *\n * - Services should not be used until after the shells exit the init barrier,\n *   to ensure service registration has completed.\n *\/\n\n#if HAVE_CONFIG_H\n#include \"config.h\"\n#endif\n#include <stdio.h>\n#include <string.h>\n#include <flux\/core.h>\n\n#include \"internal.h\"\n#include \"task.h\"\n#include \"svc.h\"\n\n#define TOPIC_STRING_SIZE  128\n\nstruct shell_svc {\n    flux_shell_t *shell;\n    uid_t uid;      \/\/ effective uid of shell\n    int *rank_table;\/\/ map shell rank to broker rank\n};\n\nstatic int lookup_rank (struct shell_svc *svc, int shell_rank, int *rank)\n{\n    if (shell_rank < 0 || shell_rank >= svc->shell->info->shell_size) {\n        errno = EINVAL;\n        return -1;\n    }\n    *rank = svc->rank_table[shell_rank];\n    return 0;\n}\n\nstatic int build_topic (struct shell_svc *svc,\n                        const char *method,\n                        char *buf,\n                        int len)\n{\n    if (snprintf (buf,\n                  len,\n                  \"shell-%ju%s%s\",\n                  (uintmax_t)svc->shell->info->jobid,\n                  method ? \".\" : \"\",\n                  method ? method : \"\") >= len) {\n        errno = EINVAL;\n        return -1;\n    }\n    return 0;\n}\n\nflux_future_t *shell_svc_vpack (struct shell_svc *svc,\n                                const char *method,\n                                int shell_rank,\n                                int flags,\n                                const  char *fmt,\n                                va_list ap)\n{\n    char topic[TOPIC_STRING_SIZE];\n    int rank;\n\n    if (lookup_rank (svc, shell_rank, &rank) < 0)\n        return NULL;\n    if (build_topic (svc, method, topic, sizeof (topic)) < 0)\n        return NULL;\n\n    return flux_rpc_vpack (svc->shell->h, topic, rank, flags, fmt, ap);\n}\n\nflux_future_t *shell_svc_pack (struct shell_svc *svc,\n                               const char *method,\n                               int shell_rank,\n                               int flags,\n                               const char *fmt, ...)\n{\n    flux_future_t *f;\n    va_list ap;\n    va_start (ap, fmt);\n    f = shell_svc_vpack (svc, method, shell_rank, flags, fmt, ap);\n    va_end (ap);\n    return f;\n}\n\nint shell_svc_allowed (struct shell_svc *svc, const flux_msg_t *msg)\n{\n    uint32_t rolemask;\n    uint32_t userid;\n\n    if (flux_msg_get_rolemask (msg, &rolemask) < 0\n            || flux_msg_get_userid (msg, &userid) < 0)\n        return -1;\n    if (!(rolemask & FLUX_ROLE_OWNER) && userid != svc->uid) {\n        errno = EPERM;\n        return -1;\n    }\n    return 0;\n}\n\nint shell_svc_register (struct shell_svc *svc,\n                        const char *method,\n                        flux_msg_handler_f cb,\n                        void *arg)\n{\n    struct flux_match match = FLUX_MATCH_REQUEST;\n    flux_msg_handler_t *mh;\n    flux_t *h = svc->shell->h;\n    char topic[TOPIC_STRING_SIZE];\n\n    if (build_topic (svc, method, topic, sizeof (topic)) < 0)\n        return -1;\n    match.topic_glob = topic;\n    if (!(mh = flux_msg_handler_create (h, match, cb, arg)))\n        return -1;\n    if (flux_aux_set (h, NULL, mh, (flux_free_f)flux_msg_handler_destroy) < 0) {\n        flux_msg_handler_destroy (mh);\n        return -1;\n    }\n    flux_msg_handler_allow_rolemask (mh, FLUX_ROLE_USER);\n    flux_msg_handler_start (mh);\n    return 0;\n}\n\nvoid shell_svc_destroy (struct shell_svc *svc)\n{\n    if (svc) {\n        int saved_errno = errno;\n        free (svc->rank_table);\n        free (svc);\n        errno = saved_errno;\n    }\n}\n\nstruct shell_svc *shell_svc_create (flux_shell_t *shell)\n{\n    struct shell_svc *svc;\n    struct rcalc_rankinfo ri;\n    int shell_size = shell->info->shell_size;\n    int i;\n\n    if (!(svc = calloc (1, sizeof (*svc))))\n        return NULL;\n    svc->shell = shell;\n    svc->uid = geteuid ();\n    if (!(svc->rank_table = calloc (shell_size, sizeof (*svc->rank_table))))\n        goto error;\n    for (i = 0; i < shell_size; i++) {\n        if (rcalc_get_nth (shell->info->rcalc, i, &ri) < 0)\n            goto error;\n        svc->rank_table[i] = ri.rank;\n    }\n    if (!shell->standalone) {\n        flux_future_t *f;\n        char name[TOPIC_STRING_SIZE];\n        if (build_topic (svc, NULL, name, sizeof (name)) < 0)\n            goto error;\n        if (!(f = flux_service_register (shell->h, name)))\n            goto error;\n        if (flux_future_get (f, NULL) < 0) {\n            flux_future_destroy (f);\n            goto error;\n        }\n        flux_future_destroy (f);\n    }\n    return svc;\nerror:\n    shell_svc_destroy (svc);\n    return NULL;\n}\n\n\/*\n * vi:tabstop=4 shiftwidth=4 expandtab\n *\/\n","lang_cluster":"C","length":195,"code_uid":"d65061d0958f492fb4f51d38020ca241"}
{"diff_hunk":"@@ -31,12 +31,11 @@\n #include <opae\/access.h>\n #include <opae\/properties.h>\n #include \"common_int.h\"\n-#include <ase_common.h>\n+#include \"ase_common.h\"\n \n fpga_result __FPGA_API__ fpgaCreateEventHandle(fpga_event_handle *handle)\n {\n \tstruct _fpga_event_handle *_eh;\n-\tfpga_result result = FPGA_OK;\n \n \tif (!handle)\n \t\treturn FPGA_INVALID_PARAM;","old_code":"\/\/ Copyright(c) 2017, Intel Corporation\n\/\/\n\/\/ Redistribution  and  use  in source  and  binary  forms,  with  or  without\n\/\/ modification, are permitted provided that the following conditions are met:\n\/\/\n\/\/ * Redistributions of  source code  must retain the  above copyright notice,\n\/\/   this list of conditions and the following disclaimer.\n\/\/ * Redistributions in binary form must reproduce the above copyright notice,\n\/\/   this list of conditions and the following disclaimer in the documentation\n\/\/   and\/or other materials provided with the distribution.\n\/\/ * Neither the name  of Intel Corporation  nor the names of its contributors\n\/\/   may be used to  endorse or promote  products derived  from this  software\n\/\/   without specific prior written permission.\n\/\/\n\/\/ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n\/\/ AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,  BUT NOT LIMITED TO,  THE\n\/\/ IMPLIED WARRANTIES OF  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n\/\/ ARE DISCLAIMED.  IN NO EVENT  SHALL THE COPYRIGHT OWNER  OR CONTRIBUTORS BE\n\/\/ LIABLE  FOR  ANY  DIRECT,  INDIRECT,  INCIDENTAL,  SPECIAL,  EXEMPLARY,  OR\n\/\/ CONSEQUENTIAL  DAMAGES  (INCLUDING,  BUT  NOT LIMITED  TO,  PROCUREMENT  OF\n\/\/ SUBSTITUTE GOODS OR SERVICES;  LOSS OF USE,  DATA, OR PROFITS;  OR BUSINESS\n\/\/ INTERRUPTION)  HOWEVER CAUSED  AND ON ANY THEORY  OF LIABILITY,  WHETHER IN\n\/\/ CONTRACT,  STRICT LIABILITY,  OR TORT  (INCLUDING NEGLIGENCE  OR OTHERWISE)\n\/\/ ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,  EVEN IF ADVISED OF THE\n\/\/ POSSIBILITY OF SUCH DAMAGE.\n\n#ifdef HAVE_CONFIG_H\n#include <config.h>\n#endif \/\/ HAVE_CONFIG_H\n\n#include <opae\/access.h>\n#include <opae\/properties.h>\n#include \"common_int.h\"\n#include <ase_common.h>\n\nfpga_result __FPGA_API__ fpgaCreateEventHandle(fpga_event_handle *handle)\n{\n\tstruct _fpga_event_handle *_eh;\n\tfpga_result result = FPGA_OK;\n\n\tif (!handle)\n\t\treturn FPGA_INVALID_PARAM;\n\n\t_eh = malloc(sizeof(struct _fpga_event_handle));\n\tif (NULL == _eh) {\n\t\tFPGA_ERR(\"Could not allocate memory for event handle\");\n\t\treturn FPGA_NO_MEMORY;\n\t}\n\n\t\/* create eventfd *\/\n\t_eh->fd = eventfd(0, 0);\n\tif (_eh->fd < 0) {\n\t\tFPGA_ERR(\"eventfd : %s\", strerror(errno));\n\t\tresult = FPGA_NOT_FOUND;\n\t\tgoto out_free;\n\t}\n\n\t*handle = (fpga_event_handle)_eh;\n\treturn FPGA_OK;\n\nout_free:\n\tfree(_eh);\n\treturn result;\n}\n\nfpga_result __FPGA_API__ fpgaDestroyEventHandle(fpga_event_handle *handle)\n{\n\tstruct _fpga_event_handle *_eh;\n\tif (!handle)\n\t\treturn FPGA_INVALID_PARAM;\n\n\t_eh = (struct _fpga_event_handle *) *handle;\n\n\tif (NULL == _eh) {\n\t\tFPGA_ERR(\"Received NULL event handle\");\n\t\treturn FPGA_INVALID_PARAM;\n\t}\n\n\tif (close(_eh->fd) < 0) {\n\t\tFPGA_ERR(\"eventfd : %s\", strerror(errno));\n\t\tif (errno == EBADF)\n\t\t\treturn FPGA_INVALID_PARAM;\n\t\telse\n\t\t\treturn FPGA_EXCEPTION;\n\t}\n\n\tfree(*handle);\n\t*handle = NULL;\n\treturn FPGA_OK;\n}\n\nfpga_result __FPGA_API__ fpgaGetOSObjectFromEventHandle(const fpga_event_handle eh,\n\t\t\t\t\t\tint *fd)\n{\n\tstruct _fpga_event_handle *_eh = (struct _fpga_event_handle *) eh;\n\tif (NULL == _eh) {\n\t\tFPGA_ERR(\"Event handle is null\");\n\t\treturn FPGA_INVALID_PARAM;\n\t}\n\n\t*fd = _eh->fd;\n\n\treturn FPGA_OK;\n}\n\nfpga_result __FPGA_API__ fpgaRegisterEvent(fpga_handle handle,\n\t\t\t\t\t   fpga_event_type type,\n\t\t\t\t\t   fpga_event_handle event_handle,\n\t\t\t\t\t   uint32_t flags)\n{\n\tUNUSED_PARAM(handle);\n\tif (type != FPGA_EVENT_INTERRUPT)\n\t\treturn FPGA_NOT_SUPPORTED;\n\n\tif (flags >= MAX_USR_INTRS)\n\t\treturn FPGA_INVALID_PARAM;\n\n\tif (register_event(FILE_DESCRIPTOR(event_handle), flags) == 0)\n\t\treturn FPGA_OK;\n\telse\n\t\treturn FPGA_EXCEPTION;\n}\n\nfpga_result __FPGA_API__ fpgaUnregisterEvent(fpga_handle handle,\n\t\t\t\t\t     fpga_event_type event_type,\n\t\t\t\t\t     fpga_event_handle event_handle)\n{\n\tUNUSED_PARAM(handle);\n\tif (event_type != FPGA_EVENT_INTERRUPT)\n\t\treturn FPGA_NOT_SUPPORTED;\n\n\tif (unregister_event(FILE_DESCRIPTOR(event_handle)) == 0)\n\t\treturn FPGA_OK;\n\telse\n\t\treturn FPGA_EXCEPTION;\n}\n","lang_cluster":"C","length":136,"code_uid":"671c18a6670d4469aa384ace3f3c4950"}
{"diff_hunk":"@@ -40,16 +40,17 @@ def get_log_env(logfile, context, original_env):\n \n # -----------------------------------------------------------------------------\n def get_check_env(path_env_extra, ld_lib_path_extra):\n-    '''\n+    \"\"\"\n     Extending the checker environment.\n     Check environment is extended to find tools if they ar not on\n-    the default places\n-    '''\n+    the default places.\n+    \"\"\"\n     new_env = os.environ.copy()\n \n     if len(path_env_extra) > 0:\n         extra_path = ':'.join(path_env_extra)\n-        LOG.debug_analyzer('Extending PATH environment variable with: ' + extra_path)\n+        LOG.debug_analyzer(\n+            'Extending PATH environment variable with: ' + extra_path)\n \n         try:\n             new_env['PATH'] = extra_path + ':' + new_env['PATH']","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n''''''\n\nimport os\n\nfrom codechecker_lib import logger\n\nLOG = logger.get_new_logger('ENV')\n\n\n# ------------------------------------------------------------------------------\ndef get_log_env(logfile, context, original_env):\n    '''\n    Environment for logging. With the ld logger.\n    Keep the original environment unmodified as possible\n    Only environment variables required for logging are changed\n    '''\n    new_env = original_env\n\n    new_env[context.env_var_cc_logger_bin] = context.path_logger_bin\n\n    new_env['LD_PRELOAD'] = context.logger_lib_name\n\n    try:\n        original_ld_library_path = new_env['LD_LIBRARY_PATH']\n        new_env['LD_LIBRARY_PATH'] = context.path_logger_lib + \\\n                                     ':' + original_ld_library_path\n    except:\n        new_env['LD_LIBRARY_PATH'] = context.path_logger_lib\n\n    # set ld logger logfile\n    new_env[context.env_var_cc_logger_file] = logfile\n\n    return new_env\n\n\n# -----------------------------------------------------------------------------\ndef get_check_env(path_env_extra, ld_lib_path_extra):\n    '''\n    Extending the checker environment.\n    Check environment is extended to find tools if they ar not on\n    the default places\n    '''\n    new_env = os.environ.copy()\n\n    if len(path_env_extra) > 0:\n        extra_path = ':'.join(path_env_extra)\n        LOG.debug_analyzer('Extending PATH environment variable with: ' + extra_path)\n\n        try:\n            new_env['PATH'] = extra_path + ':' + new_env['PATH']\n        except:\n            new_env['PATH'] = extra_path\n\n    if len(ld_lib_path_extra) > 0:\n        extra_lib = ':'.join(ld_lib_path_extra)\n        LOG.debug_analyzer('Extending LD_LIBRARY_PATH environment variable with: ' + extra_lib)\n        try:\n            original_ld_library_path = new_env['LD_LIBRARY_PATH']\n            new_env['LD_LIBRARY_PATH'] = extra_lib + ':' + original_ld_library_path\n        except:\n            new_env['LD_LIBRARY_PATH'] = extra_lib\n\n    return new_env\n","lang_cluster":"C","length":68,"code_uid":"a4957d2724d4489b91440c4c7333073d"}
{"diff_hunk":"@@ -19,6 +19,8 @@ package fv_test\n import (\n \t. \"github.com\/onsi\/ginkgo\"\n \t. \"github.com\/onsi\/gomega\"\n+\tlog \"github.com\/sirupsen\/logrus\"\n+\n \t\"github.com\/projectcalico\/felix\/fv\/containers\"\n \t\"github.com\/projectcalico\/felix\/fv\/metrics\"\n \t\"github.com\/projectcalico\/felix\/fv\/utils\"","old_code":"\/\/ +build fvtests\n\n\/\/ Copyright (c) 2017 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage fv_test\n\nimport (\n\t. \"github.com\/onsi\/ginkgo\"\n\t. \"github.com\/onsi\/gomega\"\n\t\"github.com\/projectcalico\/felix\/fv\/containers\"\n\t\"github.com\/projectcalico\/felix\/fv\/metrics\"\n\t\"github.com\/projectcalico\/felix\/fv\/utils\"\n\t\"github.com\/projectcalico\/felix\/fv\/workload\"\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/api\"\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/client\"\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/numorstring\"\n\tlog \"github.com\/sirupsen\/logrus\"\n)\n\nfunc RunEtcd() *containers.Container {\n\treturn containers.Run(\"etcd-fv\",\n\t\t\"quay.io\/coreos\/etcd\",\n\t\t\"etcd\",\n\t\t\"--advertise-client-urls\", \"http:\/\/127.0.0.1:2379\",\n\t\t\"--listen-client-urls\", \"http:\/\/0.0.0.0:2379\")\n}\n\nfunc RunFelix(etcdIP string) *containers.Container {\n\treturn containers.Run(\"felix-fv\",\n\t\t\"--privileged\",\n\t\t\"-e\", \"CALICO_DATASTORE_TYPE=etcdv2\",\n\t\t\"-e\", \"FELIX_DATASTORETYPE=etcdv2\",\n\t\t\"-e\", \"FELIX_ETCDENDPOINTS=http:\/\/\"+etcdIP+\":2379\",\n\t\t\"-e\", \"FELIX_PROMETHEUSMETRICSENABLED=true\",\n\t\t\"-e\", \"FELIX_USAGEREPORTINGENABLED=false\",\n\t\t\"-e\", \"FELIX_IPV6SUPPORT=false\",\n\t\t\"calico\/felix:latest\")\n}\n\nfunc GetEtcdClient(etcdIP string) *client.Client {\n\tclient, err := client.New(api.CalicoAPIConfig{\n\t\tSpec: api.CalicoAPIConfigSpec{\n\t\t\tDatastoreType: api.EtcdV2,\n\t\t\tEtcdConfig: api.EtcdConfig{\n\t\t\t\tEtcdEndpoints: \"http:\/\/\" + etcdIP + \":2379\",\n\t\t\t},\n\t\t},\n\t})\n\tExpect(err).NotTo(HaveOccurred())\n\treturn client\n}\n\nfunc MetricsPortReachable(felixName, felixIP string) bool {\n\t\/\/ Delete existing conntrack state for the metrics port.\n\tutils.Run(\"docker\", \"exec\", felixName,\n\t\t\"conntrack\", \"-L\")\n\tutils.Run(\"docker\", \"exec\", felixName,\n\t\t\"conntrack\", \"-L\", \"-p\", \"tcp\", \"--dport\", metrics.PortString())\n\tutils.RunMayFail(\"docker\", \"exec\", felixName,\n\t\t\"conntrack\", \"-D\", \"-p\", \"tcp\", \"--orig-port-dst\", metrics.PortString())\n\n\t\/\/ Now try to get a metric.\n\tm, err := metrics.GetFelixMetric(felixIP, \"felix_active_local_endpoints\")\n\tif err != nil {\n\t\tlog.WithError(err).Info(\"Metrics port not reachable\")\n\t\treturn false\n\t}\n\tlog.WithField(\"felix_active_local_endpoints\", m).Info(\"Metrics port reachable\")\n\treturn true\n}\n\n\/\/ Here we test reachability to a port number running on a Calico host itself, specifically Felix's\n\/\/ metrics port 9091, and how that is affected by policy, host endpoint and workload endpoint\n\/\/ configuration.\n\/\/\n\/\/ - When there is no policy or endpoint configuration, the port should be reachable.\n\/\/\n\/\/ - When there is a local workload endpoint, the port should be reachable.  (Existence of workload\n\/\/   endpoints should make no difference to reachability to ports on the host itself.)\n\/\/\n\/\/ - When a host endpoint is configured for the host's interface (eth0), but not yet any policy, the\n\/\/   port should be unreachable.\n\/\/\n\/\/   - When pre-DNAT policy is then configured, to allow ingress to that port, it should be\n\/\/     reachable again.\n\nvar _ = Context(\"with initialized Felix and etcd datastore\", func() {\n\n\tvar (\n\t\tetcd                 *containers.Container\n\t\tfelix                *containers.Container\n\t\tclient               *client.Client\n\t\tmetricsPortReachable func() bool\n\t)\n\n\tBeforeEach(func() {\n\n\t\tetcd = RunEtcd()\n\n\t\tclient = GetEtcdClient(etcd.IP)\n\t\tEventually(client.EnsureInitialized, \"10s\", \"1s\").ShouldNot(HaveOccurred())\n\n\t\tfelix = RunFelix(etcd.IP)\n\n\t\tfelixNode := api.NewNode()\n\t\tfelixNode.Metadata.Name = felix.Hostname\n\t\t_, err := client.Nodes().Create(felixNode)\n\t\tExpect(err).NotTo(HaveOccurred())\n\n\t\tmetricsPortReachable = func() bool {\n\t\t\treturn MetricsPortReachable(felix.Name, felix.IP)\n\t\t}\n\t})\n\n\tAfterEach(func() {\n\n\t\tif CurrentGinkgoTestDescription().Failed {\n\t\t\tutils.Run(\"docker\", \"logs\", felix.Name)\n\t\t\tutils.Run(\"docker\", \"exec\", felix.Name, \"iptables-save\", \"-c\")\n\t\t}\n\t\tfelix.Stop()\n\n\t\tif CurrentGinkgoTestDescription().Failed {\n\t\t\tutils.Run(\"docker\", \"exec\", etcd.Name, \"etcdctl\", \"ls\", \"--recursive\", \"\/\")\n\t\t}\n\t\tetcd.Stop()\n\t})\n\n\tIt(\"with no endpoints or policy, port should be reachable\", func() {\n\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeTrue())\n\t})\n\n\tIt(\"with a local workload, port should be reachable\", func() {\n\t\tw := workload.Run(felix, \"cali12345\", \"10.65.0.2\", \"8055\")\n\t\tw.Configure(client)\n\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeTrue())\n\t\tw.Stop()\n\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeTrue())\n\t})\n\n\tContext(\"with host endpoint defined\", func() {\n\n\t\tBeforeEach(func() {\n\t\t\thostEp := api.NewHostEndpoint()\n\t\t\thostEp.Metadata.Name = \"host-endpoint-1\"\n\t\t\thostEp.Metadata.Node = felix.Hostname\n\t\t\thostEp.Metadata.Labels = map[string]string{\"host-endpoint\": \"true\"}\n\t\t\thostEp.Spec.InterfaceName = \"eth0\"\n\t\t\t_, err := client.HostEndpoints().Create(hostEp)\n\t\t\tExpect(err).NotTo(HaveOccurred())\n\t\t})\n\n\t\tIt(\"port should not be reachable\", func() {\n\t\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeFalse())\n\t\t})\n\n\t\tContext(\"with pre-DNAT policy defined\", func() {\n\n\t\t\tBeforeEach(func() {\n\t\t\t\tpolicy := api.NewPolicy()\n\t\t\t\tpolicy.Metadata.Name = \"pre-dnat-policy-1\"\n\t\t\t\tpolicy.Spec.PreDNAT = true\n\t\t\t\tprotocol := numorstring.ProtocolFromString(\"tcp\")\n\t\t\t\tallowMetricsPortRule := api.Rule{\n\t\t\t\t\tAction:   \"allow\",\n\t\t\t\t\tProtocol: &protocol,\n\t\t\t\t\tDestination: api.EntityRule{\n\t\t\t\t\t\tPorts: []numorstring.Port{numorstring.SinglePort(uint16(metrics.Port))},\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\tpolicy.Spec.IngressRules = []api.Rule{allowMetricsPortRule}\n\t\t\t\tpolicy.Spec.Selector = \"host-endpoint=='true'\"\n\t\t\t\t_, err := client.Policies().Create(policy)\n\t\t\t\tExpect(err).NotTo(HaveOccurred())\n\t\t\t})\n\n\t\t\tIt(\"port should be reachable\", func() {\n\t\t\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeTrue())\n\t\t\t})\n\t\t})\n\t})\n})\n","lang_cluster":"C","length":194,"code_uid":"f0f38d07c4c64739bc6ee8c9e78b3a41"}
{"diff_hunk":"@@ -18,6 +18,9 @@ import portalocker\n import psutil\n import socket\n import stat\n+from libcodechecker.logger import get_logger\n+\n+LOG = get_logger('server')\n \n \n def __getInstanceDescriptorPath(folder=None):","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nInstance manager handles the state keeping of running CodeChecker instances\nfor a particular user on the local machine.\n\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport getpass\nimport json\nimport os\nimport portalocker\nimport psutil\nimport socket\nimport stat\n\n\ndef __getInstanceDescriptorPath(folder=None):\n    if not folder:\n        folder = os.path.expanduser(\"~\")\n\n    return os.path.join(folder, \".codechecker.instances.json\")\n\n\ndef __makeInstanceDescriptorFile(folder=None):\n    descriptor = __getInstanceDescriptorPath(folder)\n    if not os.path.exists(descriptor):\n        with open(descriptor, 'w') as f:\n            json.dump([], f)\n        os.chmod(descriptor, stat.S_IRUSR | stat.S_IWUSR)\n\n\ndef __checkInstance(hostname, pid):\n    \"\"\"Check if the given process on the system is a valid, running CodeChecker\n    for the current user.\"\"\"\n\n    # Instances running on a remote host with a filesystem shared with us can\n    # not usually be checked (\/proc is rarely shared across computers...),\n    # so we consider them \"alive\" servers.\n    if hostname != socket.gethostname():\n        return True\n\n    try:\n        proc = psutil.Process(pid)\n\n        return \"CodeChecker.py\" in proc.cmdline()[1] and \\\n               proc.username() == getpass.getuser()\n    except psutil.NoSuchProcess:\n        # If the process does not exist, it cannot be valid.\n        return False\n\n\ndef __rewriteInstanceFile(append, remove, folder=None):\n    \"\"\"This helper method reads the user's instance descriptor and manages it\n    eliminating dead records, appending new ones and reserialising the file.\"\"\"\n\n    __makeInstanceDescriptorFile(folder)\n    with open(__getInstanceDescriptorPath(folder), 'r+') as f:\n        portalocker.lock(f, portalocker.LOCK_EX)\n\n        # After reading, check every instance if they are still valid and\n        # make sure PID does not collide accidentally with the\n        # to-be-registered instances, if any exists in the append list as it\n        # would cause duplication.\n        #\n        # Also, we remove the records to the given PIDs, if any exists.\n        append_pids = [i['pid'] for i in append]\n        instances = [i for i in json.load(f)\n                     if i['pid'] not in append_pids and\n                     (i['hostname'] + \":\" + str(i['pid'])) not in remove and\n                     __checkInstance(i['hostname'], i['pid'])]\n\n        instances = instances + append\n\n        f.seek(0)\n        f.truncate()\n        json.dump(instances, f, indent=2)\n        portalocker.unlock(f)\n\n\ndef register(pid, workspace, port, folder=None):\n    \"\"\"\n    Adds the specified CodeChecker server instance to the user's instance\n    descriptor.\n    \"\"\"\n\n    __rewriteInstanceFile([{\"pid\": pid,\n                            \"hostname\": socket.gethostname(),\n                            \"workspace\": workspace,\n                            \"port\": port}],\n                          [],\n                          folder)\n\n\ndef unregister(pid, folder=None):\n    \"\"\"\n    Removes the specified CodeChecker server instance from the user's instance\n    descriptor.\n    \"\"\"\n\n    __rewriteInstanceFile([], [socket.gethostname() + \":\" + str(pid)], folder)\n\n\ndef get_instances(folder=None):\n    \"\"\"Returns the list of running servers for the current user.\"\"\"\n\n    # This method does NOT write the descriptor file.\n\n    descriptor = __getInstanceDescriptorPath(folder)\n    instances = []\n    if os.path.exists(descriptor):\n        with open(descriptor, 'r') as f:\n            portalocker.lock(f, portalocker.LOCK_SH)\n            instances = [i for i in json.load(f) if __checkInstance(\n                i['hostname'],\n                i['pid'])]\n            portalocker.unlock(f)\n\n    return instances\n","lang_cluster":"C","length":124,"code_uid":"879eb21083974bc89883b6c6fa393f0d"}
{"diff_hunk":"@@ -86,7 +86,7 @@ func TestNATNoBackendFromHEP(t *testing.T) {\n \t)\n \tExpect(err).NotTo(HaveOccurred())\n \n-\trunBpfTest(t, \"calico_from_host_ep\", nil, func(bpfrun bpfProgRunFn) {\n+\trunBpfTest(t, \"calico_from_host_ep\", false, nil, func(bpfrun bpfProgRunFn) {\n \t\tres, err := bpfrun(pktBytes)\n \t\tExpect(err).NotTo(HaveOccurred())\n \t\tExpect(res.RetvalStr()).To(Equal(\"TC_ACT_UNSPEC\"), \"expected program to return TC_ACT_UNSPEC\")","old_code":"\/\/ Copyright (c) 2019-2021 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage ut_test\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com\/google\/gopacket\"\n\t\"github.com\/google\/gopacket\/layers\"\n\t. \"github.com\/onsi\/gomega\"\n\n\t\"github.com\/projectcalico\/felix\/bpf\/nat\"\n)\n\nfunc TestICMPPortUnreachable(t *testing.T) {\n\tRegisterTestingT(t)\n\n\t_, ipv4, _, _, pktBytes, err := testPacketUDPDefault()\n\tExpect(err).NotTo(HaveOccurred())\n\n\trunBpfUnitTest(t, \"icmp_port_unreachable.c\", func(bpfrun bpfProgRunFn) {\n\t\tres, err := bpfrun(pktBytes)\n\t\tExpect(err).NotTo(HaveOccurred())\n\t\tExpect(res.Retval).To(Equal(0))\n\n\t\tExpect(res.dataOut).To(HaveLen(134)) \/\/ eth + ip + 64 + udp + ip + icmp\n\n\t\tpktR := gopacket.NewPacket(res.dataOut, layers.LayerTypeEthernet, gopacket.Default)\n\t\tfmt.Printf(\"pktR = %+v\\n\", pktR)\n\n\t\tcheckICMPPortUnreachable(pktR, ipv4)\n\t})\n\n}\n\nfunc TestNATNoBackendFromHEP(t *testing.T) {\n\tRegisterTestingT(t)\n\n\tiphdr := *ipv4Default\n\n\t_, ipv4, l4, _, pktBytes, err := testPacket(nil, &iphdr, nil, nil)\n\tExpect(err).NotTo(HaveOccurred())\n\n\tudp := l4.(*layers.UDP)\n\n\t\/\/ Test with count as 1 but no backend. This results in a NAT backend lookup failure\n\tnatkey := nat.NewNATKey(ipv4.DstIP, uint16(udp.DstPort), uint8(ipv4.Protocol)).AsBytes()\n\terr = natMap.Update(\n\t\tnatkey,\n\t\tnat.NewNATValue(0, 1, 0, 0).AsBytes(),\n\t)\n\tExpect(err).NotTo(HaveOccurred())\n\tdefer func() {\n\t\terr := natMap.Delete(natkey)\n\t\tExpect(err).NotTo(HaveOccurred())\n\t}()\n\n\trunBpfTest(t, \"calico_from_host_ep\", nil, func(bpfrun bpfProgRunFn) {\n\t\tres, err := bpfrun(pktBytes)\n\t\tExpect(err).NotTo(HaveOccurred())\n\t\tExpect(res.RetvalStr()).To(Equal(\"TC_ACT_UNSPEC\"), \"expected program to return TC_ACT_UNSPEC\")\n\n\t\tpktR := gopacket.NewPacket(res.dataOut, layers.LayerTypeEthernet, gopacket.Default)\n\t\tfmt.Printf(\"pktR = %+v\\n\", pktR)\n\n\t\tcheckICMPPortUnreachable(pktR, ipv4)\n\t})\n\n\t\/\/ Test with count as 0. This results in a no backend after frontend lookup as count is 0.\n\terr = natMap.Update(\n\t\tnatkey,\n\t\tnat.NewNATValue(0, 0, 0, 0).AsBytes(),\n\t)\n\tExpect(err).NotTo(HaveOccurred())\n\n\trunBpfTest(t, \"calico_from_host_ep\", nil, func(bpfrun bpfProgRunFn) {\n\t\tres, err := bpfrun(pktBytes)\n\t\tExpect(err).NotTo(HaveOccurred())\n\t\tExpect(res.RetvalStr()).To(Equal(\"TC_ACT_UNSPEC\"), \"expected program to return TC_ACT_UNSPEC\")\n\n\t\tpktR := gopacket.NewPacket(res.dataOut, layers.LayerTypeEthernet, gopacket.Default)\n\t\tfmt.Printf(\"pktR = %+v\\n\", pktR)\n\n\t\tcheckICMPPortUnreachable(pktR, ipv4)\n\t})\n}\n\nfunc checkICMPPortUnreachable(pktR gopacket.Packet, ipv4 *layers.IPv4) {\n\tipv4L := pktR.Layer(layers.LayerTypeIPv4)\n\tExpect(ipv4L).NotTo(BeNil())\n\tipv4R := ipv4L.(*layers.IPv4)\n\n\tExpect(ipv4R.Protocol).To(Equal(layers.IPProtocolICMPv4))\n\tExpect(ipv4R.SrcIP.String()).To(Equal(intfIP.String()))\n\tExpect(ipv4R.DstIP).To(Equal(ipv4.SrcIP))\n\n\ticmpL := pktR.Layer(layers.LayerTypeICMPv4)\n\tExpect(ipv4L).NotTo(BeNil())\n\ticmpR := icmpL.(*layers.ICMPv4)\n\n\tExpect(icmpR.TypeCode).To(Equal(\n\t\tlayers.CreateICMPv4TypeCode(\n\t\t\tlayers.ICMPv4TypeDestinationUnreachable,\n\t\t\tlayers.ICMPv4CodePort,\n\t\t)))\n}\n","lang_cluster":"C","length":119,"code_uid":"a0f5ceb7d9d541d38792eb9394918d61"}
{"diff_hunk":"@@ -60,7 +60,8 @@ class GenericSuppressHandler(suppress_handler.SuppressHandler):\n         ret = suppress_file_handler.write_to_suppress_file(self.suppress_file,\n                                                            bug_id,\n                                                            file_name,\n-                                                           comment)\n+                                                           comment,\n+                                                           status)\n         self.__revalidate_suppress_data()\n         return ret\n ","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nHandler for suppressing a bug.\n\"\"\"\n\nimport os\n\nfrom libcodechecker import suppress_file_handler\nfrom libcodechecker import suppress_handler\nfrom libcodechecker.logger import get_logger\n\n# Warning! this logger should only be used in this module.\nLOG = get_logger('system')\n\n\nclass GenericSuppressHandler(suppress_handler.SuppressHandler):\n\n    def __init__(self, suppress_file, allow_write):\n        \"\"\"\n        Create a new suppress handler with a suppress_file as backend.\n        \"\"\"\n        super(GenericSuppressHandler, self).__init__()\n\n        self.__suppress_info = []\n        self.__allow_write = allow_write\n\n        if suppress_file:\n            self.suppress_file = suppress_file\n            self.__have_memory_backend = True\n            self.__revalidate_suppress_data()\n        else:\n            self.__have_memory_backend = False\n            self.__arrow_write = False\n\n            if allow_write:\n                raise ValueError(\"Can't create allow_write=True suppress \"\n                                 \"handler without a backend file.\")\n\n    def __revalidate_suppress_data(self):\n        \"\"\"Reload the information in the suppress file to the memory.\"\"\"\n\n        if not self.__have_memory_backend:\n            # Do not load and have suppress data stored in memory if not\n            # needed.\n            return\n\n        with open(self.suppress_file, 'r') as file_handle:\n            self.__suppress_info = suppress_file_handler.\\\n                get_suppress_data(file_handle)\n\n    def store_suppress_bug_id(self, bug_id, file_name, comment):\n\n        if not self.__allow_write:\n            return True\n\n        ret = suppress_file_handler.write_to_suppress_file(self.suppress_file,\n                                                           bug_id,\n                                                           file_name,\n                                                           comment)\n        self.__revalidate_suppress_data()\n        return ret\n\n    def remove_suppress_bug_id(self, bug_id, file_name):\n\n        if not self.__allow_write:\n            return True\n\n        ret = suppress_file_handler.remove_from_suppress_file(\n            self.suppress_file,\n            bug_id,\n            file_name)\n        self.__revalidate_suppress_data()\n        return ret\n\n    def get_suppressed(self, bug):\n\n        return any([suppress for suppress in self.__suppress_info\n                    if suppress[0] == bug['hash_value'] and\n                    suppress[1] == os.path.basename(bug['file_path'])])\n","lang_cluster":"C","length":83,"code_uid":"9431d34b3e2d48c2b7ca3801bc190204"}
{"diff_hunk":"@@ -96,8 +96,8 @@ static void subcompositor_get_subsurface(struct wl_client *client,\n \t\tstruct wl_resource *resource, uint32_t id,\n \t\tstruct wl_resource *surface_resource,\n \t\tstruct wl_resource *parent_resource) {\n-\tstruct wlr_surface *surface = wl_resource_get_user_data(surface_resource);\n-\tstruct wlr_surface *parent = wl_resource_get_user_data(parent_resource);\n+\tstruct wlr_surface *surface = wlr_surface_from_resource(surface_resource);\n+\tstruct wlr_surface *parent = wlr_surface_from_resource(parent_resource);\n \n \tstatic const char msg[] = \"get_subsurface: wl_subsurface@\";\n ","old_code":"#include <assert.h>\n#include <stdlib.h>\n#include <wayland-server.h>\n#include <wlr\/types\/wlr_compositor.h>\n#include <wlr\/types\/wlr_region.h>\n#include <wlr\/types\/wlr_surface.h>\n#include <wlr\/util\/log.h>\n#include \"util\/signal.h\"\n\nstatic void destroy_surface_listener(struct wl_listener *listener, void *data) {\n\twl_list_remove(wl_resource_get_link(data));\n}\n\nstatic void wl_compositor_create_surface(struct wl_client *client,\n\t\tstruct wl_resource *resource, uint32_t id) {\n\tstruct wlr_compositor *compositor = wl_resource_get_user_data(resource);\n\n\tstruct wl_resource *surface_resource = wl_resource_create(client,\n\t\t&wl_surface_interface, wl_resource_get_version(resource), id);\n\tif (surface_resource == NULL) {\n\t\twl_resource_post_no_memory(resource);\n\t\treturn;\n\t}\n\n\tstruct wlr_surface *surface = wlr_surface_create(surface_resource,\n\t\tcompositor->renderer);\n\tif (surface == NULL) {\n\t\twl_resource_destroy(surface_resource);\n\t\twl_resource_post_no_memory(resource);\n\t\treturn;\n\t}\n\tsurface->compositor_data = compositor;\n\tsurface->compositor_listener.notify = &destroy_surface_listener;\n\twl_resource_add_destroy_listener(surface_resource,\n\t\t&surface->compositor_listener);\n\n\twl_list_insert(&compositor->surfaces,\n\t\twl_resource_get_link(surface_resource));\n\twlr_signal_emit_safe(&compositor->events.new_surface, surface);\n}\n\nstatic void wl_compositor_create_region(struct wl_client *client,\n\t\tstruct wl_resource *resource, uint32_t id) {\n\twlr_region_create(client, resource, id);\n}\n\nstruct wl_compositor_interface wl_compositor_impl = {\n\t.create_surface = wl_compositor_create_surface,\n\t.create_region = wl_compositor_create_region\n};\n\nstatic void wl_compositor_destroy(struct wl_resource *resource) {\n\tstruct wlr_compositor *compositor = wl_resource_get_user_data(resource);\n\tstruct wl_resource *_resource = NULL;\n\twl_resource_for_each(_resource, &compositor->wl_resources) {\n\t\tif (_resource == resource) {\n\t\t\tstruct wl_list *link = wl_resource_get_link(_resource);\n\t\t\twl_list_remove(link);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void wl_compositor_bind(struct wl_client *wl_client, void *data,\n\t\tuint32_t version, uint32_t id) {\n\tstruct wlr_compositor *compositor = data;\n\tassert(wl_client && compositor);\n\n\tstruct wl_resource *wl_resource =\n\t\twl_resource_create(wl_client, &wl_compositor_interface, version, id);\n\tif (wl_resource == NULL) {\n\t\twl_client_post_no_memory(wl_client);\n\t\treturn;\n\t}\n\twl_resource_set_implementation(wl_resource, &wl_compositor_impl,\n\t\tcompositor, wl_compositor_destroy);\n\twl_list_insert(&compositor->wl_resources,\n\t\twl_resource_get_link(wl_resource));\n}\n\nvoid wlr_compositor_destroy(struct wlr_compositor *compositor) {\n\tif (compositor == NULL) {\n\t\treturn;\n\t}\n\twl_list_remove(&compositor->display_destroy.link);\n\twl_global_destroy(compositor->wl_global);\n\tfree(compositor);\n}\n\nstatic void subcompositor_destroy(struct wl_client *client,\n\t\tstruct wl_resource *resource) {\n\twl_resource_destroy(resource);\n}\n\nstatic void subcompositor_get_subsurface(struct wl_client *client,\n\t\tstruct wl_resource *resource, uint32_t id,\n\t\tstruct wl_resource *surface_resource,\n\t\tstruct wl_resource *parent_resource) {\n\tstruct wlr_surface *surface = wl_resource_get_user_data(surface_resource);\n\tstruct wlr_surface *parent = wl_resource_get_user_data(parent_resource);\n\n\tstatic const char msg[] = \"get_subsurface: wl_subsurface@\";\n\n\tif (surface == parent) {\n\t\twl_resource_post_error(resource,\n\t\t\tWL_SUBCOMPOSITOR_ERROR_BAD_SURFACE,\n\t\t\t\"%s%d: wl_surface@%d cannot be its own parent\",\n\t\t\tmsg, id, wl_resource_get_id(surface_resource));\n\t\treturn;\n\t}\n\n\tif (surface->subsurface) {\n\t\twl_resource_post_error(resource,\n\t\t\tWL_SUBCOMPOSITOR_ERROR_BAD_SURFACE,\n\t\t\t\"%s%d: wl_surface@%d is already a sub-surface\",\n\t\t\tmsg, id, wl_resource_get_id(surface_resource));\n\t\treturn;\n\t}\n\n\tif (wlr_surface_get_main_surface(parent) == surface) {\n\t\twl_resource_post_error(resource,\n\t\t\tWL_SUBCOMPOSITOR_ERROR_BAD_SURFACE,\n\t\t\t\"%s%d: wl_surface@%d is an ancestor of parent\",\n\t\t\tmsg, id, wl_resource_get_id(surface_resource));\n\t\treturn;\n\t}\n\n\tif (wlr_surface_set_role(surface, \"wl_subsurface\", resource,\n\t\t\t\tWL_SUBCOMPOSITOR_ERROR_BAD_SURFACE) < 0) {\n\t\treturn;\n\t}\n\n\twlr_surface_make_subsurface(surface, parent, id);\n\tif (!surface->subsurface) {\n\t\twl_resource_post_no_memory(resource);\n\t\treturn;\n\t}\n}\n\n\nstatic const struct wl_subcompositor_interface subcompositor_interface = {\n\t.destroy = subcompositor_destroy,\n\t.get_subsurface = subcompositor_get_subsurface,\n};\n\nstatic void subcompositor_bind(struct wl_client *client, void *data,\n\t\tuint32_t version, uint32_t id) {\n\tstruct wlr_compositor *compositor = data;\n\tstruct wl_resource *resource =\n\t\twl_resource_create(client, &wl_subcompositor_interface, 1, id);\n\tif (resource == NULL) {\n\t\twl_client_post_no_memory(client);\n\t\treturn;\n\t}\n\twl_resource_set_implementation(resource, &subcompositor_interface,\n\t\tcompositor, NULL);\n}\n\nstatic void handle_display_destroy(struct wl_listener *listener, void *data) {\n\tstruct wlr_compositor *compositor =\n\t\twl_container_of(listener, compositor, display_destroy);\n\twlr_compositor_destroy(compositor);\n}\n\nstruct wlr_compositor *wlr_compositor_create(struct wl_display *display,\n\t\tstruct wlr_renderer *renderer) {\n\tstruct wlr_compositor *compositor =\n\t\tcalloc(1, sizeof(struct wlr_compositor));\n\tif (!compositor) {\n\t\twlr_log_errno(L_ERROR, \"Could not allocate wlr compositor\");\n\t\treturn NULL;\n\t}\n\n\tstruct wl_global *compositor_global = wl_global_create(display,\n\t\t&wl_compositor_interface, 4, compositor, wl_compositor_bind);\n\tif (!compositor_global) {\n\t\twlr_log_errno(L_ERROR, \"Could not allocate compositor global\");\n\t\tfree(compositor);\n\t\treturn NULL;\n\t}\n\tcompositor->wl_global = compositor_global;\n\tcompositor->renderer = renderer;\n\n\twl_global_create(display, &wl_subcompositor_interface, 1, compositor,\n\t\tsubcompositor_bind);\n\n\twl_list_init(&compositor->wl_resources);\n\twl_list_init(&compositor->surfaces);\n\twl_signal_init(&compositor->events.new_surface);\n\n\tcompositor->display_destroy.notify = handle_display_destroy;\n\twl_display_add_destroy_listener(display, &compositor->display_destroy);\n\n\treturn compositor;\n}\n","lang_cluster":"C","length":195,"code_uid":"b047760f02564748b93d3e796f5fdaf1"}
{"diff_hunk":"@@ -158,11 +158,11 @@ func (r *ruleRenderer) WorkloadEndpointToIptablesChains(epID *proto.WorkloadEndp\n \toutRules = append(outRules, r.DropRules(Match(), \"Drop if no profiles matched\")...)\n \n \ttoEndpointChain := Chain{\n-\t\tName:  WorkloadEndpointChainName(WorkloadToEndpointPfx, endpoint),\n+\t\tName:  EndpointChainName(WorkloadToEndpointPfx, endpoint.Name),\n \t\tRules: inRules,\n \t}\n \tfromEndpointChain := Chain{\n-\t\tName:  WorkloadEndpointChainName(WorkloadFromEndpointPfx, endpoint),\n+\t\tName:  EndpointChainName(WorkloadFromEndpointPfx, endpoint.Name),\n \t\tRules: outRules,\n \t}\n \treturn []*Chain{&toEndpointChain, &fromEndpointChain}","old_code":"\/\/ Copyright (c) 2016 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage rules\n\nimport (\n\t\"github.com\/projectcalico\/felix\/go\/felix\/hashutils\"\n\t. \"github.com\/projectcalico\/felix\/go\/felix\/iptables\"\n\t\"github.com\/projectcalico\/felix\/go\/felix\/proto\"\n)\n\nfunc (r *ruleRenderer) WorkloadDispatchChains(endpoints map[proto.WorkloadEndpointID]*proto.WorkloadEndpoint) []*Chain {\n\ttoEndpointRules := make([]Rule, 0, len(endpoints)+1)\n\tfromEndpointRules := make([]Rule, 0, len(endpoints)+1)\n\tfor _, endpoint := range endpoints {\n\t\tfromEndpointRules = append(fromEndpointRules, Rule{\n\t\t\tMatch: Match().InInterface(endpoint.Name),\n\t\t\tAction: GotoAction{\n\t\t\t\tTarget: WorkloadEndpointChainName(WorkloadFromEndpointPfx, endpoint),\n\t\t\t},\n\t\t})\n\t\ttoEndpointRules = append(toEndpointRules, Rule{\n\t\t\tMatch: Match().OutInterface(endpoint.Name),\n\t\t\tAction: GotoAction{\n\t\t\t\tTarget: WorkloadEndpointChainName(WorkloadToEndpointPfx, endpoint),\n\t\t\t},\n\t\t})\n\t}\n\n\tfromEndpointRules = append(fromEndpointRules, Rule{\n\t\tAction: DropAction{},\n\t})\n\ttoEndpointRules = append(toEndpointRules, Rule{\n\t\tAction: DropAction{},\n\t})\n\n\tfromEndpointDispatchChain := Chain{\n\t\tName:  DispatchFromWorkloadEndpoint,\n\t\tRules: fromEndpointRules,\n\t}\n\ttoEndpointDispatchChain := Chain{\n\t\tName:  DispatchToWorkloadEndpoint,\n\t\tRules: toEndpointRules,\n\t}\n\n\treturn []*Chain{&toEndpointDispatchChain, &fromEndpointDispatchChain}\n}\n\nfunc (r *ruleRenderer) WorkloadEndpointToIptablesChains(epID *proto.WorkloadEndpointID, endpoint *proto.WorkloadEndpoint) []*Chain {\n\tinRules := []Rule{}\n\toutRules := []Rule{}\n\n\t\/\/ Start by ensuring that the accept mark bit is clear, policies set that bit to indicate\n\t\/\/ that they accepted the packet.\n\tinRules = append(inRules, Rule{\n\t\tAction: ClearMarkAction{\n\t\t\tMark: r.IptablesMarkAccept,\n\t\t},\n\t})\n\toutRules = append(outRules, Rule{\n\t\tAction: ClearMarkAction{\n\t\t\tMark: r.IptablesMarkAccept,\n\t\t},\n\t})\n\n\t\/\/ TODO(smc) Police the MAC?\n\n\tfor _, tier := range endpoint.Tiers {\n\t\t\/\/ For each tier,  clear the \"accepted by tier\" mark.\n\t\tinRules = append(inRules, Rule{\n\t\t\tComment: \"Start of tier \" + tier.Name,\n\t\t\tAction: ClearMarkAction{\n\t\t\t\tMark: r.IptablesMarkNextTier,\n\t\t\t},\n\t\t})\n\t\toutRules = append(outRules, Rule{\n\t\t\tComment: \"Start of tier \" + tier.Name,\n\t\t\tAction: ClearMarkAction{\n\t\t\t\tMark: r.IptablesMarkNextTier,\n\t\t\t},\n\t\t})\n\t\t\/\/ Then, jump to each policy in turn.\n\t\tfor _, polID := range tier.Policies {\n\t\t\tinPolChainName := PolicyChainName(\n\t\t\t\tPolicyInboundPfx,\n\t\t\t\t&proto.PolicyID{Tier: tier.Name, Name: polID},\n\t\t\t)\n\t\t\tinRules = append(inRules,\n\t\t\t\tRule{\n\t\t\t\t\tMatch:  Match().MarkClear(r.IptablesMarkNextTier),\n\t\t\t\t\tAction: JumpAction{Target: inPolChainName},\n\t\t\t\t},\n\t\t\t\t\/\/ If policy marked packet as accepted, it returns, setting the\n\t\t\t\t\/\/ accept mark bit.  If that is set, return from this chain.\n\t\t\t\tRule{\n\t\t\t\t\tMatch:   Match().MarkSet(r.IptablesMarkAccept),\n\t\t\t\t\tAction:  ReturnAction{},\n\t\t\t\t\tComment: \"Return if policy accepted\",\n\t\t\t\t})\n\t\t\toutPolChainName := PolicyChainName(\n\t\t\t\tPolicyOutboundPfx,\n\t\t\t\t&proto.PolicyID{Tier: tier.Name, Name: polID},\n\t\t\t)\n\t\t\toutRules = append(outRules,\n\t\t\t\tRule{\n\t\t\t\t\tMatch:  Match().MarkClear(r.IptablesMarkNextTier),\n\t\t\t\t\tAction: JumpAction{Target: outPolChainName},\n\t\t\t\t},\n\t\t\t\t\/\/ If policy marked packet as accepted, it returns, setting the\n\t\t\t\t\/\/ accept mark bit.  If that is set, return from this chain.\n\t\t\t\tRule{\n\t\t\t\t\tMatch:   Match().MarkSet(r.IptablesMarkAccept),\n\t\t\t\t\tAction:  ReturnAction{},\n\t\t\t\t\tComment: \"Return if policy accepted\",\n\t\t\t\t})\n\t\t}\n\t\t\/\/ If no policy in the tier marked the packet as next-tier, drop the packet.\n\t\tinRules = append(inRules, r.DropRules(Match().MarkClear(r.IptablesMarkNextTier), \"Drop if no policies passed packet\")...)\n\t\toutRules = append(outRules, r.DropRules(Match().MarkClear(r.IptablesMarkNextTier), \"Drop if no policies passed packet\")...)\n\t}\n\n\t\/\/ Then, jump to each profile in turn.\n\tfor _, profileID := range endpoint.ProfileIds {\n\t\tinProfChainName := ProfileChainName(PolicyInboundPfx, &proto.ProfileID{Name: profileID})\n\t\toutProfChainName := ProfileChainName(PolicyOutboundPfx, &proto.ProfileID{Name: profileID})\n\t\tinRules = append(inRules,\n\t\t\tRule{Action: JumpAction{Target: inProfChainName}},\n\t\t\t\/\/ If policy marked packet as accepted, it returns, setting the\n\t\t\t\/\/ accept mark bit.  If that is set, return from this chain.\n\t\t\tRule{\n\t\t\t\tMatch:   Match().MarkSet(r.IptablesMarkAccept),\n\t\t\t\tAction:  ReturnAction{},\n\t\t\t\tComment: \"Return if profile accepted\",\n\t\t\t})\n\t\toutRules = append(outRules,\n\t\t\tRule{Action: JumpAction{Target: outProfChainName}},\n\t\t\t\/\/ If policy marked packet as accepted, it returns, setting the\n\t\t\t\/\/ accept mark bit.  If that is set, return from this chain.\n\t\t\tRule{\n\t\t\t\tMatch:   Match().MarkSet(r.IptablesMarkAccept),\n\t\t\t\tAction:  ReturnAction{},\n\t\t\t\tComment: \"Return if profile accepted\",\n\t\t\t})\n\t}\n\n\tinRules = append(inRules, r.DropRules(Match(), \"Drop if no profiles matched\")...)\n\toutRules = append(outRules, r.DropRules(Match(), \"Drop if no profiles matched\")...)\n\n\ttoEndpointChain := Chain{\n\t\tName:  WorkloadEndpointChainName(WorkloadToEndpointPfx, endpoint),\n\t\tRules: inRules,\n\t}\n\tfromEndpointChain := Chain{\n\t\tName:  WorkloadEndpointChainName(WorkloadFromEndpointPfx, endpoint),\n\t\tRules: outRules,\n\t}\n\treturn []*Chain{&toEndpointChain, &fromEndpointChain}\n}\n\nfunc (r *ruleRenderer) HostDispatchChains(map[proto.HostEndpointID]*proto.HostEndpoint) []*Chain {\n\tpanic(\"Not implemented\")\n\treturn nil\n}\n\nfunc (r *ruleRenderer) HostEndpointToIptablesChains(epID *proto.HostEndpointID, endpoint *proto.HostEndpoint) []*Chain {\n\tpanic(\"Not implemented\")\n\n\t\/\/ TODO(smc) Failsafe chains\n\n\treturn nil\n}\n\nfunc WorkloadEndpointChainName(prefix string, endpoint *proto.WorkloadEndpoint) string {\n\treturn hashutils.GetLengthLimitedID(\n\t\tprefix,\n\t\tendpoint.Name,\n\t\tMaxChainNameLength,\n\t)\n}\n","lang_cluster":"C","length":190,"code_uid":"13f4988ee1054fc6bc4756c4d97fcfe1"}
{"diff_hunk":"@@ -10,11 +10,14 @@ from __future__ import print_function\n from __future__ import division\n from __future__ import absolute_import\n \n+import errno\n import os\n import pickle\n import platform\n+import shutil\n import subprocess\n import sys\n+\n from uuid import uuid4\n \n from codechecker_common.logger import get_logger","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nBuild and log related functionality.\n\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport os\nimport pickle\nimport platform\nimport subprocess\nimport sys\nfrom uuid import uuid4\n\nfrom codechecker_common.logger import get_logger\n\nfrom .. import env\nfrom . import host_check\n\nLOG = get_logger('buildlogger')\n\n\ndef execute_buildcmd(command, silent=False, env=None, cwd=None):\n    \"\"\"\n    Execute the the build command and continuously write\n    the output from the process to the standard output.\n    \"\"\"\n    proc = subprocess.Popen(command,\n                            bufsize=-1,\n                            env=env,\n                            stdout=subprocess.PIPE,\n                            stderr=subprocess.STDOUT,\n                            cwd=cwd,\n                            shell=True,\n                            universal_newlines=True)\n\n    while True:\n        line = proc.stdout.readline()\n        if not line and proc.poll() is not None:\n            break\n        if not silent:\n            print(line)\n\n    return proc.returncode\n\n\ndef perform_build_command(logfile, command, context, keep_link, silent=False):\n    \"\"\"\n    Build the project and create a log file.\n    \"\"\"\n    LOG.info(\"Starting build ...\")\n\n    try:\n        original_env_file = os.environ['CODECHECKER_ORIGINAL_BUILD_ENV']\n        LOG.debug_analyzer('Loading original build env from: %s',\n                           original_env_file)\n\n        with open(original_env_file, 'rb') as env_file:\n            original_env = pickle.load(env_file)\n\n    except Exception as ex:\n        LOG.warning(str(ex))\n        LOG.warning('Failed to get saved original_env'\n                    'using a current copy for logging.')\n        original_env = os.environ.copy()\n\n    # Run user's commands with intercept.\n    if host_check.check_intercept(original_env):\n        LOG.debug_analyzer(\"with intercept ...\")\n        final_command = command\n        command = ' '.join([\"intercept-build\",\n                            \"--cdb\", logfile,\n                            \"sh -c \\\"\" + final_command + \"\\\"\"])\n        log_env = original_env\n        LOG.debug_analyzer(command)\n\n    # Run user's commands in shell.\n    else:\n        # TODO: better platform detection.\n        if platform.system() == 'Linux':\n            LOG.debug_analyzer(\"with ld logger ...\")\n            open(logfile, 'a').close()  # Same as linux's touch.\n            log_env = env.get_log_env(logfile, context, original_env)\n            if 'CC_LOGGER_GCC_LIKE' not in log_env:\n                log_env['CC_LOGGER_GCC_LIKE'] = 'gcc:g++:clang:clang++:cc:c++'\n            if keep_link or ('CC_LOGGER_KEEP_LINK' in log_env and\n                             log_env['CC_LOGGER_KEEP_LINK'] == 'true'):\n                log_env['CC_LOGGER_KEEP_LINK'] = 'true'\n        else:\n            LOG.error(\"Intercept-build is required\"\n                      \" to run CodeChecker in OS X.\")\n            sys.exit(1)\n\n    LOG.debug_analyzer(log_env)\n    try:\n        ret_code = execute_buildcmd(command, silent, log_env)\n\n        if ret_code == 0:\n            LOG.info(\"Build finished successfully.\")\n            LOG.debug_analyzer(\"The logfile is: %s\", logfile)\n        else:\n            LOG.info(\"Build failed.\")\n            sys.exit(ret_code)\n\n    except Exception as ex:\n        LOG.error(\"Calling original build command failed.\")\n        LOG.error(str(ex))\n        sys.exit(1)\n    finally:\n        # Removing flock lock file.\n        logfile_lock = logfile + '.lock'\n        if os.path.exists(logfile_lock):\n            os.remove(logfile_lock)\n\n\ndef default_compilation_db(workspace_path, run_name):\n    \"\"\"\n    Default compilation commands database file in the workspace.\n    \"\"\"\n    workspace_path = os.path.abspath(workspace_path)\n    uid = str(uuid4())[:10]  # 10 chars should be unique enough\n    cmp_json_filename = 'compilation_commands_' + run_name + '_' \\\n                        + uid + '.json'\n    compilation_commands = os.path.join(workspace_path, cmp_json_filename)\n    return compilation_commands\n","lang_cluster":"C","length":130,"code_uid":"e4d9953368ea43aab3eedd06ee5a3af7"}
{"diff_hunk":"@@ -4,6 +4,7 @@ This software is distributed under the GNU General Public License.\n See the file COPYING for details.\n *\/\n \n+#include \"dag.h\"\n #include \"dag_file.h\"\n \n #include \"xxmalloc.h\"","old_code":"\/*\nCopyright (C) 2014- The University of Notre Dame\nThis software is distributed under the GNU General Public License.\nSee the file COPYING for details.\n*\/\n\n#include \"dag_file.h\"\n\n#include \"xxmalloc.h\"\n#include \"list.h\"\n\n#include <stdlib.h>\n\nstruct dag_file * dag_file_create( const char *filename )\n{\n\tstruct dag_file *f = malloc(sizeof(*f));\n\tf->filename = xxstrdup(filename);\n\tf->needed_by = list_create();\n\tf->created_by = 0;\n\tf->ref_count = 0;\n\treturn f;\n}\n\nint dag_file_is_source( const struct dag_file *f )\n{\n\tif(f->created_by)\n\t\treturn 0;\n\telse\n\t\treturn 1;\n}\n\nint dag_file_is_sink( const struct dag_file *f )\n{\n\tif(list_size(f->needed_by) > 0)\n\t\treturn 0;\n\telse\n\t\treturn 1;\n}\n\n\/* vim: set noexpandtab tabstop=4: *\/\n","lang_cluster":"C","length":40,"code_uid":"bcf91dc8bc6946fa83e0ce7c1a8f3ac6"}
{"diff_hunk":"@@ -6,7 +6,9 @@ flux_msg_handler_create(3)\n NAME\n ----\n flux_msg_handler_create, flux_msg_handler_destroy,\n-flux_msg_handler_start, flux_msg_handler_stop -  manage message handlers\n+flux_msg_handler_start, flux_msg_handler_stop,\n+flux_msg_handler_allow_rolemask,\n+flux_msg_handler_deny_rolemask  -  manage message handlers\n \n \n SYNOPSIS","old_code":"flux_msg_handler_create(3)\n==========================\n:doctype: manpage\n\n\nNAME\n----\nflux_msg_handler_create, flux_msg_handler_destroy,\nflux_msg_handler_start, flux_msg_handler_stop -  manage message handlers\n\n\nSYNOPSIS\n--------\n #include <flux\/core.h>\n\n typedef void (*flux_msg_handler_f)(flux_t *h,\n                                    flux_msg_handler_t *mh,\n                                    const flux_msg_t *msg,\n                                    void *arg);\n\n flux_msg_handler_t *\n flux_msg_handler_create (flux_t *h,\n                          const struct flux_match match,\n                          flux_msg_handler_f callback,\n                          void *arg);\n\n void flux_msg_handler_destroy (flux_msg_handler_t *mh);\n\n void flux_msg_handler_start (flux_msg_handler_t *mh);\n\n void flux_msg_handler_stop (flux_msg_handler_t *mh);\n\n\nDESCRIPTION\n-----------\n\n`flux_msg_handler_create()` registers _callback_ to be invoked when\na message meeting _match_ criteria, as described in `flux_msg_cmp(3)`,\nis received on Flux broker handle _h_.\n\nThe message handler must be started with `flux_msg_handler_start()` in\norder to receive messages.   Conversely, `flux_msg_handler_stop()` causes\nthe message handler to stop receiving messages.  Starting and stopping\nare idempotent operations.\n\nThe handle _h_ is monitored for FLUX_POLLIN events on the flux_reactor_t\nassociated with the handle as described in `flux_set_reactor(3)`.\nThis internal \"handle watcher\" is started when the first message handler\nis started, and stopped when the last message handler is stopped.\n\nMessages arriving on _h_ are internally read and dispatched to matching\nmessage handlers.  If multiple handlers match the message, the message\nis dispatched to the most recently registered handler.\n\nFLUX_MSGTYPE_REQUEST messages with no matching message handler\nare automatically sent an ENOSYS response by the dispatcher.\n\n`flux_msg_handler_destroy()` destroys a handler, after internally\nstopping it.\n\n\nCAVEATS\n-------\n\nFLUX_MSGTYPE_EVENT messages are received on the handle only as\nrequested by `flux_event_subscribe(3)`.\n\n`flux-broker(1)` only routes FLUX_MSGTYPE_REQUEST messages to comms\nmodules according to their registered service name, which is the same as\nthe module name.  Other handle instances such as those on the local connector\ncannot yet receive requests.\n\n\nRETURN VALUE\n------------\n\n`flux_msg_handler_create()` returns a flux_msg_handler_t object on success.\nOn error, NULL is returned, and errno is set appropriately.\n\n\nERRORS\n------\n\nENOMEM::\nOut of memory.\n\n\nAUTHOR\n------\nThis page is maintained by the Flux community.\n\n\nRESOURCES\n---------\nGithub: <http:\/\/github.com\/flux-framework>\n\n\nCOPYRIGHT\n---------\ninclude::COPYRIGHT.adoc[]\n\n\nSEE ALSO\n---------\nflux_get_reactor(3), flux_reactor_start(3), flux_msg_cmp(3)\n","lang_cluster":"C","length":105,"code_uid":"e128f68d632e45cd9848f458d6f1d729"}
{"diff_hunk":"@@ -26,6 +26,7 @@ type Set interface {\n \tContains(interface{}) bool\n \tIter(func(item interface{}) error)\n \tCopy() Set\n+\tEquals(Set) bool\n }\n \n type empty struct{}","old_code":"\/\/ Copyright (c) 2016 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage set\n\nimport (\n\t\"errors\"\n\tlog \"github.com\/Sirupsen\/logrus\"\n)\n\ntype Set interface {\n\tLen() int\n\tAdd(interface{})\n\tDiscard(interface{})\n\tContains(interface{}) bool\n\tIter(func(item interface{}) error)\n\tCopy() Set\n}\n\ntype empty struct{}\n\nvar emptyValue = empty{}\n\nvar (\n\tStopIteration = errors.New(\"Stop iteration\")\n\tRemoveItem    = errors.New(\"Remove item\")\n)\n\nfunc New() Set {\n\treturn make(mapSet)\n}\n\nfunc Empty() Set {\n\treturn mapSet(nil)\n}\n\ntype mapSet map[interface{}]empty\n\nfunc (set mapSet) Len() int {\n\treturn len(set)\n}\n\nfunc (set mapSet) Add(item interface{}) {\n\tset[item] = emptyValue\n}\n\nfunc (set mapSet) Discard(item interface{}) {\n\tdelete(set, item)\n}\n\nfunc (set mapSet) Contains(item interface{}) bool {\n\t_, present := set[item]\n\treturn present\n}\n\nfunc (set mapSet) Iter(visitor func(item interface{}) error) {\nloop:\n\tfor item := range set {\n\t\terr := visitor(item)\n\t\tswitch err {\n\t\tcase StopIteration:\n\t\t\tbreak loop\n\t\tcase RemoveItem:\n\t\t\tdelete(set, item)\n\t\tcase nil:\n\t\t\tbreak\n\t\tdefault:\n\t\t\tlog.WithError(err).Panic(\"Unexpected iteration error\")\n\t\t}\n\t}\n}\n\nfunc (set mapSet) Copy() Set {\n\tcpy := New()\n\tfor item := range set {\n\t\tcpy.Add(item)\n\t}\n\treturn cpy\n}\n","lang_cluster":"C","length":90,"code_uid":"b2b8c1b167ca4b33b75667f500f21008"}
{"diff_hunk":"@@ -70,16 +70,11 @@ def setup_package():\n     # Extend the checker configuration with the server access.\n     codechecker_cfg.update(server_access)\n \n-    ret = project.clean(test_project, test_env)\n-    if ret:\n-        sys.exit(ret)\n-\n-    ret = codechecker.check_and_store(codechecker_cfg,\n-                                      test_project_name,\n-                                      test_project_path)\n+    ret = codechecker.store(codechecker_cfg,\n+                            test_project_name)\n     if ret:\n         sys.exit(1)\n-    print(\"Analyzing test project was succcessful.\")\n+    print(\"Storing the base reports was succcessful.\")\n \n     codechecker_cfg['run_names'] = [test_project_name]\n ","old_code":"# coding=utf-8\n# -----------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -----------------------------------------------------------------------------\n\n\"\"\"Setup for the package tests.\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport os\nimport shutil\nimport sys\nimport uuid\n\nfrom libtest import codechecker\nfrom libtest import env\nfrom libtest import project\n\n\nTEST_WORKSPACE = None\n\n\ndef setup_package():\n    \"\"\"Setup the environment for the tests. \"\"\"\n\n    global TEST_WORKSPACE\n    TEST_WORKSPACE = env.get_workspace('update')\n\n    os.environ['TEST_WORKSPACE'] = TEST_WORKSPACE\n\n    test_project = 'cpp'\n\n    test_config = {}\n\n    project_info = project.get_info(test_project)\n\n    test_project_path = os.path.join(TEST_WORKSPACE, \"test_proj\")\n    shutil.copytree(project.path(test_project), test_project_path)\n\n    project_info['project_path'] = test_project_path\n\n    test_project_name = project_info['name'] + '_' + uuid.uuid4().hex\n\n    test_config['test_project'] = project_info\n\n    suppress_file = None\n\n    skip_list_file = None\n\n    test_env = env.test_env(TEST_WORKSPACE)\n\n    codechecker_cfg = {\n        'suppress_file': suppress_file,\n        'skip_list_file': skip_list_file,\n        'check_env': test_env,\n        'workspace': TEST_WORKSPACE,\n        'checkers': []\n    }\n\n    # Start or connect to the running CodeChecker server and get connection\n    # details.\n    print(\"This test uses a CodeChecker server... connecting...\")\n    server_access = codechecker.start_or_get_server()\n    server_access['viewer_product'] = 'update'\n    codechecker.add_test_package_product(server_access, TEST_WORKSPACE)\n\n    # Extend the checker configuration with the server access.\n    codechecker_cfg.update(server_access)\n\n    ret = project.clean(test_project, test_env)\n    if ret:\n        sys.exit(ret)\n\n    ret = codechecker.check_and_store(codechecker_cfg,\n                                      test_project_name,\n                                      test_project_path)\n    if ret:\n        sys.exit(1)\n    print(\"Analyzing test project was succcessful.\")\n\n    codechecker_cfg['run_names'] = [test_project_name]\n\n    test_config['codechecker_cfg'] = codechecker_cfg\n\n    env.export_test_cfg(TEST_WORKSPACE, test_config)\n\n\ndef teardown_package():\n    \"\"\"Clean up after the test.\"\"\"\n\n    # TODO: if environment variable is set keep the workspace\n    # and print out the path\n    global TEST_WORKSPACE\n\n    check_env = env.import_test_cfg(TEST_WORKSPACE)[\n        'codechecker_cfg']['check_env']\n    codechecker.remove_test_package_product(TEST_WORKSPACE, check_env)\n\n    print(\"Removing: \" + TEST_WORKSPACE)\n    shutil.rmtree(TEST_WORKSPACE)\n","lang_cluster":"C","length":103,"code_uid":"2faac203170f44a4a0a3e21f7018e9d1"}
{"diff_hunk":"@@ -48,7 +48,7 @@ func MetricsPortReachable(felix *infrastructure.Felix) bool {\n }\n \n \/\/ Here we test reachability to a port number running on a Calico host itself, specifically Felix's\n-\/\/ metrics port 9091, and how that is affected by policy, host endpoint and workload endpoint\n+\/\/ metrics port 9091, and how that is affected by policy, host endpoint (eth0\/*) and workload endpoint\n \/\/ configuration.\n \/\/\n \/\/ - When there is no policy or endpoint configuration, the port should be reachable.","old_code":"\/\/ +build fvtests\n\n\/\/ Copyright (c) 2017-2018 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage fv_test\n\nimport (\n\t. \"github.com\/onsi\/ginkgo\"\n\t. \"github.com\/onsi\/gomega\"\n\tlog \"github.com\/sirupsen\/logrus\"\n\n\t\"github.com\/projectcalico\/felix\/fv\/infrastructure\"\n\t\"github.com\/projectcalico\/felix\/fv\/metrics\"\n\t\"github.com\/projectcalico\/felix\/fv\/utils\"\n\t\"github.com\/projectcalico\/felix\/fv\/workload\"\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/apiconfig\"\n\tapi \"github.com\/projectcalico\/libcalico-go\/lib\/apis\/v3\"\n\tclient \"github.com\/projectcalico\/libcalico-go\/lib\/clientv3\"\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/numorstring\"\n)\n\nfunc MetricsPortReachable(felix *infrastructure.Felix) bool {\n\t\/\/ Delete existing conntrack state for the metrics port.\n\tfelix.Exec(\"conntrack\", \"-L\")\n\tfelix.Exec(\"conntrack\", \"-L\", \"-p\", \"tcp\", \"--dport\", metrics.PortString())\n\tfelix.ExecMayFail(\"conntrack\", \"-D\", \"-p\", \"tcp\", \"--orig-port-dst\", metrics.PortString())\n\n\t\/\/ Now try to get a metric.\n\tm, err := metrics.GetFelixMetric(felix.IP, \"felix_active_local_endpoints\")\n\tif err != nil {\n\t\tlog.WithError(err).Info(\"Metrics port not reachable\")\n\t\treturn false\n\t}\n\tlog.WithField(\"felix_active_local_endpoints\", m).Info(\"Metrics port reachable\")\n\treturn true\n}\n\n\/\/ Here we test reachability to a port number running on a Calico host itself, specifically Felix's\n\/\/ metrics port 9091, and how that is affected by policy, host endpoint and workload endpoint\n\/\/ configuration.\n\/\/\n\/\/ - When there is no policy or endpoint configuration, the port should be reachable.\n\/\/\n\/\/ - When there is a local workload endpoint, the port should be reachable.  (Existence of workload\n\/\/   endpoints should make no difference to reachability to ports on the host itself.)\n\/\/\n\/\/ - When a host endpoint is configured for the host's interface (eth0), but not yet any policy, the\n\/\/   port should be unreachable.\n\/\/\n\/\/   - When pre-DNAT policy is then configured, to allow ingress to that port, it should be\n\/\/     reachable again.\n\nvar _ = infrastructure.DatastoreDescribe(\"with initialized Felix\", []apiconfig.DatastoreType{apiconfig.EtcdV3, apiconfig.Kubernetes}, func(getInfra infrastructure.InfraFactory) {\n\tvar (\n\t\tinfra                infrastructure.DatastoreInfra\n\t\tfelix                *infrastructure.Felix\n\t\tclient               client.Interface\n\t\tmetricsPortReachable func() bool\n\t)\n\n\tBeforeEach(func() {\n\t\tinfra = getInfra()\n\n\t\tfelix, client = infrastructure.StartSingleNodeTopology(infrastructure.DefaultTopologyOptions(), infra)\n\n\t\tmetricsPortReachable = func() bool {\n\t\t\treturn MetricsPortReachable(felix)\n\t\t}\n\t})\n\n\tAfterEach(func() {\n\n\t\tif CurrentGinkgoTestDescription().Failed {\n\t\t\tinfra.DumpErrorData()\n\t\t\tfelix.Exec(\"iptables-save\", \"-c\")\n\t\t\tfelix.Exec(\"ip\", \"r\")\n\t\t\tfelix.Exec(\"ip\", \"a\")\n\t\t}\n\t\tfelix.Stop()\n\t\tinfra.Stop()\n\t})\n\n\tIt(\"with no endpoints or policy, port should be reachable\", func() {\n\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeTrue())\n\t})\n\n\tIt(\"with a local workload, port should be reachable\", func() {\n\t\tw := workload.Run(felix, \"w\", \"default\", \"10.65.0.2\", \"8055\", \"tcp\")\n\t\tw.ConfigureInDatastore(infra)\n\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeTrue(), \"Not reachable with workload running\")\n\t\tw.Stop()\n\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeTrue(), \"With workload stopped, not reachable\")\n\t})\n\n\tContext(\"with host endpoint defined\", func() {\n\n\t\tBeforeEach(func() {\n\t\t\terr := infra.AddAllowToDatastore(\"host-endpoint=='true'\")\n\t\t\tExpect(err).NotTo(HaveOccurred())\n\t\t\thostEp := api.NewHostEndpoint()\n\t\t\thostEp.Name = \"host-endpoint-1\"\n\t\t\thostEp.Labels = map[string]string{\"host-endpoint\": \"true\"}\n\t\t\thostEp.Spec.Node = felix.Hostname\n\t\t\thostEp.Spec.InterfaceName = \"eth0\"\n\t\t\t_, err = client.HostEndpoints().Create(utils.Ctx, hostEp, utils.NoOptions)\n\t\t\tExpect(err).NotTo(HaveOccurred())\n\t\t})\n\n\t\tIt(\"port should not be reachable\", func() {\n\t\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeFalse())\n\t\t})\n\n\t\tContext(\"with pre-DNAT policy defined\", func() {\n\n\t\t\tBeforeEach(func() {\n\t\t\t\t\/\/ Ensure the HostEndpoint has taken effect and is blocking traffic\n\t\t\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeFalse())\n\t\t\t\tpolicy := api.NewGlobalNetworkPolicy()\n\t\t\t\tpolicy.Name = \"pre-dnat-policy-1\"\n\t\t\t\tpolicy.Spec.PreDNAT = true\n\t\t\t\tpolicy.Spec.ApplyOnForward = true\n\t\t\t\tprotocol := numorstring.ProtocolFromString(\"tcp\")\n\t\t\t\tallowMetricsPortRule := api.Rule{\n\t\t\t\t\tAction:   api.Allow,\n\t\t\t\t\tProtocol: &protocol,\n\t\t\t\t\tDestination: api.EntityRule{\n\t\t\t\t\t\tPorts: []numorstring.Port{numorstring.SinglePort(uint16(metrics.Port))},\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\tpolicy.Spec.Ingress = []api.Rule{allowMetricsPortRule}\n\t\t\t\tpolicy.Spec.Selector = \"host-endpoint=='true'\"\n\t\t\t\t_, err := client.GlobalNetworkPolicies().Create(utils.Ctx, policy, utils.NoOptions)\n\t\t\t\tExpect(err).NotTo(HaveOccurred())\n\t\t\t})\n\n\t\t\tIt(\"port should be reachable\", func() {\n\t\t\t\tEventually(metricsPortReachable, \"10s\", \"1s\").Should(BeTrue())\n\t\t\t})\n\t\t})\n\t})\n})\n","lang_cluster":"C","length":153,"code_uid":"07577775a3e743beb8d613b6a4e4fabd"}
{"diff_hunk":"@@ -64,23 +64,40 @@ flux_future_t *flux_kvs_lookupat (flux_t *h, int flags, const char *key,\n {\n     flux_future_t *f;\n     json_t *obj = NULL;\n+    struct lookup_ctx *ctx;\n \n     if (!h || !key || strlen (key) == 0 || validate_lookup_flags (flags) < 0) {\n         errno = EINVAL;\n         return NULL;\n     }\n+    if (!(ctx = alloc_ctx ()))\n+        return NULL;\n+    ctx->flags = flags;\n     if (!treeobj) {\n-        f = flux_kvs_lookup (h, flags, key);\n+        if (!(f = flux_kvs_lookup (h, flags, key))) {\n+            free_ctx (ctx);\n+            return NULL;\n+        }\n     }\n     else {\n         if (!(obj = json_loads (treeobj, 0, NULL))) {\n             errno = EINVAL;\n             return NULL;\n         }\n-        f = flux_rpc_pack (h, \"kvs.get\", FLUX_NODEID_ANY, 0, \"{s:s s:i s:O}\",\n-                                                             \"key\", key,\n-                                                             \"flags\", flags,\n-                                                             \"rootdir\", obj);\n+        if (!(f = flux_rpc_pack (h, \"kvs.get\", FLUX_NODEID_ANY, 0,\n+                                    \"{s:s s:i s:O}\", \"key\", key,\n+                                                     \"flags\", flags,\n+                                                     \"rootdir\", obj))) {\n+            free_ctx (ctx);\n+            json_decref (obj);\n+            return NULL;\n+        }\n+    }\n+    if (flux_future_aux_set (f, auxkey, ctx, (flux_free_f)free_ctx) < 0) {\n+        free_ctx (ctx);\n+        json_decref (obj);\n+        flux_future_destroy (f);\n+        return NULL;\n     }\n     json_decref (obj);\n     return f;","old_code":"\/*****************************************************************************\\\n *  Copyright (c) 2017 Lawrence Livermore National Security, LLC.  Produced at\n *  the Lawrence Livermore National Laboratory (cf, AUTHORS, DISCLAIMER.LLNS).\n *  LLNL-CODE-658032 All rights reserved.\n *\n *  This file is part of the Flux resource manager framework.\n *  For details, see https:\/\/github.com\/flux-framework.\n *\n *  This program is free software; you can redistribute it and\/or modify it\n *  under the terms of the GNU General Public License as published by the Free\n *  Software Foundation; either version 2 of the license, or (at your option)\n *  any later version.\n *\n *  Flux is distributed in the hope that it will be useful, but WITHOUT\n *  ANY WARRANTY; without even the IMPLIED WARRANTY OF MERCHANTABILITY or\n *  FITNESS FOR A PARTICULAR PURPOSE.  See the terms and conditions of the\n *  GNU General Public License for more details.\n *\n *  You should have received a copy of the GNU General Public License along\n *  with this program; if not, write to the Free Software Foundation, Inc.,\n *  59 Temple Place, Suite 330, Boston, MA 02111-1307 USA.\n *  See also:  http:\/\/www.gnu.org\/licenses\/\n\\*****************************************************************************\/\n\n#if HAVE_CONFIG_H\n#include \"config.h\"\n#endif\n#include <assert.h>\n#include <errno.h>\n#include <stdlib.h>\n#include <jansson.h>\n#include <czmq.h>\n#include <flux\/core.h>\n\n#include \"kvs_lookup.h\"\n\nstatic int validate_lookup_flags (int flags)\n{\n    switch (flags) {\n        case 0:\n        case FLUX_KVS_TREEOBJ:\n        case FLUX_KVS_READDIR:\n        case FLUX_KVS_READDIR | FLUX_KVS_TREEOBJ:\n        case FLUX_KVS_READLINK:\n            return 0;\n        default:\n            return -1;\n    }\n}\n\nflux_future_t *flux_kvs_lookup (flux_t *h, int flags, const char *key)\n{\n    if (!h || !key || strlen (key) == 0 || validate_lookup_flags (flags) < 0) {\n        errno = EINVAL;\n        return NULL;\n    }\n    return flux_rpc_pack (h, \"kvs.get\", FLUX_NODEID_ANY, 0, \"{s:s s:i}\",\n                                                            \"key\", key,\n                                                            \"flags\", flags);\n}\n\nflux_future_t *flux_kvs_lookupat (flux_t *h, int flags, const char *key,\n                                  const char *treeobj)\n{\n    flux_future_t *f;\n    json_t *obj = NULL;\n\n    if (!h || !key || strlen (key) == 0 || validate_lookup_flags (flags) < 0) {\n        errno = EINVAL;\n        return NULL;\n    }\n    if (!treeobj) {\n        f = flux_kvs_lookup (h, flags, key);\n    }\n    else {\n        if (!(obj = json_loads (treeobj, 0, NULL))) {\n            errno = EINVAL;\n            return NULL;\n        }\n        f = flux_rpc_pack (h, \"kvs.get\", FLUX_NODEID_ANY, 0, \"{s:s s:i s:O}\",\n                                                             \"key\", key,\n                                                             \"flags\", flags,\n                                                             \"rootdir\", obj);\n    }\n    json_decref (obj);\n    return f;\n}\n\nint flux_kvs_lookup_get (flux_future_t *f, const char **json_str)\n{\n    const char *auxkey = \"flux::kvs_valstr\";\n    json_t *obj;\n    char *s;\n\n    if (!(s = flux_future_aux_get (f, auxkey))) {\n        if (flux_rpc_get_unpack (f, \"{s:o}\", \"val\", &obj) < 0)\n            return -1;\n        if (!(s = json_dumps (obj, JSON_COMPACT|JSON_ENCODE_ANY))) {\n            errno = EINVAL;\n            return -1;\n        }\n        if (flux_future_aux_set (f, auxkey, s, free) < 0) {\n            free (s);\n            errno = ENOMEM;\n            return -1;\n        }\n    }\n    if (json_str)\n        *json_str = s;\n    return 0;\n}\n\nint flux_kvs_lookup_get_unpack (flux_future_t *f, const char *fmt, ...)\n{\n    va_list ap;\n    json_t *obj;\n    int rc;\n\n    if (flux_rpc_get_unpack (f, \"{s:o}\", \"val\", &obj) < 0)\n        return -1;\n    va_start (ap, fmt);\n    if ((rc = json_vunpack_ex (obj, NULL, 0, fmt, ap) < 0))\n        errno = EPROTO;\n    va_end (ap);\n\n    return rc;\n}\n\n\n\/*\n * vi:tabstop=4 shiftwidth=4 expandtab\n *\/\n","lang_cluster":"C","length":132,"code_uid":"95287b1ba7284ef19e7448028e70066b"}
{"diff_hunk":"@@ -170,6 +170,7 @@ def remove_dir(path):\n     shutil.rmtree(path, onerror=error_handler)\n \n \n+# -------------------------------------------------------------------------\n def call_command(command, env=None):\n     \"\"\" Call an external command and return with (output, return_code).\"\"\"\n ","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nUtil module.\n\"\"\"\n\nimport datetime\nimport glob\nimport hashlib\nimport ntpath\nimport os\nimport shutil\nimport socket\nimport subprocess\nimport sys\n\nfrom codechecker_lib.logger import LoggerFactory\n\n# WARNING! LOG should be only used in this module.\nLOG = LoggerFactory.get_new_logger('UTIL')\n\n\n# ---------------------------------------------------------------------\ndef get_free_port():\n    \"\"\" Get a free port from the OS. \"\"\"\n\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind(('', 0))\n    free_port = s.getsockname()[1]\n    s.close()\n\n    return free_port\n\n\n# ---------------------------------------------------------------------\ndef is_localhost(address):\n    \"\"\"\n    Check if address is one of the valid values and try to get the\n    IP-addresses from the system.\n    \"\"\"\n\n    valid_values = ['localhost', '0.0.0.0', '*']\n\n    try:\n        valid_values.append(socket.gethostbyname('localhost'))\n    except Exception:\n        # Failed to get ip address for localhost.\n        pass\n\n    try:\n        valid_values.append(socket.gethostbyname(socket.gethostname()))\n    except Exception:\n        # Failed to get ip address for host_name.\n        pass\n\n    return address in valid_values\n\n\n# ---------------------------------------------------------------------\ndef match_file_name(file_name, pattern):\n    file_name_parts = file_name.split('--')\n\n    if file_name_parts[0] == pattern:\n        return True\n    else:\n        return False\n\n\n# ---------------------------------------------------------------------\ndef get_file_last_modification_time(file):\n    \"\"\"\n    Returns the last modification time of a file.\n    \"\"\"\n    return datetime.datetime.fromtimestamp(os.path.getmtime(file))\n\n\n# ---------------------------------------------------------------------\ndef get_env_var(env_var, needed=False):\n    \"\"\"\n    Read the environment variables and handle the exception if a necessary\n    environment variable is missing.\n    \"\"\"\n\n    value = os.getenv(env_var)\n    if needed and not value:\n        LOG.critical('Failed to read necessary environment variable %s.'\n                     ' (Maybe CodeChecker was not configured properly.)'\n                     % env_var)\n        sys.exit(1)\n\n    return value\n\n\n# -------------------------------------------------------------------------\ndef get_tmp_dir_hash():\n    \"\"\"Generate a hash based on the current time and process id.\"\"\"\n\n    pid = os.getpid()\n    time = datetime.datetime.now()\n\n    data = str(pid) + str(time)\n\n    dir_hash = hashlib.md5()\n    dir_hash.update(data)\n\n    LOG.debug('The generated temporary directory hash is %s.'\n              % dir_hash.hexdigest())\n\n    return dir_hash.hexdigest()\n\n\n# -------------------------------------------------------------------------\ndef get_file_name_from_path(path):\n    \"\"\"Get the filename from a path.\"\"\"\n    head, tail = ntpath.split(path)\n    return head, tail\n\n\n# -------------------------------------------------------------------------\ndef get_obj_target(object_file_path):\n    return os.path.split(os.path.abspath(dir))[-2]\n\n\n# -------------------------------------------------------------------------\ndef create_dir(path):\n    \"\"\"Create a directory safely if it does not exist yet.\n    This may be called from several processes or threads, creating the same\n    directory, and it fails only if the directory is not created.\n    \"\"\"\n\n    if not os.path.isdir(path):\n        try:\n            LOG.debug('Creating directory %s.' % path)\n            os.makedirs(path)\n        except Exception as e:\n            if not os.path.isdir(path):\n                LOG.error('Failed to create directory %s.' % path)\n                raise e\n\n    return\n\n\n# -------------------------------------------------------------------------\ndef get_file_list(path, pattern):\n    glob_pattern = os.path.join(path, pattern)\n    return glob.glob(glob_pattern)\n\n\n# -------------------------------------------------------------------------\ndef remove_file_list(file_list):\n    for rfile in file_list:\n        LOG.debug(rfile)\n        try:\n            os.remove(rfile)\n        except OSError:\n            # Maybe another thread has already deleted it.\n            LOG.debug('Failed to remove file %s.' % rfile)\n\n    return\n\n\n# -------------------------------------------------------------------------\ndef remove_dir(path):\n    def error_handler(*args):\n        LOG.warning('Failed to remove directory %s.' % path)\n\n    shutil.rmtree(path, onerror=error_handler)\n\n\ndef call_command(command, env=None):\n    \"\"\" Call an external command and return with (output, return_code).\"\"\"\n\n    try:\n        LOG.debug('Run ' + ' '.join(command))\n        out = subprocess.check_output(command,\n                                      bufsize=-1,\n                                      env=env,\n                                      stderr=subprocess.STDOUT)\n        LOG.debug(out)\n        return out, 0\n    except subprocess.CalledProcessError as ex:\n        LOG.debug('Running command \"' + ' '.join(command) + '\" Failed.')\n        LOG.debug(str(ex.returncode))\n        LOG.debug(ex.output)\n        return ex.output, ex.returncode\n\n\ndef get_default_workspace():\n    \"\"\"\n    Default workspace in the users home directory.\n    \"\"\"\n    workspace = os.path.join(os.path.expanduser(\"~\"), '.codechecker')\n    return workspace\n","lang_cluster":"C","length":196,"code_uid":"941b6804c3ef48698b6f70e3787864fe"}
{"diff_hunk":"@@ -95,8 +95,8 @@ if __name__ == '__main__':\n         f.write(\n             json.dumps(\n                 prepare_compiler_target.prepare(\n-                    os.path.join(args.report_dir, \"compiler_target.json\"),\n-                    args.sources_root),\n+                    os.path.join(pathOptions.report_dir, \"compiler_target.json\"),\n+                    pathOptions.sources_root),\n                 indent=4))\n \n     # ctu-collect","old_code":"#!\/usr\/bin\/env python\n# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\nimport argparse\nimport json\nimport os\nimport platform\nimport subprocess\n\nimport prepare_compile_cmd\nimport prepare_compiler_includes\nimport prepare_compiler_target\nimport prepare_analyzer_cmd\n\n\ndef execute(cmd):\n    print(\"Executing command: \" + ' '.join(cmd))\n    try:\n        proc = subprocess.Popen(cmd,\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE)\n        out, err = proc.communicate()\n\n        print(\"stdout:\\n\\n\" + out.decode(\"utf-8\"))\n        print(\"stderr:\\n\\n\" + err.decode(\"utf-8\"))\n\n        if proc.returncode != 0:\n            print('Unsuccessful run: \"' + ' '.join(cmd) + '\"')\n            raise Exception(\"Unsuccessful run of command.\")\n        return out\n    except OSError:\n        print('Failed to run: \"' + ' '.join(cmd) + '\"')\n        raise\n\n\ndef get_triple_arch(analyze_command_file):\n    with open(analyze_command_file) as f:\n        cmd = f.readline()\n\n    cmd = cmd.split()\n    for flag in cmd:\n        if flag.startswith('--target='):\n            return flag[9:].split('-')[0]  # 9 == len('--target=')\n\n    return platform.machine()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='Prepare all commands '\n        'to execute in local environmennt for debugging.')\n    parser.add_argument(\n        '--sources_root',\n        default='.\/sources-root',\n        help=\"Path of the source root.\")\n    parser.add_argument(\n        '--report_dir',\n        default='..',\n        help=\"Path of the report dir.\")\n    parser.add_argument(\n        '--clang',\n        required=True,\n        help=\"Path to the clang binary.\")\n    parser.add_argument(\n        '--clang_plugin_name', default=None,\n        help=\"Name of the used clang plugin.\")\n    parser.add_argument(\n        '--clang_plugin_path', default=None,\n        help=\"Path to the used clang plugin.\")\n    args = parser.parse_args()\n\n    compile_cmd_debug = \"compile_cmd_DEBUG.json\"\n    with open(compile_cmd_debug, 'w') as f:\n        f.write(\n            json.dumps(\n                prepare_compile_cmd.prepare(\n                    os.path.join(args.report_dir, \"compile_cmd.json\"),\n                    args.sources_root),\n                indent=4))\n\n    compiler_includes_debug = \"compiler_includes_DEBUG.json\"\n    with open(compiler_includes_debug, 'w') as f:\n        f.write(\n            json.dumps(\n                prepare_compiler_includes.prepare(\n                    os.path.join(args.report_dir, \"compiler_includes.json\"),\n                    args.sources_root),\n                indent=4))\n\n    compiler_target_debug = \"compiler_target_DEBUG.json\"\n    with open(compiler_target_debug, 'wb') as f:\n        f.write(\n            json.dumps(\n                prepare_compiler_target.prepare(\n                    os.path.join(args.report_dir, \"compiler_target.json\"),\n                    args.sources_root),\n                indent=4))\n\n    # ctu-collect\n    out = execute([\"CodeChecker\", \"analyze\", \"--ctu-collect\",\n                   compile_cmd_debug,\n                   \"--compiler-includes-file\", compiler_includes_debug,\n                   \"--compiler-target-file\", compiler_target_debug,\n                   \"-o\", \"report_debug\",\n                   \"--verbose\", \"debug\"])\n\n    analyzer_command_debug = \"analyzer-command_DEBUG\"\n    target = get_triple_arch('.\/analyzer-command')\n    with open(analyzer_command_debug, 'w') as f:\n        f.write(\n            prepare_analyzer_cmd.prepare(\n                \".\/analyzer-command\",\n                prepare_analyzer_cmd.PathOptions(\n                    args.sources_root,\n                    args.clang,\n                    args.clang_plugin_name,\n                    args.clang_plugin_path,\n                    \".\/report_debug\/ctu-dir\/\" + target)))\n\n    print(\n        \"Preparation of files for debugging is done. \"\n        \"Now you can execute the generated analyzer command. \"\n        \"E.g. $ bash % s\" %\n        analyzer_command_debug)\n","lang_cluster":"C","length":127,"code_uid":"2dbd73a90a1c4069ba2b3f8e09cc9621"}
{"diff_hunk":"@@ -97,16 +97,27 @@ def perform_analysis(args, context, actions, metadata):\n                 continue\n             metadata['checkers'][analyzer].append(check)\n \n+    if ctu_collect:\n+        shutil.rmtree(ctu_dir, ignore_errors=True)\n+    elif ctu_analyze and not os.path.exists(ctu_dir):\n+        LOG.error(\"The given <ctu-dir> '\" + ctu_dir + \"' does not exist\")\n+        return\n+\n     # Run analysis.\n     LOG.info(\"Starting static analysis ...\")\n     start_time = time.time()\n \n     analysis_manager.start_workers(actions, context, config_map,\n                                    args.jobs, args.output_path,\n-                                   __get_skip_handler(args), metadata)\n+                                   __get_skip_handler(args), metadata,\n+                                   ctu_collect, ctu_analyze,\n+                                   ctu_dir, ctu_func_map_cmd)\n \n     end_time = time.time()\n     LOG.info(\"Analysis length: \" + str(end_time - start_time) + \" sec.\")\n \n     metadata['timestamps'] = {'begin': start_time,\n                               'end': end_time}\n+\n+    if ctu_collect and ctu_analyze:\n+        shutil.rmtree(ctu_dir, ignore_errors=True)","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nPrepare and start different analysis types\n\"\"\"\nimport copy\nimport shlex\nimport subprocess\nimport time\n\nfrom libcodechecker.logger import LoggerFactory\nfrom libcodechecker.analyze import analysis_manager\nfrom libcodechecker.analyze import analyzer_env\nfrom libcodechecker.analyze import skiplist_handler\nfrom libcodechecker.analyze.analyzers import analyzer_types\n\nLOG = LoggerFactory.get_new_logger('ANALYZER')\n\n\ndef prepare_actions(actions, enabled_analyzers):\n    \"\"\"\n    Set the analyzer type for each buildaction.\n    Multiple actions if multiple source analyzers are set.\n    \"\"\"\n    res = []\n\n    for ea in enabled_analyzers:\n        for action in actions:\n            new_action = copy.deepcopy(action)\n            new_action.analyzer_type = ea\n            res.append(new_action)\n    return res\n\n\ndef __get_analyzer_version(context, analyzer_config_map):\n    \"\"\"\n    Get the path and the version of the analyzer binaries.\n    \"\"\"\n    check_env = analyzer_env.get_check_env(context.path_env_extra,\n                                           context.ld_lib_path_extra)\n\n    # Get the analyzer binaries from the config_map which\n    # contains only the checked and available analyzers.\n    versions = {}\n    for _, analyzer_cfg in analyzer_config_map.items():\n        analyzer_bin = analyzer_cfg.analyzer_binary\n        version = [analyzer_bin, u' --version']\n        try:\n            output = subprocess.check_output(shlex.split(' '.join(version)),\n                                             env=check_env)\n            versions[analyzer_bin] = output\n        except (subprocess.CalledProcessError, OSError) as oerr:\n            LOG.warning(\"Failed to get analyzer version: \" + ' '.join(version))\n            LOG.warning(oerr.strerror)\n\n    return versions\n\n\ndef __get_skip_handler(args):\n    try:\n        if args.skipfile:\n            LOG.debug_analyzer(\"Creating skiplist handler.\")\n            return skiplist_handler.SkipListHandler(args.skipfile)\n    except AttributeError:\n        LOG.debug_analyzer('Skip file was not set in the command line')\n\n\ndef perform_analysis(args, context, actions, metadata):\n    \"\"\"\n    Perform static analysis via the given (or if not, all) analyzers,\n    in the given analysis context for the supplied build actions.\n    Additionally, insert statistical information into the metadata dict.\n    \"\"\"\n\n    analyzers = args.analyzers if 'analyzers' in args \\\n        else analyzer_types.supported_analyzers\n    analyzers, _ = analyzer_types.check_supported_analyzers(\n        analyzers, context)\n\n    actions = prepare_actions(actions, analyzers)\n    config_map = analyzer_types.build_config_handlers(args, context, analyzers)\n\n    # Save some metadata information.\n    versions = __get_analyzer_version(context, config_map)\n    metadata['versions'].update(versions)\n\n    metadata['checkers'] = {}\n    for analyzer in analyzers:\n        metadata['checkers'][analyzer] = []\n\n        for check, data in config_map[analyzer].checks().items():\n            enabled, _ = data\n            if not enabled:\n                continue\n            metadata['checkers'][analyzer].append(check)\n\n    # Run analysis.\n    LOG.info(\"Starting static analysis ...\")\n    start_time = time.time()\n\n    analysis_manager.start_workers(actions, context, config_map,\n                                   args.jobs, args.output_path,\n                                   __get_skip_handler(args), metadata)\n\n    end_time = time.time()\n    LOG.info(\"Analysis length: \" + str(end_time - start_time) + \" sec.\")\n\n    metadata['timestamps'] = {'begin': start_time,\n                              'end': end_time}\n","lang_cluster":"C","length":112,"code_uid":"8365a16d05df4df9aac8aa4f5c717cfe"}
{"diff_hunk":"@@ -106,19 +106,12 @@ def handle_auth(host, port, username, login=False):\n         sys.exit(1)\n \n \n-def setup_client(host, port, uri):\n-    \"\"\"\n-    Stup the thrift client and check API version and authentication needs.\n-    \"\"\"\n-    manager = session_manager.SessionManager_Client()\n-    session_token = manager.getToken(host, port)\n-\n+def perform_auth_for_handler(manager, host, port, session_token):\n     # Before actually communicating with the server,\n     # we need to check authentication first.\n     auth_client = authentication_helper.ThriftAuthHelper(host,\n                                                          port,\n-                                                         uri +\n-                                                         'Authentication',\n+                                                         '\/Authentication',\n                                                          session_token)\n     try:\n         auth_response = auth_client.getAuthParameters()","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\nimport getpass\nimport sys\n\nfrom thrift.Thrift import TApplicationException\n\nimport shared\nfrom Authentication import ttypes as AuthTypes\n\nfrom libcodechecker import session_manager\nfrom libcodechecker.logger import LoggerFactory\n\nfrom . import thrift_helper\nfrom . import authentication_helper\n\nLOG = LoggerFactory.get_new_logger('CLIENT')\nSUPPORTED_API_VERSION = '6.0'\n\n\ndef check_api_version(client):\n    \"\"\"\n    Check if server API is supported by the client.\n    \"\"\"\n\n    version = client.getAPIVersion()\n    supp_major_version = SUPPORTED_API_VERSION.split('.')[0]\n    api_major_version = version.split('.')[0]\n\n    # There is NO compatibility between major versions.\n    return supp_major_version == api_major_version\n\n\ndef handle_auth(host, port, username, login=False):\n\n    session = session_manager.SessionManager_Client()\n\n    auth_token = session.getToken(host, port)\n\n    auth_client = authentication_helper.ThriftAuthHelper(host,\n                                                         port,\n                                                         '\/Authentication',\n                                                         auth_token)\n\n    if not login:\n        logout_done = auth_client.destroySession()\n        if logout_done:\n            session.saveToken(host, port, None, True)\n            LOG.info(\"Successfully logged out.\")\n        return\n\n    try:\n        handshake = auth_client.getAuthParameters()\n\n        if not handshake.requiresAuthentication:\n            LOG.info(\"This server does not require privileged access.\")\n            return\n\n        if auth_token and handshake.sessionStillActive:\n            LOG.info(\"You are already logged in.\")\n            return\n        else:\n            LOG.info(\"Server requires authentication to access. Please use \"\n                     \"'CodeChecker cmd login' to authenticate.\")\n\n    except TApplicationException:\n        LOG.info(\"This server does not support privileged access.\")\n        return\n\n    methods = auth_client.getAcceptedAuthMethods()\n    # Attempt username-password auth first.\n    if 'Username:Password' in str(methods):\n\n        # Try to use a previously saved credential from configuration file.\n        saved_auth = session.getAuthString(host, port)\n\n        if saved_auth:\n            LOG.info(\"Logging in using preconfigured credentials...\")\n            username = saved_auth.split(\":\")[0]\n            pwd = saved_auth.split(\":\")[1]\n        else:\n            LOG.info(\"Logging in using credentials from command line...\")\n            pwd = getpass.getpass(\"Please provide password for user '{0}'\"\n                                  .format(username))\n\n        LOG.debug(\"Trying to login as {0} to {1}:{2}\"\n                  .format(username, host, port))\n        try:\n            session_token = auth_client.performLogin(\"Username:Password\",\n                                                     username + \":\" +\n                                                     pwd)\n\n            session.saveToken(host, port, session_token)\n            LOG.info(\"Server reported successful authentication.\")\n        except shared.ttypes.RequestFailed as reqfail:\n            LOG.error(\"Authentication failed! Please check your credentials.\")\n            LOG.error(reqfail.message)\n            sys.exit(1)\n    else:\n        LOG.critical(\"No authentication methods were reported by the server \"\n                     \"that this client could support.\")\n        sys.exit(1)\n\n\ndef setup_client(host, port, uri):\n    \"\"\"\n    Stup the thrift client and check API version and authentication needs.\n    \"\"\"\n    manager = session_manager.SessionManager_Client()\n    session_token = manager.getToken(host, port)\n\n    # Before actually communicating with the server,\n    # we need to check authentication first.\n    auth_client = authentication_helper.ThriftAuthHelper(host,\n                                                         port,\n                                                         uri +\n                                                         'Authentication',\n                                                         session_token)\n    try:\n        auth_response = auth_client.getAuthParameters()\n    except TApplicationException as tex:\n        auth_response = AuthTypes.HandshakeInformation()\n        auth_response.requiresAuthentication = False\n\n    if auth_response.requiresAuthentication and \\\n            not auth_response.sessionStillActive:\n        print_err = False\n\n        if manager.is_autologin_enabled():\n            auto_auth_string = manager.getAuthString(host, port)\n            if auto_auth_string:\n                # Try to automatically log in with a saved credential\n                # if it exists for the server.\n                try:\n                    session_token = auth_client.performLogin(\n                        \"Username:Password\",\n                        auto_auth_string)\n                    manager.saveToken(host, port, session_token)\n                    LOG.info(\"Authenticated using pre-configured \"\n                             \"credentials.\")\n                except shared.ttypes.RequestFailed:\n                    print_err = True\n            else:\n                print_err = True\n        else:\n            print_err = True\n\n        if print_err:\n            LOG.error(\"Access denied. This server requires authentication.\")\n            LOG.error(\"Please log in onto the server using 'CodeChecker cmd \"\n                      \"login'.\")\n            sys.exit(1)\n\n    client = thrift_helper.ThriftClientHelper(host, port, uri, session_token)\n    # Test if client can work with the server's API.\n    if not check_api_version(client):\n        LOG.critical(\"The server uses a newer version of the API which is \"\n                     \"incompatible with this client. Please update client.\")\n        sys.exit(1)\n\n    return client\n","lang_cluster":"C","length":165,"code_uid":"e777f68cec2644d4bf310bed397b3ba0"}
{"diff_hunk":"@@ -74,9 +74,13 @@ func (poc *PolicySorter) Sorted() *tierInfo {\n \treturn tierInfo\n }\n \n+\/\/ Note: PolKV is really internal to the calc package.  It is named with an initial capital so that\n+\/\/ the test package calc_test can also use it.\n type PolKV struct {\n-\tKey   model.PolicyKey\n-\tValue *model.Policy\n+\tKey     model.PolicyKey\n+\tValue   *model.Policy\n+\tIngress *bool\n+\tEgress  *bool\n }\n \n func (p PolKV) String() string {","old_code":"\/\/ Copyright (c) 2016-2017 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage calc\n\nimport (\n\t\"fmt\"\n\t\"sort\"\n\n\tlog \"github.com\/sirupsen\/logrus\"\n\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/backend\/api\"\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/backend\/model\"\n)\n\ntype PolicySorter struct {\n\ttier *tierInfo\n}\n\nfunc NewPolicySorter() *PolicySorter {\n\treturn &PolicySorter{\n\t\ttier: &tierInfo{\n\t\t\tName:     \"default\",\n\t\t\tPolicies: make(map[model.PolicyKey]*model.Policy),\n\t\t},\n\t}\n}\n\nfunc (poc *PolicySorter) OnUpdate(update api.Update) (dirty bool) {\n\tswitch key := update.Key.(type) {\n\tcase model.PolicyKey:\n\t\toldPolicy := poc.tier.Policies[key]\n\t\tif update.Value != nil {\n\t\t\tnewPolicy := update.Value.(*model.Policy)\n\t\t\tif oldPolicy == nil ||\n\t\t\t\toldPolicy.Order != newPolicy.Order ||\n\t\t\t\toldPolicy.DoNotTrack != newPolicy.DoNotTrack ||\n\t\t\t\toldPolicy.PreDNAT != newPolicy.PreDNAT {\n\t\t\t\tdirty = true\n\t\t\t}\n\t\t\tpoc.tier.Policies[key] = newPolicy\n\t\t} else {\n\t\t\tif oldPolicy != nil {\n\t\t\t\tdelete(poc.tier.Policies, key)\n\t\t\t\tdirty = true\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc (poc *PolicySorter) Sorted() *tierInfo {\n\ttierInfo := poc.tier\n\ttierInfo.OrderedPolicies = make([]PolKV, 0, len(tierInfo.Policies))\n\tfor k, v := range tierInfo.Policies {\n\t\ttierInfo.OrderedPolicies = append(tierInfo.OrderedPolicies, PolKV{Key: k, Value: v})\n\t}\n\t\/\/ Note: using explicit Debugf() here rather than WithFields(); we want the []PolKV slice\n\t\/\/ to be stringified with %v rather than %#v (as used by WithField()).\n\tlog.Debugf(\"Order before sorting: %v\", tierInfo.OrderedPolicies)\n\tsort.Sort(PolicyByOrder(tierInfo.OrderedPolicies))\n\tlog.Debugf(\"Order after sorting: %v\", tierInfo.OrderedPolicies)\n\treturn tierInfo\n}\n\ntype PolKV struct {\n\tKey   model.PolicyKey\n\tValue *model.Policy\n}\n\nfunc (p PolKV) String() string {\n\torderStr := \"nil policy\"\n\tif p.Value != nil {\n\t\tif p.Value.Order != nil {\n\t\t\torderStr = fmt.Sprintf(\"%v\", *p.Value.Order)\n\t\t} else {\n\t\t\torderStr = \"default\"\n\t\t}\n\t}\n\treturn fmt.Sprintf(\"%s(%s)\", p.Key.Name, orderStr)\n}\n\ntype PolicyByOrder []PolKV\n\nfunc (a PolicyByOrder) Len() int      { return len(a) }\nfunc (a PolicyByOrder) Swap(i, j int) { a[i], a[j] = a[j], a[i] }\nfunc (a PolicyByOrder) Less(i, j int) bool {\n\tbothNil := a[i].Value.Order == nil && a[j].Value.Order == nil\n\tbothSet := a[i].Value.Order != nil && a[j].Value.Order != nil\n\tordersEqual := bothNil || bothSet && (*a[i].Value.Order == *a[j].Value.Order)\n\n\tif ordersEqual {\n\t\t\/\/ Use name as tie-break.\n\t\tresult := a[i].Key.Name < a[j].Key.Name\n\t\treturn result\n\t}\n\n\t\/\/ nil order maps to \"infinity\"\n\tif a[i].Value.Order == nil {\n\t\treturn false\n\t} else if a[j].Value.Order == nil {\n\t\treturn true\n\t}\n\n\t\/\/ Otherwise, use numeric comparison.\n\treturn *a[i].Value.Order < *a[j].Value.Order\n}\n\ntype tierInfo struct {\n\tName            string\n\tValid           bool\n\tOrder           *float64\n\tPolicies        map[model.PolicyKey]*model.Policy\n\tOrderedPolicies []PolKV\n}\n\nfunc NewTierInfo(name string) *tierInfo {\n\treturn &tierInfo{\n\t\tName:     name,\n\t\tPolicies: make(map[model.PolicyKey]*model.Policy),\n\t}\n}\n\nfunc (t tierInfo) String() string {\n\tpolicies := make([]string, len(t.OrderedPolicies))\n\tfor ii, pol := range t.OrderedPolicies {\n\t\tpolType := \"t\"\n\t\tif pol.Value != nil {\n\t\t\tif pol.Value.DoNotTrack {\n\t\t\t\tpolType = \"u\"\n\t\t\t} else if pol.Value.PreDNAT {\n\t\t\t\tpolType = \"p\"\n\t\t\t}\n\t\t}\n\t\tpolicies[ii] = fmt.Sprintf(\"%v(%v)\", pol.Key.Name, polType)\n\t}\n\treturn fmt.Sprintf(\"%v -> %v\", t.Name, policies)\n}\n","lang_cluster":"C","length":149,"code_uid":"9f677c03926243b497adde8d47a4f221"}
{"diff_hunk":"@@ -76,19 +76,14 @@ another key.  _target_ need not exist.\n `flux_kvs_txn_put_raw()` sets _key_ to a value containing raw data\n referred to by _data_ of length _len_.\n \n+`flux_kvs_txn_put_treeobj()` sets _key_ to an RFC 11 object, encoded\n+as a JSON string.\n+\n \n FLAGS\n -----\n \n-The _flags_ argument may be zero, or a bitmask of one or more of the\n-following flags:\n-\n-FLUX_KVS_TREEOBJ::\n-The specified value is interpreted as an RFC 11 tree object (KVS meta data)\n-rather than the actual value.  Currently the only way to obtain a valid\n-tree object is with `flux_kvs_lookup(3)` or `flux_kvs_lookupat(3)`.  In\n-the future, other methods may be available.  Note: this flag is only\n-valid for `flux_kvs_txn_put()` and `flux_kvs_txn_pack()`.\n+The _flags_ argument is currently unused and must be zero.\n \n \n include::JSON_PACK.adoc[]","old_code":"flux_kvs_txn_create(3)\n======================\n:doctype: manpage\n\n\nNAME\n----\nflux_kvs_txn_create, flux_kvs_txn_destroy, flux_kvs_txn_put, flux_kvs_txn_pack, flux_kvs_txn_vpack, flux_kvs_txn_mkdir, flux_kvs_txn_unlink, flux_kvs_txn_symlink, flux_kvs_txn_put_raw - operate on a KVS transaction object\n\n\nSYNOPSIS\n--------\n #include <flux\/core.h>\n\n flux_kvs_txn_t *flux_kvs_txn_create (void);\n\n void flux_kvs_txn_destroy (flux_kvs_txn_t *txn);\n\n int flux_kvs_txn_put (flux_kvs_txn_t *txn, int flags,\n                       const char *key, const char *json_str);\n\n int flux_kvs_txn_pack (flux_kvs_txn_t *txn, int flags,\n                        const char *key, const char *fmt, ...);\n\n int flux_kvs_txn_vpack (flux_kvs_txn_t *txn, int flags,\n                         const char *key, const char *fmt, va_list ap);\n\n int flux_kvs_txn_mkdir (flux_kvs_txn_t *txn, int flags,\n                         const char *key);\n\n int flux_kvs_txn_unlink (flux_kvs_txn_t *txn, int flags,\n                          const char *key);\n\n int flux_kvs_txn_symlink (flux_kvs_txn_t *txn, int flags,\n                           const char *key, const char *target);\n\n int flux_kvs_txn_put_raw (flux_kvs_txn_t *txn, int flags,\n                           const char *key, const void *data, int len);\n\n\n\nDESCRIPTION\n-----------\n\n`flux_kvs_txn_create()` creates a KVS transaction object that may be\npassed to `flux_kvs_commit(3)` or `flux_kvs_fence(3)`.  The transaction\nconsists of a list of operations that are applied to the KVS together,\nin order.  The entire transaction either succeeds or fails.  After commit\nor fence, the object must be destroyed with `flux_kvs_txn_destroy()`.\n\n`flux_kvs_txn_put()` sets _key_ to a value represented by _json_str_.\nIf _key_ does not exist it is created.  _key_ is hierarchical, with period\n(\".\") used as a path separator.  \".\" represents the root of the namespace\nand is optional at the beginning of _key_.  Any path components in _key_\nthat do not exist is created.  Any path components in _key_ that must be\nconverted to directories are overwritten.  The value _json_str_ may be be\nany bare JSON value (except null), a JSON array, or a JSON object, encoded\nas a string.  Alternatively, the FLUX_KVS_TREEOBJ flag may be specified\nindicating that the _json_str_ value is to be interpreted as an RFC 11\ntree object, as described in FLAGS below.  A NULL _json_str_ value is\nequivalent to calling `flux_kvs_txn_unlink()` on _key_.\n\n`flux_kvs_txn_pack()` is identical to `flux_kvs_txn_put()`, except\n`json_pack()` style arguments (see below) are used to construct the\nvalue.  `flux_kvs_txn_vpack()` is a variant that accepts a _va_list_\nargument.\n\n`flux_kvs_txn_mkdir()` sets _key_ to an empty directory.\n\n`flux_kvs_txn_unlink()` removes _key_.  If _key_ is a directory,\nall its contents are removed as well.\n\n`flux_kvs_txn_symlink()` sets _key_ to a symbolic link pointing to _target_,\nanother key.  _target_ need not exist.\n\n`flux_kvs_txn_put_raw()` sets _key_ to a value containing raw data\nreferred to by _data_ of length _len_.\n\n\nFLAGS\n-----\n\nThe _flags_ argument may be zero, or a bitmask of one or more of the\nfollowing flags:\n\nFLUX_KVS_TREEOBJ::\nThe specified value is interpreted as an RFC 11 tree object (KVS meta data)\nrather than the actual value.  Currently the only way to obtain a valid\ntree object is with `flux_kvs_lookup(3)` or `flux_kvs_lookupat(3)`.  In\nthe future, other methods may be available.  Note: this flag is only\nvalid for `flux_kvs_txn_put()` and `flux_kvs_txn_pack()`.\n\n\ninclude::JSON_PACK.adoc[]\n\n\nRETURN VALUE\n------------\n\n`flux_kvs_txn_create()` returns a `flux_kvs_txn_t` object on success,\nor NULL on failure with errno set appropriately.\n\n`flux_kvs_txn_put()`, `flux_kvs_txn_pack()`, `flux_kvs_txn_mkdir()`,\n`flux_kvs_txn_unlink()`, `flux_kvs_txn_symlink()`, and `flux_kvs_txn_put_raw()`\nreturns 0 on success, or -1 on failure with errno set appropriately.\n\nERRORS\n------\n\nEINVAL::\nOne of the arguments was invalid.\n\nENOMEM::\nOut of memory.\n\n\nAUTHOR\n------\nThis page is maintained by the Flux community.\n\n\nRESOURCES\n---------\nGithub: <http:\/\/github.com\/flux-framework>\n\n\nCOPYRIGHT\n---------\ninclude::COPYRIGHT.adoc[]\n\n\nSEE ALSO\n---------\nflux_kvs_commit(3)\n\nhttps:\/\/github.com\/flux-framework\/rfc\/blob\/master\/spec_11.adoc[RFC 11: Key Value Store Tree Object Format v1]\n","lang_cluster":"C","length":136,"code_uid":"5a9239ea8696494f9678d9fbbdc2dbbf"}
{"diff_hunk":"@@ -16,6 +16,7 @@ from codechecker_lib import analysis_manager\n from codechecker_lib import client\n from codechecker_lib import logger\n from codechecker_lib import skiplist_handler\n+from codechecker_lib import analyzer_env\n from codechecker_lib.analyzers import analyzer_types\n \n LOG = logger.get_new_logger('ANALYZER')","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nPrepare and start different analisys types\n\"\"\"\nimport copy\nimport json\nimport os\nimport sys\nimport time\n\nfrom codechecker_lib import analysis_manager\nfrom codechecker_lib import client\nfrom codechecker_lib import logger\nfrom codechecker_lib import skiplist_handler\nfrom codechecker_lib.analyzers import analyzer_types\n\nLOG = logger.get_new_logger('ANALYZER')\n\n\ndef prepare_actions(actions, enabled_analyzers):\n    \"\"\"\n    Set the analyzer type for each buildaction.\n    Multiple actions if multiple source analyzers are set.\n    \"\"\"\n    res = []\n\n    for ea in enabled_analyzers:\n        for action in actions:\n            new_action = copy.deepcopy(action)\n            new_action.analyzer_type = ea\n            res.append(new_action)\n    return res\n\n\ndef run_check(args, actions, context):\n    \"\"\"\n    Prepare:\n    - analyzer config handlers\n    - skiplist handling\n    - analyzer severity levels\n\n    Stores analysis related data to the database and starts the analysis.\n    \"\"\"\n\n    if args.jobs <= 0:\n        args.jobs = 1\n\n    LOG.debug_analyzer(\"Checking supported analyzers.\")\n    enabled_analyzers = analyzer_types.check_supported_analyzers(\n        args.analyzers,\n        context)\n\n    # Load severity map from config file.\n    LOG.debug_analyzer(\"Loading checker severity map.\")\n    if os.path.exists(context.checkers_severity_map_file):\n        with open(context.checkers_severity_map_file, 'r') as sev_conf_file:\n            severity_config = sev_conf_file.read()\n\n        context.severity_map = json.loads(severity_config)\n\n    actions = prepare_actions(actions, enabled_analyzers)\n\n    package_version = context.version['major'] + '.' + context.version['minor']\n\n    suppress_file = ''\n    try:\n        suppress_file = os.path.realpath(args.suppress)\n    except AttributeError:\n        LOG.debug_analyzer('Suppress file was not set in the command line')\n\n    # Create one skip list handler shared between the analysis manager workers.\n    skip_handler = None\n    try:\n        if args.skipfile:\n            LOG.debug_analyzer(\"Creating skiplist handler.\")\n            skip_handler = skiplist_handler.SkipListHandler(args.skipfile)\n    except AttributeError:\n        LOG.debug_analyzer('Skip file was not set in the command line')\n\n    with client.get_connection() as connection:\n        context.run_id = connection.add_checker_run(' '.join(sys.argv),\n                                                    args.name,\n                                                    package_version,\n                                                    args.force)\n\n        # Clean previous suppress information.\n        client.clean_suppress(connection, context.run_id)\n\n        if os.path.exists(suppress_file):\n            client.send_suppress(context.run_id, connection, suppress_file)\n\n        analyzer_config_map = \\\n            analyzer_types.build_config_handlers(args,\n                                                 context,\n                                                 enabled_analyzers,\n                                                 connection)\n        if skip_handler:\n            connection.add_skip_paths(context.run_id,\n                                      skip_handler.get_skiplist())\n\n    LOG.info(\"Static analysis is starting ...\")\n    start_time = time.time()\n\n    analysis_manager.start_workers(args,\n                                   actions,\n                                   context,\n                                   analyzer_config_map,\n                                   skip_handler)\n\n    end_time = time.time()\n\n    with client.get_connection() as connection:\n        connection.finish_checker_run(context.run_id)\n\n    LOG.info(\"Analysis length: \" + str(end_time - start_time) + \" sec.\")\n\n\ndef run_quick_check(args,\n                    context,\n                    actions):\n    \"\"\"\n    This function implements the \"quickcheck\" feature.\n    No result is stored to a database.\n    \"\"\"\n\n    enabled_analyzers = analyzer_types.check_supported_analyzers(args.analyzers,\n                                                                 context)\n\n    actions = prepare_actions(actions, enabled_analyzers)\n\n    analyzer_config_map = \\\n        analyzer_types.build_config_handlers(args,\n                                             context,\n                                             enabled_analyzers)\n\n    analysis_manager.start_workers(args, actions, context, analyzer_config_map,\n                                   None, False)\n","lang_cluster":"C","length":141,"code_uid":"c85d6ee084f742d0b18e23bcc95115e1"}
{"diff_hunk":"@@ -100,11 +100,15 @@ func (epmm *DefaultEPMarkManager) GetEndpointMark(ep string) (uint32, error) {\n \t\treturn 0, errors.New(\"No mark left for endpoint\")\n \t}\n \n-\tmark, err := epmm.markBitsManager.MapNumberToMark(prospect)\n+\treturn epmm.allocateOnePosition(ep, prospect)\n+}\n+\n+func (epmm *DefaultEPMarkManager) allocateOnePosition(ep string, pos int) (uint32, error) {\n+\tmark, err := epmm.markBitsManager.MapNumberToMark(pos)\n \tif err != nil {\n \t\treturn 0, err\n \t}\n-\tepmm.setMark(ep, prospect, mark)\n+\tepmm.setMark(ep, pos, mark)\n \treturn mark, nil\n }\n ","old_code":"\/\/ Copyright (c) 2018 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage rules\n\nimport (\n\t\"errors\"\n\t\"hash\/fnv\"\n\t\"io\"\n\n\t\"github.com\/projectcalico\/felix\/markbits\"\n)\n\n\/\/ Endpoint Mark Mapper (EPM) provides set of functions to manage allocation\/free endpoint mark bit\n\/\/ given a mark bit mask. Note: This is not thread safe.\ntype EndpointMarkMapper interface {\n\tGetMask() uint32\n\tGetEndpointMark(ep string) (uint32, error)\n\tReleaseEndpointMark(ep string)\n\tSetEndpointMark(ep string, mark uint32) error\n}\n\ntype DefaultEPMarkManager struct {\n\tmarkBitsManager *markbits.MarkBitsManager\n\tmaxPosition     int\n\n\thash32 HashCalculator32\n\n\tactiveEndpointToPosition map[string]int\n\tactiveEndpointToMark     map[string]uint32\n\tactivePositionToEndpoint map[int]string\n\tactiveMarkToEndpoint     map[uint32]string\n}\n\nfunc NewEndpointMarkMapper(markMask uint32) EndpointMarkMapper {\n\treturn NewEndpointMarkMapperWithShim(markMask, fnv.New32())\n}\n\nfunc NewEndpointMarkMapperWithShim(markMask uint32, hash32 HashCalculator32) EndpointMarkMapper {\n\tmarkBitsManager := markbits.NewMarkBitsManager(markMask, \"endpoint-iptable-mark\")\n\n\treturn &DefaultEPMarkManager{\n\t\tmarkBitsManager: markBitsManager,\n\t\tmaxPosition:     markBitsManager.CurrentFreeNumberOfMark(), \/\/ This includes zero\n\t\thash32:          hash32,\n\t\tactiveEndpointToPosition: map[string]int{},\n\t\tactiveEndpointToMark:     map[string]uint32{},\n\t\tactivePositionToEndpoint: map[int]string{},\n\t\tactiveMarkToEndpoint:     map[uint32]string{},\n\t}\n}\n\nfunc (epmm *DefaultEPMarkManager) GetMask() uint32 {\n\treturn epmm.markBitsManager.GetMask()\n}\n\nfunc (epmm *DefaultEPMarkManager) GetEndpointMark(ep string) (uint32, error) {\n\tlength := len(ep)\n\tif length == 0 {\n\t\treturn 0, errors.New(\"Invalid endpoint name\")\n\t}\n\n\t\/\/ Return current mark for Endpoint if it already has one.\n\tif mark, ok := epmm.activeEndpointToMark[ep]; ok {\n\t\treturn mark, nil\n\t}\n\n\t\/\/ Try to allocate a position based on hash from endpoint name.\n\tepmm.hash32.Write([]byte(ep))\n\ttotal := int(epmm.hash32.Sum32())\n\tepmm.hash32.Reset()\n\n\tvar prospect int\n\tgotOne := false\n\tfor i := 0; i < epmm.maxPosition; i++ {\n\t\tprospect = (total + i) % epmm.maxPosition\n\t\tif prospect == 0 {\n\t\t\t\/\/ Make sure we get non zero position number.\n\t\t\tcontinue\n\t\t}\n\t\t_, alreadyAlloced := epmm.activePositionToEndpoint[prospect]\n\t\tif !alreadyAlloced {\n\t\t\tgotOne = true\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif !gotOne {\n\t\treturn 0, errors.New(\"No mark left for endpoint\")\n\t}\n\n\tmark, err := epmm.markBitsManager.MapNumberToMark(prospect)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tepmm.setMark(ep, prospect, mark)\n\treturn mark, nil\n}\n\nfunc (epmm *DefaultEPMarkManager) ReleaseEndpointMark(ep string) {\n\tif mark, ok := epmm.activeEndpointToMark[ep]; ok {\n\t\tepmm.deleteMark(ep, epmm.activeEndpointToPosition[ep], mark)\n\t}\n}\n\n\/\/ This is used to set a mark for an endpoint from previous allocated mark.\n\/\/ The endpoint should not have a mark already.\nfunc (epmm *DefaultEPMarkManager) SetEndpointMark(ep string, mark uint32) error {\n\tif currentMark, ok := epmm.activeEndpointToMark[ep]; ok {\n\t\t\/\/ We got a endpoint with mark already.\n\t\tif currentMark != mark {\n\t\t\treturn errors.New(\"Different mark already exists\")\n\t\t}\n\t\treturn nil\n\t}\n\tif currentEP, ok := epmm.activeMarkToEndpoint[mark]; ok {\n\t\t\/\/ We got a mark with endpoint already.\n\t\tif currentEP != ep {\n\t\t\treturn errors.New(\"Endpoint with this mark already exists\")\n\t\t}\n\t\treturn nil\n\t}\n\n\tpos, err := epmm.markBitsManager.MapMarkToNumber(mark)\n\tif err != nil {\n\t\treturn err\n\t}\n\tepmm.setMark(ep, pos, mark)\n\treturn nil\n}\n\nfunc (epmm *DefaultEPMarkManager) deleteMark(ep string, pos int, mark uint32) {\n\tdelete(epmm.activePositionToEndpoint, pos)\n\tdelete(epmm.activeMarkToEndpoint, mark)\n\tdelete(epmm.activeEndpointToPosition, ep)\n\tdelete(epmm.activeEndpointToMark, ep)\n}\n\nfunc (epmm *DefaultEPMarkManager) setMark(ep string, pos int, mark uint32) {\n\tepmm.activePositionToEndpoint[pos] = ep\n\tepmm.activeEndpointToPosition[ep] = pos\n\tepmm.activeEndpointToMark[ep] = mark\n\tepmm.activeMarkToEndpoint[mark] = ep\n}\n\n\/\/ This interface has subset of functions of built in hash32 interface.\ntype HashCalculator32 interface {\n\t\/\/ Write (via the embedded io.Writer interface) adds more data to the running hash.\n\t\/\/ It never returns an error.\n\tio.Writer\n\n\t\/\/ Sum32 returns a hash result of uint32.\n\t\/\/ It does not change the underlying hash state.\n\tSum32() uint32\n\n\t\/\/ Reset resets the Hash to its initial state.\n\tReset()\n}\n","lang_cluster":"C","length":169,"code_uid":"279f41f9a07c4bc189f15c580ffa8cb2"}
{"diff_hunk":"@@ -31,11 +31,11 @@ static struct wlr_input_device *allocate_device(struct wlr_backend_state *state,\n \t\/\/ TODO: any way to retrieve those information?\n \tint vendor = 0;\n \tint product = 0;\n-\tconst char *name = \"unknown;wayland\";\n+\tconst char *name = \"wayland\";\n \tstruct wlr_input_device *wlr_device = wlr_input_device_create(\n \t\ttype, &input_device_impl, devstate,\n \t\tname, vendor, product);\n-\tif(!wlr_device) {\n+\tif (!wlr_device) {\n \t\tfree(devstate);\n \t\treturn NULL;\n \t}","old_code":"#define _XOPEN_SOURCE 500\n#include <assert.h>\n#include <stdlib.h>\n#include <stdint.h>\n#include <string.h>\n#include <wayland-client.h>\n#include <wlr\/interfaces\/wlr_output.h>\n#include <wlr\/interfaces\/wlr_input_device.h>\n#include <wlr\/interfaces\/wlr_pointer.h>\n#include <wlr\/interfaces\/wlr_keyboard.h>\n#include <wlr\/util\/log.h>\n#include \"backend\/wayland.h\"\n\nstatic void wlr_wl_device_destroy(struct wlr_input_device_state *state) {\n\tfree(state);\n}\n\nstatic struct wlr_input_device_impl input_device_impl = {\n\t.destroy = wlr_wl_device_destroy\n};\n\nstatic struct wlr_input_device *allocate_device(struct wlr_backend_state *state,\n\t\tenum wlr_input_device_type type) {\n\tstruct wlr_input_device_state *devstate =\n\t\tcalloc(1, sizeof(struct wlr_input_device_state));\n\tif(!devstate) {\n\t\twlr_log(L_ERROR, \"Allocation failed: %s\", strerror(errno));\n\t\treturn NULL;\n\t}\n\n\t\/\/ TODO: any way to retrieve those information?\n\tint vendor = 0;\n\tint product = 0;\n\tconst char *name = \"unknown;wayland\";\n\tstruct wlr_input_device *wlr_device = wlr_input_device_create(\n\t\ttype, &input_device_impl, devstate,\n\t\tname, vendor, product);\n\tif(!wlr_device) {\n\t\tfree(devstate);\n\t\treturn NULL;\n\t}\n\n\tlist_add(state->devices, wlr_device);\n\treturn wlr_device;\n}\n\nstatic void seat_handle_capabilities(void *data, struct wl_seat *wl_seat,\n\t\tenum wl_seat_capability caps) {\n\tstruct wlr_backend_state *state = data;\n\tassert(state->seat == wl_seat);\n\n\t\/\/ TODO: add listeners and receive input\n\tif ((caps & WL_SEAT_CAPABILITY_POINTER)) {\n\t\twlr_log(L_DEBUG, \"seat %p offered pointer\", wl_seat);\n\t\tstruct wl_pointer *wl_pointer = wl_seat_get_pointer(wl_seat);\n\n\t\tstruct wlr_input_device *wlr_device = allocate_device(state,\n\t\t\tWLR_INPUT_DEVICE_POINTER);\n\t\tif(!wlr_device) {\n\t\t\twl_pointer_destroy(wl_pointer);\n\t\t\twlr_log(L_ERROR, \"Unable to allocate wl_pointer device\");\n\t\t\treturn;\n\t\t}\n\n\t\twlr_device->pointer = wlr_pointer_create(NULL, NULL);\n\t\tlist_add(state->devices, wlr_device);\n\t\twl_signal_emit(&state->backend->events.input_add, wlr_device);\n\t}\n\tif ((caps & WL_SEAT_CAPABILITY_KEYBOARD)) {\n\t\twlr_log(L_DEBUG, \"seat %p offered keyboard\", wl_seat);\n\t\tstruct wl_keyboard *wl_keyboard = wl_seat_get_keyboard(wl_seat);\n\t\tstruct wlr_input_device *wlr_device = allocate_device(state,\n\t\t\tWLR_INPUT_DEVICE_KEYBOARD);\n\t\tif(!wlr_device) {\n\t\t\twl_keyboard_release(wl_keyboard);\n\t\t\twlr_log(L_ERROR, \"Unable to allocate wl_pointer device\");\n\t\t\treturn;\n\t\t}\n\n\t\twlr_device->keyboard = wlr_keyboard_create(NULL, NULL);\n\t\tlist_add(state->devices, wlr_device);\n\t\twl_signal_emit(&state->backend->events.input_add, wlr_device);\n\t}\n\n\t\/\/ TODO: touch\n}\n\nstatic void seat_handle_name(void *data, struct wl_seat *wl_seat, const char *name) {\n\tstruct wlr_backend_state *state = data;\n\tassert(state->seat == wl_seat);\n\tstate->seatName = strdup(name);\n}\n\nconst struct wl_seat_listener seat_listener = {\n\t.capabilities = seat_handle_capabilities,\n\t.name = seat_handle_name,\n};\n","lang_cluster":"C","length":97,"code_uid":"1d56accb492047518a01b529609a1f59"}
{"diff_hunk":"@@ -166,7 +166,7 @@ class PlistToDB(ResultHandler):\n                 connection.finish_build_action(analysis_id, msg)\n                 return 1\n \n-            self.__store_bugs(files, bugs, connection, analysis_id)\n+            self.__store_bugs(files, reports, connection, analysis_id)\n \n             connection.finish_build_action(analysis_id, self.analyzer_stderr)\n ","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\nfrom abc import ABCMeta\nimport ntpath\nimport os\nimport zlib\n\nimport shared\n\nfrom libcodechecker import client\nfrom libcodechecker import logger\nfrom libcodechecker import suppress_handler\nfrom libcodechecker.analyze import plist_parser\nfrom libcodechecker.analyze.analyzers.result_handler_base import ResultHandler\nfrom libcodechecker.logger import LoggerFactory\n\nLOG = LoggerFactory.get_new_logger('PLIST TO DB')\n\n\nclass PlistToDB(ResultHandler):\n    \"\"\"\n    Result handler for processing a plist file with the\n    analysis results and stores them to the database.\n    \"\"\"\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, buildaction, workspace, run_id):\n        super(PlistToDB, self).__init__(buildaction, workspace)\n        self.__run_id = run_id\n\n    def __store_bugs(self, files, bugs, connection, analisys_id):\n        file_ids = {}\n        # Send content of file to the server if needed.\n        for file_name in files:\n            file_descriptor = connection.need_file_content(self.__run_id,\n                                                           file_name)\n            file_ids[file_name] = file_descriptor.fileId\n\n            # Sometimes the file doesn't exist, e.g. when the input of the\n            # analysis is pure plist files.\n            if not os.path.isfile(file_name):\n                LOG.debug(file_name + ' not found, and will not be stored.')\n                continue\n\n            if file_descriptor.needed:\n                with open(file_name, 'r') as source_file:\n                    file_content = source_file.read()\n                compressed_file = zlib.compress(file_content,\n                                                zlib.Z_BEST_COMPRESSION)\n                # TODO: we may not use the file content in the end\n                # depending on skippaths.\n                LOG.debug('storing file content to the database')\n                connection.add_file_content(file_descriptor.fileId,\n                                            compressed_file)\n\n        # Skipping bugs in header files handled here.\n        report_ids = []\n        for bug in bugs:\n            events = bug.events()\n\n            # Skip list handler can be None if no config file is set.\n            if self.skiplist_handler:\n                if events and self.skiplist_handler.should_skip(\n                        events[-1].start_pos.file_path):\n                    # Issue #20: this bug is in a file which should be skipped\n                    LOG.debug(bug.hash_value + ' is skipped (in ' +\n                              events[-1].start_pos.file_path + \")\")\n                    continue\n\n            # Create remaining data for bugs and send them to the server.\n            bug_paths = []\n            for path in bug.paths():\n                bug_paths.append(\n                    shared.ttypes.BugPathPos(path.start_pos.line,\n                                             path.start_pos.col,\n                                             path.end_pos.line,\n                                             path.end_pos.col,\n                                             file_ids[\n                                                 path.start_pos.file_path]))\n\n            bug_events = []\n            for event in bug.events():\n                bug_events.append(shared.ttypes.BugPathEvent(\n                    event.start_pos.line,\n                    event.start_pos.col,\n                    event.end_pos.line,\n                    event.end_pos.col,\n                    event.msg,\n                    file_ids[event.start_pos.file_path]))\n\n            bug_hash = bug.hash_value\n\n            severity_name = self.severity_map.get(bug.checker_name,\n                                                  'UNSPECIFIED')\n            severity = shared.ttypes.Severity._NAMES_TO_VALUES[severity_name]\n\n            sp_handler = suppress_handler.SourceSuppressHandler(bug)\n\n            # Check for suppress comment.\n            supp = sp_handler.get_suppressed()\n            if supp:\n                connection.add_suppress_bug(self.__run_id, [supp])\n\n            LOG.debug('Storing check results to the database.')\n\n            report_id = connection.add_report(analisys_id,\n                                              file_ids[bug.file_path],\n                                              bug_hash,\n                                              bug.msg,\n                                              bug_paths,\n                                              bug_events,\n                                              bug.checker_name,\n                                              bug.category,\n                                              bug.type,\n                                              severity,\n                                              supp is not None)\n\n            report_ids.append(report_id)\n\n    def handle_results(self):\n        \"\"\"\n        Send the plist content to the database.\n        Server API calls should be used in one connection.\n         - addBuildAction\n         - addReport\n         - needFileContent\n         - addFileContent\n         - finishBuildAction\n        \"\"\"\n\n        with client.get_connection() as connection:\n\n            LOG.debug('Storing original build and analyzer command '\n                      'to the database.')\n\n            _, source_file_name = ntpath.split(self.analyzed_source_file)\n\n            if LoggerFactory.get_log_level() == logger.DEBUG:\n                analyzer_cmd = ' '.join(self.analyzer_cmd)\n            else:\n                analyzer_cmd = ''\n\n            build_cmd_hash = self.buildaction.original_command_hash\n            analysis_id = \\\n                connection.add_build_action(self.__run_id,\n                                            build_cmd_hash,\n                                            analyzer_cmd,\n                                            self.buildaction.analyzer_type,\n                                            source_file_name)\n\n            assert self.analyzer_returncode == 0\n\n            plist_file = self.analyzer_result_file\n\n            try:\n                files, bugs = plist_parser.parse_plist(plist_file)\n            except Exception as ex:\n                LOG.debug(str(ex))\n                msg = 'Parsing the generated result file failed.'\n                LOG.error(msg + ' ' + plist_file)\n                connection.finish_build_action(analysis_id, msg)\n                return 1\n\n            self.__store_bugs(files, bugs, connection, analysis_id)\n\n            connection.finish_build_action(analysis_id, self.analyzer_stderr)\n\n    def postprocess_result(self):\n        \"\"\"\n        No postprocessing required for plists.\n        \"\"\"\n        pass\n","lang_cluster":"C","length":177,"code_uid":"06a9e494390f474a8058586ae1ad0764"}
{"diff_hunk":"@@ -90,9 +90,7 @@ int main()\n   int* p = 0;\n \n   i = *p + 42;\n-}\"\"\"\n-        elif version == 4:\n-            source = \"\"\"\n+}\"\"\", \"\"\"\n \n \n int main()","old_code":"#\n# -----------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -----------------------------------------------------------------------------\n\n\"\"\" detection_status function test. \"\"\"\nimport json\nimport os\nimport unittest\n\nimport shared\n\nfrom libtest import codechecker\nfrom libtest import env\n\n\nclass TestDetectionStatus(unittest.TestCase):\n\n    def setUp(self):\n        # TEST_WORKSPACE is automatically set by test package __init__.py .\n        self.test_workspace = os.environ['TEST_WORKSPACE']\n\n        test_class = self.__class__.__name__\n        print('Running ' + test_class + ' tests in ' + self.test_workspace)\n\n        self._codechecker_cfg = env.import_codechecker_cfg(self.test_workspace)\n\n        # Get the CodeChecker cmd if needed for the tests.\n        self._codechecker_cmd = env.codechecker_cmd()\n        self._test_dir = os.path.join(self.test_workspace, 'test_files')\n\n        try:\n            os.makedirs(self._test_dir)\n        except os.error:\n            # Directory already exists.\n            pass\n\n        # Setup a viewer client to test viewer API calls.\n        self._cc_client = env.setup_viewer_client(self.test_workspace)\n        self.assertIsNotNone(self._cc_client)\n\n        # Change working dir to testfile dir so CodeChecker can be run easily.\n        self.__old_pwd = os.getcwd()\n        os.chdir(self._test_dir)\n\n        self._source_file = \"main.cpp\"\n\n        # Init project dir.\n        makefile = \"all:\\n\\t$(CXX) -c main.cpp -o \/dev\/null\\n\"\n        project_info = {\n            \"name\": \"hello\",\n            \"clean_cmd\": \"\",\n            \"build_cmd\": \"make\"\n        }\n\n        with open(os.path.join(self._test_dir, 'Makefile'), 'w') as f:\n            f.write(makefile)\n        with open(os.path.join(self._test_dir, 'project_info.json'), 'w') as f:\n            json.dump(project_info, f)\n\n    def tearDown(self):\n        \"\"\"Restore environment after tests have ran.\"\"\"\n        os.chdir(self.__old_pwd)\n\n    def _create_source_file(self, version):\n        if version == 1:\n            source = \"\"\"\nint main()\n{\n  int i = 1 \/ 0;\n}\"\"\"\n        elif version == 2:\n            source = \"\"\"\nint main()\n{\n  int i = 1 \/ 0;\n\n  int* p = 0;\n\n  i = *p + 42;\n}\"\"\"\n        elif version == 3:\n            source = \"\"\"\nint main()\n{\n  int i = 1 \/ 2;\n\n  int* p = 0;\n\n  i = *p + 42;\n}\"\"\"\n        elif version == 4:\n            source = \"\"\"\n\n\nint main()\n{\n  int i = 1 \/ 0;\n\n  int* p = 0;\n\n  i = *p + 42;\n}\"\"\"\n\n        with open(os.path.join(self._test_dir, self._source_file), 'w') as f:\n            f.write(source)\n\n        codechecker.check(self._codechecker_cfg,\n                          'hello',\n                          self._test_dir)\n\n    def test_same_file_change(self):\n        \"\"\"\n        This tests the change of the detection status of bugs when the file\n        content changes.\n        \"\"\"\n\n        # Check the first file version\n        self._create_source_file(1)\n\n        runs = self._cc_client.getRunData(None)\n        run_id = max(map(lambda run: run.runId, runs))\n\n        reports = self._cc_client.getRunResults([run_id], 100, 0, [], [])\n        print(reports)\n        self.assertEqual(len(reports), 2)\n        self.assertTrue(all(map(\n            lambda r: r.detectionStatus == shared.ttypes.DetectionStatus.NEW,\n            reports)))\n\n        # Check the second file version\n        self._create_source_file(2)\n        reports = self._cc_client.getRunResults([run_id], 100, 0, [], [])\n        for report in reports:\n            if report.detectionStatus == \\\n                    shared.ttypes.DetectionStatus.UNRESOLVED:\n                self.assertIn(report.bugHash,\n                              ['209be2f6905590d99853ce01d52a78e0',\n                               'e8f47588c8095f02a53e338984ce52ba'])\n            elif report.detectionStatus == \\\n                    shared.ttypes.DetectionStatus.NEW:\n                self.assertIn(report.bugHash,\n                              ['cbd629ba2ee25c41cdbf5e2e336b1b1c'])\n            else:\n                self.assertTrue(False)\n\n        # Check the third file version\n        self._create_source_file(3)\n        reports = self._cc_client.getRunResults([run_id], 100, 0, [], [])\n        for report in reports:\n            if report.detectionStatus == \\\n                    shared.ttypes.DetectionStatus.RESOLVED:\n                self.assertIn(report.bugHash,\n                              ['209be2f6905590d99853ce01d52a78e0',\n                               'e8f47588c8095f02a53e338984ce52ba'])\n            elif report.detectionStatus == \\\n                    shared.ttypes.DetectionStatus.NEW:\n                self.assertIn(report.bugHash,\n                              ['ac147b31a745d91be093bd70bbc5567c'])\n            elif report.detectionStatus == \\\n                    shared.ttypes.DetectionStatus.UNRESOLVED:\n                self.assertIn(report.bugHash,\n                              ['cbd629ba2ee25c41cdbf5e2e336b1b1c'])\n            else:\n                self.assertTrue(False)\n\n        # Check the second file version again\n        self._create_source_file(2)\n        reports = self._cc_client.getRunResults([run_id], 100, 0, [], [])\n        for report in reports:\n            if report.detectionStatus == \\\n                    shared.ttypes.DetectionStatus.UNRESOLVED:\n                self.assertIn(report.bugHash,\n                              ['cbd629ba2ee25c41cdbf5e2e336b1b1c'])\n            elif report.detectionStatus == \\\n                    shared.ttypes.DetectionStatus.REOPENED:\n                self.assertIn(report.bugHash,\n                              ['209be2f6905590d99853ce01d52a78e0',\n                               'e8f47588c8095f02a53e338984ce52ba'])\n            elif report.detectionStatus == \\\n                    shared.ttypes.DetectionStatus.RESOLVED:\n                self.assertIn(report.bugHash,\n                              ['ac147b31a745d91be093bd70bbc5567c'])\n\n        # Check the fourth file version\n        self._create_source_file(4)\n        reports = self._cc_client.getRunResults([run_id], 100, 0, [], [])\n        for report in reports:\n            if report.detectionStatus == \\\n                    shared.ttypes.DetectionStatus.UNRESOLVED:\n                self.assertIn(report.bugHash,\n                              ['209be2f6905590d99853ce01d52a78e0',\n                               'e8f47588c8095f02a53e338984ce52ba',\n                               'cbd629ba2ee25c41cdbf5e2e336b1b1c'])\n            elif report.detectionStatus == \\\n                    shared.ttypes.DetectionStatus.RESOLVED:\n                self.assertIn(report.bugHash,\n                              ['ac147b31a745d91be093bd70bbc5567c'])\n","lang_cluster":"C","length":200,"code_uid":"9c1e30d675684ec2828697d719dc49f9"}
{"diff_hunk":"@@ -44,9 +44,8 @@\n \n \/* global list of tokens we've seen *\/\n static struct _fpga_feature_token *ftoken_root;\n-\/** Mutex to protect feature tokens *\/\n-pthread_mutex_t ftoken_lock = PTHREAD_MUTEX_INITIALIZER;\n \n+extern pthread_mutex_t global_lock;\n \/**\n  * @brief Add entry to linked list for feature tokens\n  *\tWill allocate memory (which is freed by feature_token_cleanup())","old_code":"\/\/ Copyright(c) 2017-2018, Intel Corporation\n\/\/\n\/\/ Redistribution  and  use  in source  and  binary  forms,  with  or  without\n\/\/ modification, are permitted provided that the following conditions are met:\n\/\/\n\/\/ * Redistributions of  source code  must retain the  above copyright notice,\n\/\/   this list of conditions and the following disclaimer.\n\/\/ * Redistributions in binary form must reproduce the above copyright notice,\n\/\/   this list of conditions and the following disclaimer in the documentation\n\/\/   and\/or other materials provided with the distribution.\n\/\/ * Neither the name  of Intel Corporation  nor the names of its contributors\n\/\/   may be used to  endorse or promote  products derived  from this  software\n\/\/   without specific prior written permission.\n\/\/\n\/\/ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n\/\/ AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,  BUT NOT LIMITED TO,  THE\n\/\/ IMPLIED WARRANTIES OF  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n\/\/ ARE DISCLAIMED.  IN NO EVENT  SHALL THE COPYRIGHT OWNER  OR CONTRIBUTORS BE\n\/\/ LIABLE  FOR  ANY  DIRECT,  INDIRECT,  INCIDENTAL,  SPECIAL,  EXEMPLARY,  OR\n\/\/ CONSEQUENTIAL  DAMAGES  (INCLUDING,  BUT  NOT LIMITED  TO,  PROCUREMENT  OF\n\/\/ SUBSTITUTE GOODS OR SERVICES;  LOSS OF USE,  DATA, OR PROFITS;  OR BUSINESS\n\/\/ INTERRUPTION)  HOWEVER CAUSED  AND ON ANY THEORY  OF LIABILITY,  WHETHER IN\n\/\/ CONTRACT,  STRICT LIABILITY,  OR TORT  (INCLUDING NEGLIGENCE  OR OTHERWISE)\n\/\/ ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,  EVEN IF ADVISED OF THE\n\/\/ POSSIBILITY OF SUCH DAMAGE.\n\n#ifdef HAVE_CONFIG_H\n#include <config.h>\n#endif \/\/ HAVE_CONFIG_H\n#include <uuid\/uuid.h>\n#ifndef _GNU_SOURCE\n#define _GNU_SOURCE\n#endif\n#include <pthread.h>\n#include <time.h>\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n#undef _GNU_SOURCE\n\n#include \"safe_string\/safe_string.h\"\n#include \"types_int.h\"\n#include \"feature_token_list_int.h\"\n\n\/* global list of tokens we've seen *\/\nstatic struct _fpga_feature_token *ftoken_root;\n\/** Mutex to protect feature tokens *\/\npthread_mutex_t ftoken_lock = PTHREAD_MUTEX_INITIALIZER;\n\n\/**\n * @brief Add entry to linked list for feature tokens\n *\tWill allocate memory (which is freed by feature_token_cleanup())\n *\n * @param type\n * @param guid\n * @param handle\n *\n * @return\n *\/\nstruct _fpga_feature_token *feature_token_add(uint32_t type, uint32_t mmio_num, fpga_guid guid,\n\t\t\t\t\t      uint64_t offset, fpga_handle handle)\n{\n\tstruct _fpga_feature_token *tmp;\n\terrno_t e;\n\tint err = 0;\n\n\tif (pthread_mutex_lock(&ftoken_lock)) {\n\t\tFPGA_ERR(\"Failed to lock feature token mutex\");\n\t\treturn NULL;\n\t}\n\n\t\/* Prevent duplicate entries. *\/\n\tfor (tmp = ftoken_root; NULL != tmp; tmp = tmp->next) {\n\t\tif ((uuid_compare(guid, tmp->feature_guid)) == 0) {\n\t\t\terr = pthread_mutex_unlock(&ftoken_lock);\n\t\t\tif (err) {\n\t\t\t\tFPGA_ERR(\"pthread_mutex_unlock() failed: %S\",\n\t\t\t\t\t strerror(err));\n\t\t\t}\n\t\t\treturn tmp;\n\t\t}\n\t}\n\n\ttmp = (struct _fpga_feature_token *)malloc(\n\t\tsizeof(struct _fpga_feature_token));\n\tif (NULL == tmp) {\n\t\tFPGA_ERR(\"Failed to allocate memory for fhandle\");\n\t\treturn NULL;\n\t}\n\n\tuuid_clear(tmp->feature_guid);\n\ttmp->magic = FPGA_FEATURE_TOKEN_MAGIC;\n\ttmp->feature_type = type;\n\ttmp->mmio_num = mmio_num;\n\ttmp->csr_offset = offset;\n\ttmp->handle = handle;\n\ttmp->next = NULL;\n\n\te = memcpy_s(tmp->feature_guid, sizeof(fpga_guid), guid,\n\t\t     sizeof(fpga_guid));\n\n\tif (EOK != e) {\n\t\tFPGA_ERR(\"memcpy_s failed\");\n\t\tgoto out_free;\n\t}\n\n\ttmp->next = ftoken_root;\n\tftoken_root = tmp;\n\n\terr = pthread_mutex_unlock(&ftoken_lock);\n\tif (err) {\n\t\tFPGA_ERR(\"pthread_mutex_unlock() failed: %S\", strerror(err));\n\t\tgoto out_free;\n\t}\n\n\treturn tmp;\n\nout_free:\n\tfree(tmp);\n\terr = pthread_mutex_unlock(&ftoken_lock);\n\tif (err) {\n\t\tFPGA_ERR(\"pthread_mutex_unlock() failed: %S\", strerror(err));\n\t}\n\treturn NULL;\n}\n\n\/*\n * Clean up remaining entries in linked list\n * Will delete all remaining entries\n *\/\nvoid feature_token_cleanup(void)\n{\n\tint err = 0;\n\tstruct _fpga_feature_token *current = ftoken_root;\n\terr = pthread_mutex_lock(&ftoken_lock);\n\tif (err) {\n\t\tFPGA_ERR(\"pthread_mutex_lock() failed: %s\", strerror(err));\n\t\treturn;\n\t}\n\n\tif (!ftoken_root)\n\t\tgoto out_unlock;\n\n\twhile (current) {\n\t\tstruct _fpga_feature_token *tmp = current;\n\t\tcurrent = current->next;\n\n\t\t\/\/ invalidate magic (just in case)\n\t\ttmp->magic = FPGA_INVALID_MAGIC;\n\t\tfree(tmp);\n\t\ttmp = NULL;\n\t}\n\n\tftoken_root = NULL;\n\nout_unlock:\n\terr = pthread_mutex_unlock(&ftoken_lock);\n\tif (err) {\n\t\tFPGA_ERR(\"pthread_mutex_unlock() failed: %s\", strerror(err));\n\t}\n}\n","lang_cluster":"C","length":161,"code_uid":"33bdb52c72034f5f995d2e2d7eeddea7"}
{"diff_hunk":"@@ -176,11 +176,13 @@ int main() {\n \t\tfree(keymap);\n \t\tbreak;\n \t}\n+\tstate.xwayland = wlr_xwayland_create(compositor.display, state.wlr_compositor);\n \n \tcompositor.keyboard_key_cb = handle_keyboard_key;\n \n \twl_display_run(compositor.display);\n \n+\twlr_xwayland_destroy(state.xwayland);\n \tclose(state.keymap_fd);\n \twlr_seat_destroy(state.wl_seat);\n \twlr_data_device_manager_destroy(state.data_device_manager);","old_code":"#define _POSIX_C_SOURCE 199309L\n#include <string.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <inttypes.h>\n#include <unistd.h>\n#include <sys\/mman.h>\n#include <wayland-server.h>\n#include <wlr\/backend.h>\n#include <wlr\/backend\/session.h>\n#include <wlr\/render.h>\n#include <wlr\/render\/matrix.h>\n#include <wlr\/render\/gles2.h>\n#include <wlr\/types\/wlr_output.h>\n#include <wlr\/types\/wlr_surface.h>\n#include <wlr\/types\/wlr_wl_shell.h>\n#include <wlr\/types\/wlr_xdg_shell_v6.h>\n#include <wlr\/types\/wlr_seat.h>\n#include <wlr\/types\/wlr_data_device_manager.h>\n#include \"wlr\/types\/wlr_compositor.h\"\n#include <xkbcommon\/xkbcommon.h>\n#include <wlr\/util\/log.h>\n#include \"shared.h\"\n\n\/\/ TODO: move to common header?\nint os_create_anonymous_file(off_t size);\n\nstruct sample_state {\n\tstruct wlr_renderer *renderer;\n\tstruct wlr_compositor *wlr_compositor;\n\tstruct wlr_wl_shell *wl_shell;\n\tstruct wlr_seat *wl_seat;\n\tstruct wlr_xdg_shell_v6 *xdg_shell;\n\tstruct wlr_data_device_manager *data_device_manager;\n\tstruct wl_resource *focus;\n\tstruct wl_listener keyboard_bound;\n\tint keymap_fd;\n\tsize_t keymap_size;\n\tuint32_t serial;\n};\n\n\/*\n * Convert timespec to milliseconds\n *\/\nstatic inline int64_t timespec_to_msec(const struct timespec *a) {\n\treturn (int64_t)a->tv_sec * 1000 + a->tv_nsec \/ 1000000;\n}\n\nstatic void output_frame_handle_surface(struct sample_state *sample,\n\t\tstruct wlr_output *wlr_output, struct timespec *ts,\n\t\tstruct wl_resource *_res) {\n\tstruct wlr_surface *surface = wl_resource_get_user_data(_res);\n\tfloat matrix[16];\n\tfloat transform[16];\n\twlr_surface_flush_damage(surface);\n\tif (surface->texture->valid) {\n\t\twlr_matrix_translate(&transform, 200, 200, 0);\n\t\twlr_surface_get_matrix(surface, &matrix,\n\t\t\t&wlr_output->transform_matrix, &transform);\n\t\twlr_render_with_matrix(sample->renderer, surface->texture, &matrix);\n\n\t\tstruct wlr_frame_callback *cb, *cnext;\n\t\twl_list_for_each_safe(cb, cnext, &surface->frame_callback_list, link) {\n\t\t\twl_callback_send_done(cb->resource, timespec_to_msec(ts));\n\t\t\twl_resource_destroy(cb->resource);\n\t\t}\n\t}\n}\nstatic void handle_output_frame(struct output_state *output, struct timespec *ts) {\n\tstruct compositor_state *state = output->compositor;\n\tstruct sample_state *sample = state->data;\n\tstruct wlr_output *wlr_output = output->output;\n\n\twlr_output_make_current(wlr_output);\n\twlr_renderer_begin(sample->renderer, wlr_output);\n\n\tstruct wlr_wl_shell_surface *wl_shell_surface;\n\twl_list_for_each(wl_shell_surface, &sample->wl_shell->surfaces, link) {\n\t\toutput_frame_handle_surface(sample, wlr_output, ts, wl_shell_surface->surface);\n\t}\n\tstruct wlr_xdg_surface_v6 *xdg_surface;\n\twl_list_for_each(xdg_surface, &sample->xdg_shell->surfaces, link) {\n\t\toutput_frame_handle_surface(sample, wlr_output, ts, xdg_surface->surface);\n\t}\n\n\twlr_renderer_end(sample->renderer);\n\twlr_output_swap_buffers(wlr_output);\n}\n\nstatic void handle_keyboard_key(struct keyboard_state *keyboard, uint32_t keycode,\n\t \txkb_keysym_t sym, enum wlr_key_state key_state) {\n\tstruct compositor_state *state = keyboard->compositor;\n\tstruct sample_state *sample = state->data;\n\n\tstruct wl_resource *res = NULL;\n\tstruct wlr_seat_handle *seat_handle = NULL;\n\twl_list_for_each(res, &sample->wlr_compositor->surfaces, link) {\n\t\tbreak;\n\t}\n\n\tif (res) {\n\t\tseat_handle = wlr_seat_handle_for_client(sample->wl_seat,\n\t\t\twl_resource_get_client(res));\n\t}\n\n\tif (res != sample->focus && seat_handle && seat_handle->keyboard) {\n\t\tstruct wl_array keys;\n\t\twl_array_init(&keys);\n\t\twl_keyboard_send_enter(seat_handle->keyboard, ++sample->serial, res, &keys);\n\t\tsample->focus = res;\n\t}\n\n\tif (seat_handle && seat_handle->keyboard) {\n\t\tuint32_t depressed = xkb_state_serialize_mods(keyboard->xkb_state,\n\t\t\tXKB_STATE_MODS_DEPRESSED);\n\t\tuint32_t latched = xkb_state_serialize_mods(keyboard->xkb_state,\n\t\t\tXKB_STATE_MODS_LATCHED);\n\t\tuint32_t locked = xkb_state_serialize_mods(keyboard->xkb_state,\n\t\t\tXKB_STATE_MODS_LOCKED);\n\t\tuint32_t group = xkb_state_serialize_layout(keyboard->xkb_state,\n\t\t\tXKB_STATE_LAYOUT_EFFECTIVE);\n\t\twl_keyboard_send_modifiers(seat_handle->keyboard, ++sample->serial, depressed,\n\t\t\tlatched, locked, group);\n\t\twl_keyboard_send_key(seat_handle->keyboard, ++sample->serial, 0, keycode, key_state);\n\t}\n}\n\nstatic void handle_keyboard_bound(struct wl_listener *listener, void *data) {\n\tstruct wlr_seat_handle *handle = data;\n\tstruct sample_state *state = wl_container_of(listener, state, keyboard_bound);\n\n\twl_keyboard_send_keymap(handle->keyboard, WL_KEYBOARD_KEYMAP_FORMAT_XKB_V1,\n\t\tstate->keymap_fd, state->keymap_size);\n\n\tif (wl_resource_get_version(handle->keyboard) >= 2) {\n\t\twl_keyboard_send_repeat_info(handle->keyboard, 25, 600);\n\t}\n}\n\nint main() {\n\tstruct sample_state state = { 0 };\n\tstruct compositor_state compositor = { 0,\n\t\t.data = &state,\n\t\t.output_frame_cb = handle_output_frame,\n\t};\n\tcompositor_init(&compositor);\n\n\tstate.renderer = wlr_gles2_renderer_create(compositor.backend);\n\tif (!state.renderer) {\n\t\twlr_log(L_ERROR, \"Could not start compositor, OOM\");\n\t\texit(EXIT_FAILURE);\n\t}\n\twl_display_init_shm(compositor.display);\n\tstate.wlr_compositor = wlr_compositor_create(compositor.display, state.renderer);\n\tstate.wl_shell = wlr_wl_shell_create(compositor.display);\n\tstate.xdg_shell = wlr_xdg_shell_v6_create(compositor.display);\n\tstate.data_device_manager = wlr_data_device_manager_create(compositor.display);\n\n\tstate.wl_seat = wlr_seat_create(compositor.display, \"seat0\");\n\tstate.keyboard_bound.notify = handle_keyboard_bound;\n\twl_signal_add(&state.wl_seat->events.keyboard_bound, &state.keyboard_bound);\n\twlr_seat_set_capabilities(state.wl_seat, WL_SEAT_CAPABILITY_KEYBOARD\n\t\t| WL_SEAT_CAPABILITY_POINTER | WL_SEAT_CAPABILITY_TOUCH);\n\n\tstruct keyboard_state *kbstate;\n\twl_list_for_each(kbstate, &compositor.keyboards, link) {\n\t\tchar *keymap = xkb_keymap_get_as_string(kbstate->keymap,\n\t\t\tXKB_KEYMAP_FORMAT_TEXT_V1);\n\t\tstate.keymap_size = strlen(keymap);\n\t\tstate.keymap_fd = os_create_anonymous_file(state.keymap_size);\n\t\tvoid *ptr = mmap(NULL, state.keymap_size,\n\t\t\t\t     PROT_READ | PROT_WRITE,\n\t\t\t\t     MAP_SHARED, state.keymap_fd, 0);\n\t\tstrcpy(ptr, keymap);\n\t\tfree(keymap);\n\t\tbreak;\n\t}\n\n\tcompositor.keyboard_key_cb = handle_keyboard_key;\n\n\twl_display_run(compositor.display);\n\n\tclose(state.keymap_fd);\n\twlr_seat_destroy(state.wl_seat);\n\twlr_data_device_manager_destroy(state.data_device_manager);\n\twlr_xdg_shell_v6_destroy(state.xdg_shell);\n\twlr_wl_shell_destroy(state.wl_shell);\n\twlr_compositor_destroy(state.wlr_compositor);\n\twlr_renderer_destroy(state.renderer);\n\tcompositor_fini(&compositor);\n}\n","lang_cluster":"C","length":192,"code_uid":"c13d7aaeae5948c192a53bb6b7a727d1"}
{"diff_hunk":"@@ -51,12 +51,12 @@ static void send_chunk(h2o_ostream_t *_self, h2o_req_t *req, h2o_iovec_t *inbufs\n         outbufcnt += inbufcnt;\n         if (state != H2O_SEND_STATE_ERROR) {\n             outbufs[outbufcnt].base = \"\\r\\n0\\r\\n\\r\\n\";\n-            outbufs[outbufcnt].len = state == H2O_SEND_STATE_FINAL ? (req->send_server_timing ? 5 : 7) : 2;\n+            outbufs[outbufcnt].len = state == H2O_SEND_STATE_FINAL ? (req->send_server_timing_trailer ? 5 : 7) : 2;\n             outbufcnt++;\n         }\n     } else if (state == H2O_SEND_STATE_FINAL) {\n         outbufs[outbufcnt].base = \"0\\r\\n\\r\\n\";\n-        outbufs[outbufcnt].len = req->send_server_timing ? 3 : 5;\n+        outbufs[outbufcnt].len = req->send_server_timing_trailer ? 3 : 5;\n         outbufcnt++;\n     }\n ","old_code":"\/*\n * Copyright (c) 2014 DeNA Co., Ltd.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n *\/\n#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include \"h2o.h\"\n\ntypedef struct st_chunked_encoder_t {\n    h2o_ostream_t super;\n    char buf[64];\n} chunked_encoder_t;\n\nstatic void send_chunk(h2o_ostream_t *_self, h2o_req_t *req, h2o_iovec_t *inbufs, size_t inbufcnt, h2o_send_state_t state)\n{\n    chunked_encoder_t *self = (void *)_self;\n    h2o_iovec_t *outbufs = alloca(sizeof(h2o_iovec_t) * (inbufcnt + 2));\n    size_t chunk_size, outbufcnt = 0, i;\n\n    \/* calc chunk size *\/\n    chunk_size = 0;\n    for (i = 0; i != inbufcnt; ++i)\n        chunk_size += inbufs[i].len;\n    req->bytes_sent += chunk_size;\n\n    \/* create chunk header and output data *\/\n    if (chunk_size != 0) {\n        outbufs[outbufcnt].base = self->buf;\n        outbufs[outbufcnt].len = sprintf(self->buf, \"%zx\\r\\n\", chunk_size);\n        assert(outbufs[outbufcnt].len < sizeof(self->buf));\n        outbufcnt++;\n        memcpy(outbufs + outbufcnt, inbufs, sizeof(h2o_iovec_t) * inbufcnt);\n        outbufcnt += inbufcnt;\n        if (state != H2O_SEND_STATE_ERROR) {\n            outbufs[outbufcnt].base = \"\\r\\n0\\r\\n\\r\\n\";\n            outbufs[outbufcnt].len = state == H2O_SEND_STATE_FINAL ? (req->send_server_timing ? 5 : 7) : 2;\n            outbufcnt++;\n        }\n    } else if (state == H2O_SEND_STATE_FINAL) {\n        outbufs[outbufcnt].base = \"0\\r\\n\\r\\n\";\n        outbufs[outbufcnt].len = req->send_server_timing ? 3 : 5;\n        outbufcnt++;\n    }\n\n    \/* if state is error, send a broken chunk to pass the error down to the browser *\/\n    if (state == H2O_SEND_STATE_ERROR) {\n        outbufs[outbufcnt].base = \"\\r\\n1\\r\\n\";\n        outbufs[outbufcnt].len = 5;\n        outbufcnt++;\n    }\n\n    h2o_ostream_send_next(&self->super, req, outbufs, outbufcnt, state);\n}\n\nstatic void on_setup_ostream(h2o_filter_t *self, h2o_req_t *req, h2o_ostream_t **slot)\n{\n    chunked_encoder_t *encoder;\n\n    \/* TODO: make chunked filter a submodule of lib\/http1.c so that we could eliminate this flag, protocol version checks, etc. *\/\n    if (req->is_subrequest)\n        goto Next;\n\n    \/* do nothing if not HTTP\/1.1 or content-length is known *\/\n    if (req->res.content_length != SIZE_MAX || req->version != 0x101)\n        goto Next;\n    \/* RFC 2616 4.4 states that the following status codes (and response to a HEAD method) should not include message body *\/\n    if ((100 <= req->res.status && req->res.status <= 199) || req->res.status == 204 || req->res.status == 304)\n        goto Next;\n    else if (h2o_memis(req->input.method.base, req->input.method.len, H2O_STRLIT(\"HEAD\")))\n        goto Next;\n    \/* we cannot handle certain responses (like 101 switching protocols) *\/\n    if (req->res.status != 200) {\n        req->http1_is_persistent = 0;\n        goto Next;\n    }\n    \/* skip if content-encoding header is being set *\/\n    if (h2o_find_header(&req->res.headers, H2O_TOKEN_TRANSFER_ENCODING, -1) != -1)\n        goto Next;\n\n    \/* set content-encoding header *\/\n    h2o_add_header(&req->pool, &req->res.headers, H2O_TOKEN_TRANSFER_ENCODING, NULL, H2O_STRLIT(\"chunked\"));\n\n    \/* set the flag that tells finalostream that req->bytes_sent is already counted *\/\n    req->bytes_counted_by_ostream = 1;\n\n    \/* setup filter *\/\n    encoder = (void *)h2o_add_ostream(req, H2O_ALIGNOF(*encoder), sizeof(*encoder), slot);\n    encoder->super.do_send = send_chunk;\n    slot = &encoder->super.next;\n\nNext:\n    h2o_setup_next_ostream(req, slot);\n}\n\nvoid h2o_chunked_register(h2o_pathconf_t *pathconf)\n{\n    h2o_filter_t *self = h2o_create_filter(pathconf, sizeof(*self));\n    self->on_setup_ostream = on_setup_ostream;\n}\n","lang_cluster":"C","length":117,"code_uid":"0f2410eb73f04f9aa34e40fa2a0f5bcc"}
{"diff_hunk":"@@ -139,22 +139,30 @@ class TestSuppress(unittest.TestCase):\n                                                    'status': rw_status}\n \n         run_results = get_all_run_results(self._cc_client, runid)\n+        logging.debug(\"Run results:\")\n+        [logging.debug(x) for x in run_results]\n         self.assertIsNotNone(run_results)\n         self.assertNotEqual(len(run_results), 0)\n \n         for bug_hash in hash_to_suppress_msgs:\n-            expected = hash_to_suppress_msgs[bug_hash]\n-            report = [x for x in run_results if x.bugHash == bug_hash][0]\n+            expected_data = hash_to_suppress_msgs[bug_hash]\n+            report_data_of_bug = [\n+                report_data for report_data in run_results\n+                if report_data.bugHash == bug_hash]\n+            self.assertEqual(len(report_data_of_bug), 1)\n+            report_data = report_data_of_bug[0]\n \n             # Check the stored suppress comment\n-            self.assertEqual(report.reviewData.comment, expected['message'])\n-            self.assertEqual(report.reviewData.status, expected['status'])\n+            self.assertEqual(report_data.reviewData.comment,\n+                             expected_data['message'])\n+            self.assertEqual(report_data.reviewData.status,\n+                             expected_data['status'])\n \n             # Change review status to confirmed bug.\n             review_comment = \"This is really a bug\"\n             status = ReviewStatus.CONFIRMED\n             success = self._cc_client.changeReviewStatus(\n-                report.reportId, status, review_comment)\n+                report_data.reportId, status, review_comment)\n \n             self.assertTrue(success)\n             logging.debug(\"Bug review status changed successfully\")","old_code":"# -------------------------------------------------------------------------\n#\n#  Part of the CodeChecker project, under the Apache License v2.0 with\n#  LLVM Exceptions. See LICENSE for license information.\n#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n#\n# -------------------------------------------------------------------------\n\"\"\"\nTest source-code level suppression data writing to suppress file.\n\"\"\"\n\n\nimport logging\nimport os\nimport shlex\nimport sys\nimport subprocess\nfrom subprocess import CalledProcessError\nimport unittest\n\nfrom codechecker_api.codeCheckerDBAccess_v6.ttypes import ReviewStatus\n\nfrom libtest import env\nfrom libtest import codechecker\nfrom libtest.thrift_client_to_db import get_all_run_results\n\n\ndef call_cmd(command, cwd, env):\n    try:\n        print(' '.join(command))\n        proc = subprocess.Popen(\n            shlex.split(' '.join(command)),\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env=env, encoding=\"utf-8\", errors=\"ignore\")\n        out, err = proc.communicate()\n        print(out)\n        print(err)\n        return proc.returncode\n    except CalledProcessError as cerr:\n        print(\"Failed to call:\\n\" + ' '.join(cerr.cmd))\n        return cerr.returncode\n\n\nclass TestSuppress(unittest.TestCase):\n    \"\"\"\n    Test source-code level suppression data writing to suppress file.\n    \"\"\"\n\n    def setUp(self):\n        self._test_workspace = os.environ['TEST_WORKSPACE']\n\n        self._testproject_data = env.setup_test_proj_cfg(self._test_workspace)\n        self.assertIsNotNone(self._testproject_data)\n\n        self._test_project_path = self._testproject_data['project_path']\n\n        self._cc_client = env.setup_viewer_client(self._test_workspace)\n        self.assertIsNotNone(self._cc_client)\n\n        # Get the run names which belong to this test\n        run_names = env.get_run_names(self._test_workspace)\n\n        runs = self._cc_client.getRunData(None, None, 0, None)\n\n        test_runs = [run for run in runs if run.name in run_names]\n\n        self.assertEqual(len(test_runs), 1,\n                         'There should be only one run for this test.')\n        self._runid = test_runs[0].runId\n        self._run_name = test_runs[0].name\n\n    def test_suppress_import(self):\n        \"\"\"\n        Test the suppress file importing.\n        \"\"\"\n\n        generated_file = os.path.join(self._test_workspace,\n                                      \"generated.suppress\")\n\n        extract_cmd = ['CodeChecker', 'parse',\n                       os.path.join(self._test_workspace, \"reports\"),\n                       \"--suppress\", generated_file,\n                       \"--export-source-suppress\"\n                       ]\n\n        ret = call_cmd(extract_cmd,\n                       self._test_project_path,\n                       env.test_env(self._test_workspace))\n        self.assertEqual(ret, 2, \"Failed to generate suppress file.\")\n\n        codechecker_cfg = env.import_test_cfg(\n            self._test_workspace)['codechecker_cfg']\n\n        product_url = env.parts_to_url(codechecker_cfg)\n        import_cmd = ['CodeChecker', 'cmd', 'suppress', '-i', generated_file,\n                      '--url', product_url, self._run_name]\n\n        print(import_cmd)\n        ret = call_cmd(import_cmd,\n                       self._test_project_path,\n                       env.test_env(self._test_workspace))\n        self.assertEqual(ret, 0, \"Failed to import suppress file.\")\n\n    def test_suppress_comment_in_db(self):\n        \"\"\"\n        Exported source suppress comment stored as a review status in the db.\n        \"\"\"\n        runid = self._runid\n        logging.debug(\"Get all run results from the db for runid: \" +\n                      str(runid))\n\n        hash_to_suppress_msgs = {}\n        with open(os.path.join(self._test_project_path, \"suppress.expected\"),\n                  'r', encoding=\"utf-8\", errors=\"ignore\") as expected:\n            for line in expected:\n                src_code_info = line.strip().split('||')\n\n                status = None\n                if len(src_code_info) == 4:\n                    # Newest source code comment format where status is given.\n                    bug_hash, _, msg, status = src_code_info\n                elif len(src_code_info) == 3:\n                    # Old format where review status is not given.\n                    bug_hash, _, msg = src_code_info\n                else:\n                    # Oldest source code comment format where status and file\n                    # name are not given.\n                    bug_hash, msg = src_code_info\n\n                rw_status = ReviewStatus.FALSE_POSITIVE\n                if status == 'confirmed':\n                    rw_status = ReviewStatus.CONFIRMED\n                elif status == 'intentional':\n                    rw_status = ReviewStatus.INTENTIONAL\n\n                hash_to_suppress_msgs[bug_hash] = {'message': msg,\n                                                   'status': rw_status}\n\n        run_results = get_all_run_results(self._cc_client, runid)\n        self.assertIsNotNone(run_results)\n        self.assertNotEqual(len(run_results), 0)\n\n        for bug_hash in hash_to_suppress_msgs:\n            expected = hash_to_suppress_msgs[bug_hash]\n            report = [x for x in run_results if x.bugHash == bug_hash][0]\n\n            # Check the stored suppress comment\n            self.assertEqual(report.reviewData.comment, expected['message'])\n            self.assertEqual(report.reviewData.status, expected['status'])\n\n            # Change review status to confirmed bug.\n            review_comment = \"This is really a bug\"\n            status = ReviewStatus.CONFIRMED\n            success = self._cc_client.changeReviewStatus(\n                report.reportId, status, review_comment)\n\n            self.assertTrue(success)\n            logging.debug(\"Bug review status changed successfully\")\n\n        # Get the results to compare.\n        updated_results = get_all_run_results(self._cc_client, self._runid)\n        self.assertIsNotNone(updated_results)\n        self.assertNotEqual(len(updated_results), 0)\n\n        for bug_hash in hash_to_suppress_msgs:\n            report = [x for x in updated_results if x.bugHash == bug_hash][0]\n\n            # Check the stored suppress comment\n            self.assertEqual(report.reviewData.comment, \"This is really a bug\")\n            self.assertEqual(report.reviewData.status, ReviewStatus.CONFIRMED)\n\n        # Check the same project again.\n        codechecker_cfg = env.import_test_cfg(\n            self._test_workspace)['codechecker_cfg']\n\n        initial_test_project_name = self._run_name\n\n        ret = codechecker.check_and_store(codechecker_cfg,\n                                          initial_test_project_name,\n                                          self._test_project_path)\n        if ret:\n            sys.exit(1)\n\n        # Get the results to compare.\n        updated_results = get_all_run_results(self._cc_client, self._runid)\n        self.assertIsNotNone(updated_results)\n        self.assertNotEqual(len(updated_results), 0)\n\n        for bug_hash in hash_to_suppress_msgs:\n            expected = hash_to_suppress_msgs[bug_hash]\n            report = [x for x in updated_results if x.bugHash == bug_hash][0]\n\n            # Check that source code comments in the database are changed back\n            # after storage.\n            self.assertEqual(report.reviewData.comment, expected['message'])\n            self.assertEqual(report.reviewData.status, expected['status'])\n","lang_cluster":"C","length":198,"code_uid":"810077cd36904039be73196040d3dc22"}
{"diff_hunk":"@@ -43,6 +43,9 @@ const (\n \n \tIPSetIDAllHostIPs = \"all-hosts\"\n \n+\tChainFipDnat = ChainNamePrefix + \"-fip-dnat\"\n+\tChainFipSnat = ChainNamePrefix + \"-fip-snat\"\n+\n \tPolicyInboundPfx  = ChainNamePrefix + \"pi-\"\n \tPolicyOutboundPfx = ChainNamePrefix + \"po-\"\n ","old_code":"\/\/ Copyright (c) 2016-2017 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage rules\n\nimport (\n\tlog \"github.com\/Sirupsen\/logrus\"\n\t\"github.com\/projectcalico\/felix\/go\/felix\/ipsets\"\n\t\"github.com\/projectcalico\/felix\/go\/felix\/iptables\"\n\t\"github.com\/projectcalico\/felix\/go\/felix\/proto\"\n\t\"net\"\n\t\"strings\"\n)\n\nconst (\n\tChainNamePrefix = \"cali\"\n\tIPSetNamePrefix = \"cali\"\n\n\tChainFilterInput   = ChainNamePrefix + \"-INPUT\"\n\tChainFilterForward = ChainNamePrefix + \"-FORWARD\"\n\tChainFilterOutput  = ChainNamePrefix + \"-OUTPUT\"\n\n\tChainFailsafeIn  = ChainNamePrefix + \"-failsafe-in\"\n\tChainFailsafeOut = ChainNamePrefix + \"-failsafe-out\"\n\n\tChainNATPrerouting  = ChainNamePrefix + \"-PREROUTING\"\n\tChainNATPostrouting = ChainNamePrefix + \"-POSTROUTING\"\n\tChainNATOutgoing    = ChainNamePrefix + \"-nat-outgoing\"\n\n\tIPSetIDNATOutgoingAllPools  = \"all-ipam-pools\"\n\tIPSetIDNATOutgoingMasqPools = \"masq-ipam-pools\"\n\n\tIPSetIDAllHostIPs = \"all-hosts\"\n\n\tPolicyInboundPfx  = ChainNamePrefix + \"pi-\"\n\tPolicyOutboundPfx = ChainNamePrefix + \"po-\"\n\n\tChainWorkloadToHost       = ChainNamePrefix + \"-wl-to-host\"\n\tChainFromWorkloadDispatch = ChainNamePrefix + \"-from-wl-dispatch\"\n\tChainToWorkloadDispatch   = ChainNamePrefix + \"-to-wl-dispatch\"\n\n\tChainDispatchToHostEndpoint   = ChainNamePrefix + \"-to-host-endpoint\"\n\tChainDispatchFromHostEndpoint = ChainNamePrefix + \"-from-host-endpoint\"\n\n\tWorkloadToEndpointPfx   = ChainNamePrefix + \"tw-\"\n\tWorkloadFromEndpointPfx = ChainNamePrefix + \"fw-\"\n\n\tHostToEndpointPfx   = ChainNamePrefix + \"th-\"\n\tHostFromEndpointPfx = ChainNamePrefix + \"fh-\"\n\n\tRuleHashPrefix = \"cali:\"\n\n\t\/\/ HistoricNATRuleInsertRegex is a regex pattern to match to match\n\t\/\/ special-case rules inserted by old versions of felix.  Specifically,\n\t\/\/ Python felix used to insert a masquerade rule directly into the\n\t\/\/ POSTROUTING chain.\n\t\/\/\n\t\/\/ Note: this regex depends on the output format of iptables-save so,\n\t\/\/ where possible, it's best to match only on part of the rule that\n\t\/\/ we're sure can't change (such as the ipset name in the masquerade\n\t\/\/ rule).\n\tHistoricInsertedNATRuleRegex = `-A POSTROUTING .* felix-masq-ipam-pools .*|` +\n\t\t`-A POSTROUTING -o tunl0 -m addrtype ! --src-type LOCAL --limit-iface-out -m addrtype --src-type LOCAL -j MASQUERADE`\n)\n\nvar (\n\t\/\/ AllHistoricChainNamePrefixes lists all the prefixes that we've used for chains.  Keeping\n\t\/\/ track of the old names lets us clean them up.\n\tAllHistoricChainNamePrefixes = []string{\"felix-\", \"cali\"}\n\t\/\/ AllHistoricIPSetNamePrefixes, similarly contains all the prefixes we've ever used for IP\n\t\/\/ sets.\n\tAllHistoricIPSetNamePrefixes = []string{\"felix-\", \"cali\"}\n\t\/\/ LegacyV4IPSetNames contains some extra IP set names that were used in older versions of\n\t\/\/ Felix and don't fit our versioned pattern.\n\tLegacyV4IPSetNames = []string{\"felix-masq-ipam-pools\", \"felix-all-ipam-pools\"}\n)\n\ntype RuleRenderer interface {\n\tStaticFilterTableChains(ipVersion uint8) []*iptables.Chain\n\tStaticNATTableChains(ipVersion uint8) []*iptables.Chain\n\n\tWorkloadDispatchChains(map[proto.WorkloadEndpointID]*proto.WorkloadEndpoint) []*iptables.Chain\n\tWorkloadEndpointToIptablesChains(epID *proto.WorkloadEndpointID, endpoint *proto.WorkloadEndpoint) []*iptables.Chain\n\n\tHostDispatchChains(map[string]proto.HostEndpointID) []*iptables.Chain\n\tHostEndpointToIptablesChains(ifaceName string, endpoint *proto.HostEndpoint) []*iptables.Chain\n\n\tPolicyToIptablesChains(policyID *proto.PolicyID, policy *proto.Policy, ipVersion uint8) []*iptables.Chain\n\tProfileToIptablesChains(profileID *proto.ProfileID, policy *proto.Profile, ipVersion uint8) []*iptables.Chain\n\tProtoRuleToIptablesRules(pRule *proto.Rule, ipVersion uint8) []iptables.Rule\n\n\tNATOutgoingChain(active bool, ipVersion uint8) *iptables.Chain\n}\n\ntype DefaultRuleRenderer struct {\n\tConfig\n\n\tdropActions        []iptables.Action\n\tinputAcceptActions []iptables.Action\n}\n\nfunc (r *DefaultRuleRenderer) ipSetConfig(ipVersion uint8) *ipsets.IPVersionConfig {\n\tif ipVersion == 4 {\n\t\treturn r.IPSetConfigV4\n\t} else if ipVersion == 6 {\n\t\treturn r.IPSetConfigV6\n\t} else {\n\t\tlog.WithField(\"version\", ipVersion).Panic(\"Unknown IP version\")\n\t\treturn nil\n\t}\n}\n\ntype Config struct {\n\tIPSetConfigV4 *ipsets.IPVersionConfig\n\tIPSetConfigV6 *ipsets.IPVersionConfig\n\n\tWorkloadIfacePrefixes []string\n\n\tIptablesMarkAccept   uint32\n\tIptablesMarkNextTier uint32\n\n\tOpenStackMetadataIP          net.IP\n\tOpenStackMetadataPort        uint16\n\tOpenStackSpecialCasesEnabled bool\n\n\tIPIPEnabled       bool\n\tIPIPTunnelAddress net.IP\n\n\tDropLogPrefix        string\n\tActionOnDrop         string\n\tEndpointToHostAction string\n\n\tFailsafeInboundHostPorts  []uint16\n\tFailsafeOutboundHostPorts []uint16\n}\n\nfunc NewRenderer(config Config) RuleRenderer {\n\tlog.WithField(\"config\", config).Info(\"Creating rule renderer.\")\n\t\/\/ Convert configured actions to rule slices.  First, what should we actually do when we'd\n\t\/\/ normally drop a packet?  For sandbox mode, we support allowing the packet instead, or\n\t\/\/ logging it.\n\tvar dropActions []iptables.Action\n\tif strings.HasPrefix(config.ActionOnDrop, \"LOG-\") {\n\t\tlog.Warn(\"Action on drop includes LOG.  All dropped packets will be logged.\")\n\t\tlogPrefix := \"calico-drop\"\n\t\tif config.DropLogPrefix != \"\" {\n\t\t\tlogPrefix = config.DropLogPrefix\n\t\t}\n\t\tdropActions = append(dropActions, iptables.LogAction{Prefix: logPrefix})\n\t}\n\tif strings.HasSuffix(config.ActionOnDrop, \"ACCEPT\") {\n\t\tlog.Warn(\"Action on drop set to ACCEPT.  Calico security is disabled!\")\n\t\tdropActions = append(dropActions, iptables.AcceptAction{})\n\t} else {\n\t\tdropActions = append(dropActions, iptables.DropAction{})\n\t}\n\n\t\/\/ Second, what should we do with packets that come from workloads to the host itself.\n\tvar inputAcceptActions []iptables.Action\n\tswitch config.EndpointToHostAction {\n\tcase \"DROP\":\n\t\tlog.Info(\"Workload to host packets will be dropped.\")\n\t\tinputAcceptActions = dropActions\n\tcase \"ACCEPT\":\n\t\tlog.Info(\"Workload to host packets will be accepted.\")\n\t\tinputAcceptActions = []iptables.Action{iptables.AcceptAction{}}\n\tdefault:\n\t\tlog.Info(\"Workload to host packets will be returned to INPUT chain.\")\n\t\tinputAcceptActions = []iptables.Action{iptables.ReturnAction{}}\n\t}\n\n\treturn &DefaultRuleRenderer{\n\t\tConfig:             config,\n\t\tdropActions:        dropActions,\n\t\tinputAcceptActions: inputAcceptActions,\n\t}\n}\n","lang_cluster":"C","length":188,"code_uid":"cf23dd88e8334da6b19eceffac5adacb"}
{"diff_hunk":"@@ -176,7 +176,7 @@ class PlistToDB(ResultHandler):\n \n             assert self.analyzer_returncode == 0\n \n-            plist_file = self.get_analyzer_result_file()\n+            plist_file = self.analyzer_result_file\n \n             try:\n                 files, bugs = plist_parser.parse_plist(plist_file)","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\nimport ntpath\nimport os\nimport zlib\nfrom abc import ABCMeta\n\nimport shared\n\nfrom codechecker_lib import client\nfrom codechecker_lib import plist_parser\nfrom codechecker_lib import suppress_handler\nfrom codechecker_lib.logger import LoggerFactory\nfrom codechecker_lib.analyzers.result_handler_base import ResultHandler\n\nLOG = LoggerFactory.get_new_logger('PLIST TO DB')\n\n\nclass PlistToDB(ResultHandler):\n    \"\"\"\n    Result handler for processing a plist file with the\n    analysis results and stores them to the database.\n    \"\"\"\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self, buildaction, workspace, run_id):\n        super(PlistToDB, self).__init__(buildaction, workspace)\n        self.__run_id = run_id\n\n    def __store_bugs(self, files, bugs, connection, analisys_id):\n        file_ids = {}\n        # Send content of file to the server if needed.\n        for file_name in files:\n            file_descriptor = connection.need_file_content(self.__run_id,\n                                                           file_name)\n            file_ids[file_name] = file_descriptor.fileId\n\n            # Sometimes the file doesn't exist, e.g. when the input of the\n            # analysis is pure plist files.\n            if not os.path.isfile(file_name):\n                LOG.debug(file_name + ' not found, and will not be stored.')\n                continue\n\n            if file_descriptor.needed:\n                with open(file_name, 'r') as source_file:\n                    file_content = source_file.read()\n                compressed_file = zlib.compress(file_content,\n                                                zlib.Z_BEST_COMPRESSION)\n                # TODO: we may not use the file content in the end\n                # depending on skippaths.\n                LOG.debug('storing file content to the database')\n                connection.add_file_content(file_descriptor.fileId,\n                                            compressed_file)\n\n        # Skipping bugs in header files handled here.\n        report_ids = []\n        for bug in bugs:\n            events = bug.events()\n\n            # Skip list handler can be None if no config file is set.\n            if self.skiplist_handler:\n                if events and self.skiplist_handler.should_skip(\n                        events[-1].start_pos.file_path):\n                    # Issue #20: this bug is in a file which should be skipped\n                    LOG.debug(bug.hash_value + ' is skipped (in ' +\n                              events[-1].start_pos.file_path + \")\")\n                    continue\n\n            # Create remaining data for bugs and send them to the server.\n            bug_paths = []\n            for path in bug.paths():\n                bug_paths.append(\n                    shared.ttypes.BugPathPos(path.start_pos.line,\n                                             path.start_pos.col,\n                                             path.end_pos.line,\n                                             path.end_pos.col,\n                                             file_ids[\n                                                 path.start_pos.file_path]))\n\n            bug_events = []\n            for event in bug.events():\n                bug_events.append(shared.ttypes.BugPathEvent(\n                    event.start_pos.line,\n                    event.start_pos.col,\n                    event.end_pos.line,\n                    event.end_pos.col,\n                    event.msg,\n                    file_ids[event.start_pos.file_path]))\n\n            bug_hash = bug.hash_value\n\n            severity_name = self.severity_map.get(bug.checker_name,\n                                                  'UNSPECIFIED')\n            severity = shared.ttypes.Severity._NAMES_TO_VALUES[severity_name]\n\n            sp_handler = suppress_handler.SourceSuppressHandler(bug)\n\n            # Check for suppress comment.\n            supp = sp_handler.get_suppressed()\n            if supp:\n                connection.add_suppress_bug(self.__run_id, [supp])\n\n            LOG.debug('Storing check results to the database.')\n\n            report_id = connection.add_report(analisys_id,\n                                              file_ids[bug.file_path],\n                                              bug_hash,\n                                              bug.msg,\n                                              bug_paths,\n                                              bug_events,\n                                              bug.checker_name,\n                                              bug.category,\n                                              bug.type,\n                                              severity,\n                                              supp is not None)\n\n            report_ids.append(report_id)\n\n    def handle_plist(self, plist):\n        with client.get_connection() as connection:\n            # TODO: When the analyzer name can be read from PList, then it\n            # should be passed too.\n            # TODO: File name should be read from the PList and passed.\n            analysis_id = connection. \\\n                add_build_action(self.__run_id,\n                                 plist,\n                                 'Build action from plist',\n                                 '',\n                                 '')\n\n            try:\n                files, bugs = plist_parser.parse_plist(plist)\n            except Exception as ex:\n                msg = 'Parsing the generated result file failed.'\n                LOG.error(msg + ' ' + plist)\n                LOG.error(str(ex))\n                connection.finish_build_action(analysis_id, msg)\n                return 1\n\n            self.__store_bugs(files, bugs, connection, analysis_id)\n\n            connection.finish_build_action(analysis_id, self.analyzer_stderr)\n\n    def handle_results(self):\n        \"\"\"\n        Send the plist content to the database.\n        Server API calls should be used in one connection.\n         - addBuildAction\n         - addReport\n         - needFileContent\n         - addFileContent\n         - finishBuildAction\n        \"\"\"\n\n        with client.get_connection() as connection:\n\n            LOG.debug('Storing original build and analyzer command '\n                      'to the database.')\n\n            _, source_file_name = ntpath.split(self.analyzed_source_file)\n\n            analysis_id = \\\n                connection.add_build_action(self.__run_id,\n                                            self.buildaction.original_command,\n                                            ' '.join(\n                                                self.analyzer_cmd),\n                                            self.buildaction.analyzer_type,\n                                            source_file_name)\n\n            # Store buildaction and analyzer command to the database.\n\n            assert self.analyzer_returncode == 0\n\n            plist_file = self.get_analyzer_result_file()\n\n            try:\n                files, bugs = plist_parser.parse_plist(plist_file)\n            except Exception as ex:\n                LOG.debug(str(ex))\n                msg = 'Parsing the generated result file failed.'\n                LOG.error(msg + ' ' + plist_file)\n                connection.finish_build_action(analysis_id, msg)\n                return 1\n\n            self.__store_bugs(files, bugs, connection, analysis_id)\n\n            connection.finish_build_action(analysis_id, self.analyzer_stderr)\n","lang_cluster":"C","length":192,"code_uid":"ec5595a5d6b94310b0c47556af239cf4"}
{"diff_hunk":"@@ -54,25 +54,30 @@ class PlistToHtmlTest(unittest.TestCase):\n                         plist_file.truncate()\n                         plist_file.write(new_content)\n \n-    def __test_html_builder(self, proj: str):\n+    def __test_html_builder(self, proj: str) -> str:\n         \"\"\"\n         Test building html file from the given proj's plist file.\n         \"\"\"\n-        proj_dir = os.path.join(self.test_workspace, 'test_files', proj)\n-        plist_file = os.path.join(proj_dir, f\"{proj}.plist\")\n-\n-        reports = report_file.get_reports(plist_file)\n+        html_builder = report_to_html.HtmlBuilder(self.layout_dir)\n \n+        proj_dir = os.path.join(self.test_workspace, 'test_files', proj)\n         output_dir = os.path.join(proj_dir, 'html')\n         if not os.path.exists(output_dir):\n             os.mkdir(output_dir)\n \n-        output_path = os.path.join(output_dir, f\"{proj}.plist.html\")\n+        processed_path_hashes = set()\n+        for file_path in glob.glob(os.path.join(proj_dir, f\"*.plist\")):\n+            file_name = os.path.basename(file_path)\n+            output_path = os.path.join(output_dir, f\"{file_name}.html\")\n \n-        html_builder = report_to_html.HtmlBuilder(self.layout_dir)\n-        report_to_html.convert(plist_file, reports, output_dir, html_builder)\n+            reports = report_file.get_reports(file_path)\n+            reports = reports_helper.skip(\n+                reports, processed_path_hashes)\n+\n+            report_to_html.convert(\n+                file_path, reports, output_dir, html_builder)\n \n-        self.assertTrue(os.path.exists(output_path))\n+            self.assertTrue(os.path.exists(output_path))\n \n         html_builder.create_index_html(output_dir)\n         html_builder.create_statistics_html(output_dir)","old_code":"# -------------------------------------------------------------------------\n#\n#  Part of the CodeChecker project, under the Apache License v2.0 with\n#  LLVM Exceptions. See LICENSE for license information.\n#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n#\n# -------------------------------------------------------------------------\n\n\nimport os\nimport shutil\nimport unittest\n\nfrom typing import ClassVar\n\nfrom libtest import env\n\nfrom codechecker_report_converter.report.output.html import \\\n    html as report_to_html\nfrom codechecker_report_converter.report import report_file\n\n\ndef get_project_path(test_project) -> str:\n    \"\"\" Return project path for the given project. \"\"\"\n    return os.path.join(env.test_proj_root(), test_project)\n\n\nclass PlistToHtmlTest(unittest.TestCase):\n    test_workspace: ClassVar[str]\n    layout_dir: ClassVar[str]\n\n    @classmethod\n    def setUpClass(self):\n        \"\"\" Initialize test files. \"\"\"\n        self.test_workspace = os.environ['TEST_WORKSPACE']\n        self.layout_dir = os.environ['LAYOUT_DIR']\n\n        test_file_dir_path = os.path.join(self.test_workspace, \"test_files\")\n\n        test_projects = ['notes', 'macros', 'simple']\n        for test_project in test_projects:\n            test_project_path = os.path.join(test_file_dir_path, test_project)\n            shutil.copytree(get_project_path(test_project), test_project_path)\n\n            for test_file in os.listdir(test_project_path):\n                if test_file.endswith(\".plist\"):\n                    test_file_path = os.path.join(test_project_path, test_file)\n                    with open(test_file_path, 'r+',\n                              encoding='utf-8', errors='ignore') as plist_file:\n                        content = plist_file.read()\n                        new_content = content.replace(\"$FILE_PATH$\",\n                                                      test_project_path)\n                        plist_file.seek(0)\n                        plist_file.truncate()\n                        plist_file.write(new_content)\n\n    def __test_html_builder(self, proj: str):\n        \"\"\"\n        Test building html file from the given proj's plist file.\n        \"\"\"\n        proj_dir = os.path.join(self.test_workspace, 'test_files', proj)\n        plist_file = os.path.join(proj_dir, f\"{proj}.plist\")\n\n        reports = report_file.get_reports(plist_file)\n\n        output_dir = os.path.join(proj_dir, 'html')\n        if not os.path.exists(output_dir):\n            os.mkdir(output_dir)\n\n        output_path = os.path.join(output_dir, f\"{proj}.plist.html\")\n\n        html_builder = report_to_html.HtmlBuilder(self.layout_dir)\n        report_to_html.convert(plist_file, reports, output_dir, html_builder)\n\n        self.assertTrue(os.path.exists(output_path))\n\n        html_builder.create_index_html(output_dir)\n        html_builder.create_statistics_html(output_dir)\n\n        index_html = os.path.join(output_dir, 'index.html')\n        self.assertTrue(os.path.exists(index_html))\n\n    def test_get_report_data_notes(self):\n        \"\"\" Get report data for plist which contains notes. \"\"\"\n        proj_notes = os.path.join(self.test_workspace, 'test_files', 'notes')\n        plist_file = os.path.join(proj_notes, 'notes.plist')\n\n        reports = report_file.get_reports(plist_file)\n\n        html_builder = report_to_html.HtmlBuilder(self.layout_dir)\n        html_builder._add_html_reports(reports)\n\n        self.assertEqual(len(html_builder.files), 1)\n\n        html_reports = html_builder.html_reports\n        self.assertEqual(len(html_reports), 1)\n\n        report = html_reports[0]\n        self.assertEqual(len(report['notes']), 1)\n        self.assertEqual(len(report['macros']), 0)\n        self.assertGreaterEqual(len(report['events']), 1)\n        self.assertEqual(report['checkerName'], 'alpha.clone.CloneChecker')\n\n    def test_get_report_data_macros(self):\n        \"\"\" Get report data for plist which contains macro expansion. \"\"\"\n        proj_macros = os.path.join(self.test_workspace, 'test_files', 'macros')\n        plist_file = os.path.join(proj_macros, 'macros.plist')\n\n        reports = report_file.get_reports(plist_file)\n\n        html_builder = report_to_html.HtmlBuilder(self.layout_dir)\n        html_builder._add_html_reports(reports)\n\n        self.assertEqual(len(html_builder.files), 1)\n\n        html_reports = html_builder.html_reports\n        self.assertEqual(len(html_reports), 1)\n\n        report = html_reports[0]\n        self.assertEqual(len(reports), 1)\n\n        report = html_reports[0]\n        self.assertEqual(len(report['notes']), 0)\n        self.assertEqual(len(report['macros']), 1)\n        self.assertGreaterEqual(len(report['events']), 1)\n        self.assertEqual(report['checkerName'], 'core.NullDereference')\n\n    def test_get_report_data_simple(self):\n        \"\"\" Get report data for plist which contains simple reports. \"\"\"\n        proj_simple = os.path.join(self.test_workspace, 'test_files', 'simple')\n        plist_file = os.path.join(proj_simple, 'simple.plist')\n\n        reports = report_file.get_reports(plist_file)\n\n        html_builder = report_to_html.HtmlBuilder(self.layout_dir)\n        html_builder._add_html_reports(reports)\n\n        self.assertEqual(len(html_builder.files), 1)\n\n        html_reports = html_builder.html_reports\n        self.assertEqual(len(html_reports), 2)\n\n        dead_stores = [r for r in html_reports if\n                       r['checkerName'] == 'deadcode.DeadStores'][0]\n        self.assertEqual(len(dead_stores['notes']), 0)\n        self.assertEqual(len(dead_stores['macros']), 0)\n        self.assertGreaterEqual(len(dead_stores['events']), 1)\n\n        divide_zero = [r for r in html_reports if\n                       r['checkerName'] == 'core.DivideZero'][0]\n        self.assertEqual(len(divide_zero['notes']), 0)\n        self.assertEqual(len(divide_zero['macros']), 0)\n        self.assertGreaterEqual(len(divide_zero['events']), 1)\n\n    def test_html_builder(self):\n        \"\"\" Test building html files from plist files on multiple projects. \"\"\"\n        self.__test_html_builder('notes')\n        self.__test_html_builder('macros')\n        self.__test_html_builder('simple')\n","lang_cluster":"C","length":159,"code_uid":"c4df427a1f824de5883b1921a5cf46a3"}
{"diff_hunk":"@@ -31,6 +31,7 @@ type Set interface {\n \tIter(func(item interface{}) error)\n \tCopy() Set\n \tEquals(Set) bool\n+\tContainsAll(Set) bool\n }\n \n type empty struct{}","old_code":"\/\/ Copyright (c) 2016-2017 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage set\n\nimport (\n\t\"errors\"\n\t\"reflect\"\n\n\tlog \"github.com\/Sirupsen\/logrus\"\n)\n\ntype Set interface {\n\tLen() int\n\tAdd(interface{})\n\tAddAll(itemArray interface{})\n\tDiscard(interface{})\n\tClear()\n\tContains(interface{}) bool\n\tIter(func(item interface{}) error)\n\tCopy() Set\n\tEquals(Set) bool\n}\n\ntype empty struct{}\n\nvar emptyValue = empty{}\n\nvar (\n\tStopIteration = errors.New(\"Stop iteration\")\n\tRemoveItem    = errors.New(\"Remove item\")\n)\n\nfunc New() Set {\n\treturn make(mapSet)\n}\n\nfunc From(members ...interface{}) Set {\n\ts := New()\n\ts.AddAll(members)\n\treturn s\n}\n\nfunc FromArray(membersArray interface{}) Set {\n\ts := New()\n\ts.AddAll(membersArray)\n\treturn s\n}\n\nfunc Empty() Set {\n\treturn mapSet(nil)\n}\n\ntype mapSet map[interface{}]empty\n\nfunc (set mapSet) Len() int {\n\treturn len(set)\n}\n\nfunc (set mapSet) Add(item interface{}) {\n\tset[item] = emptyValue\n}\n\nfunc (set mapSet) AddAll(itemArray interface{}) {\n\n\tarrVal := reflect.ValueOf(itemArray)\n\tfor i := 0; i < arrVal.Len(); i++ {\n\t\tset.Add(arrVal.Index(i).Interface())\n\t}\n}\n\nfunc (set mapSet) Discard(item interface{}) {\n\tdelete(set, item)\n}\n\nfunc (set mapSet) Clear() {\n\tfor item := range set {\n\t\tdelete(set, item)\n\t}\n}\n\nfunc (set mapSet) Contains(item interface{}) bool {\n\t_, present := set[item]\n\treturn present\n}\n\nfunc (set mapSet) Iter(visitor func(item interface{}) error) {\nloop:\n\tfor item := range set {\n\t\terr := visitor(item)\n\t\tswitch err {\n\t\tcase StopIteration:\n\t\t\tbreak loop\n\t\tcase RemoveItem:\n\t\t\tdelete(set, item)\n\t\tcase nil:\n\t\t\tbreak\n\t\tdefault:\n\t\t\tlog.WithError(err).Panic(\"Unexpected iteration error\")\n\t\t}\n\t}\n}\n\nfunc (set mapSet) Copy() Set {\n\tcpy := New()\n\tfor item := range set {\n\t\tcpy.Add(item)\n\t}\n\treturn cpy\n}\n\nfunc (set mapSet) Equals(other Set) bool {\n\tif set.Len() != other.Len() {\n\t\treturn false\n\t}\n\tfor item := range set {\n\t\tif !other.Contains(item) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n","lang_cluster":"C","length":133,"code_uid":"0e90aa5f649544ccb1ebc0a495fe2512"}
{"diff_hunk":"@@ -14,7 +14,7 @@ from thrift.protocol import TJSONProtocol\n from thrift.protocol.TProtocol import TProtocolException\n \n import shared\n-from Authentication import codeCheckerAuthentication\n+from Authentication_v6 import codeCheckerAuthentication\n \n from libcodechecker import session_manager\n ","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\nimport os\nimport sys\n# import datetime\nimport socket\n\nfrom thrift.transport import THttpClient\nfrom thrift.protocol import TJSONProtocol\nfrom thrift.protocol.TProtocol import TProtocolException\n\nimport shared\nfrom Authentication import codeCheckerAuthentication\n\nfrom libcodechecker import session_manager\n\n\nclass ThriftAuthHelper():\n    def __init__(self, host, port, uri, session_token=None):\n        self.__host = host\n        self.__port = port\n        self.transport = THttpClient.THttpClient(self.__host, self.__port, uri)\n        self.protocol = TJSONProtocol.TJSONProtocol(self.transport)\n        self.client = codeCheckerAuthentication.Client(self.protocol)\n\n        if session_token:\n            headers = {'Cookie': session_manager.SESSION_COOKIE_NAME +\n                       \"=\" + session_token}\n            self.transport.setCustomHeaders(headers)\n\n            # ------------------------------------------------------------\n\n    def ThriftClientCall(function):\n        # print type(function)\n        funcName = function.__name__\n\n        def wrapper(self, *args, **kwargs):\n            # print('['+host+':'+str(port)+'] >>>>> ['+funcName+']')\n            # before = datetime.datetime.now()\n            self.transport.open()\n            func = getattr(self.client, funcName)\n            try:\n                res = func(*args, **kwargs)\n\n            except shared.ttypes.RequestFailed as reqfailure:\n                if reqfailure.errorCode == shared.ttypes.ErrorCode.DATABASE:\n                    print('Database error on server')\n                    print(str(reqfailure.message))\n                elif reqfailure.errorCode ==\\\n                        shared.ttypes.ErrorCode.AUTH_DENIED:\n                    print('Authentication denied.')\n                    raise reqfailure\n                elif reqfailure.errorCode ==\\\n                        shared.ttypes.ErrorCode.UNAUTHORIZED:\n                    print('Unauthorised.')\n                    raise reqfailure\n                else:\n                    print('Other error')\n                    print(str(reqfailure))\n\n                sys.exit(1)\n            except TProtocolException as ex:\n                print(\"Connection failed to {0}:{1}\"\n                      .format(self.__host, self.__port))\n                sys.exit(1)\n            except socket.error as serr:\n                errCause = os.strerror(serr.errno)\n                print(errCause)\n                print(str(serr))\n                sys.exit(1)\n\n            # after = datetime.datetime.now()\n            # timediff = after - before\n            # diff = timediff.microseconds\/1000\n            # print('['+str(diff)+'ms] <<<<< ['+host+':'+str(port)+']')\n            # print res\n            self.transport.close()\n            return res\n\n        return wrapper\n\n    # ============= Authentication and session handling =============\n    @ThriftClientCall\n    def getAuthParameters(self):\n        pass\n\n    @ThriftClientCall\n    def getAcceptedAuthMethods(self):\n        pass\n\n    @ThriftClientCall\n    def performLogin(self, auth_method, auth_string):\n        pass\n\n    @ThriftClientCall\n    def destroySession(self):\n        pass\n\n    # ============= Authorization, permission management =============\n    @ThriftClientCall\n    def getPermissions(self, scope):\n        pass\n\n    @ThriftClientCall\n    def getPermissionsForUser(self, scope, extra_params, filter):\n        pass\n\n    @ThriftClientCall\n    def getAuthorisedNames(self, permission, extra_params):\n        pass\n\n    @ThriftClientCall\n    def addPermission(self, permission, auth_name, is_group, extra_params):\n        pass\n\n    @ThriftClientCall\n    def removePermission(self, permission, auth_name, is_group, extra_params):\n        pass\n\n    @ThriftClientCall\n    def hasPermission(self, permission, extra_params):\n        pass\n","lang_cluster":"C","length":126,"code_uid":"df5333e6b2754cdabda901c709d8ac0d"}
{"diff_hunk":"@@ -1,5 +1,5 @@\n \/* **********************************************************\n- * Copyright (c) 2010-2019 Google, Inc.  All rights reserved.\n+ * Copyright (c) 2010-2021 Google, Inc.  All rights reserved.\n  * Copyright (c) 2003-2010 VMware, Inc.  All rights reserved.\n  * **********************************************************\/\n ","old_code":"\/* **********************************************************\n * Copyright (c) 2010-2019 Google, Inc.  All rights reserved.\n * Copyright (c) 2003-2010 VMware, Inc.  All rights reserved.\n * **********************************************************\/\n\n\/*\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * * Redistributions of source code must retain the above copyright notice,\n *   this list of conditions and the following disclaimer.\n *\n * * Redistributions in binary form must reproduce the above copyright notice,\n *   this list of conditions and the following disclaimer in the documentation\n *   and\/or other materials provided with the distribution.\n *\n * * Neither the name of VMware, Inc. nor the names of its contributors may be\n *   used to endorse or promote products derived from this software without\n *   specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL VMWARE, INC. OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n *\/\n\n\/* Copyright (c) 2003-2007 Determina Corp. *\/\n\n\/*\n * ntdll_shared.c\n * Routines for calling Windows system calls via the ntdll.dll wrappers,\n * meant for sharing beyond the core DR library.\n * Xref i#1409 on splitting files up and eliminating the NOT_DYNAMORIO_CORE\n * ifdefs that today select portions of files.\n *\/\n#include \"configure.h\"\n#ifdef NOT_DYNAMORIO_CORE\n#    include \"globals_shared.h\"\n#    define ASSERT(x)\n#    define ASSERT_CURIOSITY(x)\n#    define ASSERT_NOT_REACHED()\n#    define ASSERT_NOT_IMPLEMENTED(x)\n#    define DODEBUG(x)\n#    define DOCHECK(n, x)\n#    define DEBUG_DECLARE(x)\n#    pragma warning(disable : 4210) \/\/ nonstd extension: function given file scope\n#    pragma warning(disable : 4204) \/\/ nonstd extension: non-constant aggregate\n                                    \/\/ initializer\n#    define INVALID_FILE INVALID_HANDLE_VALUE\n#    define STATUS_NOT_IMPLEMENTED ((NTSTATUS)0xC0000002L)\n#else\n\/* we include globals.h mainly for ASSERT, even though we're\n * used by preinject.\n * preinject just defines its own d_r_internal_error!\n *\/\n#    include \"..\/globals.h\"\n#    include \"..\/module_shared.h\"\n#endif\n\n\/* We have to hack away things we use here that won't work for non-core *\/\n#if defined(NOT_DYNAMORIO_CORE_PROPER) || defined(NOT_DYNAMORIO_CORE)\n#    undef ASSERT_OWN_NO_LOCKS\n#    define ASSERT_OWN_NO_LOCKS() \/* who cares if not the core *\/\n#endif\n\n#include \"ntdll_shared.h\"\n\n#ifndef X64\nNTSTATUS\nnt_wow64_read_virtual_memory64(HANDLE process, uint64 base, void *buffer,\n                               size_t buffer_length, size_t *bytes_read)\n{\n    \/* This syscall was added in 2003 so we can't statically link. *\/\n    typedef NTSTATUS(NTAPI * NtWow64ReadVirtualMemory64_t)(\n        HANDLE ProcessHandle, IN PVOID64 BaseAddress, OUT PVOID Buffer,\n        IN ULONGLONG BufferSize, OUT PULONGLONG NumberOfBytesRead);\n    static NtWow64ReadVirtualMemory64_t ntcall;\n    NTSTATUS res;\n    if (ntcall == NULL) {\n#    if !defined(NOT_DYNAMORIO_CORE) && !defined(NOT_DYNAMORIO_CORE_PROPER)\n        \/* The first call may not be during init so we have to unprot *\/\n        if (dynamo_initialized)\n            SELF_UNPROTECT_DATASEC(DATASEC_RARELY_PROT);\n#    endif\n        ntcall = (NtWow64ReadVirtualMemory64_t)\n#    ifdef NOT_DYNAMORIO_CORE\n            GetProcAddress(GetModuleHandle(\"ntdll.dll\"), \"NtWow64ReadVirtualMemory64\");\n#    else\n            d_r_get_proc_address(get_ntdll_base(), \"NtWow64ReadVirtualMemory64\");\n#    endif\n#    if !defined(NOT_DYNAMORIO_CORE) && !defined(NOT_DYNAMORIO_CORE_PROPER)\n        if (dynamo_initialized)\n            SELF_PROTECT_DATASEC(DATASEC_RARELY_PROT);\n#    endif\n    }\n    if (ntcall == NULL) {\n        \/* We do not need to fall back to NtReadVirtualMemory, b\/c\n         * NtWow64ReadVirtualMemory64 was added in xp64==2003 and so should\n         * always exist if we are in a WOW64 process: and we should only be\n         * called from a WOW64 process.\n         *\/\n        ASSERT_NOT_REACHED();\n        res = STATUS_NOT_IMPLEMENTED;\n    } else {\n        uint64 len;\n        res = ntcall(process, (PVOID64)base, buffer, (ULONGLONG)buffer_length, &len);\n        if (bytes_read != NULL)\n            *bytes_read = (size_t)len;\n    }\n    return res;\n}\n#endif\n","lang_cluster":"C","length":119,"code_uid":"a45ad509d300423aa03af6c20aa8b08e"}
{"diff_hunk":"@@ -126,7 +126,7 @@ class AnalyzerConfigHandler(object):\n             else:\n                 # Turn default checkers on.\n                 for checker in checker_config['default']:\n-                    self.set_checker_enabled(checker, True)\n+                    self.set_checker_enabled(checker)\n \n         # If enable_all is given, almost all checkers should be enabled.\n         if enable_all:","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nStatic analyzer configuration handler.\n\"\"\"\n\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nfrom abc import ABCMeta\nimport collections\nimport os\nimport platform\nimport sys\n\nfrom codechecker_common.logger import get_logger\n\nLOG = get_logger('system')\n\n\nclass AnalyzerConfigHandler(object):\n    \"\"\"\n    Handle the checker configurations and enabled disabled checkers lists.\n    \"\"\"\n    __metaclass__ = ABCMeta\n\n    def __init__(self):\n\n        self.analyzer_binary = None\n        self.analyzer_plugins_dir = None\n        self.compiler_resource_dir = ''\n        self.analyzer_extra_arguments = []\n        self.checker_config = ''\n        self.report_hash = None\n\n        # The key is the checker name, the value is a tuple.\n        # False if disabled (should be by default).\n        # True if checker is enabled.\n        # (False\/True, 'checker_description')\n        self.__available_checkers = collections.OrderedDict()\n\n    @property\n    def analyzer_plugins(self):\n        \"\"\"\n        Full path of the analyzer plugins.\n        \"\"\"\n        plugin_dir = self.analyzer_plugins_dir\n        if not plugin_dir or not os.path.exists(plugin_dir):\n            return []\n\n        analyzer_plugins = [os.path.join(plugin_dir, f)\n                            for f in os.listdir(plugin_dir)\n                            if os.path.isfile(os.path.join(plugin_dir, f))\n                            and f.endswith(\".so\")]\n        return analyzer_plugins\n\n    def add_checker(self, checker_name, enabled, description):\n        \"\"\"\n        Add additional checker.\n        Tuple of (checker_name, True or False).\n        \"\"\"\n        self.__available_checkers[checker_name] = (enabled, description)\n\n    def set_checker_enabled(self, checker_name, enabled=True):\n        \"\"\"\n        Enable checker, keep description if already set.\n        \"\"\"\n        for ch_name, values in self.__available_checkers.items():\n            if ch_name.startswith(checker_name) or \\\n               ch_name.endswith(checker_name):\n                _, description = values\n                self.__available_checkers[ch_name] = (enabled, description)\n\n    def checks(self):\n        \"\"\"\n        Return the checkers.\n        \"\"\"\n        return self.__available_checkers\n\n    def __gen_name_variations(self):\n        \"\"\"\n        Generate all applicable name variations from the given checker list.\n        \"\"\"\n        checker_names = (name for name in self.__available_checkers)\n        reserved_names = []\n\n        for name in checker_names:\n            delim = '.' if '.' in name else '-'\n            parts = name.split(delim)\n            # Creates a list of variations from a checker name, e.g.\n            # ['security', 'security.insecureAPI', 'security.insecureAPI.gets']\n            # from 'security.insecureAPI.gets' or\n            # ['misc', 'misc-dangling', 'misc-dangling-handle']\n            # from 'misc-dangling-handle'.\n            v = [delim.join(parts[:(i + 1)]) for i in range(len(parts))]\n            reserved_names += v\n\n        return reserved_names\n\n    def initialize_checkers(self,\n                            available_profiles,\n                            package_root,\n                            checkers,\n                            checker_config=None,\n                            cmdline_checkers=None,\n                            enable_all=False):\n        \"\"\"\n        Initializes the checker list for the specified config handler based on\n        given checker profiles, commandline arguments and the\n        analyzer-retrieved checker list.\n        \"\"\"\n\n        # By default disable all checkers.\n        for checker_name, description in checkers:\n            self.add_checker(checker_name, False, description)\n\n        # Set default enabled or disabled checkers, based on the config file.\n        if checker_config:\n            # Check whether a default profile exists.\n            if 'default' not in checker_config:\n                LOG.warning(\"No default profile found!\")\n            else:\n                # Turn default checkers on.\n                for checker in checker_config['default']:\n                    self.set_checker_enabled(checker, True)\n\n        # If enable_all is given, almost all checkers should be enabled.\n        if enable_all:\n            for checker_name, enabled in checkers:\n                if not checker_name.startswith(\"alpha.\") and \\\n                        not checker_name.startswith(\"debug.\") and \\\n                        not checker_name.startswith(\"osx.\"):\n                    # There are a few exceptions, though, which still need to\n                    # be manually enabled by the user: alpha and debug.\n                    self.set_checker_enabled(checker_name)\n\n                if checker_name.startswith(\"osx.\") and \\\n                        platform.system() == 'Darwin':\n                    # OSX checkers are only enable-all'd if we are on OSX.\n                    self.set_checker_enabled(checker_name)\n\n        # Set user defined enabled or disabled checkers from the command line.\n        if cmdline_checkers:\n\n            # Construct a list of reserved checker names.\n            # (It is used to check if a profile name is valid.)\n            reserved_names = self.__gen_name_variations()\n\n            for identifier, enabled in cmdline_checkers:\n\n                # The identifier is a profile name.\n                if identifier in available_profiles:\n                    profile_name = identifier\n\n                    if profile_name == \"list\":\n                        LOG.error(\"'list' is a reserved profile keyword. \")\n                        LOG.error(\"Please choose another profile name in \"\n                                  \"'%s'\/config\/config.json and rebuild.\",\n                                  package_root)\n                        sys.exit(1)\n\n                    if profile_name in reserved_names:\n                        LOG.warning(\"Profile name '%s' conflicts with a \"\n                                    \"checker(-group) name.\", profile_name)\n\n                    # Enable or disable all checkers belonging to the profile.\n                    for checker in checker_config[profile_name]:\n                        self.set_checker_enabled(checker, enabled)\n\n                # The identifier is a checker(-group) name.\n                else:\n                    checker_name = identifier\n                    self.set_checker_enabled(checker_name, enabled)\n","lang_cluster":"C","length":177,"code_uid":"709125056a94432e8f0cdc2bfb02c101"}
{"diff_hunk":"@@ -24,7 +24,7 @@ class ClangSAConfigHandler(config_handler.AnalyzerConfigHandler):\n     Configuration handler for the clang static analyzer.\n     \"\"\"\n \n-    def __init__(self, environ):\n+    def __init__(self, environ, analyzer_binary):\n         super(ClangSAConfigHandler, self).__init__()\n         self.ctu_dir = ''\n         self.ctu_on_demand = False","old_code":"# -------------------------------------------------------------------------\n#\n#  Part of the CodeChecker project, under the Apache License v2.0 with\n#  LLVM Exceptions. See LICENSE for license information.\n#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n#\n# -------------------------------------------------------------------------\n\"\"\"\nClang Static analyzer configuration handler.\n\"\"\"\nimport os\n\nfrom codechecker_analyzer import env\nfrom codechecker_common.logger import get_logger\nfrom .ctu_autodetection import CTUAutodetection\n\nfrom .. import config_handler\n\nLOG = get_logger('analyzer.clangsa')\n\n\nclass ClangSAConfigHandler(config_handler.AnalyzerConfigHandler):\n    \"\"\"\n    Configuration handler for the clang static analyzer.\n    \"\"\"\n\n    def __init__(self, environ):\n        super(ClangSAConfigHandler, self).__init__()\n        self.ctu_dir = ''\n        self.ctu_on_demand = False\n        self.log_file = ''\n        self.path_env_extra = ''\n        self.ld_lib_path_extra = ''\n        self.enable_z3 = False\n        self.enable_z3_refutation = False\n        self.environ = environ\n        self.version_info = None\n\n    @property\n    def analyzer_plugins(self):\n        \"\"\" Full path of the analyzer plugins. \"\"\"\n        plugin_dir = self.analyzer_plugins_dir\n\n        clangsa_plugin_dir = env.get_clangsa_plugin_dir()\n        is_analyzer_from_path = env.is_analyzer_from_path()\n        if is_analyzer_from_path:\n            if not clangsa_plugin_dir:\n                return []\n\n            # If the CC_ANALYZERS_FROM_PATH and CC_CLANGSA_PLUGIN_DIR\n            # environment variables are set we will use this value as the\n            # plugin directory.\n            plugin_dir = clangsa_plugin_dir\n\n        if not plugin_dir or not os.path.exists(plugin_dir):\n            return []\n\n        return [os.path.join(plugin_dir, f)\n                for f in os.listdir(plugin_dir)\n                if os.path.isfile(os.path.join(plugin_dir, f))\n                and f.endswith(\".so\")]\n\n    @property\n    def ctu_capability(self):\n        return CTUAutodetection(self.analyzer_binary, self.environ)\n","lang_cluster":"C","length":65,"code_uid":"30d3939827ed4e1da026e0b39fca3b0b"}
{"diff_hunk":"@@ -69,7 +69,10 @@ char *gpu_name_get()\n \t}\n \n \tchar *gpu_name = get_line(pipe);\n-\tfclose(pipe);\n+\n+\tstring_chomp(gpu_name);\n+\n+\tpclose(pipe);\n \n \treturn gpu_name;\n }","old_code":"\/*\nCopyright (C) 2013- The University of Notre Dame\nThis software is distributed under the GNU General Public License.\nSee the file COPYING for details.\n*\/\n\n#include \"gpu_info.h\"\n#include \"stringtools.h\"\n#include \"get_line.h\"\n\n#include <stddef.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys\/types.h>\n#include <sys\/wait.h>\n#include <sys\/stat.h>\n#include <unistd.h>\n#include <fcntl.h>\n\n\n\n#define GPU_AUTODETECT \"cctools_gpu_autodetect\"\n\nint gpu_info_get()\n{\n\tint pipefd[2];\n\tpipe(pipefd);\n\n\tpid_t pid = fork();\n\n\tif(pid<0) {\n\t\treturn 0;\n\t} else if(pid==0) {\n\t\tclose(pipefd[0]);\n\t\tdup2(pipefd[1], fileno(stdout));\n\t\tchar *args[] = {GPU_AUTODETECT, NULL};\n\t\tif(!access(GPU_AUTODETECT, R_OK|X_OK)){\n\t\t\texecv(GPU_AUTODETECT, args);\n\t\t} else {\n\t\t\texecvp(GPU_AUTODETECT, args);\n\t\t}\n\t\t_exit(0);\n\t} else {\n\t\tclose(pipefd[1]);\n\t\tint status = 0;\n\t\tint gpu_count = 0;\n\t\tchar buffer[10]; \/* Enough characters to hold a decimal representation of a 32 bit int. *\/\n\t\tif(read(pipefd[0], buffer, 10)){\n\t\t\twaitpid(pid, &status, 0);\n\t\t\tgpu_count = atoi(buffer);\n\t\t}\n\n\t\tclose(pipefd[0]);\n\t\treturn gpu_count;\n\t}\n}\n\nchar *gpu_name_get()\n{\n\tchar *nvidia_cmd = \"\/bin\/nvidia-smi\";\n\n\tif(access(nvidia_cmd, X_OK) != 0) {\n\t\treturn NULL;\n\t}\n\n\tFILE *pipe = popen(\"\/bin\/nvidia-smi --query-gpu=gpu_name --format=csv,noheader\", \"r\");\n\tif(!pipe) {\n\t\treturn NULL;\n\t}\n\n\tchar *gpu_name = get_line(pipe);\n\tfclose(pipe);\n\n\treturn gpu_name;\n}\n\n\/* vim: set noexpandtab tabstop=4: *\/\n","lang_cluster":"C","length":77,"code_uid":"9e7712d445e54306bcbf9fcd294b8b2f"}
{"diff_hunk":"@@ -1,5 +1,4 @@\n using System;\n-using System.Reflection;\n using System.Threading;\n using System.Threading.Tasks;\n using Datadog.Trace.ClrProfiler.Emit;","old_code":"using System;\nusing System.Reflection;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Datadog.Trace.ClrProfiler.Emit;\nusing Datadog.Trace.ClrProfiler.Helpers;\nusing Datadog.Trace.Logging;\n\nnamespace Datadog.Trace.ClrProfiler.Integrations\n{\n    \/\/\/ <summary>\n    \/\/\/ Traces an Elasticsearch pipeline\n    \/\/\/ <\/summary>\n    public static class ElasticsearchNet5Integration\n    {\n        private const string IntegrationName = \"ElasticsearchNet5\";\n        private const string Version5 = \"5\";\n        private const string ElasticsearchAssembly = \"Elasticsearch.Net\";\n        private const string RequestPipelineInterface = \"Elasticsearch.Net.IRequestPipeline\";\n\n        private static readonly ILog Log = LogProvider.GetLogger(typeof(ElasticsearchNet5Integration));\n        private static readonly Type ElasticsearchResponseType = Type.GetType(\"Elasticsearch.Net.ElasticsearchResponse`1, Elasticsearch.Net\", throwOnError: false);\n\n        \/\/\/ <summary>\n        \/\/\/ Traces a synchronous call to Elasticsearch.\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"TResponse\">The type of the response<\/typeparam>\n        \/\/\/ <param name=\"pipeline\">The pipeline for the original method<\/param>\n        \/\/\/ <param name=\"requestData\">The request data<\/param>\n        \/\/\/ <param name=\"opCode\">The OpCode used in the original method call.<\/param>\n        \/\/\/ <param name=\"mdToken\">The mdToken of the original method call.<\/param>\n        \/\/\/ <param name=\"moduleVersionPtr\">A pointer to the module version GUID.<\/param>\n        \/\/\/ <returns>The original result<\/returns>\n        [InterceptMethod(\n            CallerAssembly = ElasticsearchAssembly,\n            TargetAssembly = ElasticsearchAssembly,\n            TargetType = RequestPipelineInterface,\n            TargetSignatureTypes = new[] { \"Elasticsearch.Net.ElasticsearchResponse`1<T>\", \"Elasticsearch.Net.RequestData\" },\n            TargetMinimumVersion = Version5,\n            TargetMaximumVersion = Version5)]\n        public static object CallElasticsearch<TResponse>(\n            object pipeline,\n            object requestData,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            const string methodName = nameof(CallElasticsearch);\n            Func<object, object, object> callElasticSearch;\n            var pipelineType = pipeline.GetType();\n            var genericArgument = typeof(TResponse);\n\n            try\n            {\n                callElasticSearch =\n                    MethodBuilder<Func<object, object, object>>\n                       .Start(moduleVersionPtr, mdToken, opCode, methodName)\n                       .WithConcreteType(pipelineType)\n                       .WithMethodGenerics(genericArgument)\n                       .WithParameters(requestData)\n                       .Build();\n            }\n            catch (Exception ex)\n            {\n                \/\/ profiled app will not continue working as expected without this method\n                Log.ErrorException($\"Error retrieving {pipelineType.Name}.{methodName}(RequestData requestData)\", ex);\n                throw;\n            }\n\n            using (var scope = ElasticsearchNetCommon.CreateScope(Tracer.Instance, IntegrationName, pipeline, requestData))\n            {\n                try\n                {\n                    return callElasticSearch(pipeline, requestData);\n                }\n                catch (Exception ex) when (scope?.Span.SetExceptionForFilter(ex) ?? false)\n                {\n                    \/\/ unreachable code\n                    throw;\n                }\n            }\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Traces an asynchronous call to Elasticsearch.\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"TResponse\">Type type of the response<\/typeparam>\n        \/\/\/ <param name=\"pipeline\">The pipeline for the original method<\/param>\n        \/\/\/ <param name=\"requestData\">The request data<\/param>\n        \/\/\/ <param name=\"cancellationTokenSource\">A cancellation token<\/param>\n        \/\/\/ <param name=\"opCode\">The OpCode used in the original method call.<\/param>\n        \/\/\/ <param name=\"mdToken\">The mdToken of the original method call.<\/param>\n        \/\/\/ <param name=\"moduleVersionPtr\">A pointer to the module version GUID.<\/param>\n        \/\/\/ <returns>The original result<\/returns>\n        [InterceptMethod(\n            CallerAssembly = ElasticsearchAssembly,\n            TargetAssembly = ElasticsearchAssembly,\n            TargetType = RequestPipelineInterface,\n            TargetSignatureTypes = new[] { \"System.Threading.Tasks.Task`1<Elasticsearch.Net.ElasticsearchResponse`1<T>>\", \"Elasticsearch.Net.RequestData\", ClrNames.CancellationToken },\n            TargetMinimumVersion = Version5,\n            TargetMaximumVersion = Version5)]\n        public static object CallElasticsearchAsync<TResponse>(\n            object pipeline,\n            object requestData,\n            object cancellationTokenSource,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            var tokenSource = cancellationTokenSource as CancellationTokenSource;\n            var cancellationToken = tokenSource?.Token ?? CancellationToken.None;\n\n            var genericArgument = typeof(TResponse);\n            var genericResponseType = ElasticsearchResponseType.MakeGenericType(genericArgument);\n\n            Func<object, object, CancellationToken, object> instrumentedMethod;\n\n            try\n            {\n                instrumentedMethod =\n                    MethodBuilder<Func<object, object, CancellationToken, object>>\n                       .Start(moduleVersionPtr, mdToken, opCode, nameof(CallElasticsearchAsync))\n                       .WithConcreteType(pipeline.GetType())\n                       .WithMethodGenerics(genericArgument)\n                       .WithParameters(requestData, cancellationToken)\n                       .ForceMethodDefinitionResolution()\n                       .Build();\n            }\n            catch (Exception ex)\n            {\n                Log.ErrorException($\"Error resolving Elasticsearch.Net.IRequestPipeline.{nameof(CallElasticsearchAsync)}<T>(...)\", ex);\n                throw;\n            }\n\n            return AsyncHelper.InvokeGenericTaskDelegate(\n                owningType: ElasticsearchNetCommon.RequestPipelineType,\n                taskResultType: genericResponseType,\n                nameOfIntegrationMethod: nameof(CallElasticsearchAsyncInternal),\n                integrationType: typeof(ElasticsearchNet5Integration),\n                pipeline,\n                requestData,\n                cancellationToken,\n                instrumentedMethod);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Traces an asynchronous call to Elasticsearch.\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"T\">Type type of the Task<\/typeparam>\n        \/\/\/ <param name=\"pipeline\">The pipeline for the original method<\/param>\n        \/\/\/ <param name=\"requestData\">The request data<\/param>\n        \/\/\/ <param name=\"cancellationToken\">A cancellation token<\/param>\n        \/\/\/ <param name=\"originalMethod\">A delegate for the method we are instrumenting<\/param>\n        \/\/\/ <returns>The original result<\/returns>\n        private static async Task<T> CallElasticsearchAsyncInternal<T>(\n            object pipeline,\n            object requestData,\n            CancellationToken cancellationToken,\n            Func<object, object, CancellationToken, object> originalMethod)\n        {\n            using (var scope = ElasticsearchNetCommon.CreateScope(Tracer.Instance, IntegrationName, pipeline, requestData))\n            {\n                try\n                {\n                    var task = (Task<T>)originalMethod(pipeline, requestData, cancellationToken);\n                    return await task.ConfigureAwait(false);\n                }\n                catch (Exception ex) when (scope?.Span.SetExceptionForFilter(ex) ?? false)\n                {\n                    \/\/ unreachable code\n                    throw;\n                }\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":176,"code_uid":"5f67b6208468488bbd92db4370880924"}
{"diff_hunk":"@@ -31,13 +31,15 @@ namespace Nethermind.Blockchain.Find\n         private readonly IReceiptsRecovery _receiptsRecovery;\n         private readonly int _maxBlockDepth;\n         private readonly IBlockFinder _blockFinder;\n+        private readonly ILogger _logger;\n \n-        public LogFinder(IBlockFinder blockFinder, IReceiptStorage receiptStorage, IBloomStorage bloomStorage, IReceiptsRecovery receiptsRecovery, int maxBlockDepth = 1000)\n+        public LogFinder(IBlockFinder blockFinder, IReceiptStorage receiptStorage, IBloomStorage bloomStorage, IReceiptsRecovery receiptsRecovery, ILogManager logManager, int maxBlockDepth = 1000)\n         {\n             _blockFinder = blockFinder ?? throw new ArgumentNullException(nameof(blockFinder));\n             _receiptStorage = receiptStorage ?? throw new ArgumentNullException(nameof(receiptStorage));\n             _bloomStorage = bloomStorage ?? throw new ArgumentNullException(nameof(bloomStorage));\n             _receiptsRecovery = receiptsRecovery ?? throw new ArgumentNullException(nameof(receiptsRecovery));\n+            _logger = logManager?.GetClassLogger<LogFinder>() ?? throw new ArgumentNullException(nameof(logManager));\n             _maxBlockDepth = maxBlockDepth;\n         }\n ","old_code":"\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing Nethermind.Blockchain.Filters;\nusing Nethermind.Blockchain.Receipts;\nusing Nethermind.Core;\nusing Nethermind.Store.Bloom;\n\nnamespace Nethermind.Blockchain.Find\n{\n    public class LogFinder : ILogFinder\n    {\n        private readonly IReceiptStorage _receiptStorage;\n        private readonly IBloomStorage _bloomStorage;\n        private readonly IReceiptsRecovery _receiptsRecovery;\n        private readonly int _maxBlockDepth;\n        private readonly IBlockFinder _blockFinder;\n\n        public LogFinder(IBlockFinder blockFinder, IReceiptStorage receiptStorage, IBloomStorage bloomStorage, IReceiptsRecovery receiptsRecovery, int maxBlockDepth = 1000)\n        {\n            _blockFinder = blockFinder ?? throw new ArgumentNullException(nameof(blockFinder));\n            _receiptStorage = receiptStorage ?? throw new ArgumentNullException(nameof(receiptStorage));\n            _bloomStorage = bloomStorage ?? throw new ArgumentNullException(nameof(bloomStorage));\n            _receiptsRecovery = receiptsRecovery ?? throw new ArgumentNullException(nameof(receiptsRecovery));\n            _maxBlockDepth = maxBlockDepth;\n        }\n\n        public IEnumerable<FilterLog> FindLogs(LogFilter filter)\n        {\n            BlockHeader FindHeader(BlockParameter blockParameter, string name) => _blockFinder.FindHeader(blockParameter) ?? throw new ArgumentException(ILogFinder.NotFoundError, name);\n\n            var toBlock = FindHeader(filter.ToBlock, nameof(filter.ToBlock));\n            var fromBlock = FindHeader(filter.FromBlock, nameof(filter.FromBlock));\n\n            if (fromBlock.Number > toBlock.Number && toBlock.Number != 0)\n            {\n                throw new ArgumentException(\"'From' block is later than 'to' block.\");\n            }\n\n            return ShouldUseBloomDatabase(fromBlock, toBlock) && CanUseBloomDatabase(toBlock, fromBlock)\n                ? FilterLogsWithBloomsIndex(filter, fromBlock, toBlock) \n                : FilterLogsIteratively(filter, fromBlock, toBlock);\n        }\n\n        private bool ShouldUseBloomDatabase(BlockHeader fromBlock, BlockHeader toBlock)\n        {\n            var blocksToSearch = toBlock.Number - fromBlock.Number + 1;\n            return blocksToSearch > 1; \/\/ if we are searching only in 1 block skip bloom index altogether, this can be tweaked\n        }\n\n        private IEnumerable<FilterLog> FilterLogsWithBloomsIndex(LogFilter filter, BlockHeader fromBlock, BlockHeader toBlock)\n        {\n            var enumeration = _bloomStorage.GetBlooms(fromBlock.Number, toBlock.Number);\n            foreach (var bloom in enumeration)\n            {\n                if (filter.Matches(bloom) && enumeration.TryGetBlockNumber(out var blockNumber))\n                {\n                    foreach (var filterLog in FindLogsInBlock(filter, _blockFinder.FindBlock(blockNumber)))\n                    {\n                        yield return filterLog;\n                    }\n                }\n            }\n        }\n\n        private bool CanUseBloomDatabase(BlockHeader toBlock, BlockHeader fromBlock) => _bloomStorage.ContainsRange(fromBlock.Number, toBlock.Number) && _blockFinder.IsMainChain(toBlock) && _blockFinder.IsMainChain(fromBlock);\n\n        private IEnumerable<FilterLog> FilterLogsIteratively(LogFilter filter, BlockHeader fromBlock, BlockHeader toBlock)\n        {\n            int count = 0;\n\n            while (count < _maxBlockDepth && toBlock.Number >= (fromBlock?.Number ?? long.MaxValue))\n            {\n                foreach (var filterLog in FindLogsInBlock(filter, toBlock))\n                {\n                    yield return filterLog;\n                }\n\n                if (!TryGetParentBlock(toBlock, out toBlock))\n                {\n                    break;\n                }\n\n                count++;\n            }\n        }\n\n        private IEnumerable<FilterLog> FindLogsInBlock(LogFilter filter, BlockHeader block) =>\n            filter.Matches(block.Bloom)\n                ? FindLogsInBlock(filter, _blockFinder.FindBlock(block.Hash))\n                : Enumerable.Empty<FilterLog>();\n\n        private IEnumerable<FilterLog> FindLogsInBlock(LogFilter filter, Block block)\n        {\n            var receipts = _receiptStorage.FindForBlock(block, _receiptsRecovery);\n            long logIndexInBlock = 0;\n            foreach (var receipt in receipts)\n            {\n                if (receipt == null)\n                {\n                    continue;\n                }\n\n                if (filter.Matches(receipt.Bloom))\n                {\n                    for (var index = 0; index < receipt.Logs.Length; index++)\n                    {\n                        var log = receipt.Logs[index];\n                        if (filter.Accepts(log))\n                        {\n                            yield return new FilterLog(logIndexInBlock, index, receipt, log);\n                        }\n\n                        logIndexInBlock++;\n                    }\n                }\n                else\n                {\n                    logIndexInBlock += receipt.Logs.Length;\n                }\n            }\n        }\n\n        private bool TryGetParentBlock(BlockHeader currentBlock, out BlockHeader parentHeader)\n        {\n            if (currentBlock.IsGenesis)\n            {\n                parentHeader = null;\n                return false;\n            }\n\n            parentHeader = _blockFinder.FindParentHeader(currentBlock, BlockTreeLookupOptions.TotalDifficultyNotNeeded);\n            return true;\n        }\n    }\n}","lang_cluster":"C#","length":152,"code_uid":"f6f37c5e9c2c4cb28cffc710a97a96e7"}
{"diff_hunk":"@@ -81,9 +81,13 @@ namespace Datadog.Trace.DuckTyping\n         public static T DuckAs<T>(this object instance)\n             where T : class\n         {\n-            if (DuckType.CanCreate<T>(instance))\n+            if (instance is not null)\n             {\n-                return DuckType.Create<T>(instance);\n+                DuckType.CreateTypeResult proxyResult = DuckType.CreateCache<T>.GetProxy(instance.GetType());\n+                if (proxyResult.Success)\n+                {\n+                    return proxyResult.CreateInstance<T>(instance);\n+                }\n             }\n \n             return null;","old_code":"using System;\nusing System.Runtime.CompilerServices;\nusing Datadog.Trace.Logging;\n\nnamespace Datadog.Trace.DuckTyping\n{\n    \/\/\/ <summary>\n    \/\/\/ Duck type extensions\n    \/\/\/ <\/summary>\n    public static class DuckTypeExtensions\n    {\n        private static readonly IDatadogLogger Log = DatadogLogging.GetLoggerFor(typeof(DuckType));\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the duck type instance for the object implementing a base class or interface T\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"instance\">Object instance<\/param>\n        \/\/\/ <typeparam name=\"T\">Target type<\/typeparam>\n        \/\/\/ <returns>DuckType instance<\/returns>\n        [MethodImpl(MethodImplOptions.AggressiveInlining)]\n        public static T DuckCast<T>(this object instance)\n            => DuckType.Create<T>(instance);\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the duck type instance for the object implementing a base class or interface T\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"instance\">Object instance<\/param>\n        \/\/\/ <param name=\"targetType\">Target type<\/param>\n        \/\/\/ <returns>DuckType instance<\/returns>\n        [MethodImpl(MethodImplOptions.AggressiveInlining)]\n        public static object DuckCast(this object instance, Type targetType)\n            => DuckType.Create(targetType, instance);\n\n        \/\/\/ <summary>\n        \/\/\/ Tries to ducktype the object implementing a base class or interface T\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"T\">Target type<\/typeparam>\n        \/\/\/ <param name=\"instance\">Object instance<\/param>\n        \/\/\/ <param name=\"value\">Ducktype instance<\/param>\n        \/\/\/ <returns>true if the object instance was ducktyped; otherwise, false.<\/returns>\n        [MethodImpl(MethodImplOptions.AggressiveInlining)]\n        public static bool TryDuckCast<T>(this object instance, out T value)\n        {\n            if (DuckType.CanCreate<T>(instance))\n            {\n                value = DuckType.Create<T>(instance);\n                return true;\n            }\n\n            value = default;\n            return false;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Tries to ducktype the object implementing a base class or interface T\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"instance\">Object instance<\/param>\n        \/\/\/ <param name=\"targetType\">Target type<\/param>\n        \/\/\/ <param name=\"value\">Ducktype instance<\/param>\n        \/\/\/ <returns>true if the object instance was ducktyped; otherwise, false.<\/returns>\n        [MethodImpl(MethodImplOptions.AggressiveInlining)]\n        public static bool TryDuckCast(this object instance, Type targetType, out object value)\n        {\n            if (DuckType.CanCreate(targetType, instance))\n            {\n                value = DuckType.Create(targetType, instance);\n                return true;\n            }\n\n            value = default;\n            return false;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the duck type instance for the object implementing a base class or interface T\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"instance\">Object instance<\/param>\n        \/\/\/ <typeparam name=\"T\">Target type<\/typeparam>\n        \/\/\/ <returns>DuckType instance<\/returns>\n        [MethodImpl(MethodImplOptions.AggressiveInlining)]\n        public static T DuckAs<T>(this object instance)\n            where T : class\n        {\n            if (DuckType.CanCreate<T>(instance))\n            {\n                return DuckType.Create<T>(instance);\n            }\n\n            return null;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the duck type instance for the object implementing a base class or interface T\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"instance\">Object instance<\/param>\n        \/\/\/ <param name=\"targetType\">Target type<\/param>\n        \/\/\/ <returns>DuckType instance<\/returns>\n        [MethodImpl(MethodImplOptions.AggressiveInlining)]\n        public static object DuckAs(this object instance, Type targetType)\n        {\n            if (DuckType.CanCreate(targetType, instance))\n            {\n                return DuckType.Create(targetType, instance);\n            }\n\n            return null;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets if a proxy can be created\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"instance\">Instance object<\/param>\n        \/\/\/ <typeparam name=\"T\">Duck type<\/typeparam>\n        \/\/\/ <returns>true if the proxy can be created; otherwise, false<\/returns>\n        [MethodImpl(MethodImplOptions.AggressiveInlining)]\n        public static bool DuckIs<T>(this object instance)\n            => DuckType.CanCreate<T>(instance);\n\n        \/\/\/ <summary>\n        \/\/\/ Gets if a proxy can be created\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"instance\">Instance object<\/param>\n        \/\/\/ <param name=\"targetType\">Duck type<\/param>\n        \/\/\/ <returns>true if the proxy can be created; otherwise, false<\/returns>\n        [MethodImpl(MethodImplOptions.AggressiveInlining)]\n        public static bool DuckIs(this object instance, Type targetType)\n            => DuckType.CanCreate(targetType, instance);\n    }\n}\n","lang_cluster":"C#","length":129,"code_uid":"b3363dcc50ca40109dd3875966ce8477"}
{"diff_hunk":"@@ -60,6 +60,20 @@ namespace Datadog.Trace.PlatformHelpers\n \n         public string ResourceId { get; }\n \n+        public AzureContext AzureContext { get; private set; } = AzureContext.AzureAppService;\n+\n+        public string FunctionsExtensionVersion { get; }\n+\n+        public string FunctionsWorkerRuntime { get; }\n+\n+        public string InstanceName { get; }\n+\n+        public string InstanceId { get; }\n+\n+        public string OperatingSystem { get; }\n+\n+        public string Runtime { get; }\n+\n         private string CompileResourceId()\n         {\n             string resourceId = null;","old_code":"using System;\nusing System.Collections;\nusing Datadog.Trace.ExtensionMethods;\nusing Datadog.Trace.Logging;\nusing Datadog.Trace.Util;\n\nnamespace Datadog.Trace.PlatformHelpers\n{\n    internal class AzureAppServices\n    {\n        \/\/\/ <summary>\n        \/\/\/ Configuration key which is used as a flag to tell us whether we are running in the context of Azure App Services.\n        \/\/\/ <\/summary>\n        internal static readonly string AzureAppServicesContextKey = \"DD_AZURE_APP_SERVICES\";\n\n        \/\/\/ <summary>\n        \/\/\/ Example: 8c56d827-5f07-45ce-8f2b-6c5001db5c6f+apm-dotnet-EastUSwebspace\n        \/\/\/ Format: {subscriptionId}+{planResourceGroup}-{hostedInRegion}\n        \/\/\/ <\/summary>\n        internal static readonly string WebsiteOwnerNameKey = \"WEBSITE_OWNER_NAME\";\n\n        \/\/\/ <summary>\n        \/\/\/ This is the name of the resource group the site instance is assigned to.\n        \/\/\/ <\/summary>\n        internal static readonly string ResourceGroupKey = \"WEBSITE_RESOURCE_GROUP\";\n\n        \/\/\/ <summary>\n        \/\/\/ This is the unique name of the website instance within azure app services.\n        \/\/\/ <\/summary>\n        internal static readonly string SiteNameKey = \"WEBSITE_DEPLOYMENT_ID\";\n\n        private static readonly Vendors.Serilog.ILogger Log = DatadogLogging.GetLogger(typeof(AzureAppServices));\n\n        static AzureAppServices()\n        {\n            Metadata = new AzureAppServices(EnvironmentHelpers.GetEnvironmentVariables());\n        }\n\n        public AzureAppServices(IDictionary environmentVariables)\n        {\n            IsRelevant = GetVariableIfExists(AzureAppServicesContextKey, environmentVariables)?.ToBoolean() ?? false;\n            if (IsRelevant)\n            {\n                SubscriptionId = GetSubscriptionId(environmentVariables);\n                ResourceGroup = GetVariableIfExists(ResourceGroupKey, environmentVariables);\n                SiteName = GetVariableIfExists(SiteNameKey, environmentVariables);\n                ResourceId = CompileResourceId();\n            }\n        }\n\n        public static AzureAppServices Metadata { get; set; }\n\n        public bool IsRelevant { get; }\n\n        public string SubscriptionId { get; }\n\n        public string ResourceGroup { get; }\n\n        public string SiteName { get; }\n\n        public string ResourceId { get; }\n\n        private string CompileResourceId()\n        {\n            string resourceId = null;\n\n            try\n            {\n                var success = true;\n                if (SubscriptionId == null)\n                {\n                    success = false;\n                    Log.Warning(\"Could not successfully retrieve the subscription ID from variable: {0}\", WebsiteOwnerNameKey);\n                }\n\n                if (SiteName == null)\n                {\n                    success = false;\n                    Log.Warning(\"Could not successfully retrieve the deployment ID from variable: {0}\", SiteNameKey);\n                }\n\n                if (ResourceGroup == null)\n                {\n                    success = false;\n                    Log.Warning(\"Could not successfully retrieve the resource group name from variable: {0}\", ResourceGroupKey);\n                }\n\n                if (success)\n                {\n                    resourceId = $\"\/subscriptions\/{SubscriptionId}\/resourcegroups\/{ResourceGroup}\/providers\/microsoft.web\/sites\/{SiteName}\".ToLowerInvariant();\n                }\n            }\n            catch (Exception ex)\n            {\n                Log.SafeLogError(ex, \"Could not successfully setup the resource id for azure app services.\");\n            }\n\n            return resourceId;\n        }\n\n        private string GetSubscriptionId(IDictionary environmentVariables)\n        {\n            try\n            {\n                var websiteOwner = GetVariableIfExists(WebsiteOwnerNameKey, environmentVariables);\n                if (!string.IsNullOrWhiteSpace(websiteOwner))\n                {\n                    var plusSplit = websiteOwner.Split('+');\n                    if (plusSplit.Length > 0 && !string.IsNullOrWhiteSpace(plusSplit[0]))\n                    {\n                        return plusSplit[0];\n                    }\n                }\n            }\n            catch (Exception ex)\n            {\n                Log.SafeLogError(ex, \"Could not successfully retrieve the subscription id for azure app services.\");\n            }\n\n            return null;\n        }\n\n        private string GetVariableIfExists(string key, IDictionary environmentVariables)\n        {\n            if (environmentVariables.Contains(key))\n            {\n                return environmentVariables[key]?.ToString();\n            }\n\n            return null;\n        }\n    }\n}\n","lang_cluster":"C#","length":133,"code_uid":"95490f0436b5433daf064a4d2892905e"}
{"diff_hunk":"@@ -65,16 +65,19 @@ namespace Microsoft.AspNet.Server.Kestrel.Http\n             return tcs.Task;\n         }\n \n-        private void OnConnection(UvStreamHandle listenSocket, int status)\n-        {\n-            var acceptSocket = new UvTcpHandle();\n-            acceptSocket.Init(Thread.Loop, Thread.QueueCloseHandle);\n-            listenSocket.Accept(acceptSocket);\n+        \/\/\/ <summary>\n+        \/\/\/ Creates the socket used to listen for incoming connections\n+        \/\/\/ <\/summary>\n+        protected abstract T CreateListenSocket(string host, int port);\n \n-            DispatchConnection(acceptSocket);\n-        }\n+        \/\/\/ <summary>\n+        \/\/\/ Handles an incoming connection\n+        \/\/\/ <\/summary>\n+        \/\/\/ <param name=\"listenSocket\">Socket being used to listen on<\/param>\n+        \/\/\/ <param name=\"status\">Connection status<\/param>\n+        protected abstract void OnConnection(T listenSocket, int status);\n \n-        protected virtual void DispatchConnection(UvTcpHandle socket)\n+        protected virtual void DispatchConnection(T socket)\n         {\n             var connection = new Connection(this, socket);\n             connection.Start();","old_code":"\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing Microsoft.AspNet.Server.Kestrel.Infrastructure;\nusing Microsoft.AspNet.Server.Kestrel.Networking;\nusing System;\nusing System.Diagnostics;\nusing System.Net;\nusing System.Threading.Tasks;\n\nnamespace Microsoft.AspNet.Server.Kestrel.Http\n{\n    \/\/\/ <summary>\n    \/\/\/ Summary description for Accept\n    \/\/\/ <\/summary>\n    public class Listener : ListenerContext, IDisposable\n    {\n        private static readonly Action<UvStreamHandle, int, Exception, object> _connectionCallback = ConnectionCallback;\n\n        UvTcpHandle ListenSocket { get; set; }\n\n        private static void ConnectionCallback(UvStreamHandle stream, int status, Exception error, object state)\n        {\n            if (error != null)\n            {\n                Trace.WriteLine(\"Listener.ConnectionCallback \" + error.ToString());\n            }\n            else\n            {\n                ((Listener)state).OnConnection(stream, status);\n            }\n        }\n\n        public Listener(IMemoryPool memory)\n        {\n            Memory = memory;\n        }\n\n        public Task StartAsync(\n            string scheme,\n            string host,\n            int port,\n            KestrelThread thread,\n            Func<Frame, Task> application)\n        {\n            Thread = thread;\n            Application = application;\n\n            var tcs = new TaskCompletionSource<int>();\n            Thread.Post(_ =>\n            {\n                try\n                {\n                    ListenSocket = new UvTcpHandle();\n                    ListenSocket.Init(Thread.Loop, Thread.QueueCloseHandle);\n                    ListenSocket.Bind(new IPEndPoint(IPAddress.Any, port));\n                    ListenSocket.Listen(Constants.ListenBacklog, _connectionCallback, this);\n                    tcs.SetResult(0);\n                }\n                catch (Exception ex)\n                {\n                    tcs.SetException(ex);\n                }\n            }, null);\n            return tcs.Task;\n        }\n\n        private void OnConnection(UvStreamHandle listenSocket, int status)\n        {\n            var acceptSocket = new UvTcpHandle();\n            acceptSocket.Init(Thread.Loop, Thread.QueueCloseHandle);\n            listenSocket.Accept(acceptSocket);\n\n            DispatchConnection(acceptSocket);\n        }\n\n        protected virtual void DispatchConnection(UvTcpHandle socket)\n        {\n            var connection = new Connection(this, socket);\n            connection.Start();\n        }\n\n        public void Dispose()\n        {\n            \/\/ Ensure the event loop is still running.\n            \/\/ If the event loop isn't running and we try to wait on this Post\n            \/\/ to complete, then KestrelEngine will never be disposed and\n            \/\/ the exception that stopped the event loop will never be surfaced.\n            if (Thread.FatalError == null)\n            {\n                var tcs = new TaskCompletionSource<int>();\n                Thread.Post(\n                    _ =>\n                    {\n                        try\n                        {\n                            ListenSocket.Dispose();\n                            tcs.SetResult(0);\n                        }\n                        catch (Exception ex)\n                        {\n                            tcs.SetException(ex);\n                        }\n                    },\n                    null);\n\n                \/\/ REVIEW: Should we add a timeout here to be safe?\n                tcs.Task.Wait();\n            }\n\n            ListenSocket = null;\n        }\n    }\n}\n","lang_cluster":"C#","length":114,"code_uid":"20a7331e8b7d4d43ab2196ef92cde128"}
{"diff_hunk":"@@ -78,11 +78,6 @@ namespace OpenTelemetry.Context\n         \/\/\/ <param name=\"value\">The value of the context entry.<\/param>\n         public DistributedContext(string key, string value)\n         {\n-            if (key is null || value is null)\n-            {\n-                throw new ArgumentNullException(key is null ? nameof(key) : nameof(value));\n-            }\n-\n             this.entries = carrier is NoopDistributedContextCarrier ? emptyList : new List<DistributedContextEntry>(1) { new DistributedContextEntry(key, value) };\n         }\n ","old_code":"\ufeff\/\/ <copyright file=\"DistributedContext.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright 2018, OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Runtime.Serialization.Formatters;\n\nnamespace OpenTelemetry.Context\n{\n    \/\/\/ <summary>\n    \/\/\/ Distributed context.\n    \/\/\/ <\/summary>\n    public readonly struct DistributedContext : IEquatable<DistributedContext>\n    {\n        private static DistributedContextCarrier carrier = NoopDistributedContextCarrier.Instance;\n        private static List<DistributedContextEntry> emptyList = new List<DistributedContextEntry>();\n        private readonly IEnumerable<DistributedContextEntry> entries;\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"DistributedContext\"\/> struct.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"entries\">Entries for distributed context.<\/param>\n        public DistributedContext(IEnumerable<DistributedContextEntry> entries)\n        {\n            if (carrier is NoopDistributedContextCarrier || entries is null || entries.Count() == 0)\n            {\n                this.entries = emptyList;\n            }\n            else\n            {\n                \/\/ Filter the default and duplicate entries.\n                List<DistributedContextEntry> list = new List<DistributedContextEntry>(entries.Count());\n                for (int i = 0; i < entries.Count(); i++)\n                {\n                    DistributedContextEntry entry = entries.ElementAt(i);\n                    if (entry == default)\n                    {\n                        continue;\n                    }\n\n                    int j;\n                    for (j = entries.Count() - 1; j > i; j--)\n                    {\n                        if (entry.Key == entries.ElementAt(j).Key)\n                        {\n                            break;\n                        }\n                    }\n\n                    if (j <= i)\n                    {\n                        list.Add(entry);\n                    }\n                }\n\n                this.entries = list;\n            }\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"DistributedContext\"\/> struct.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"key\">The key of the context entry.<\/param>\n        \/\/\/ <param name=\"value\">The value of the context entry.<\/param>\n        public DistributedContext(string key, string value)\n        {\n            if (key is null || value is null)\n            {\n                throw new ArgumentNullException(key is null ? nameof(key) : nameof(value));\n            }\n\n            this.entries = carrier is NoopDistributedContextCarrier ? emptyList : new List<DistributedContextEntry>(1) { new DistributedContextEntry(key, value) };\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"DistributedContext\"\/> struct.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"entry\">The distributed context entry.<\/param>\n        public DistributedContext(DistributedContextEntry entry)\n        {\n            this.entries = carrier is NoopDistributedContextCarrier || entry == default ? emptyList : new List<DistributedContextEntry>(1) { entry };\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets empty object of <see cref=\"DistributedContext\"\/> struct.\n        \/\/\/ <\/summary>\n        public static DistributedContext Empty { get; } = new DistributedContext(emptyList);\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the current <see cref=\"DistributedContext\"\/>.\n        \/\/\/ <\/summary>\n        public static DistributedContext Current => carrier.Current;\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets the default carrier instance of the <see cref=\"DistributedContextCarrier\"\/> class.\n        \/\/\/ SDK will need to override the value to AsyncLocalDistributedContextCarrier.Instance.\n        \/\/\/ <\/summary>\n        public static DistributedContextCarrier Carrier\n        {\n            get => carrier;\n            set\n            {\n                if (value is null)\n                {\n                    throw new ArgumentNullException(nameof(value));\n                }\n\n                carrier = value;\n            }\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets all the <see cref=\"DistributedContextEntry\"\/> in this <see cref=\"DistributedContext\"\/>.\n        \/\/\/ <\/summary>\n        public IEnumerable<DistributedContextEntry> Entries => this.entries;\n\n        \/\/\/ <summary>\n        \/\/\/ Sets the current <see cref=\"DistributedContext\"\/>.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"context\">Context to set as current.<\/param>\n        \/\/\/ <returns>Scope object. On disposal - original context will be restored.<\/returns>\n        public static IDisposable SetCurrent(in DistributedContext context) => carrier.SetCurrent(context);\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the <see cref=\"DistributedContextEntry\"\/> with the specified name.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"key\">Name of the <see cref=\"DistributedContextEntry\"\/> to get.<\/param>\n        \/\/\/ <returns>The <see cref=\"DistributedContextEntry\"\/> with the specified name. If not found - null.<\/returns>\n        public string GetEntryValue(string key) => this.entries.LastOrDefault(x => x.Key == key).Value;\n\n        \/\/\/ <inheritdoc\/>\n        public bool Equals(DistributedContext other)\n        {\n            if (this.entries.Count() != other.entries.Count())\n            {\n                return false;\n            }\n\n            foreach (DistributedContextEntry entry in this.entries)\n            {\n                if (other.GetEntryValue(entry.Key) != entry.Value)\n                {\n                    return false;\n                }\n            }\n\n            return true;\n        }\n    }\n}\n","lang_cluster":"C#","length":164,"code_uid":"3bcc224535ea45ff9bb15a1c63dac190"}
{"diff_hunk":"@@ -3,6 +3,7 @@\n \n using System;\n using System.IO;\n+using System.Net;\n using Microsoft.AspNetCore.Builder;\n using Microsoft.AspNetCore.Hosting;\n using Microsoft.AspNetCore.Http;","old_code":"\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System;\nusing System.IO;\nusing Microsoft.AspNetCore.Builder;\nusing Microsoft.AspNetCore.Hosting;\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.Extensions.Logging;\n\nnamespace SampleApp\n{\n    public class Startup\n    {\n        public void Configure(IApplicationBuilder app, ILoggerFactory loggerFactory)\n        {\n            loggerFactory.AddConsole(LogLevel.Trace);\n            var logger = loggerFactory.CreateLogger(\"Default\");\n\n            app.Run(async context =>\n            {\n                var connectionFeature = context.Connection;\n                logger.LogDebug($\"Peer: {connectionFeature.RemoteIpAddress?.ToString()}:{connectionFeature.RemotePort}\"\n                    + $\"{Environment.NewLine}\"\n                    + $\"Sock: {connectionFeature.LocalIpAddress?.ToString()}:{connectionFeature.LocalPort}\");\n\n                var response = $\"hello, world{Environment.NewLine}\";\n                context.Response.ContentLength = response.Length;\n                context.Response.ContentType = \"text\/plain\";\n                await context.Response.WriteAsync(response);\n            });\n        }\n\n        public static void Main(string[] args)\n        {\n            var host = new WebHostBuilder()\n                .UseKestrel(options =>\n                {\n                    \/\/ options.ThreadCount = 4;\n                    options.NoDelay = true;\n                    options.UseHttps(\"testCert.pfx\", \"testPassword\");\n                    options.UseConnectionLogging();\n                })\n                .UseUrls(\"http:\/\/localhost:5000\", \"https:\/\/localhost:5001\")\n                .UseContentRoot(Directory.GetCurrentDirectory())\n                .UseStartup<Startup>()\n                .Build();\n\n            \/\/ The following section should be used to demo sockets\n            \/\/var addresses = application.GetAddresses();\n            \/\/addresses.Clear();\n            \/\/addresses.Add(\"http:\/\/unix:\/tmp\/kestrel-test.sock\");\n\n            host.Run();\n        }\n    }\n}","lang_cluster":"C#","length":57,"code_uid":"97dc0b8a36e543eeb6a2e8767e507c90"}
{"diff_hunk":"@@ -16,6 +16,7 @@\n \/\/ \n \n using System.Collections.Generic;\n+using Nethermind.Abi;\n using Nethermind.Consensus.Transactions;\n using Nethermind.Core;\n ","old_code":"\ufeff\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\/\/ \n\nusing System.Collections.Generic;\nusing Nethermind.Consensus.Transactions;\nusing Nethermind.Core;\n\nnamespace Nethermind.Consensus.AuRa.Validators\n{\n    public partial class ContractBasedValidator : ITxSource\n    {\n        private readonly long _posdaoTransition;\n        \n        public IEnumerable<Transaction> GetTransactions(BlockHeader parent, long gasLimit)\n        {\n            if (ForSealing)\n            {\n                var newBlockNumber = parent.Number + 1;\n                if (newBlockNumber < _posdaoTransition)\n                {\n                    if (_logger.IsTrace) _logger.Trace(\"Skipping a call to emitInitiateChange\");\n                }\n                else\n                {\n                    bool emitInitChangeCallable = false;\n\n                    try\n                    {\n                        emitInitChangeCallable = ValidatorContract.EmitInitiateChangeCallable(parent);\n                    }\n                    catch (AuRaException e)\n                    {\n                        if (_logger.IsError) _logger.Error($\"Call to {nameof(ValidatorContract.EmitInitiateChangeCallable)} failed.\", e);\n                    }\n\n                    if (emitInitChangeCallable)\n                    {\n                        if (_logger.IsTrace) _logger.Trace($\"New block #{newBlockNumber} issued \u2015 calling emitInitiateChange()\");\n                        Metrics.EmitInitiateChange++;\n                        yield return ValidatorContract.EmitInitiateChange();\n                    }\n                    else\n                    {\n                        if (_logger.IsTrace) _logger.Trace($\"New block #{newBlockNumber} issued \u2015 no need to call emitInitiateChange()\");\n                    }\n                }\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":64,"code_uid":"2410dae4a277461dbe8591ed2344e5b3"}
{"diff_hunk":"@@ -100,15 +100,15 @@ namespace Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.DataCollection\n             bool isRunStartingNow,\n             ITestMessageEventHandler runEventsHandler)\n         {\n-            bool areTestCaseLevelEventsRequired = false;\n-            bool isDataCollectionStarted = false;\n+            var areTestCaseLevelEventsRequired = false;\n+            var isDataCollectionStarted = false;\n             IDictionary<string, string> environmentVariables = null;\n \n             var dataCollectionEventsPort = 0;\n             this.InvokeDataCollectionServiceAction(\n             () =>\n             {\n-                var result = this.dataCollectionRequestSender.SendBeforeTestRunStartAndGetResult(settingsXml);\n+                var result = this.dataCollectionRequestSender.SendBeforeTestRunStartAndGetResult(this.settingsXml, runEventsHandler);\n                 areTestCaseLevelEventsRequired = result.AreTestCaseLevelEventsRequired;\n                 environmentVariables = result.EnvironmentVariables;\n                 dataCollectionEventsPort = result.DataCollectionEventsPort;","old_code":"\/\/ Copyright (c) Microsoft Corporation. All rights reserved.\n\/\/ Licensed under the MIT license. See LICENSE file in the project root for full license information.\n\nnamespace Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.DataCollection\n{\n    using System;\n    using System.Collections.Generic;\n    using System.Collections.ObjectModel;\n\n    using Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.DataCollection.Interfaces;\n    using Microsoft.VisualStudio.TestPlatform.ObjectModel;\n    using Microsoft.VisualStudio.TestPlatform.ObjectModel.Client;\n    using Microsoft.VisualStudio.TestPlatform.CommunicationUtilities.DataCollection.Interfaces;\n    using Microsoft.VisualStudio.TestPlatform.CommunicationUtilities.DataCollection;\n\n    \/\/\/ <summary>\n    \/\/\/ The test data collection client.\n    \/\/\/ <\/summary>\n    internal class ProxyDataCollectionManager : IProxyDataCollectionManager\n    {\n        private const string PortOption = \"--port\";\n\n        private IDataCollectionRequestSender dataCollectionRequestSender;\n        private IDataCollectionLauncher dataCollectionLauncher;\n        private string settingsXml;\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"ProxyDataCollectionManager\"\/> class.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"arch\">\n        \/\/\/ The arch.\n        \/\/\/ <\/param>\n        \/\/\/ <param name=\"settingsXml\">\n        \/\/\/ The settings Xml.\n        \/\/\/ <\/param>\n        public ProxyDataCollectionManager(Architecture arch, string settingsXml)\n            : this(arch, settingsXml, new DataCollectionRequestSender(), new DataCollectionLauncher())\n        {\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"ProxyDataCollectionManager\"\/> class.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"dataCollectionRequestSender\">\n        \/\/\/ The data collection request sender.\n        \/\/\/ <\/param>\n        \/\/\/ <param name=\"dataCollectionLauncher\">\n        \/\/\/ The data collection launcher.\n        \/\/\/ <\/param>\n        internal ProxyDataCollectionManager(Architecture arch, string settingsXml, IDataCollectionRequestSender dataCollectionRequestSender, IDataCollectionLauncher dataCollectionLauncher)\n        {\n            this.settingsXml = settingsXml;\n            this.dataCollectionRequestSender = dataCollectionRequestSender;\n            this.dataCollectionLauncher = dataCollectionLauncher;\n            this.InitializeSocketCommunication(arch);\n        }\n\n\n        \/\/\/ <summary>\n        \/\/\/ Invoked after ending of test run\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"isCanceled\">\n        \/\/\/ The is Canceled.\n        \/\/\/ <\/param>\n        \/\/\/ <param name=\"runEventsHandler\">\n        \/\/\/ The run Events Handler.\n        \/\/\/ <\/param>\n        \/\/\/ <returns>\n        \/\/\/ The <see cref=\"Collection\"\/>.\n        \/\/\/ <\/returns>\n        public Collection<AttachmentSet> AfterTestRunEnd(bool isCanceled, ITestMessageEventHandler runEventsHandler)\n        {\n            Collection<AttachmentSet> attachmentSet = null;\n            this.InvokeDataCollectionServiceAction(\n           () =>\n           {\n               attachmentSet = this.dataCollectionRequestSender.SendAfterTestRunStartAndGetResult();\n           },\n                runEventsHandler);\n            return attachmentSet;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Invoked before starting of test run\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"resetDataCollectors\">\n        \/\/\/ The reset Data Collectors.\n        \/\/\/ <\/param>\n        \/\/\/ <param name=\"isRunStartingNow\">\n        \/\/\/ The is Run Starting Now.\n        \/\/\/ <\/param>\n        \/\/\/ <param name=\"runEventsHandler\">\n        \/\/\/ The run Events Handler.\n        \/\/\/ <\/param>\n        \/\/\/ <returns>\n        \/\/\/ BeforeTestRunStartResult object\n        \/\/\/ <\/returns>\n        public DataCollectionParameters BeforeTestRunStart(\n            bool resetDataCollectors,\n            bool isRunStartingNow,\n            ITestMessageEventHandler runEventsHandler)\n        {\n            bool areTestCaseLevelEventsRequired = false;\n            bool isDataCollectionStarted = false;\n            IDictionary<string, string> environmentVariables = null;\n\n            var dataCollectionEventsPort = 0;\n            this.InvokeDataCollectionServiceAction(\n            () =>\n            {\n                var result = this.dataCollectionRequestSender.SendBeforeTestRunStartAndGetResult(settingsXml);\n                areTestCaseLevelEventsRequired = result.AreTestCaseLevelEventsRequired;\n                environmentVariables = result.EnvironmentVariables;\n                dataCollectionEventsPort = result.DataCollectionEventsPort;\n            },\n                runEventsHandler);\n            return new DataCollectionParameters(\n                            isDataCollectionStarted,\n                            areTestCaseLevelEventsRequired,\n                            environmentVariables,\n                            dataCollectionEventsPort);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ The dispose.\n        \/\/\/ <\/summary>\n        public void Dispose()\n        {\n            this.dataCollectionRequestSender.Close();\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ The initialize socket communication.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"arch\">\n        \/\/\/ The arch.\n        \/\/\/ <\/param>\n        internal void InitializeSocketCommunication(Architecture arch)\n        {\n            var port = this.dataCollectionRequestSender.InitializeCommunication();\n\n            this.dataCollectionLauncher.Initialize(arch);\n            this.dataCollectionLauncher.LaunchDataCollector(null, this.GetCommandLineArguments(port));\n            this.dataCollectionRequestSender.WaitForRequestHandlerConnection(connectionTimeout: 5000);\n        }\n\n        private void InvokeDataCollectionServiceAction(Action action, ITestMessageEventHandler runEventsHandler)\n        {\n            try\n            {\n                if (EqtTrace.IsVerboseEnabled)\n                {\n                    EqtTrace.Verbose(\"ProxyDataCollectionManager.InvokeDataCollectionServiceAction: Starting.\");\n                }\n\n                action();\n                if (EqtTrace.IsInfoEnabled)\n                {\n                    EqtTrace.Info(\"ProxyDataCollectionManager.InvokeDataCollectionServiceAction: Completed.\");\n                }\n            }\n            catch (Exception ex)\n            {\n                if (EqtTrace.IsWarningEnabled)\n                {\n                    EqtTrace.Warning(\"ProxyDataCollectionManager.InvokeDataCollectionServiceAction: TestPlatformException = {0}.\", ex);\n                }\n\n                this.HandleExceptionMessage(runEventsHandler, ex);\n            }\n        }\n\n        private void HandleExceptionMessage(ITestMessageEventHandler runEventsHandler, Exception exception)\n        {\n            if (EqtTrace.IsErrorEnabled)\n            {\n                EqtTrace.Error(exception);\n            }\n\n            runEventsHandler.HandleLogMessage(ObjectModel.Logging.TestMessageLevel.Error, exception.Message);\n        }\n\n        private IList<string> GetCommandLineArguments(int portNumber)\n        {\n            var commandlineArguments = new List<string>();\n\n            commandlineArguments.Add(PortOption);\n            commandlineArguments.Add(portNumber.ToString());\n\n            return commandlineArguments;\n        }\n    }\n}","lang_cluster":"C#","length":193,"code_uid":"39703b71bc4c48fdaaefd31827803791"}
{"diff_hunk":"@@ -42,7 +42,7 @@ namespace OpenTelemetry.Context.Propagation\n         void Inject<T>(PropagationContext context, T carrier, Action<T, string, string> setter);\n \n         \/\/\/ <summary>\n-        \/\/\/ Extracts activity context from textual representation.\n+        \/\/\/ Extracts the context from a carrier.\n         \/\/\/ <\/summary>\n         \/\/\/ <typeparam name=\"T\">Type of object to extract context from. Typically HttpRequest or similar.<\/typeparam>\n         \/\/\/ <param name=\"context\">The default context to be used if Extract fails.<\/param>","old_code":"\/\/ <copyright file=\"IPropagator.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Collections.Generic;\n\nnamespace OpenTelemetry.Context.Propagation\n{\n    \/\/\/ <summary>\n    \/\/\/ Text format wire context propagator. Helps to extract and inject context from textual\n    \/\/\/ representation (typically http headers or metadata collection).\n    \/\/\/ <\/summary>\n    public interface IPropagator\n    {\n        \/\/\/ <summary>\n        \/\/\/ Gets the list of headers used by propagator. The use cases of this are:\n        \/\/\/   * allow pre-allocation of fields, especially in systems like gRPC Metadata\n        \/\/\/   * allow a single-pass over an iterator (ex OpenTracing has no getter in TextMap).\n        \/\/\/ <\/summary>\n        ISet<string> Fields { get; }\n\n        \/\/\/ <summary>\n        \/\/\/ Injects textual representation of activity context to transmit over the wire.\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"T\">Type of an object to set context on. Typically HttpRequest or similar.<\/typeparam>\n        \/\/\/ <param name=\"context\">The default context to transmit over the wire.<\/param>\n        \/\/\/ <param name=\"carrier\">Object to set context on. Instance of this object will be passed to setter.<\/param>\n        \/\/\/ <param name=\"setter\">Action that will set name and value pair on the object.<\/param>\n        void Inject<T>(PropagationContext context, T carrier, Action<T, string, string> setter);\n\n        \/\/\/ <summary>\n        \/\/\/ Extracts activity context from textual representation.\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"T\">Type of object to extract context from. Typically HttpRequest or similar.<\/typeparam>\n        \/\/\/ <param name=\"context\">The default context to be used if Extract fails.<\/param>\n        \/\/\/ <param name=\"carrier\">Object to extract context from. Instance of this object will be passed to the getter.<\/param>\n        \/\/\/ <param name=\"getter\">Function that will return string value of a key with the specified name.<\/param>\n        \/\/\/ <returns>Context from it's text representation.<\/returns>\n        PropagationContext Extract<T>(PropagationContext context, T carrier, Func<T, string, IEnumerable<string>> getter);\n    }\n}\n","lang_cluster":"C#","length":54,"code_uid":"95f2152a4e864dfea68d4823b967173a"}
{"diff_hunk":"@@ -31,9 +31,7 @@ namespace Nethermind.Db\n         public IDb ChtDb { get; } = new MemDb();\n         public IDb BeamStateDb { get; } = new MemDb();\n \n-        public IDb BaselineTreeDb { get; } = new MemDb();\n-\n-        public IDb BaselineTreeMetadataDb { get; } = new MemDb();\n+        public IEnumerable<IDb> OtherDbs => _otherDbs;\n \n         public void Dispose()\n         {","old_code":"\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nnamespace Nethermind.Db\n{\n    public class MemDbProvider : IDbProvider\n    {\n        public ISnapshotableDb StateDb { get; } = new StateDb();\n        public ISnapshotableDb CodeDb { get; } = new StateDb();\n        public IColumnsDb<ReceiptsColumns> ReceiptsDb { get; } = new MemColumnsDb<ReceiptsColumns>();\n        public IDb BlocksDb { get; } = new MemDb();\n        public IDb HeadersDb { get; } = new MemDb();\n        public IDb BlockInfosDb { get; } = new MemDb();\n        public IDb PendingTxsDb { get; } = new MemDb();\n        public IDb ConfigsDb { get; } = new MemDb();\n        public IDb EthRequestsDb { get; } = new MemDb();\n        public IDb BloomDb { get; } = new MemDb();\n        public IDb ChtDb { get; } = new MemDb();\n        public IDb BeamStateDb { get; } = new MemDb();\n\n        public IDb BaselineTreeDb { get; } = new MemDb();\n\n        public IDb BaselineTreeMetadataDb { get; } = new MemDb();\n\n        public void Dispose()\n        {\n            StateDb?.Dispose();\n            CodeDb?.Dispose();\n            ReceiptsDb?.Dispose();\n            BlocksDb?.Dispose();\n            BlockInfosDb?.Dispose();\n            PendingTxsDb?.Dispose();\n            ConfigsDb?.Dispose();\n            EthRequestsDb?.Dispose();\n            BloomDb?.Dispose();\n            ChtDb?.Dispose();\n            BaselineTreeDb?.Dispose();\n            BaselineTreeMetadataDb?.Dispose();\n        }\n    }\n}\n","lang_cluster":"C#","length":54,"code_uid":"35df634c86a6421d89e8d70d6602f899"}
{"diff_hunk":"@@ -20,7 +20,7 @@ namespace Datadog.Trace.ClrProfiler.IntegrationTests.AdoNet\n         {\n             foreach (object[] item in PackageVersions.MySqlData)\n             {\n-                if ((string)item[0] == string.Empty || !((string)item[0]).StartsWith(\"8\"))\n+                if (!((string)item[0]).StartsWith(\"8\"))\n                 {\n                     continue;\n                 }","old_code":"using System.Collections.Generic;\nusing System.Linq;\nusing Datadog.Core.Tools;\nusing Datadog.Trace.Configuration;\nusing Datadog.Trace.TestHelpers;\nusing Xunit;\nusing Xunit.Abstractions;\n\nnamespace Datadog.Trace.ClrProfiler.IntegrationTests.AdoNet\n{\n    public class MySqlCommandTests : TestHelper\n    {\n        public MySqlCommandTests(ITestOutputHelper output)\n            : base(\"MySql\", output)\n        {\n            SetServiceVersion(\"1.0.0\");\n        }\n\n        public static IEnumerable<object[]> GetMySql8Data()\n        {\n            foreach (object[] item in PackageVersions.MySqlData)\n            {\n                if ((string)item[0] == string.Empty || !((string)item[0]).StartsWith(\"8\"))\n                {\n                    continue;\n                }\n\n                yield return item.Concat(new object[] { false, false, }).ToArray();\n                yield return item.Concat(new object[] { true, false, }).ToArray();\n                yield return item.Concat(new object[] { true, true, }).ToArray();\n            }\n        }\n\n        public static IEnumerable<object[]> GetOldMySqlData()\n        {\n            foreach (object[] item in PackageVersions.MySqlData)\n            {\n                if ((string)item[0] == string.Empty || ((string)item[0]).StartsWith(\"8\"))\n                {\n                    continue;\n                }\n\n                yield return item.Concat(new object[] { false, false, }).ToArray();\n                yield return item.Concat(new object[] { true, false, }).ToArray();\n                yield return item.Concat(new object[] { true, true, }).ToArray();\n            }\n        }\n\n        [Theory]\n        [MemberData(nameof(GetMySql8Data))]\n        [Trait(\"Category\", \"EndToEnd\")]\n        public void SubmitsTracesWithNetStandardInMySql8(string packageVersion, bool enableCallTarget, bool enableInlining)\n        {\n            SubmitsTracesWithNetStandard(packageVersion, enableCallTarget, enableInlining);\n        }\n\n        [Theory]\n        [MemberData(nameof(GetOldMySqlData))]\n        [Trait(\"Category\", \"EndToEnd\")]\n        [Trait(\"Category\", \"ArmUnsupported\")]\n        public void SubmitsTracesWithNetStandardInOldMySql(string packageVersion, bool enableCallTarget, bool enableInlining)\n        {\n            SubmitsTracesWithNetStandard(packageVersion, enableCallTarget, enableInlining);\n        }\n\n        [Theory]\n        [InlineData(false, false)]\n        [InlineData(true, false)]\n        [InlineData(true, true)]\n        [Trait(\"Category\", \"EndToEnd\")]\n        public void SpansDisabledByAdoNetExcludedTypes(bool enableCallTarget, bool enableInlining)\n        {\n            SetCallTargetSettings(enableCallTarget, enableInlining);\n\n            var totalSpanCount = 21;\n\n            const string dbType = \"mysql\";\n            const string expectedOperationName = dbType + \".query\";\n\n            SetEnvironmentVariable(ConfigurationKeys.AdoNetExcludedTypes, \"System.Data.SqlClient.SqlCommand;Microsoft.Data.SqlClient.SqlCommand;MySql.Data.MySqlClient.MySqlCommand;Npgsql.NpgsqlCommand\");\n\n            int agentPort = TcpPortProvider.GetOpenPort();\n\n            using (var agent = new MockTracerAgent(agentPort))\n            using (ProcessResult processResult = RunSampleAndWaitForExit(agent.Port))\n            {\n                Assert.True(processResult.ExitCode >= 0, $\"Process exited with code {processResult.ExitCode}\");\n\n                var spans = agent.WaitForSpans(totalSpanCount, returnAllOperations: true);\n                Assert.NotEmpty(spans);\n                Assert.Empty(spans.Where(s => s.Name.Equals(expectedOperationName)));\n            }\n        }\n\n        private void SubmitsTracesWithNetStandard(string packageVersion, bool enableCallTarget, bool enableInlining)\n        {\n            SetCallTargetSettings(enableCallTarget, enableInlining);\n\n            \/\/ Note: The automatic instrumentation currently bails out on the generic wrappers.\n            \/\/ Once this is implemented, this will add another 1 group for the direct assembly reference\n            \/\/ and another 1 group for the netstandard assembly reference\n#if NET452\n            var expectedSpanCount = 50; \/\/ 7 queries * 7 groups + 1 internal query\n#else\n            var expectedSpanCount = 78; \/\/ 7 queries * 11 groups + 1 internal query\n            if (packageVersion == \"6.8.8\")\n            {\n                expectedSpanCount = 76; \/\/ For this version the callsite instrumentation returns 2 spans less.\n            }\n#endif\n\n            if (enableCallTarget)\n            {\n#if NET452\n                expectedSpanCount = 62;\n#else\n                expectedSpanCount = 97;\n#endif\n            }\n\n            const string dbType = \"mysql\";\n            const string expectedOperationName = dbType + \".query\";\n            const string expectedServiceName = \"Samples.MySql-\" + dbType;\n\n            \/\/ NOTE: opt into the additional instrumentation of calls into netstandard.dll\n            SetEnvironmentVariable(\"DD_TRACE_NETSTANDARD_ENABLED\", \"true\");\n\n            int agentPort = TcpPortProvider.GetOpenPort();\n\n            using (var agent = new MockTracerAgent(agentPort))\n            using (ProcessResult processResult = RunSampleAndWaitForExit(agent.Port, packageVersion: packageVersion))\n            {\n                Assert.True(processResult.ExitCode >= 0, $\"Process exited with code {processResult.ExitCode}\");\n\n                var spans = agent.WaitForSpans(expectedSpanCount, operationName: expectedOperationName);\n                Assert.Equal(expectedSpanCount, spans.Count);\n\n                foreach (var span in spans)\n                {\n                    Assert.Equal(expectedOperationName, span.Name);\n                    Assert.Equal(expectedServiceName, span.Service);\n                    Assert.Equal(SpanTypes.Sql, span.Type);\n                    Assert.Equal(dbType, span.Tags[Tags.DbType]);\n                    Assert.False(span.Tags?.ContainsKey(Tags.Version), \"External service span should not have service version tag.\");\n                }\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":149,"code_uid":"f80122f2643642258722d825374e1e35"}
{"diff_hunk":"@@ -14,6 +14,8 @@\n \/\/  You should have received a copy of the GNU Lesser General Public License\n \/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n \n+using Nethermind.Core;\n+\n namespace Nethermind.KeyStore.Config\n {\n     public class KeyStoreConfig : IKeyStoreConfig","old_code":"\ufeff\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nnamespace Nethermind.KeyStore.Config\n{\n    public class KeyStoreConfig : IKeyStoreConfig\n    {\n        public string KeyStoreDirectory { get; set; } = \"keystore\";\n        public string KeyStoreEncoding { get; set; } = \"UTF-8\";\n        public string Kdf { get; set; } = \"scrypt\";\n        public string Cipher { get; set; } = \"aes-128-ctr\";\n        public int KdfparamsDklen { get; set; } = 32;\n        public int KdfparamsN { get; set; } = 262144;\n        public int KdfparamsP { get; set; } = 1;\n        public int KdfparamsR { get; set; } = 8;\n        \n        public int KdfparamsSaltLen { get; set; } = 32;\n        public int SymmetricEncrypterBlockSize { get; set; } = 128;\n        public int SymmetricEncrypterKeySize { get; set; } = 128;\n        public int IVSize { get; set; } = 16;\n        public string TestNodeKey { get; set; }\n    }\n}","lang_cluster":"C#","length":36,"code_uid":"749859fc392149afad46925318a80a49"}
{"diff_hunk":"@@ -112,6 +112,13 @@ namespace Datadog.Trace.Tests.Logging\n             Assert.DoesNotContain(CorrelationIdentifier.TraceIdKey, logEvent.Properties.GetKeys());\n             Assert.DoesNotContain(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n \n+            \/\/ Scope: N\/A\n+            \/\/ Custom property: N\/A\n+            logEvent = filteredLogs[logIndex++];\n+            Assert.DoesNotContain(CorrelationIdentifier.SpanIdKey, logEvent.Properties.GetKeys());\n+            Assert.DoesNotContain(CorrelationIdentifier.TraceIdKey, logEvent.Properties.GetKeys());\n+            Assert.DoesNotContain(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n+\n             \/\/ Scope: N\/A\n             \/\/ Custom property: SET\n             logEvent = filteredLogs[logIndex++];","old_code":"using System.Collections.Generic;\nusing System.IO;\nusing System.Reflection;\nusing Datadog.Trace.Logging;\nusing Datadog.Trace.Logging.LogProviders;\nusing log4net.Appender;\nusing log4net.Config;\nusing log4net.Core;\nusing log4net.Layout;\nusing Newtonsoft.Json;\nusing Xunit;\n\nnamespace Datadog.Trace.Tests.Logging\n{\n    [Collection(nameof(Datadog.Trace.Tests.Logging))]\n    public class Log4NetLogProviderTests\n    {\n        private readonly ILogProvider _logProvider;\n        private readonly ILog _logger;\n        private readonly MemoryAppender _memoryAppender;\n\n        public Log4NetLogProviderTests()\n        {\n            _memoryAppender = new MemoryAppender();\n            var repository = log4net.LogManager.GetRepository(Assembly.GetAssembly(typeof(log4net.LogManager)));\n            BasicConfigurator.Configure(repository, _memoryAppender);\n\n            _logProvider = new Log4NetLogProvider();\n            LogProvider.SetCurrentLogProvider(_logProvider);\n            _logger = new LoggerExecutionWrapper(_logProvider.GetLogger(\"Test\"));\n        }\n\n        [Fact]\n        public void EnabledLibLogSubscriberAddsTraceData()\n        {\n            \/\/ Assert that the Log4Net log provider is correctly being used\n            Assert.IsType<Log4NetLogProvider>(LogProvider.CurrentLogProvider);\n\n            \/\/ Instantiate a tracer for this test with default settings and set LogsInjectionEnabled to TRUE\n            var tracer = LoggingProviderTestHelpers.InitializeTracer(enableLogsInjection: true);\n            LoggingProviderTestHelpers.PerformParentChildScopeSequence(tracer, _logger, _logProvider.OpenMappedContext, out var parentScope, out var childScope);\n\n            \/\/ Filter the logs\n            List<LoggingEvent> filteredLogs = new List<LoggingEvent>(_memoryAppender.GetEvents());\n            filteredLogs.RemoveAll(log => !log.MessageObject.ToString().Contains(LoggingProviderTestHelpers.LogPrefix));\n\n            int logIndex = 0;\n            LoggingEvent logEvent;\n\n            \/\/ Scope: Parent scope\n            \/\/ Custom property: N\/A\n            logEvent = filteredLogs[logIndex++];\n            logEvent.Contains(parentScope);\n            Assert.DoesNotContain(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n\n            \/\/ Scope: Parent scope\n            \/\/ Custom property: SET\n            logEvent = filteredLogs[logIndex++];\n            logEvent.Contains(parentScope);\n            Assert.Contains(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n            Assert.Equal<int>(LoggingProviderTestHelpers.CustomPropertyValue, int.Parse(logEvent.Properties[LoggingProviderTestHelpers.CustomPropertyName].ToString()));\n\n            \/\/ Scope: Child scope\n            \/\/ Custom property: SET\n            logEvent = filteredLogs[logIndex++];\n            logEvent.Contains(childScope);\n            Assert.Contains(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n            Assert.Equal<int>(LoggingProviderTestHelpers.CustomPropertyValue, int.Parse(logEvent.Properties[LoggingProviderTestHelpers.CustomPropertyName].ToString()));\n\n            \/\/ Scope: Parent scope\n            \/\/ Custom property: SET\n            logEvent = filteredLogs[logIndex++];\n            logEvent.Contains(parentScope);\n            Assert.Contains(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n            Assert.Equal<int>(LoggingProviderTestHelpers.CustomPropertyValue, int.Parse(logEvent.Properties[LoggingProviderTestHelpers.CustomPropertyName].ToString()));\n\n            \/\/ EXISTING: Verify the log event is decorated with the parent scope properties\n            \/\/ Scope: Parent scope\n            \/\/ Custom property: N\/A\n            logEvent = filteredLogs[logIndex++];\n            logEvent.Contains(parentScope);\n            Assert.DoesNotContain(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n\n            \/\/ Scope: Default values of TraceId=0,SpanId=0\n            \/\/ Custom property: N\/A\n            logEvent = filteredLogs[logIndex++];\n            logEvent.Contains(traceId: 0, spanId: 0);\n            Assert.DoesNotContain(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n        }\n\n        [Fact]\n        public void DisabledLibLogSubscriberDoesNotAddTraceData()\n        {\n            \/\/ Assert that the Log4Net log provider is correctly being used\n            Assert.IsType<Log4NetLogProvider>(LogProvider.CurrentLogProvider);\n\n            \/\/ Instantiate a tracer for this test with default settings and set LogsInjectionEnabled to TRUE\n            var tracer = LoggingProviderTestHelpers.InitializeTracer(enableLogsInjection: false);\n            LoggingProviderTestHelpers.PerformParentChildScopeSequence(tracer, _logger, _logProvider.OpenMappedContext, out var parentScope, out var childScope);\n\n            \/\/ Filter the logs\n            List<LoggingEvent> filteredLogs = new List<LoggingEvent>(_memoryAppender.GetEvents());\n            filteredLogs.RemoveAll(log => !log.MessageObject.ToString().Contains(LoggingProviderTestHelpers.LogPrefix));\n\n            int logIndex = 0;\n            LoggingEvent logEvent;\n\n            \/\/ Scope: N\/A\n            \/\/ Custom property: N\/A\n            logEvent = filteredLogs[logIndex++];\n            Assert.DoesNotContain(CorrelationIdentifier.SpanIdKey, logEvent.Properties.GetKeys());\n            Assert.DoesNotContain(CorrelationIdentifier.TraceIdKey, logEvent.Properties.GetKeys());\n            Assert.DoesNotContain(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n\n            \/\/ Scope: N\/A\n            \/\/ Custom property: SET\n            logEvent = filteredLogs[logIndex++];\n            Assert.DoesNotContain(CorrelationIdentifier.SpanIdKey, logEvent.Properties.GetKeys());\n            Assert.DoesNotContain(CorrelationIdentifier.TraceIdKey, logEvent.Properties.GetKeys());\n            Assert.Contains(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n            Assert.Equal<int>(LoggingProviderTestHelpers.CustomPropertyValue, int.Parse(logEvent.Properties[LoggingProviderTestHelpers.CustomPropertyName].ToString()));\n\n            \/\/ Scope: N\/A\n            \/\/ Custom property: SET\n            logEvent = filteredLogs[logIndex++];\n            Assert.DoesNotContain(CorrelationIdentifier.SpanIdKey, logEvent.Properties.GetKeys());\n            Assert.DoesNotContain(CorrelationIdentifier.TraceIdKey, logEvent.Properties.GetKeys());\n            Assert.Contains(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n            Assert.Equal<int>(LoggingProviderTestHelpers.CustomPropertyValue, int.Parse(logEvent.Properties[LoggingProviderTestHelpers.CustomPropertyName].ToString()));\n\n            \/\/ Scope: N\/A\n            \/\/ Custom property: SET\n            logEvent = filteredLogs[logIndex++];\n            Assert.DoesNotContain(CorrelationIdentifier.SpanIdKey, logEvent.Properties.GetKeys());\n            Assert.DoesNotContain(CorrelationIdentifier.TraceIdKey, logEvent.Properties.GetKeys());\n            Assert.Contains(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n            Assert.Equal<int>(LoggingProviderTestHelpers.CustomPropertyValue, int.Parse(logEvent.Properties[LoggingProviderTestHelpers.CustomPropertyName].ToString()));\n\n            \/\/ Scope: N\/A\n            \/\/ Custom property: N\/A\n            logEvent = filteredLogs[logIndex++];\n            Assert.DoesNotContain(CorrelationIdentifier.SpanIdKey, logEvent.Properties.GetKeys());\n            Assert.DoesNotContain(CorrelationIdentifier.TraceIdKey, logEvent.Properties.GetKeys());\n            Assert.DoesNotContain(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n\n            \/\/ Scope: N\/A\n            \/\/ Custom property: N\/A\n            logEvent = filteredLogs[logIndex++];\n            Assert.DoesNotContain(CorrelationIdentifier.SpanIdKey, logEvent.Properties.GetKeys());\n            Assert.DoesNotContain(CorrelationIdentifier.TraceIdKey, logEvent.Properties.GetKeys());\n            Assert.DoesNotContain(LoggingProviderTestHelpers.CustomPropertyName, logEvent.Properties.GetKeys());\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Lightweight JSON-formatter for Log4Net inspired by https:\/\/github.com\/Litee\/log4net.Layout.Json\n        \/\/\/ <\/summary>\n        internal class Log4NetJsonLayout : LayoutSkeleton\n        {\n            public override void ActivateOptions()\n            {\n            }\n\n            public override void Format(TextWriter writer, LoggingEvent e)\n            {\n                var dic = new Dictionary<string, object>\n                {\n                    [\"level\"] = e.Level.DisplayName,\n                    [\"messageObject\"] = e.MessageObject,\n                    [\"renderedMessage\"] = e.RenderedMessage,\n                    [\"timestampUtc\"] = e.TimeStamp.ToUniversalTime().ToString(\"O\"),\n                    [\"logger\"] = e.LoggerName,\n                    [\"thread\"] = e.ThreadName,\n                    [\"exceptionObject\"] = e.ExceptionObject,\n                    [\"exceptionObjectString\"] = e.ExceptionObject == null ? null : e.GetExceptionString(),\n                    [\"userName\"] = e.UserName,\n                    [\"domain\"] = e.Domain,\n                    [\"identity\"] = e.Identity,\n                    [\"location\"] = e.LocationInformation.FullInfo,\n                    [\"properties\"] = e.GetProperties()\n                };\n                writer.Write(JsonConvert.SerializeObject(dic));\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":185,"code_uid":"2dbd3446caee4c73bc78b45040b53dd8"}
{"diff_hunk":"@@ -8,10 +8,11 @@\n namespace MvvmCross.iOS.Views\n {\n     using System;\n-\n+    using Foundation;\n     using MvvmCross.Binding.BindingContext;\n     using MvvmCross.Core.ViewModels;\n     using MvvmCross.Platform.iOS.Views;\n+    using UIKit;\n \n     public class MvxTabBarViewController\n         : MvxEventSourceTabBarController","old_code":"\/\/ MvxTabBarViewController.cs\n\n\/\/ MvvmCross is licensed using Microsoft Public License (Ms-PL)\n\/\/ Contributions and inspirations noted in readme.md and license.txt\n\/\/\n\/\/ Project Lead - Stuart Lodge, @slodge, me@slodge.com\n\nnamespace MvvmCross.iOS.Views\n{\n    using System;\n\n    using MvvmCross.Binding.BindingContext;\n    using MvvmCross.Core.ViewModels;\n    using MvvmCross.Platform.iOS.Views;\n\n    public class MvxTabBarViewController\n        : MvxEventSourceTabBarController\n          , IMvxIosView\n    {\n        protected MvxTabBarViewController()\n        {\n            this.AdaptForBinding();\n        }\n\n        protected MvxTabBarViewController(IntPtr handle)\n            : base(handle)\n        {\n            this.AdaptForBinding();\n        }\n\n        public object DataContext\n        {\n            get\n            {\n                \/\/ special code needed in TabBar because View is initialized during construction\n                return this.BindingContext?.DataContext;\n            }\n            set { this.BindingContext.DataContext = value; }\n        }\n\n        public IMvxViewModel ViewModel\n        {\n            get { return this.DataContext as IMvxViewModel; }\n            set { this.DataContext = value; }\n        }\n\n        public MvxViewModelRequest Request { get; set; }\n\n        public IMvxBindingContext BindingContext { get; set; }\n    }\n\n    public class MvxTabBarViewController<TViewModel>\n        : MvxTabBarViewController\n          , IMvxIosView<TViewModel> where TViewModel : class, IMvxViewModel\n    {\n        protected MvxTabBarViewController()\n        {\n        }\n\n        protected MvxTabBarViewController(IntPtr handle)\n            : base(handle)\n        {\n        }\n\n        public new TViewModel ViewModel\n        {\n            get { return (TViewModel)base.ViewModel; }\n            set { base.ViewModel = value; }\n        }\n    }\n}","lang_cluster":"C#","length":71,"code_uid":"f7e2c864d5ac48ea97b22dd2d3d68f5c"}
{"diff_hunk":"@@ -26,8 +26,7 @@ namespace OpenTelemetry.Logs\n     public class OpenTelemetryLoggerProvider : ILoggerProvider, ISupportExternalScope\n     {\n         internal BaseProcessor<LogRecord> Processor;\n-        private readonly OpenTelemetryLoggerOptions options;\n-        private readonly IDictionary<string, ILogger> loggers;\n+        private readonly IDictionary<string, OpenTelemetryLogger> loggers = new Dictionary<string, OpenTelemetryLogger>(StringComparer.Ordinal);\n         private bool disposed;\n         private IExternalScopeProvider scopeProvider;\n ","old_code":"\/\/ <copyright file=\"OpenTelemetryLoggerProvider.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\n#if NET461 || NETSTANDARD2_0\nusing System;\nusing System.Collections.Generic;\nusing Microsoft.Extensions.Logging;\nusing Microsoft.Extensions.Options;\n\nnamespace OpenTelemetry.Logs\n{\n    [ProviderAlias(\"OpenTelemetry\")]\n    public class OpenTelemetryLoggerProvider : ILoggerProvider, ISupportExternalScope\n    {\n        internal BaseProcessor<LogRecord> Processor;\n        private readonly OpenTelemetryLoggerOptions options;\n        private readonly IDictionary<string, ILogger> loggers;\n        private bool disposed;\n        private IExternalScopeProvider scopeProvider;\n\n        static OpenTelemetryLoggerProvider()\n        {\n            \/\/ Accessing Sdk class is just to trigger its static ctor,\n            \/\/ which sets default Propagators and default Activity Id format\n            _ = Sdk.SuppressInstrumentation;\n        }\n\n        public OpenTelemetryLoggerProvider(IOptionsMonitor<OpenTelemetryLoggerOptions> options)\n            : this(options?.CurrentValue)\n        {\n        }\n\n        internal OpenTelemetryLoggerProvider(OpenTelemetryLoggerOptions options)\n        {\n            this.options = options ?? throw new ArgumentNullException(nameof(options));\n            this.loggers = new Dictionary<string, ILogger>(StringComparer.Ordinal);\n\n            foreach (var processor in options.Processors)\n            {\n                this.AddProcessor(processor);\n            }\n        }\n\n        internal IExternalScopeProvider ScopeProvider\n        {\n            get\n            {\n                if (this.scopeProvider == null)\n                {\n                    this.scopeProvider = new LoggerExternalScopeProvider();\n                }\n\n                return this.scopeProvider;\n            }\n        }\n\n        void ISupportExternalScope.SetScopeProvider(IExternalScopeProvider scopeProvider)\n        {\n            \/\/ TODO: set existing loggers\n            this.scopeProvider = scopeProvider;\n        }\n\n        public ILogger CreateLogger(string categoryName)\n        {\n            lock (this.loggers)\n            {\n                ILogger logger;\n\n                if (this.loggers.TryGetValue(categoryName, out logger))\n                {\n                    return logger;\n                }\n\n                logger = new OpenTelemetryLogger(categoryName, this);\n                this.loggers.Add(categoryName, logger);\n                return logger;\n            }\n        }\n\n        \/\/\/ <inheritdoc\/>\n        public void Dispose()\n        {\n            this.Dispose(true);\n            GC.SuppressFinalize(this);\n        }\n\n        internal OpenTelemetryLoggerProvider AddProcessor(BaseProcessor<LogRecord> processor)\n        {\n            if (processor == null)\n            {\n                throw new ArgumentNullException(nameof(processor));\n            }\n\n            if (this.Processor == null)\n            {\n                this.Processor = processor;\n            }\n            else if (this.Processor is CompositeProcessor<LogRecord> compositeProcessor)\n            {\n                compositeProcessor.AddProcessor(processor);\n            }\n            else\n            {\n                this.Processor = new CompositeProcessor<LogRecord>(new[]\n                {\n                    this.Processor,\n                    processor,\n                });\n            }\n\n            return this;\n        }\n\n        protected virtual void Dispose(bool disposing)\n        {\n            if (this.disposed)\n            {\n                return;\n            }\n\n            if (disposing)\n            {\n                \/\/ Wait for up to 5 seconds grace period\n                this.Processor?.Shutdown(5000);\n                this.Processor?.Dispose();\n            }\n\n            this.disposed = true;\n        }\n    }\n}\n#endif\n","lang_cluster":"C#","length":145,"code_uid":"90f04a069a9a4a08b1ae5e525ed53bee"}
{"diff_hunk":"@@ -17,12 +17,17 @@\n using System;\n using System.Collections.Concurrent;\n using System.Collections.Generic;\n+using System.IO;\n+using System.Linq;\n+using System.Timers;\n+using Nethermind.Core.Caching;\n+using Nethermind.Core.Timers;\n using Nethermind.Logging;\n using Nethermind.Stats.Model;\n \n namespace Nethermind.Stats\n {\n-    public class NodeStatsManager : INodeStatsManager\n+    public class NodeStatsManager : INodeStatsManager, IDisposable\n     {\n         private class NodeComparer : IEqualityComparer<Node>\n         {","old_code":"\ufeff\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing System.Collections.Concurrent;\nusing System.Collections.Generic;\nusing Nethermind.Logging;\nusing Nethermind.Stats.Model;\n\nnamespace Nethermind.Stats\n{\n    public class NodeStatsManager : INodeStatsManager\n    {\n        private class NodeComparer : IEqualityComparer<Node>\n        {\n            public bool Equals(Node x, Node y)\n            {\n                if (ReferenceEquals(x, null))\n                {\n                    return ReferenceEquals(y, null);\n                }\n\n                if (ReferenceEquals(y, null))\n                {\n                    return false;\n                }\n\n                return x.Id == y.Id;\n            }\n\n            public int GetHashCode(Node obj)\n            {\n                return obj?.GetHashCode() ?? 0;\n            }\n        }\n        \n        private readonly ILogger _logger;\n        private readonly ConcurrentDictionary<Node, INodeStats> _nodeStats = new ConcurrentDictionary<Node, INodeStats>(new NodeComparer());\n\n        public NodeStatsManager(ILogManager logManager)\n        {\n            _logger = logManager?.GetClassLogger() ?? throw new ArgumentNullException(nameof(logManager));\n        }\n\n        private INodeStats AddStats(Node node)\n        {\n            return new NodeStatsLight(node);\n        }\n        \n        public INodeStats GetOrAdd(Node node)\n        {\n            if (node == null)\n            {\n                return null;\n            }\n\n            \/\/ to avoid allocations\n            if (_nodeStats.TryGetValue(node, out INodeStats stats))\n            {\n                return stats;\n            }\n            \n            return _nodeStats.GetOrAdd(node, AddStats);\n        }\n\n        public void ReportHandshakeEvent(Node node, ConnectionDirection direction)\n        {\n            INodeStats stats = GetOrAdd(node);\n            stats.AddNodeStatsHandshakeEvent(direction);\n        }\n\n        public void ReportSyncEvent(Node node, NodeStatsEventType nodeStatsEvent)\n        {\n            INodeStats stats = GetOrAdd(node);\n            stats.AddNodeStatsSyncEvent(nodeStatsEvent);\n        }\n        \n        public void ReportEvent(Node node, NodeStatsEventType eventType)\n        {\n            INodeStats stats = GetOrAdd(node);\n            stats.AddNodeStatsEvent(eventType);\n        }\n\n        public (bool Result, NodeStatsEventType? DelayReason) IsConnectionDelayed(Node node)\n        {\n            INodeStats stats = GetOrAdd(node);\n            return stats.IsConnectionDelayed();\n        }\n\n        public CompatibilityValidationType? FindCompatibilityValidationResult(Node node)\n        {\n            INodeStats stats = GetOrAdd(node);\n            return stats.FailedCompatibilityValidation;\n        }\n\n        public long GetCurrentReputation(Node node)\n        {\n            INodeStats stats = GetOrAdd(node);\n            return stats.CurrentNodeReputation;\n        }\n\n        public void ReportP2PInitializationEvent(Node node, P2PNodeDetails p2PNodeDetails)\n        {\n            INodeStats stats = GetOrAdd(node);\n            stats.AddNodeStatsP2PInitializedEvent(p2PNodeDetails);\n        }\n\n        public void ReportSyncPeerInitializeEvent(string protocol, Node node, SyncPeerNodeDetails syncPeerNodeDetails)\n        {\n            INodeStats stats = GetOrAdd(node);\n            if (protocol == \"eth\")\n                stats.AddNodeStatsEth62InitializedEvent(syncPeerNodeDetails);\n            else if (protocol == \"les\")\n                stats.AddNodeStatsLesInitializedEvent(syncPeerNodeDetails);\n            else\n                throw new ArgumentException($\"Unknown protocol: {protocol}\");\n        }\n\n        public void ReportFailedValidation(Node node, CompatibilityValidationType validationType)\n        {\n            INodeStats stats = GetOrAdd(node);\n            stats.FailedCompatibilityValidation = validationType;\n        }\n\n        public void ReportDisconnect(Node node, DisconnectType disconnectType, DisconnectReason disconnectReason)\n        {\n            INodeStats stats = GetOrAdd(node);\n            stats.AddNodeStatsDisconnectEvent(disconnectType, disconnectReason);\n        }\n\n        public long GetNewPersistedReputation(Node node)\n        {\n            INodeStats stats = GetOrAdd(node);\n            return stats.NewPersistedNodeReputation;\n        }\n\n        public long GetCurrentPersistedReputation(Node node)\n        {\n            INodeStats stats = GetOrAdd(node);\n            return stats.CurrentPersistedNodeReputation;\n        }\n\n        public bool HasFailedValidation(Node node)\n        {\n            INodeStats stats = GetOrAdd(node);\n            return stats.FailedCompatibilityValidation != null;\n        }\n\n        public void ReportTransferSpeedEvent(Node node, TransferSpeedType type, long value)\n        {\n            INodeStats stats = GetOrAdd(node);\n            stats.AddTransferSpeedCaptureEvent(type, value);\n        }\n    }\n}\n","lang_cluster":"C#","length":168,"code_uid":"917a71f733214bf99791d5f25f09fe64"}
{"diff_hunk":"@@ -24,7 +24,7 @@ using OpenTelemetry.Internal;\n \n namespace OpenTelemetry.Trace.Internal\n {\n-    internal class BroadcastActivityProcessor : ActivityProcessor, IDisposable\n+    internal class BroadcastActivityProcessor : ActivityProcessor\n     {\n         private readonly IEnumerable<ActivityProcessor> processors;\n         private bool isDisposed;","old_code":"\ufeff\/\/ <copyright file=\"BroadcastActivityProcessor.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.Linq;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing OpenTelemetry.Internal;\n\nnamespace OpenTelemetry.Trace.Internal\n{\n    internal class BroadcastActivityProcessor : ActivityProcessor, IDisposable\n    {\n        private readonly IEnumerable<ActivityProcessor> processors;\n        private bool isDisposed;\n\n        public BroadcastActivityProcessor(IEnumerable<ActivityProcessor> processors)\n        {\n            if (processors == null)\n            {\n                throw new ArgumentNullException(nameof(processors));\n            }\n\n            if (!processors.Any())\n            {\n                throw new ArgumentException($\"{nameof(processors)} collection is empty\");\n            }\n\n            this.processors = processors;\n        }\n\n        public override void OnEnd(Activity activity)\n        {\n            foreach (var processor in this.processors)\n            {\n                try\n                {\n                    processor.OnEnd(activity);\n                }\n                catch (Exception e)\n                {\n                    OpenTelemetrySdkEventSource.Log.SpanProcessorException(\"OnEnd\", e);\n                }\n            }\n        }\n\n        public override void OnStart(Activity activity)\n        {\n            foreach (var processor in this.processors)\n            {\n                try\n                {\n                    processor.OnStart(activity);\n                }\n                catch (Exception e)\n                {\n                    OpenTelemetrySdkEventSource.Log.SpanProcessorException(\"OnStart\", e);\n                }\n            }\n        }\n\n        public override Task ShutdownAsync(CancellationToken cancellationToken)\n        {\n            var tasks = new List<Task>();\n            foreach (var processor in this.processors)\n            {\n                tasks.Add(processor.ShutdownAsync(cancellationToken));\n            }\n\n            return Task.WhenAll(tasks);\n        }\n\n        public override Task ForceFlushAsync(CancellationToken cancellationToken)\n        {\n            var tasks = new List<Task>(this.processors.Count());\n            foreach (var processor in this.processors)\n            {\n                tasks.Add(processor.ForceFlushAsync(cancellationToken));\n            }\n\n            return Task.WhenAll(tasks);\n        }\n\n        public void Dispose()\n        {\n            this.Dispose(true);\n        }\n\n        protected virtual void Dispose(bool disposing)\n        {\n            try\n            {\n                this.ShutdownAsync(CancellationToken.None).GetAwaiter().GetResult();\n            }\n            catch (Exception ex)\n            {\n                OpenTelemetrySdkEventSource.Log.SpanProcessorException(nameof(this.Dispose), ex);\n            }\n\n            if (disposing && !this.isDisposed)\n            {\n                foreach (var processor in this.processors)\n                {\n                    try\n                    {\n                        if (processor is IDisposable disposable)\n                        {\n                            disposable.Dispose();\n                        }\n                    }\n                    catch (Exception e)\n                    {\n                        OpenTelemetrySdkEventSource.Log.SpanProcessorException(\"Dispose\", e);\n                    }\n                }\n\n                this.isDisposed = true;\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":136,"code_uid":"8182f3e457194e61b6e2f19d7f057590"}
{"diff_hunk":"@@ -6,18 +6,24 @@\n \/\/ Project Lead - Stuart Lodge, @slodge, me@slodge.com\n \n using System;\n+using System.Collections.Generic;\n+using System.Linq;\n using Windows.UI.Core;\n+using Windows.UI.Xaml;\n+using Windows.UI.Xaml.Controls;\n+using Windows.UI.Xaml.Media;\n using MvvmCross.Core.Navigation;\n using MvvmCross.Core.ViewModels;\n using MvvmCross.Core.Views;\n using MvvmCross.Platform;\n using MvvmCross.Platform.Exceptions;\n using MvvmCross.Platform.Platform;\n+using MvvmCross.Uwp.Attributes;\n \n namespace MvvmCross.Uwp.Views\n {\n     public class MvxWindowsViewPresenter\n-        : MvxViewPresenter, IMvxWindowsViewPresenter\n+        : MvxViewPresenter, IMvxWindowsViewPresenter, IMvxAttributeViewPresenter\n     {\n         protected readonly IMvxWindowsFrame _rootFrame;\n ","old_code":"\ufeff\/\/ MvxStoreViewPresenter.cs\n\n\/\/ MvvmCross is licensed using Microsoft Public License (Ms-PL)\n\/\/ Contributions and inspirations noted in readme.md and license.txt\n\/\/ \n\/\/ Project Lead - Stuart Lodge, @slodge, me@slodge.com\n\nusing System;\nusing Windows.UI.Core;\nusing MvvmCross.Core.Navigation;\nusing MvvmCross.Core.ViewModels;\nusing MvvmCross.Core.Views;\nusing MvvmCross.Platform;\nusing MvvmCross.Platform.Exceptions;\nusing MvvmCross.Platform.Platform;\n\nnamespace MvvmCross.Uwp.Views\n{\n    public class MvxWindowsViewPresenter\n        : MvxViewPresenter, IMvxWindowsViewPresenter\n    {\n        protected readonly IMvxWindowsFrame _rootFrame;\n\n        public MvxWindowsViewPresenter(IMvxWindowsFrame rootFrame)\n        {\n            _rootFrame = rootFrame;\n\n            SystemNavigationManager.GetForCurrentView().BackRequested += BackButtonOnBackRequested;\n        }\n\n        private IMvxViewModelTypeFinder _viewModelTypeFinder;\n        public IMvxViewModelTypeFinder ViewModelTypeFinder\n        {\n            get\n            {\n                if (_viewModelTypeFinder == null)\n                    _viewModelTypeFinder = Mvx.Resolve<IMvxViewModelTypeFinder>();\n                return _viewModelTypeFinder;\n            }\n            set\n            {\n                _viewModelTypeFinder = value;\n            }\n        }\n\n        private IMvxViewsContainer _viewsContainer;\n        public IMvxViewsContainer ViewsContainer\n        {\n            get\n            {\n                if (_viewsContainer == null)\n                    _viewsContainer = Mvx.Resolve<IMvxViewsContainer>();\n                return _viewsContainer;\n            }\n            set\n            {\n                _viewsContainer = value;\n            }\n        }\n\n        protected virtual async void BackButtonOnBackRequested(object sender, BackRequestedEventArgs backRequestedEventArgs)\n        {\n            if (backRequestedEventArgs.Handled)\n                return;\n\n            var currentView = _rootFrame.Content as IMvxView;\n            if (currentView == null)\n            {\n                Mvx.Warning(\"Ignoring close for viewmodel - rootframe has no current page\");\n                return;\n            }\n\n            var navigationService = Mvx.Resolve<IMvxNavigationService>();\n\r            backRequestedEventArgs.Handled = await navigationService.Close(currentView.ViewModel);\n        }\n\n        public override void Show(MvxViewModelRequest request)\n        {\n            try\n            {\n                var requestText = GetRequestText(request);\n                var viewsContainer = Mvx.Resolve<IMvxViewsContainer>();\n                var viewType = viewsContainer.GetViewType(request.ViewModelType);\n\n                _rootFrame.Navigate(viewType, requestText); \/\/Frame won't allow serialization of it's nav-state if it gets a non-simple type as a nav param\n\n                HandleBackButtonVisibility();\n            }\n            catch (Exception exception)\n            {\n                MvxTrace.Trace(\"Error seen during navigation request to {0} - error {1}\", request.ViewModelType.Name,\n                               exception.ToLongString());\n            }\n        }\n\n        protected virtual string GetRequestText(MvxViewModelRequest request)\n        {\n            var requestTranslator = Mvx.Resolve<IMvxWindowsViewModelRequestTranslator>();\n            string requestText = string.Empty;\n            if (request is MvxViewModelInstanceRequest)\n            {\n                requestText = requestTranslator.GetRequestTextWithKeyFor(((MvxViewModelInstanceRequest)request).ViewModelInstance);\n            }\n            else\n            {\n                requestText = requestTranslator.GetRequestTextFor(request);\n            }\n\n            return requestText;\n        }\n\n        public override void ChangePresentation(MvxPresentationHint hint)\n        {\n            if (HandlePresentationChange(hint)) return;\n\n            if (hint is MvxClosePresentationHint)\n            {\n                Close((hint as MvxClosePresentationHint).ViewModelToClose);\n                return;\n            }\n\n            MvxTrace.Warning(\"Hint ignored {0}\", hint.GetType().Name);\n        }\n\n        public override void Close(IMvxViewModel viewModel)\n        {\n            var currentView = _rootFrame.Content as IMvxView;\n            if (currentView == null)\n            {\n                Mvx.Warning(\"Ignoring close for viewmodel - rootframe has no current page\");\n                return;\n            }\n\n            if (currentView.ViewModel != viewModel)\n            {\n                Mvx.Warning(\"Ignoring close for viewmodel - rootframe's current page is not the view for the requested viewmodel\");\n                return;\n            }\n\n            if (!_rootFrame.CanGoBack)\n            {\n                Mvx.Warning(\"Ignoring close for viewmodel - rootframe refuses to go back\");\n                return;\n            }\n\n            _rootFrame.GoBack();\n\n            HandleBackButtonVisibility();\n        }\n\n        protected virtual void HandleBackButtonVisibility()\n        {\n            SystemNavigationManager.GetForCurrentView().AppViewBackButtonVisibility =\n                _rootFrame.CanGoBack ? AppViewBackButtonVisibility.Visible : AppViewBackButtonVisibility.Collapsed;\n        }\n    }\n}","lang_cluster":"C#","length":158,"code_uid":"b3b2e585d0ed49e9b15a8e10a5feb0db"}
{"diff_hunk":"@@ -120,7 +120,7 @@ namespace MvvmCross.Plugins.PictureChooser.iOS\n             }\n \n             _picker.DismissViewController(true, () => { });\n-            _modalHost.NativeModalViewControllerDisappearedOnItsOwn();\n+            _viewPresenter.CloseModalViewControllers();\n         }\n \n         private void Picker_FinishedPickingMedia(object sender, UIImagePickerMediaPickedEventArgs e)","old_code":"\ufeff\/\/ MvxImagePickerTask.cs\n\/\/ (c) Copyright Cirrious Ltd. http:\/\/www.cirrious.com\n\/\/ MvvmCross is licensed using Microsoft Public License (Ms-PL)\n\/\/ Contributions and inspirations noted in readme.md and license.txt\n\/\/\n\/\/ Project Lead - Stuart Lodge, @slodge, me@slodge.com\n\nusing System;\nusing System.IO;\nusing System.Runtime.InteropServices;\nusing System.Threading.Tasks;\nusing CoreGraphics;\nusing Foundation;\nusing MvvmCross.Platform;\nusing MvvmCross.Platform.iOS.Platform;\nusing MvvmCross.Platform.iOS.Views;\nusing MvvmCross.Platform.Logging;\nusing UIKit;\n\nnamespace MvvmCross.Plugins.PictureChooser.iOS\n{\n    [MvvmCross.Platform.Preserve(AllMembers = true)]\n\tpublic class MvxImagePickerTask\n        : MvxIosTask, IMvxPictureChooserTask\n    {\n        private readonly UIImagePickerController _picker;\n        private readonly IMvxIosModalHost _modalHost;\n        private bool _currentlyActive;\n        private int _maxPixelDimension;\n        private int _percentQuality;\n        private Action<Stream, string> _pictureAvailable;\n        private Action _assumeCancelled;\n\n        public MvxImagePickerTask()\n        {\n            _modalHost = Mvx.Resolve<IMvxIosModalHost>();\n            _picker = new UIImagePickerController\n            {\n                \/\/CameraCaptureMode = UIImagePickerControllerCameraCaptureMode.Photo,\n                \/\/CameraDevice = UIImagePickerControllerCameraDevice.Front\n            };\n            _picker.FinishedPickingMedia += Picker_FinishedPickingMedia;\n            _picker.FinishedPickingImage += Picker_FinishedPickingImage;\n            _picker.Canceled += Picker_Canceled;\n        }\n\n        public void ChoosePictureFromLibrary(int maxPixelDimension, int percentQuality, Action<Stream, string> pictureAvailable,\n                                     Action assumeCancelled)\n        {\n            _picker.SourceType = UIImagePickerControllerSourceType.PhotoLibrary;\n            ChoosePictureCommon(maxPixelDimension, percentQuality, pictureAvailable, assumeCancelled);\n        }\n\n        public void ChoosePictureFromLibrary(int maxPixelDimension, int percentQuality, Action<Stream> pictureAvailable,\n                                             Action assumeCancelled)\n        {\n            ChoosePictureFromLibrary(maxPixelDimension, percentQuality, (stream, name) => pictureAvailable(stream), assumeCancelled);\n        }\n\n        public void TakePicture(int maxPixelDimension, int percentQuality, Action<Stream> pictureAvailable,\n                                Action assumeCancelled)\n        {\n            _picker.SourceType = UIImagePickerControllerSourceType.Camera;\n            ChoosePictureCommon(maxPixelDimension, percentQuality, (stream, name) => pictureAvailable(stream), assumeCancelled);\n        }\n\n        public Task<Stream> ChoosePictureFromLibrary(int maxPixelDimension, int percentQuality)\n        {\n            var task = new TaskCompletionSource<Stream>();\n            ChoosePictureFromLibrary(maxPixelDimension, percentQuality, task.SetResult, () => task.SetResult(null));\n            return task.Task;\n        }\n\n        public Task<Stream> TakePicture(int maxPixelDimension, int percentQuality)\n        {\n            var task = new TaskCompletionSource<Stream>();\n            TakePicture(maxPixelDimension, percentQuality, task.SetResult, () => task.SetResult(null));\n            return task.Task;\n        }\n\n        public void ContinueFileOpenPicker(object args)\n        {\n        }\n\n        private void ChoosePictureCommon(int maxPixelDimension, int percentQuality,\n                                         Action<Stream, string> pictureAvailable, Action assumeCancelled)\n        {\n            SetCurrentlyActive();\n            _maxPixelDimension = maxPixelDimension;\n            _percentQuality = percentQuality;\n            _pictureAvailable = pictureAvailable;\n            _assumeCancelled = assumeCancelled;\n\n            _modalHost.PresentModalViewController(_picker, true);\n        }\n\n        private void HandleImagePick(UIImage image, string name)\n        {\n            ClearCurrentlyActive();\n            if (image != null)\n            {\n                if (_maxPixelDimension > 0 && (image.Size.Height > _maxPixelDimension || image.Size.Width > _maxPixelDimension))\n                {\n                    \/\/ resize the image\n                    image = image.ImageToFitSize(new CGSize(_maxPixelDimension, _maxPixelDimension));\n                }\n\n                using (NSData data = image.AsJPEG(_percentQuality \/ 100f))\n                {\n                    var byteArray = new byte[data.Length];\n                    Marshal.Copy(data.Bytes, byteArray, 0, Convert.ToInt32(data.Length));\n\n                    var imageStream = new MemoryStream(byteArray, false);\n                    _pictureAvailable?.Invoke(imageStream, name);\n                }\n            }\n            else\n            {\n                _assumeCancelled?.Invoke();\n            }\n\n            _picker.DismissViewController(true, () => { });\n            _modalHost.NativeModalViewControllerDisappearedOnItsOwn();\n        }\n\n        private void Picker_FinishedPickingMedia(object sender, UIImagePickerMediaPickedEventArgs e)\n        {\n            NSUrl referenceURL = e.Info[new NSString(\"UIImagePickerControllerReferenceURL\")] as NSUrl;\n            var image = e.EditedImage ?? e.OriginalImage;\n            HandleImagePick(image, referenceURL != null ? referenceURL.AbsoluteString : string.Empty);\n        }\n\n        private void Picker_FinishedPickingImage(object sender, UIImagePickerImagePickedEventArgs e)\n        {\n            NSUrl referenceURL = e.EditingInfo[\"UIImagePickerControllerReferenceURL\"] as NSUrl;\n            var image = e.Image;\n            HandleImagePick(image, referenceURL != null ? referenceURL.AbsoluteString : string.Empty);\n        }\n\n        private void Picker_Canceled(object sender, EventArgs e)\n        {\n            ClearCurrentlyActive();\n            _assumeCancelled?.Invoke();\n            _picker.DismissViewController(true, () => { });\n            _modalHost.NativeModalViewControllerDisappearedOnItsOwn();\n        }\n\n        private void SetCurrentlyActive()\n        {\n            if (_currentlyActive)\n                MvxPluginLog.Instance.Warn(\"MvxImagePickerTask called when task already active\");\n            _currentlyActive = true;\n        }\n\n        private void ClearCurrentlyActive()\n        {\n            if (!_currentlyActive)\n                MvxPluginLog.Instance.Warn(\"Tried to clear currently active - but already cleared\");\n            _currentlyActive = false;\n        }\n    }\n}\n","lang_cluster":"C#","length":162,"code_uid":"dfac93377ecc4812b658a5ab99a8eadf"}
{"diff_hunk":"@@ -63,9 +63,26 @@ namespace OpenTelemetry.Instrumentation.SqlClient\n         \/\/\/ <remarks>\n         \/\/\/ <para><see cref=\"Activity\"\/>: the activity being enriched.<\/para>\n         \/\/\/ <para>string: the name of the event.<\/para>\n-        \/\/\/ <para>object: the raw object from which additional information can be extracted to enrich the activity.\n-        \/\/\/ The type of this object depends on the event, which is given by the above parameter.<\/para>\n+        \/\/\/ <para>object: the raw <c>SqlCommand<\/c> object from which additional information can be extracted to enrich the activity.<\/para>\n+        \/\/\/ <para>See also: <a href=\"https:\/\/github.com\/open-telemetry\/opentelemetry-dotnet\/tree\/master\/src\/OpenTelemetry.Instrumentation.SqlClient#Enrich\">example<\/a>.<\/para>\n         \/\/\/ <\/remarks>\n+        \/\/\/ <example>\n+        \/\/\/ <code>\n+        \/\/\/ using var tracerProvider = Sdk.CreateTracerProviderBuilder()\n+        \/\/\/     .AddSqlClientInstrumentation(opt => opt.Enrich\n+        \/\/\/         = (activity, eventName, rawObject) =>\n+        \/\/\/      {\n+        \/\/\/         if (eventName.Equals(\"OnCustom\"))\n+        \/\/\/         {\n+        \/\/\/             if (rawObject is SqlCommand cmd)\n+        \/\/\/             {\n+        \/\/\/                 activity.SetTag(\"db.commandTimeout\", cmd.CommandTimeout);\n+        \/\/\/             }\n+        \/\/\/         }\n+        \/\/\/      })\n+        \/\/\/     .Build();\n+        \/\/\/ <\/code>\n+        \/\/\/ <\/example>\n         public Action<Activity, string, object> Enrich { get; set; }\n \n         internal static SqlConnectionDetails ParseDataSource(string dataSource)","old_code":"\/\/ <copyright file=\"SqlClientInstrumentationOptions.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Collections.Concurrent;\nusing System.Data;\nusing System.Diagnostics;\nusing System.Text.RegularExpressions;\nusing OpenTelemetry.Trace;\n\nnamespace OpenTelemetry.Instrumentation.SqlClient\n{\n    \/\/\/ <summary>\n    \/\/\/ Options for <see cref=\"SqlClientInstrumentation\"\/>.\n    \/\/\/ <\/summary>\n    public class SqlClientInstrumentationOptions\n    {\n        \/*\n         * Match...\n         *  serverName\n         *  serverName[ ]\\\\[ ]instanceName\n         *  serverName[ ],[ ]port\n         *  serverName[ ]\\\\[ ]instanceName[ ],[ ]port\n         * [ ] can be any number of white-space, SQL allows it for some reason.\n         *\/\n        private static readonly Regex DataSourceRegex = new Regex(\"^(.*?)\\\\s*(?:[\\\\\\\\,]|$)\\\\s*(.*?)\\\\s*(?:,|$)\\\\s*(.*)$\", RegexOptions.Compiled);\n        private static readonly ConcurrentDictionary<string, SqlConnectionDetails> ConnectionDetailCache = new ConcurrentDictionary<string, SqlConnectionDetails>(StringComparer.OrdinalIgnoreCase);\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets a value indicating whether or not the <see cref=\"SqlClientInstrumentation\"\/> should add the names of <see cref=\"CommandType.StoredProcedure\"\/> commands as the <see cref=\"SemanticConventions.AttributeDbStatement\"\/> tag. Default value: True.\n        \/\/\/ <\/summary>\n        public bool SetStoredProcedureCommandName { get; set; } = true;\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets a value indicating whether or not the <see cref=\"SqlClientInstrumentation\"\/> should add the text of <see cref=\"CommandType.Text\"\/> commands as the <see cref=\"SemanticConventions.AttributeDbStatement\"\/> tag. Default value: False.\n        \/\/\/ <\/summary>\n        public bool SetTextCommandContent { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets a value indicating whether or not the <see cref=\"SqlClientInstrumentation\"\/> should parse the DataSource on a SqlConnection into server name, instance name, and\/or port connection-level attribute tags. Default value: False.\n        \/\/\/ <\/summary>\n        \/\/\/ <remarks>\n        \/\/\/ The default behavior is to set the SqlConnection DataSource as the <see cref=\"SemanticConventions.AttributePeerService\"\/> tag. If enabled, SqlConnection DataSource will be parsed and the server name will be sent as the <see cref=\"SemanticConventions.AttributeNetPeerName\"\/> or <see cref=\"SemanticConventions.AttributeNetPeerIp\"\/> tag, the instance name will be sent as the <see cref=\"SemanticConventions.AttributeDbMsSqlInstanceName\"\/> tag, and the port will be sent as the <see cref=\"SemanticConventions.AttributeNetPeerPort\"\/> tag if it is not 1433 (the default port).\n        \/\/\/ <\/remarks>\n        public bool EnableConnectionLevelAttributes { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets an action to enrich an Activity.\n        \/\/\/ <\/summary>\n        \/\/\/ <remarks>\n        \/\/\/ <para><see cref=\"Activity\"\/>: the activity being enriched.<\/para>\n        \/\/\/ <para>string: the name of the event.<\/para>\n        \/\/\/ <para>object: the raw object from which additional information can be extracted to enrich the activity.\n        \/\/\/ The type of this object depends on the event, which is given by the above parameter.<\/para>\n        \/\/\/ <\/remarks>\n        public Action<Activity, string, object> Enrich { get; set; }\n\n        internal static SqlConnectionDetails ParseDataSource(string dataSource)\n        {\n            Match match = DataSourceRegex.Match(dataSource);\n\n            string serverHostName = match.Groups[1].Value;\n            string serverIpAddress = null;\n\n            var uriHostNameType = Uri.CheckHostName(serverHostName);\n            if (uriHostNameType == UriHostNameType.IPv4 || uriHostNameType == UriHostNameType.IPv6)\n            {\n                serverIpAddress = serverHostName;\n                serverHostName = null;\n            }\n\n            string instanceName;\n            string port;\n            if (match.Groups[3].Length > 0)\n            {\n                instanceName = match.Groups[2].Value;\n                port = match.Groups[3].Value;\n                if (port == \"1433\")\n                {\n                    port = null;\n                }\n            }\n            else if (int.TryParse(match.Groups[2].Value, out int parsedPort))\n            {\n                port = parsedPort == 1433 ? null : match.Groups[2].Value;\n                instanceName = null;\n            }\n            else\n            {\n                instanceName = match.Groups[2].Value;\n\n                if (string.IsNullOrEmpty(instanceName))\n                {\n                    instanceName = null;\n                }\n\n                port = null;\n            }\n\n            return new SqlConnectionDetails\n            {\n                ServerHostName = serverHostName,\n                ServerIpAddress = serverIpAddress,\n                InstanceName = instanceName,\n                Port = port,\n            };\n        }\n\n        internal void AddConnectionLevelDetailsToActivity(string dataSource, Activity sqlActivity)\n        {\n            if (!this.EnableConnectionLevelAttributes)\n            {\n                sqlActivity.SetTag(SemanticConventions.AttributePeerService, dataSource);\n            }\n            else\n            {\n                if (!ConnectionDetailCache.TryGetValue(dataSource, out SqlConnectionDetails connectionDetails))\n                {\n                    connectionDetails = ParseDataSource(dataSource);\n                    ConnectionDetailCache.TryAdd(dataSource, connectionDetails);\n                }\n\n                if (!string.IsNullOrEmpty(connectionDetails.ServerHostName))\n                {\n                    sqlActivity.SetTag(SemanticConventions.AttributeNetPeerName, connectionDetails.ServerHostName);\n                }\n                else\n                {\n                    sqlActivity.SetTag(SemanticConventions.AttributeNetPeerIp, connectionDetails.ServerIpAddress);\n                }\n\n                if (!string.IsNullOrEmpty(connectionDetails.InstanceName))\n                {\n                    sqlActivity.SetTag(SemanticConventions.AttributeDbMsSqlInstanceName, connectionDetails.InstanceName);\n                }\n\n                if (!string.IsNullOrEmpty(connectionDetails.Port))\n                {\n                    sqlActivity.SetTag(SemanticConventions.AttributeNetPeerPort, connectionDetails.Port);\n                }\n            }\n        }\n\n        internal class SqlConnectionDetails\n        {\n            public string ServerHostName { get; set; }\n\n            public string ServerIpAddress { get; set; }\n\n            public string InstanceName { get; set; }\n\n            public string Port { get; set; }\n        }\n    }\n}\n","lang_cluster":"C#","length":168,"code_uid":"bdbf50a9d9e3497eac77b810c39515e3"}
{"diff_hunk":"@@ -55,6 +55,12 @@ namespace OpenTelemetry.Instrumentation.AspNet\n \n                 if (propagationContext.Baggage != default)\n                 {\n+                    \/\/ todo: RestoreActivityIfNeeded below compensates for\n+                    \/\/ AsyncLocal Activity.Current being lost. Baggage\n+                    \/\/ potentially will suffer from the same issue, but we can\u2019t\n+                    \/\/ simply add it to context.Items because any change results\n+                    \/\/ in a new instance. Probably need to save it at the end of\n+                    \/\/ each OnExecuteRequestStep.\n                     Baggage.Current = propagationContext.Baggage;\n                 }\n ","old_code":"\/\/ <copyright file=\"ActivityHelper.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Collections;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.Web;\nusing OpenTelemetry.Context.Propagation;\n\nnamespace OpenTelemetry.Instrumentation.AspNet\n{\n    \/\/\/ <summary>\n    \/\/\/ Activity helper class.\n    \/\/\/ <\/summary>\n    internal static class ActivityHelper\n    {\n        \/\/\/ <summary>\n        \/\/\/ Key to store the activity in HttpContext.\n        \/\/\/ <\/summary>\n        public const string ActivityKey = \"__AspnetActivity__\";\n\n        private static readonly ActivitySource AspNetSource = new ActivitySource(TelemetryHttpModule.AspNetSourceName);\n        private static readonly Func<HttpRequest, string, IEnumerable<string>> HttpRequestHeaderValuesGetter = (request, name) => request.Headers.GetValues(name);\n\n        \/\/\/ <summary>\n        \/\/\/ Creates root (first level) activity that describes incoming request.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"textMapPropagator\"><see cref=\"TextMapPropagator\"\/>.<\/param>\n        \/\/\/ <param name=\"context\">Current HttpContext.<\/param>\n        \/\/\/ <param name=\"onRequestStartedCallback\">Callback action.<\/param>\n        \/\/\/ <returns>New root activity.<\/returns>\n        public static Activity StartAspNetActivity(TextMapPropagator textMapPropagator, HttpContext context, Action<Activity, HttpContext> onRequestStartedCallback)\n        {\n            PropagationContext propagationContext = textMapPropagator.Extract(default, context.Request, HttpRequestHeaderValuesGetter);\n\n            Activity activity = AspNetSource.CreateActivity(TelemetryHttpModule.AspNetActivityName, ActivityKind.Server, propagationContext.ActivityContext);\n\n            if (activity != null)\n            {\n                context.Items[ActivityKey] = activity;\n\n                if (propagationContext.Baggage != default)\n                {\n                    Baggage.Current = propagationContext.Baggage;\n                }\n\n                try\n                {\n                    onRequestStartedCallback?.Invoke(activity, context);\n                }\n                catch (Exception callbackEx)\n                {\n                    AspNetTelemetryEventSource.Log.CallbackException(activity, \"OnStarted\", callbackEx);\n                }\n\n                AspNetTelemetryEventSource.Log.ActivityStarted(activity);\n            }\n\n            return activity;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Stops the activity and notifies listeners about it.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"context\">Current HttpContext.<\/param>\n        \/\/\/ <param name=\"onRequestStoppedCallback\">Callback action.<\/param>\n        public static void StopAspNetActivity(HttpContext context, Action<Activity, HttpContext> onRequestStoppedCallback)\n        {\n            var contextItems = context.Items;\n            var currentActivity = Activity.Current;\n            Activity aspNetActivity = (Activity)contextItems[ActivityKey];\n\n            if (currentActivity != aspNetActivity)\n            {\n                Activity.Current = aspNetActivity;\n            }\n\n            if (aspNetActivity != null)\n            {\n                aspNetActivity.Stop();\n                contextItems[ActivityKey] = null;\n\n                try\n                {\n                    onRequestStoppedCallback?.Invoke(aspNetActivity, context);\n                }\n                catch (Exception callbackEx)\n                {\n                    AspNetTelemetryEventSource.Log.CallbackException(aspNetActivity, \"OnStopped\", callbackEx);\n                }\n\n                AspNetTelemetryEventSource.Log.ActivityStopped(currentActivity);\n            }\n\n            if (currentActivity != aspNetActivity)\n            {\n                Activity.Current = currentActivity;\n            }\n        }\n\n        public static void WriteActivityException(IDictionary contextItems, Exception exception, Action<Activity, Exception> onExceptionCallback)\n        {\n            Activity aspNetActivity = (Activity)contextItems[ActivityKey];\n\n            if (aspNetActivity != null)\n            {\n                try\n                {\n                    onExceptionCallback?.Invoke(aspNetActivity, exception);\n                }\n                catch (Exception callbackEx)\n                {\n                    AspNetTelemetryEventSource.Log.CallbackException(aspNetActivity, \"OnException\", callbackEx);\n                }\n\n                AspNetTelemetryEventSource.Log.ActivityException(aspNetActivity, exception);\n            }\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ It's possible that a request is executed in both native threads and managed threads,\n        \/\/\/ in such case Activity.Current will be lost during native thread and managed thread switch.\n        \/\/\/ This method is intended to restore the current activity in order to correlate the child\n        \/\/\/ activities with the root activity of the request.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"contextItems\">HttpContext.Items dictionary.<\/param>\n        internal static void RestoreActivityIfNeeded(IDictionary contextItems)\n        {\n            if (Activity.Current == null)\n            {\n                Activity aspNetActivity = (Activity)contextItems[ActivityKey];\n                if (aspNetActivity != null)\n                {\n                    Activity.Current = aspNetActivity;\n                    AspNetTelemetryEventSource.Log.ActivityRestored(aspNetActivity);\n                }\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":154,"code_uid":"7b3f496d86624b739ca15f8714313d4a"}
{"diff_hunk":"@@ -55,14 +55,14 @@ namespace OpenTelemetry.Trace.Test\n         [Fact]\n         public void ThrowsInExporter()\n         {\n-            this.activityExporter = new TestActivityExporter(_ => throw new ArgumentException(\"123\"));\n-            this.openTelemetry = Sdk.CreateTracerProvider(b => b\n-                        .AddActivitySource(\"cijo\")\n-                        .AddProcessorPipeline(p => p\n-                        .SetExporter(this.activityExporter)\n-                        .SetExportingProcessor(e => new SimpleActivityProcessor(e))));\n-\n-            ActivitySource source = new ActivitySource(\"cijo\");\n+            var activityExporter = new TestActivityExporter(_ => throw new ArgumentException(\"123\"));\n+            using var openTelemetry = Sdk.CreateTracerProviderBuilder()\n+                        .AddSource(\"random\")\n+                        .SetSampler(new AlwaysOnSampler())\n+                        .AddProcessor(new SimpleActivityProcessor(activityExporter))\n+                        .Build();\n+\n+            ActivitySource source = new ActivitySource(\"random\");\n             var activity = source.StartActivity(\"somename\");\n \n             \/\/ does not throw","old_code":"\ufeff\/\/ <copyright file=\"SimpleActivityProcessorTest.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Diagnostics;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing OpenTelemetry.Testing.Export;\nusing OpenTelemetry.Trace.Samplers;\nusing Xunit;\n\nnamespace OpenTelemetry.Trace.Test\n{\n    public class SimpleActivityProcessorTest : IDisposable\n    {\n        private const string SpanName1 = \"MySpanName\/1\";\n        private const string SpanName2 = \"MySpanName\/2\";\n        private const string ActivitySourceName = \"defaultactivitysource\";\n\n        private TestActivityExporter activityExporter;\n        private TracerProvider openTelemetry;\n        private ActivitySource activitySource;\n\n        public SimpleActivityProcessorTest()\n        {\n            this.activityExporter = new TestActivityExporter(null);\n            this.openTelemetry = Sdk.CreateTracerProvider(b => b\n                        .AddActivitySource(ActivitySourceName)\n                        .AddProcessorPipeline(p => p\n                        .SetExporter(this.activityExporter)\n                        .SetExportingProcessor(e => new SimpleActivityProcessor(e)))\n                .SetSampler(new AlwaysOnSampler()));\n            this.activitySource = new ActivitySource(ActivitySourceName);\n        }\n\n        [Fact]\n        public void ThrowsOnNullExporter()\n        {\n            Assert.Throws<ArgumentNullException>(() => new SimpleActivityProcessor(null));\n        }\n\n        [Fact]\n        public void ThrowsInExporter()\n        {\n            this.activityExporter = new TestActivityExporter(_ => throw new ArgumentException(\"123\"));\n            this.openTelemetry = Sdk.CreateTracerProvider(b => b\n                        .AddActivitySource(\"cijo\")\n                        .AddProcessorPipeline(p => p\n                        .SetExporter(this.activityExporter)\n                        .SetExportingProcessor(e => new SimpleActivityProcessor(e))));\n\n            ActivitySource source = new ActivitySource(\"cijo\");\n            var activity = source.StartActivity(\"somename\");\n\n            \/\/ does not throw\n            activity.Stop();\n        }\n\n        [Fact]\n        public void ProcessorDoesNotBlockOnExporter()\n        {\n            this.activityExporter = new TestActivityExporter(async _ => await Task.Delay(500));\n            this.openTelemetry = Sdk.CreateTracerProvider(b => b\n                        .AddActivitySource(\"cijo\")\n                        .AddProcessorPipeline(p => p\n                        .SetExporter(this.activityExporter)\n                        .SetExportingProcessor(e => new SimpleActivityProcessor(e))));\n\n            ActivitySource source = new ActivitySource(\"cijo\");\n            var activity = source.StartActivity(\"somename\");\n\n            \/\/ does not block\n            var sw = Stopwatch.StartNew();\n            activity.Stop();\n            sw.Stop();\n\n            Assert.InRange(sw.Elapsed, TimeSpan.Zero, TimeSpan.FromMilliseconds(100));\n\n            var exported = this.WaitForSpans(this.activityExporter, 1, TimeSpan.FromMilliseconds(600));\n\n            Assert.Single(exported);\n        }\n\n        [Fact]\n        public async Task ShutdownTwice()\n        {\n            var activityProcessor = new SimpleActivityProcessor(new TestActivityExporter(null));\n\n            await activityProcessor.ShutdownAsync(CancellationToken.None).ConfigureAwait(false);\n\n            \/\/ does not throw\n            await activityProcessor.ShutdownAsync(CancellationToken.None).ConfigureAwait(false);\n        }\n\n        [Fact]\n        public async Task ForceFlushReturnsCompletedTask()\n        {\n            var activityProcessor = new SimpleActivityProcessor(new TestActivityExporter(null));\n\n            var forceFlushTask = activityProcessor.ForceFlushAsync(CancellationToken.None);\n            await forceFlushTask;\n\n            Assert.True(forceFlushTask.IsCompleted);\n        }\n\n        [Fact]\n        public void ExportDifferentSampledSpans()\n        {\n            var span1 = this.CreateSampledEndedSpan(SpanName1);\n            var span2 = this.CreateSampledEndedSpan(SpanName2);\n\n            var exported = this.WaitForSpans(this.activityExporter, 2, TimeSpan.FromMilliseconds(100));\n            Assert.Equal(2, exported.Length);\n            Assert.Contains(span1, exported);\n            Assert.Contains(span2, exported);\n        }\n\n        [Fact(Skip = \"Reenable once AlwaysParentSampler is added\")]\n        public void ExportNotSampledSpans()\n        {\n            var span1 = this.CreateNotSampledEndedSpan(SpanName1);\n            var span2 = this.CreateSampledEndedSpan(SpanName2);\n\n            \/\/ Spans are recorded and exported in the same order as they are ended, we test that a non\n            \/\/ sampled span is not exported by creating and ending a sampled span after a non sampled span\n            \/\/ and checking that the first exported span is the sampled span (the non sampled did not get\n            \/\/ exported).\n\n            var exported = this.WaitForSpans(this.activityExporter, 1, TimeSpan.FromMilliseconds(100));\n\n            \/\/ Need to check this because otherwise the variable span1 is unused, other option is to not\n            \/\/ have a span1 variable.\n            Assert.Single(exported);\n            Assert.Contains(span2, exported);\n        }\n\n        public void Dispose()\n        {\n            this.activityExporter.ShutdownAsync(CancellationToken.None);\n            Activity.Current = null;\n        }\n\n        private Activity CreateSampledEndedSpan(string spanName)\n        {\n            var context = new ActivityContext(ActivityTraceId.CreateRandom(), ActivitySpanId.CreateRandom(), ActivityTraceFlags.Recorded);\n\n            var activity = this.activitySource.StartActivity(spanName, ActivityKind.Internal, context);\n            activity.Stop();\n            return activity;\n        }\n\n        private Activity CreateNotSampledEndedSpan(string spanName)\n        {\n            var context = new ActivityContext(ActivityTraceId.CreateRandom(), ActivitySpanId.CreateRandom(), ActivityTraceFlags.None);\n            var activity = this.activitySource.StartActivity(spanName, ActivityKind.Internal, context);\n            activity.Stop();\n            return activity;\n        }\n\n        private Activity[] WaitForSpans(TestActivityExporter exporter, int spanCount, TimeSpan timeout)\n        {\n            Assert.True(\n                SpinWait.SpinUntil(\n                    () =>\n                    {\n                        Thread.Sleep(0);\n                        return exporter.ExportedActivities.Length >= spanCount;\n                    }, timeout + TimeSpan.FromMilliseconds(20)));\n\n            return exporter.ExportedActivities;\n        }\n    }\n}\n","lang_cluster":"C#","length":186,"code_uid":"02eb7a06c41f4011b49054186a3f1630"}
{"diff_hunk":"@@ -96,5 +96,14 @@ namespace SampleApp\n                 \n             return hostBuilder.Build().RunAsync();\n         }\n+\n+        private static void ShowConfig(IConfiguration config)\n+        {\n+            foreach (var pair in config.GetChildren())\n+            {\n+                Console.WriteLine($\"{pair.Path} - {pair.Value}\");\n+                ShowConfig(pair);\n+            }\n+        }\n     }\n }","old_code":"\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System;\nusing System.Diagnostics;\nusing System.Globalization;\nusing System.IO;\nusing System.Net;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Builder;\nusing Microsoft.AspNetCore.Hosting;\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.AspNetCore.Server.Kestrel.Transport.Abstractions.Internal;\nusing Microsoft.Extensions.Configuration;\nusing Microsoft.Extensions.Logging;\n\nnamespace SampleApp\n{\n    public class Startup\n    {\n        public void Configure(IApplicationBuilder app, ILoggerFactory loggerFactory)\n        {\n            var logger = loggerFactory.CreateLogger(\"Default\");\n\n            app.Run(async context =>\n            {\n                var connectionFeature = context.Connection;\n                logger.LogDebug($\"Peer: {connectionFeature.RemoteIpAddress?.ToString()}:{connectionFeature.RemotePort}\"\n                    + $\"{Environment.NewLine}\"\n                    + $\"Sock: {connectionFeature.LocalIpAddress?.ToString()}:{connectionFeature.LocalPort}\");\n\n                var response = $\"hello, world{Environment.NewLine}\";\n                context.Response.ContentLength = response.Length;\n                context.Response.ContentType = \"text\/plain\";\n                await context.Response.WriteAsync(response);\n            });\n        }\n\n        public static Task Main(string[] args)\n        {\n            TaskScheduler.UnobservedTaskException += (sender, e) =>\n            {\n                Console.WriteLine(\"Unobserved exception: {0}\", e.Exception);\n            };\n\n            var hostBuilder = new WebHostBuilder()\n                .ConfigureLogging((_, factory) =>\n                {\n                    factory.AddConsole();\n                })\n                .UseKestrel((context, options) =>\n                {\n                    var basePort = context.Configuration.GetValue<int?>(\"BASE_PORT\") ?? 5000;\n\n                    \/\/ Run callbacks on the transport thread\n                    options.ApplicationSchedulingMode = SchedulingMode.Inline;\n\n                    options.Listen(IPAddress.Loopback, basePort, listenOptions =>\n                    {\n                        \/\/ Uncomment the following to enable Nagle's algorithm for this endpoint.\n                        \/\/listenOptions.NoDelay = false;\n\n                        listenOptions.UseConnectionLogging();\n                    });\n\n                    options.Listen(IPAddress.Loopback, basePort + 1, listenOptions =>\n                    {\n                        listenOptions.UseHttps(\"testCert.pfx\", \"testPassword\");\n                        listenOptions.UseConnectionLogging();\n                    });\n\n                    options.ListenLocalhost(basePort + 2, listenOptions =>\n                    {\n                        listenOptions.UseHttps(\"testCert.pfx\", \"testPassword\");\n                    });\n\n                    options.ListenAnyIP(basePort + 3);\n\n                    options.UseSystemd();\n\n                    \/\/ The following section should be used to demo sockets\n                    \/\/options.ListenUnixSocket(\"\/tmp\/kestrel-test.sock\");\n                })\n                .UseContentRoot(Directory.GetCurrentDirectory())\n                .UseStartup<Startup>();\n\n            if (string.Equals(Process.GetCurrentProcess().Id.ToString(), Environment.GetEnvironmentVariable(\"LISTEN_PID\")))\n            {\n                \/\/ Use libuv if activated by systemd, since that's currently the only transport that supports being passed a socket handle.\n                hostBuilder.UseLibuv(options =>\n                 {\n                     \/\/ Uncomment the following line to change the default number of libuv threads for all endpoints.\n                     \/\/ options.ThreadCount = 4;\n                 });\n            }\n                \n            return hostBuilder.Build().RunAsync();\n        }\n    }\n}","lang_cluster":"C#","length":100,"code_uid":"a2f6f88905b64c6b8259a1b948adc1d4"}
{"diff_hunk":"@@ -50,7 +50,7 @@ namespace System.Drawing.Tests\n         {\n             try\n             {\n-                return new [] \n+                return new []\n                 {\n                     new ImageTestData(ImageFormat.Bmp),\n                     new ImageTestData(ImageFormat.Jpeg),","old_code":"\ufeff\/\/ Licensed to the .NET Foundation under one or more agreements.\n\/\/ The .NET Foundation licenses this file to you under the MIT license.\n\/\/ See the LICENSE file in the project root for more information.\n\nusing System.Collections.Generic;\nusing System.Drawing.Imaging;\nusing System.IO;\nusing System.Runtime.InteropServices;\nusing BenchmarkDotNet.Attributes;\nusing MicroBenchmarks;\n\nnamespace System.Drawing.Tests\n{\n    [BenchmarkCategory(Categories.Libraries)]\n    public class Perf_Image_Load\n    {\n        \/\/ this field is lazy to avoid the exception during static ctor initialization of this type (harder to catch and handle properly)\n        private static readonly Lazy<ImageTestData[]> LazyTestCases = new Lazy<ImageTestData[]>(CreateTestCases);\n\n        public IEnumerable<object> ImageFormats() => LazyTestCases.Value;\n\n        [Benchmark]\n        [ArgumentsSource(nameof(ImageFormats))]\n        public void Bitmap_FromStream(ImageTestData format)\n        {\n            using (new Bitmap(format.Stream))\n            {\n            }\n        }\n\n        [Benchmark]\n        [ArgumentsSource(nameof(ImageFormats))]\n        public void Image_FromStream(ImageTestData format)\n        {\n            using (Image.FromStream(format.Stream))\n            {\n            }\n        }\n\n        [Benchmark]\n        [ArgumentsSource(nameof(ImageFormats))]\n        public void Image_FromStream_NoValidation(ImageTestData format)\n        {\n            using (Image.FromStream(format.Stream, false, false))\n            {\n            }\n        }\n\n        private static ImageTestData[] CreateTestCases()\n        {\n            try\n            {\n                return new [] \n                {\n                    new ImageTestData(ImageFormat.Bmp),\n                    new ImageTestData(ImageFormat.Jpeg),\n                    new ImageTestData(ImageFormat.Png),\n                    new ImageTestData(ImageFormat.Gif)\n                };\n            }\n            catch (Exception) when (RuntimeInformation.IsOSPlatform(OSPlatform.Linux))\n            {\n                Console.ForegroundColor = ConsoleColor.Red;\n                Console.WriteLine(\"libgdiplus is missing, you can install it by running 'apt-get install libgdiplus'\");\n                Console.ResetColor();\n\n                throw;\n            }\n        }\n\n        public class ImageTestData\n        {\n            public Stream Stream { get; }\n            private string FormatName { get; }\n\n            public ImageTestData(ImageFormat format)\n            {\n                Stream = CreateTestImage(format);\n                FormatName = format.ToString();\n            }\n\n            \/\/ the value returned by ToString is used in the text representation of Benchmark ID in our reporting system\n            public override string ToString() => FormatName;\n\n            private static Stream CreateTestImage(ImageFormat format)\n            {\n                Random r = new Random(1066); \/\/ the seed must not be changed\n\n                const int Size = 1000;\n                Point RandomPoint() => new Point(r.Next(Size), r.Next(Size));\n\n                var result = new MemoryStream();\n\n                using (Bitmap bitmap = new Bitmap(Size, Size))\n                using (Pen pen = new Pen(Color.Blue))\n                using (Graphics graphics = Graphics.FromImage(bitmap))\n                {\n                    for (int i = 0; i < 100; i++)\n                    {\n                        graphics.DrawBezier(pen, RandomPoint(), RandomPoint(), RandomPoint(), RandomPoint());\n                    }\n\n                    bitmap.Save(result, format);\n                }\n\n                return result;\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":110,"code_uid":"30d673364c384225ac049db5fd015007"}
{"diff_hunk":"@@ -152,6 +152,7 @@ namespace Microsoft.AspNetCore.Server.Kestrel.Core.Internal\n             finally\n             {\n                 CloseRawPipes();\n+                _adaptedPipelineTcs.TrySetResult(null);\n             }\n         }\n ","old_code":"\ufeff\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Server.Kestrel.Core.Adapter;\nusing Microsoft.AspNetCore.Server.Kestrel.Core.Adapter.Internal;\nusing Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Http;\nusing Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Infrastructure;\nusing Microsoft.AspNetCore.Server.Kestrel.Internal.System.IO.Pipelines;\nusing Microsoft.AspNetCore.Server.Kestrel.Transport.Abstractions;\nusing Microsoft.Extensions.Internal;\nusing Microsoft.Extensions.Logging;\n\nnamespace Microsoft.AspNetCore.Server.Kestrel.Core.Internal\n{\n    public class FrameConnection : IConnectionContext\n    {\n        private readonly FrameConnectionContext _context;\n        private readonly Frame _frame;\n        private readonly List<IConnectionAdapter> _connectionAdapters;\n        private readonly TaskCompletionSource<object> _frameStartedTcs = new TaskCompletionSource<object>();\n\n        private AdaptedPipeline _adaptedPipeline;\n        private Stream _filteredStream;\n        private Task _adaptedPipelineTask = TaskCache.CompletedTask;\n\n        public FrameConnection(FrameConnectionContext context)\n        {\n            _context = context;\n            _frame = context.Frame;\n            _connectionAdapters = context.ConnectionAdapters;\n        }\n\n        public string ConnectionId => _context.ConnectionId;\n        public IPipeWriter Input => _context.Input.Writer;\n        public IPipeReader Output => _context.Output.Reader;\n\n        private PipeFactory PipeFactory => _context.PipeFactory;\n\n        \/\/ Internal for testing\n        internal PipeOptions AdaptedPipeOptions => new PipeOptions\n        {\n            ReaderScheduler = InlineScheduler.Default,\n            WriterScheduler = InlineScheduler.Default,\n            MaximumSizeHigh = _context.ServiceContext.ServerOptions.Limits.MaxRequestBufferSize ?? 0,\n            MaximumSizeLow = _context.ServiceContext.ServerOptions.Limits.MaxRequestBufferSize ?? 0\n        };\n\n        private IKestrelTrace Log => _context.ServiceContext.Log;\n\n        public void StartRequestProcessing()\n        {\n            _frame.Input = _context.Input.Reader;\n            _frame.Output = _context.OutputProducer;\n\n            if (_connectionAdapters.Count == 0)\n            {\n                _frame.Start();\n                _frameStartedTcs.SetResult(null);\n            }\n            else\n            {\n                \/\/ Ensure that IConnectionAdapter.OnConnectionAsync does not run on the transport thread.\n                _context.ServiceContext.ThreadPool.UnsafeRun(state =>\n                {\n                    \/\/ ApplyConnectionAdaptersAsync should never throw. If it succeeds, it will call _frame.Start().\n                    \/\/ Otherwise, it will close the connection.\n                    var ignore = ((FrameConnection)state).ApplyConnectionAdaptersAsync();\n                }, this);\n            }\n        }\n\n        public void OnConnectionClosed()\n        {\n            Log.ConnectionStop(ConnectionId);\n            KestrelEventSource.Log.ConnectionStop(this);\n        }\n\n        public async Task StopAsync()\n        {\n            await _frameStartedTcs.Task;\n            await _frame.StopAsync();\n            await _adaptedPipelineTask;\n        }\n\n        public void Abort(Exception ex)\n        {\n            _frame.Abort(ex);\n        }\n\n        public void Timeout()\n        {\n            _frame.SetBadRequestState(RequestRejectionReason.RequestTimeout);\n        }\n\n        private async Task ApplyConnectionAdaptersAsync()\n        {\n            try\n            {\n                var rawSocketOutput = _frame.Output;\n                var rawStream = new RawStream(_frame.Input, rawSocketOutput);\n                var adapterContext = new ConnectionAdapterContext(rawStream);\n                var adaptedConnections = new IAdaptedConnection[_connectionAdapters.Count];\n\n                for (var i = 0; i < _connectionAdapters.Count; i++)\n                {\n                    var adaptedConnection = await _connectionAdapters[i].OnConnectionAsync(adapterContext);\n                    adaptedConnections[i] = adaptedConnection;\n                    adapterContext = new ConnectionAdapterContext(adaptedConnection.ConnectionStream);\n                }\n\n                if (adapterContext.ConnectionStream != rawStream)\n                {\n                    _filteredStream = adapterContext.ConnectionStream;\n                    _adaptedPipeline = new AdaptedPipeline(\n                        adapterContext.ConnectionStream,\n                        PipeFactory.Create(AdaptedPipeOptions),\n                        PipeFactory.Create(AdaptedPipeOptions));\n\n                    _frame.Input = _adaptedPipeline.Input.Reader;\n                    _frame.Output = _adaptedPipeline.Output;\n\n                    _adaptedPipelineTask = RunAdaptedPipeline();\n                }\n\n                _frame.AdaptedConnections = adaptedConnections;\n                _frame.Start();\n                _frameStartedTcs.SetResult(null);\n            }\n            catch (Exception ex)\n            {\n                Log.LogError(0, ex, $\"Uncaught exception from the {nameof(IConnectionAdapter.OnConnectionAsync)} method of an {nameof(IConnectionAdapter)}.\");\n                _frameStartedTcs.SetResult(null);\n                CloseRawPipes();\n            }\n        }\n\n        private async Task RunAdaptedPipeline()\n        {\n            try\n            {\n                await _adaptedPipeline.RunAsync();\n            }\n            catch (Exception ex)\n            {\n                \/\/ adaptedPipeline.RunAsync() shouldn't throw.\n                Log.LogError(0, ex, $\"{nameof(FrameConnection)}.{nameof(ApplyConnectionAdaptersAsync)}\");\n            }\n            finally\n            {\n                CloseRawPipes();\n            }\n        }\n\n        private void CloseRawPipes()\n        {\n            _filteredStream?.Dispose();\n            _context.OutputProducer.Dispose();\n            _context.Input.Reader.Complete();\n        }\n    }\n}\n","lang_cluster":"C#","length":165,"code_uid":"18fb7ed37cc849a880ad945f5185ad6d"}
{"diff_hunk":"@@ -13,11 +13,13 @@\n \/\/ See the License for the specific language governing permissions and\n \/\/ limitations under the License.\n \/\/ <\/copyright>\n+using System;\n using System.Collections.Generic;\n using System.Linq;\n using System.Text;\n using System.Threading;\n using System.Threading.Tasks;\n+using OpenTelemetry.Resources;\n using Thrift.Protocols;\n using Thrift.Protocols.Entities;\n ","old_code":"\ufeff\/\/ <copyright file=\"Process.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright 2018, OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Thrift.Protocols;\nusing Thrift.Protocols.Entities;\n\nnamespace OpenTelemetry.Exporter.Jaeger.Implementation\n{\n    public class Process : TAbstractBase\n    {\n        public Process()\n        {\n        }\n\n        public Process(string serviceName, IDictionary<string, object> processTags)\n            : this()\n        {\n            this.ServiceName = serviceName;\n\n            if (processTags != null)\n            {\n                this.Tags = new List<JaegerTag>();\n                this.Tags.AddRange(processTags.Select(pt => pt.ToJaegerTag()));\n            }\n        }\n\n        public string ServiceName { get; set; }\n\n        public List<JaegerTag> Tags { get; set; }\n\n        public async Task WriteAsync(TProtocol oprot, CancellationToken cancellationToken)\n        {\n            oprot.IncrementRecursionDepth();\n\n            try\n            {\n                var struc = new TStruct(\"Process\");\n                await oprot.WriteStructBeginAsync(struc, cancellationToken);\n\n                var field = new TField\n                {\n                    Name = \"serviceName\",\n                    Type = TType.String,\n                    ID = 1,\n                };\n\n                await oprot.WriteFieldBeginAsync(field, cancellationToken);\n                await oprot.WriteStringAsync(this.ServiceName, cancellationToken);\n                await oprot.WriteFieldEndAsync(cancellationToken);\n\n                if (this.Tags != null)\n                {\n                    field.Name = \"tags\";\n                    field.Type = TType.List;\n                    field.ID = 2;\n\n                    await oprot.WriteFieldBeginAsync(field, cancellationToken);\n                    {\n                        await oprot.WriteListBeginAsync(new TList(TType.Struct, this.Tags.Count), cancellationToken);\n\n                        foreach (JaegerTag jt in this.Tags)\n                        {\n                            await jt.WriteAsync(oprot, cancellationToken);\n                        }\n\n                        await oprot.WriteListEndAsync(cancellationToken);\n                    }\n\n                    await oprot.WriteFieldEndAsync(cancellationToken);\n                }\n\n                await oprot.WriteFieldStopAsync(cancellationToken);\n                await oprot.WriteStructEndAsync(cancellationToken);\n            }\n            finally\n            {\n                oprot.DecrementRecursionDepth();\n            }\n        }\n\n        public override string ToString()\n        {\n            var sb = new StringBuilder(\"Process(\");\n            sb.Append(\", ServiceName: \");\n            sb.Append(this.ServiceName);\n\n            if (this.Tags != null)\n            {\n                sb.Append(\", Tags: \");\n                sb.Append(this.Tags);\n            }\n\n            sb.Append(\")\");\n            return sb.ToString();\n        }\n    }\n}\n","lang_cluster":"C#","length":114,"code_uid":"670c9e135b2240a4b4f18e24a6b9590e"}
{"diff_hunk":"@@ -69,7 +69,7 @@ namespace Nethermind.Runner.Ethereum.Steps\n             \n             var validator = new AuRaValidatorProcessorFactory(\n                     readOnlyTxProcessingEnv.StateProvider,\n-                    new AbiEncoder(),\n+                    _context.AbiEncoder,\n                     readOnlyTxProcessingEnv.TransactionProcessor,\n                     new ReadOnlyTransactionProcessorSource(readOnlyTxProcessingEnv),\n                     readOnlyTxProcessingEnv.BlockTree,","old_code":"\ufeff\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing Nethermind.Abi;\nusing Nethermind.Blockchain;\nusing Nethermind.Blockchain.Processing;\nusing Nethermind.Consensus.AuRa;\nusing Nethermind.Consensus.AuRa.Config;\nusing Nethermind.Db;\nusing Nethermind.Logging;\nusing Nethermind.Runner.Ethereum.Context;\nusing Nethermind.Store;\n\nnamespace Nethermind.Runner.Ethereum.Steps\n{\n    [RunnerStepDependencies(typeof(InitializeNetwork), typeof(InitializeFinalizationAuRa), typeof(SetupKeyStore))]\n    public class StartBlockProducerAuRa : StartBlockProducer\n    {\n        private readonly AuRaEthereumRunnerContext _context;\n\n        public StartBlockProducerAuRa(AuRaEthereumRunnerContext context) : base(context)\n        {\n            _context = context;\n        }\n\n        protected override void BuildProducer()\n        {\n            if (_context.NodeKey == null) throw new StepDependencyException(nameof(_context.NodeKey));\n            if (_context.ChainSpec == null) throw new StepDependencyException(nameof(_context.ChainSpec));\n            \n            ILogger logger = _context.LogManager.GetClassLogger();\n            if (logger.IsWarn) logger.Warn(\"Starting AuRa block producer & sealer\");\n            \n            IAuRaStepCalculator stepCalculator = new AuRaStepCalculator(_context.ChainSpec.AuRa.StepDuration, _context.Timestamper);\n            BlockProducerContext producerContext = GetProducerChain();\n            var auraConfig = _context.Config<IAuraConfig>();\n            _context.BlockProducer = new AuRaBlockProducer(\n                producerContext.PendingTxSelector,\n                producerContext.ChainProcessor,\n                producerContext.ReadOnlyStateProvider,\n                _context.Sealer,\n                _context.BlockTree,\n                _context.BlockProcessingQueue,\n                _context.Timestamper,\n                _context.LogManager,\n                stepCalculator,\n                auraConfig,\n                _context.NodeKey.Address);\n        }\n\n        protected override BlockProcessor CreateBlockProcessor(ReadOnlyTxProcessingEnv readOnlyTxProcessingEnv, IReadOnlyDbProvider readOnlyDbProvider)\n        {\n            if (_context.RewardCalculatorSource == null) throw new StepDependencyException(nameof(_context.RewardCalculatorSource));\n            if (_context.ValidatorStore == null) throw new StepDependencyException(nameof(_context.ValidatorStore));\n            if (_context.ChainSpec == null) throw new StepDependencyException(nameof(_context.ChainSpec));\n            \n            var validator = new AuRaValidatorProcessorFactory(\n                    readOnlyTxProcessingEnv.StateProvider,\n                    new AbiEncoder(),\n                    readOnlyTxProcessingEnv.TransactionProcessor,\n                    new ReadOnlyTransactionProcessorSource(readOnlyTxProcessingEnv),\n                    readOnlyTxProcessingEnv.BlockTree,\n                    _context.ReceiptStorage,\n                    _context.ValidatorStore,\n                    _context.LogManager)\n                .CreateValidatorProcessor(_context.ChainSpec.AuRa.Validators);\n            \n            var blockProducer = new AuRaBlockProcessor(\n                _context.SpecProvider,\n                _context.BlockValidator,\n                _context.RewardCalculatorSource.Get(readOnlyTxProcessingEnv.TransactionProcessor),\n                readOnlyTxProcessingEnv.TransactionProcessor,\n                readOnlyDbProvider.StateDb,\n                readOnlyDbProvider.CodeDb,\n                readOnlyTxProcessingEnv.StateProvider,\n                readOnlyTxProcessingEnv.StorageProvider,\n                _context.TxPool,\n                _context.ReceiptStorage,\n                _context.LogManager, \n                validator);\n            \n            validator.SetFinalizationManager(_context.FinalizationManager, true);\n\n            return blockProducer;\n        }\n    }\n}","lang_cluster":"C#","length":100,"code_uid":"8fffd4f6f79548e980600cc7f3c56de6"}
{"diff_hunk":"@@ -117,8 +117,8 @@ namespace Nethermind.Consensus.AuRa.Transactions\n         {\n             public const int MaxCacheSize = 4096;\n             \n-            internal ICache<(Keccak ParentHash, Address Sender), ITransactionPermissionContract.TxPermissions?> Permissions { get; } =\n-                new LruCache<(Keccak ParentHash, Address Sender), ITransactionPermissionContract.TxPermissions?>(MaxCacheSize, \"TxPermissions\");\n+            internal ICache<(Keccak ParentHash, Address Sender), (ITransactionPermissionContract.TxPermissions Permissions, bool ContractExists)> Permissions { get; } =\n+                new LruCache<(Keccak ParentHash, Address Sender), (ITransactionPermissionContract.TxPermissions Permissions, bool ContractExists)>(MaxCacheSize, \"TxPermissions\");\n         }\n     }\n }","old_code":"\ufeff\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\/\/ \n\nusing System;\nusing System.Diagnostics;\nusing Nethermind.Abi;\nusing Nethermind.Consensus.AuRa.Contracts;\nusing Nethermind.Consensus.Transactions;\nusing Nethermind.Core;\nusing Nethermind.Core.Caching;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Logging;\nusing Nethermind.State;\n\nnamespace Nethermind.Consensus.AuRa.Transactions\n{\n    public class PermissionBasedTxFilter : ITxFilter\n    {\n        private readonly VersionedContract<ITransactionPermissionContract> _contract;\n        private readonly Cache _cache;\n        private readonly IReadOnlyStateProvider _stateProvider;\n        private readonly ILogger _logger;\n\n        public PermissionBasedTxFilter(\n            VersionedContract<ITransactionPermissionContract> contract,\n            Cache cache,\n            IReadOnlyStateProvider stateProvider,\n            ILogManager logManager)\n        {\n            _contract = contract ?? throw new ArgumentNullException(nameof(contract));\n            _cache = cache ?? throw new ArgumentNullException(nameof(cache));\n            _stateProvider = stateProvider ?? throw new ArgumentNullException(nameof(stateProvider));\n            _logger = logManager?.GetClassLogger<PermissionBasedTxFilter>() ?? throw new ArgumentNullException(nameof(logManager));\n        }\n        \n        public (bool Allowed, string Reason) IsAllowed(Transaction tx, BlockHeader parentHeader)\n        {\n            if (parentHeader.Number + 1 < _contract.Activation)\n            {\n                return (true, string.Empty);\n            }\n\n            var txType = GetTxType(tx);\n            var txPermissions = GetPermissions(tx, parentHeader);\n            if (_logger.IsTrace) _logger.Trace($\"Given transaction: {tx.Hash} sender: {tx.SenderAddress} to: {tx.To} value: {tx.Value}, gas_price: {tx.GasPrice}. \" +\n                                               $\"Permissions required: {txType}, got: {txPermissions}.\");\n            return (txPermissions & txType) == txType ? (true, string.Empty) : (false, \"permission denied\");\n        }\n\n        private ITransactionPermissionContract.TxPermissions GetPermissions(Transaction tx, BlockHeader parentHeader)\n        {\n            var key = (parentHeader.Hash, tx.SenderAddress);\n            var txCachedPermissions = _cache.Permissions.Get(key);\n            return txCachedPermissions ?? GetPermissionsFromContract(tx, parentHeader, key);\n        }\n\n        private ITransactionPermissionContract.TxPermissions GetPermissionsFromContract(\n            Transaction tx,\n            BlockHeader parentHeader,\n            in (Keccak Hash, Address SenderAddress) key)\n        {\n            ITransactionPermissionContract.TxPermissions txPermissions = ITransactionPermissionContract.TxPermissions.None;\n            bool shouldCache = true;\n            \n            ITransactionPermissionContract versionedContract = GetVersionedContract(parentHeader);\n            if (versionedContract is null)\n            {\n                if (_logger.IsError) _logger.Error(\"Unknown version of tx permissions contract is used.\");\n            }\n            else\n            {\n                if (_logger.IsTrace) _logger.Trace($\"Version of tx permission contract: {versionedContract.Version}.\");\n                \n                try\n                {\n                    (txPermissions, shouldCache) = versionedContract.AllowedTxTypes(parentHeader, tx);\n                }\n                catch (AbiException e)\n                {\n                    if (_logger.IsError) _logger.Error($\"Error calling tx permissions contract on {parentHeader.ToString(BlockHeader.Format.FullHashAndNumber)} for tx {tx.ToShortString()} {new StackTrace()}.\", e);\n                }\n            }\n\n            if (shouldCache)\n            {\n                _cache.Permissions.Set(key, txPermissions);\n            }\n\n            return txPermissions;\n        }\n\n        private ITransactionPermissionContract? GetVersionedContract(BlockHeader blockHeader)\n            => _contract.ResolveVersion(blockHeader);\n\n        private ITransactionPermissionContract.TxPermissions GetTxType(Transaction tx) =>\n            tx.IsContractCreation\n                ? ITransactionPermissionContract.TxPermissions.Create\n                : (_stateProvider.GetCode(tx.To) ?? Array.Empty<byte>()).Length != 0\n                    ? ITransactionPermissionContract.TxPermissions.Call\n                    : ITransactionPermissionContract.TxPermissions.Basic;\n        \n        public class Cache\n        {\n            public const int MaxCacheSize = 4096;\n            \n            internal ICache<(Keccak ParentHash, Address Sender), ITransactionPermissionContract.TxPermissions?> Permissions { get; } =\n                new LruCache<(Keccak ParentHash, Address Sender), ITransactionPermissionContract.TxPermissions?>(MaxCacheSize, \"TxPermissions\");\n        }\n    }\n}\n","lang_cluster":"C#","length":124,"code_uid":"33824f8702964d419d495031d559fa95"}
{"diff_hunk":"@@ -38,7 +38,7 @@ namespace Datadog.Trace.ClrProfiler.IntegrationTests\n     public class AspNetMvc4TestsCallTargetClassic : AspNetMvc4Tests\n     {\n         public AspNetMvc4TestsCallTargetClassic(IisFixture iisFixture, ITestOutputHelper output)\n-            : base(iisFixture, output, enableCallTarget: true, classicMode: true)\n+            : base(iisFixture, output, enableCallTarget: true, classicMode: true, enableFeatureFlag: false)\n         {\n         }\n     }","old_code":"\/\/ <copyright file=\"AspNetMvc4Tests.cs\" company=\"Datadog\">\n\/\/ Unless explicitly stated otherwise all files in this repository are licensed under the Apache 2 License.\n\/\/ This product includes software developed at Datadog (https:\/\/www.datadoghq.com\/). Copyright 2017 Datadog, Inc.\n\/\/ <\/copyright>\n\n#if NET461\n#pragma warning disable SA1402 \/\/ File may only contain a single class\n#pragma warning disable SA1649 \/\/ File name must match first type name\n\nusing System.Net;\nusing System.Threading.Tasks;\nusing Datadog.Trace.TestHelpers;\nusing Xunit;\nusing Xunit.Abstractions;\n\nnamespace Datadog.Trace.ClrProfiler.IntegrationTests\n{\n    [CollectionDefinition(\"IisTests\", DisableParallelization = true)]\n    [Collection(\"IisTests\")]\n    public class AspNetMvc4TestsCallsiteClassic : AspNetMvc4Tests\n    {\n        public AspNetMvc4TestsCallsiteClassic(IisFixture iisFixture, ITestOutputHelper output)\n            : base(iisFixture, output, enableCallTarget: false, classicMode: true)\n        {\n        }\n    }\n\n    [Collection(\"IisTests\")]\n    public class AspNetMvc4TestsCallsiteIntegrated : AspNetMvc4Tests\n    {\n        public AspNetMvc4TestsCallsiteIntegrated(IisFixture iisFixture, ITestOutputHelper output)\n            : base(iisFixture, output, enableCallTarget: false, classicMode: false)\n        {\n        }\n    }\n\n    [Collection(\"IisTests\")]\n    public class AspNetMvc4TestsCallTargetClassic : AspNetMvc4Tests\n    {\n        public AspNetMvc4TestsCallTargetClassic(IisFixture iisFixture, ITestOutputHelper output)\n            : base(iisFixture, output, enableCallTarget: true, classicMode: true)\n        {\n        }\n    }\n\n    [Collection(\"IisTests\")]\n    public class AspNetMvc4TestsCallTargetIntegrated : AspNetMvc4Tests\n    {\n        public AspNetMvc4TestsCallTargetIntegrated(IisFixture iisFixture, ITestOutputHelper output)\n            : base(iisFixture, output, enableCallTarget: true, classicMode: false)\n        {\n        }\n    }\n\n    public abstract class AspNetMvc4Tests : TestHelper, IClassFixture<IisFixture>\n    {\n        private readonly IisFixture _iisFixture;\n\n        public AspNetMvc4Tests(IisFixture iisFixture, ITestOutputHelper output, bool enableCallTarget, bool classicMode)\n            : base(\"AspNetMvc4\", @\"test\\test-applications\\aspnet\", output)\n        {\n            SetServiceVersion(\"1.0.0\");\n            SetCallTargetSettings(enableCallTarget);\n\n            _iisFixture = iisFixture;\n            _iisFixture.TryStartIis(this, classicMode);\n        }\n\n        [Theory]\n        [Trait(\"Category\", \"EndToEnd\")]\n        [Trait(\"RunOnWindows\", \"True\")]\n        [Trait(\"LoadFromGAC\", \"True\")]\n        [MemberData(nameof(AspNetMvc4TestData.WithoutFeatureFlag), MemberType = typeof(AspNetMvc4TestData))]\n        public async Task SubmitsTraces(\n            string path,\n            string expectedAspNetResourceName,\n            string expectedResourceName,\n            HttpStatusCode expectedStatusCode,\n            bool isError,\n            string expectedErrorType,\n            string expectedErrorMessage,\n            SerializableDictionary tags)\n        {\n            await AssertWebServerSpan(\n                path,\n                _iisFixture.Agent,\n                _iisFixture.HttpPort,\n                expectedStatusCode,\n                isError,\n                expectedAspNetErrorType: expectedErrorType,\n                expectedAspNetErrorMessage: expectedErrorMessage,\n                expectedErrorType: expectedErrorType,\n                expectedErrorMessage: expectedErrorMessage,\n                \"web\",\n                \"aspnet-mvc.request\",\n                expectedAspNetResourceName,\n                expectedResourceName,\n                \"1.0.0\",\n                tags);\n        }\n    }\n}\n#endif\n","lang_cluster":"C#","length":103,"code_uid":"925fbbb01b77454ab84598a29602fb80"}
{"diff_hunk":"@@ -12,7 +12,7 @@ using Datadog.Trace.DuckTyping;\n namespace Datadog.Trace.ClrProfiler.AutoInstrumentation.Testing.MsTestV2\n {\n     \/\/\/ <summary>\n-    \/\/\/ Microsoft.VisualStudio.TestPlatform.TestFramework.Execute calltarget instrumentation\n+    \/\/\/ Microsoft.VisualStudio.TestPlatform.MSTest.TestAdapter.Execution.TestMethodRunner.Execute calltarget instrumentation\n     \/\/\/ <\/summary>\n     [InstrumentMethod(\n         AssemblyName = \"Microsoft.VisualStudio.TestPlatform.MSTest.TestAdapter\",","old_code":"\/\/ <copyright file=\"TestMethodRunnerExecuteIntegration.cs\" company=\"Datadog\">\n\/\/ Unless explicitly stated otherwise all files in this repository are licensed under the Apache 2 License.\n\/\/ This product includes software developed at Datadog (https:\/\/www.datadoghq.com\/). Copyright 2017 Datadog, Inc.\n\/\/ <\/copyright>\n\nusing System;\nusing System.ComponentModel;\nusing Datadog.Trace.Ci.Tags;\nusing Datadog.Trace.ClrProfiler.CallTarget;\nusing Datadog.Trace.DuckTyping;\n\nnamespace Datadog.Trace.ClrProfiler.AutoInstrumentation.Testing.MsTestV2\n{\n    \/\/\/ <summary>\n    \/\/\/ Microsoft.VisualStudio.TestPlatform.TestFramework.Execute calltarget instrumentation\n    \/\/\/ <\/summary>\n    [InstrumentMethod(\n        AssemblyName = \"Microsoft.VisualStudio.TestPlatform.MSTest.TestAdapter\",\n        TypeName = \"Microsoft.VisualStudio.TestPlatform.MSTest.TestAdapter.Execution.TestMethodRunner\",\n        MethodName = \"Execute\",\n        ReturnTypeName = \"Microsoft.VisualStudio.TestPlatform.MSTest.TestAdapter.ObjectModel.UnitTestResult\",\n        ParameterTypeNames = new string[0],\n        MinimumVersion = \"14.0.0\",\n        MaximumVersion = \"14.*.*\",\n        IntegrationName = MsTestIntegration.IntegrationName)]\n    [Browsable(false)]\n    [EditorBrowsable(EditorBrowsableState.Never)]\n    public class TestMethodRunnerExecuteIntegration\n    {\n        \/\/\/ <summary>\n        \/\/\/ OnMethodBegin callback\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"TTarget\">Type of the target<\/typeparam>\n        \/\/\/ <param name=\"instance\">Instance value, aka `this` of the instrumented method.<\/param>\n        \/\/\/ <returns>Calltarget state value<\/returns>\n        public static CallTargetState OnMethodBegin<TTarget>(TTarget instance)\n        {\n            if (!MsTestIntegration.IsEnabled)\n            {\n                return CallTargetState.GetDefault();\n            }\n\n            return new CallTargetState((Scope)null, instance);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ OnAsyncMethodEnd callback\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"TTarget\">Type of the target<\/typeparam>\n        \/\/\/ <typeparam name=\"TReturn\">Type of the return value<\/typeparam>\n        \/\/\/ <param name=\"instance\">Instance value, aka `this` of the instrumented method.<\/param>\n        \/\/\/ <param name=\"returnValue\">Return value<\/param>\n        \/\/\/ <param name=\"exception\">Exception instance in case the original code threw an exception.<\/param>\n        \/\/\/ <param name=\"state\">Calltarget state value<\/param>\n        \/\/\/ <returns>A response value, in an async scenario will be T of Task of T<\/returns>\n        public static CallTargetReturn<TReturn> OnMethodEnd<TTarget, TReturn>(TTarget instance, TReturn returnValue, Exception exception, CallTargetState state)\n            where TTarget : ITestMethodRunner\n        {\n            if (returnValue is Array returnValueArray && returnValueArray.Length == 1)\n            {\n                object unitTestResultObject = returnValueArray.GetValue(0);\n                if (unitTestResultObject != null && unitTestResultObject.TryDuckCast<UnitTestResultStruct>(out var unitTestResult))\n                {\n                    var outcome = unitTestResult.Outcome;\n                    if (outcome == UnitTestResultOutcome.Inconclusive || outcome == UnitTestResultOutcome.NotRunnable || outcome == UnitTestResultOutcome.Ignored)\n                    {\n                        \/\/ This instrumentation catches all tests being ignored\n                        if (state.State != null && state.State.TryDuckCast<ITestMethodRunner>(out var testMethodRunner))\n                        {\n                            var scope = MsTestIntegration.OnMethodBegin(testMethodRunner.TestMethodInfo, testMethodRunner.GetType());\n                            scope.Span.SetTag(TestTags.Status, TestTags.StatusSkip);\n                            scope.Span.SetTag(TestTags.SkipReason, unitTestResult.ErrorMessage);\n                            scope.Dispose();\n                        }\n                    }\n                }\n            }\n\n            return new CallTargetReturn<TReturn>(returnValue);\n        }\n    }\n}\n","lang_cluster":"C#","length":82,"code_uid":"196e8eb3f0ed44c2aac89567fbba472f"}
{"diff_hunk":"@@ -118,29 +118,30 @@ namespace MvvmCross.Droid.Support.V7.RecyclerView\n         public IEnumerable ItemsSource\n         {\n             get { return Adapter.ItemsSource; }\n-            set { Adapter.ItemsSource = value; }\n+            set\n+            {\n+                var adapter = Adapter;\n+                if (adapter != null)\n+                    adapter.ItemsSource = value;\n+            }\n         }\n \n         public int ItemTemplateId\n         {\n             get\n             {\n-                var singleItemDefaultTemplateSelector = ItemTemplateSelector as MvxDefaultTemplateSelector;\n-\n-                if (singleItemDefaultTemplateSelector == null)\n+                if (!(ItemTemplateSelector is MvxDefaultTemplateSelector singleItemDefaultTemplateSelector))\n                     throw new InvalidOperationException(\n-                        $\"If you wan't to use single item-template RecyclerView Adapter you can't change it's\" +\n+                        $\"If you don't want to use single item-template RecyclerView Adapter you can't change it's\" +\n                         $\"{nameof(IMvxTemplateSelector)} to anything other than {nameof(MvxDefaultTemplateSelector)}\");\n \n                 return singleItemDefaultTemplateSelector.ItemTemplateId;\n             }\n             set\n             {\n-                var singleItemDefaultTemplateSelector = ItemTemplateSelector as MvxDefaultTemplateSelector;\n-\n-                if (singleItemDefaultTemplateSelector == null)\n+                if (!(ItemTemplateSelector is MvxDefaultTemplateSelector singleItemDefaultTemplateSelector))\n                     throw new InvalidOperationException(\n-                        $\"If you wan't to use single item-template RecyclerView Adapter you can't change it's\" +\n+                        $\"If you don't want to use single item-template RecyclerView Adapter you can't change it's\" +\n                         $\"{nameof(IMvxTemplateSelector)} to anything other than {nameof(MvxDefaultTemplateSelector)}\");\n \n                 singleItemDefaultTemplateSelector.ItemTemplateId = value;","old_code":"\ufeff\/\/ Licensed to the .NET Foundation under one or more agreements.\n\/\/ The .NET Foundation licenses this file to you under the MS-PL license.\n\/\/ See the LICENSE file in the project root for more information.\n\nusing System;\nusing System.Collections;\nusing System.Windows.Input;\nusing Android.Content;\nusing Android.Runtime;\nusing Android.Util;\nusing MvvmCross.Binding.Attributes;\nusing MvvmCross.Droid.Support.V7.RecyclerView.AttributeHelpers;\nusing MvvmCross.Droid.Support.V7.RecyclerView.ItemTemplates;\nusing MvvmCross.Platforms.Android.Binding.Views;\n\nnamespace MvvmCross.Droid.Support.V7.RecyclerView\n{\n    [Register(\"mvvmcross.droid.support.v7.recyclerview.MvxRecyclerView\")]\n    public class MvxRecyclerView : Android.Support.V7.Widget.RecyclerView\n    {\n        public MvxRecyclerView(IntPtr javaReference, JniHandleOwnership transfer)\n            : base(javaReference, transfer)\n        {\n        }\n\n        public MvxRecyclerView(Context context, IAttributeSet attrs) :\n            this(context, attrs, 0, new MvxRecyclerAdapter())\n        {\n        }\n\n        public MvxRecyclerView(Context context, IAttributeSet attrs, int defStyle) \n            : this(context, attrs, defStyle, new MvxRecyclerAdapter())\n        {\n        }\n\n        public MvxRecyclerView(Context context, IAttributeSet attrs, int defStyle, IMvxRecyclerAdapter adapter) \n            : base(context, attrs, defStyle)\n        {\n            \/\/ Note: Any calling derived class passing a null adapter is responsible for setting\n            \/\/ it's own ItemTemplateSelector\n            if (adapter == null)\n                return;\n\n            var currentLayoutManager = GetLayoutManager();\n\n            \/\/ Love you Android\n            \/\/ https:\/\/code.google.com\/p\/android\/issues\/detail?id=77846#c10\n            \/\/ Don't believe those bastards, it's not fixed - workaround hack hack hack\n            if (currentLayoutManager == null)\n                SetLayoutManager(new MvxGuardedLinearLayoutManager(context));\n\n            var itemTemplateId = MvxAttributeHelpers.ReadListItemTemplateId(context, attrs);\n            var itemTemplateSelector = MvxRecyclerViewAttributeExtensions.BuildItemTemplateSelector(context, attrs, itemTemplateId);\n\n            adapter.ItemTemplateSelector = itemTemplateSelector;\n            Adapter = adapter;\n\n            if (itemTemplateId == 0)\n                itemTemplateId = global::Android.Resource.Layout.SimpleListItem1;\n\n            if (itemTemplateSelector.GetType() == typeof(MvxDefaultTemplateSelector))\n                ItemTemplateId = itemTemplateId;\n        }\n\n        public sealed override void SetLayoutManager(LayoutManager layout)\n        {\n            base.SetLayoutManager(layout);\n        }\n\n        protected override void OnDetachedFromWindow()\n        {\n            base.OnDetachedFromWindow();\n\n            \/\/ Remove all the views that are currently in play.\n            \/\/ This clears out all of the ViewHolder DataContexts by detaching the ViewHolder.\n            \/\/ Eventually the GC will come along and clear out the binding contexts.\n            \/\/ Issue #1405\n            GetLayoutManager()?.RemoveAllViews();\n        }\n\n        public new IMvxRecyclerAdapter Adapter\n        {\n            get\n            {\n                return GetAdapter() as IMvxRecyclerAdapter;\n            }\n            set\n            {\n                var existing = Adapter;\n\n                if (existing == value)\n                    return;\n\n                \/\/ Support lib doesn't seem to have anything similar to IListAdapter yet\n                \/\/ hence cast to Adapter.\n                if (value != null && existing != null)\n                {\n                    value.ItemsSource = existing.ItemsSource;\n                    value.ItemTemplateSelector = existing.ItemTemplateSelector;\n                    value.ItemClick = existing.ItemClick;\n                    value.ItemLongClick = existing.ItemLongClick;\n\n                    SwapAdapter((Adapter)value, false);\n                }\n                else\n                {\n                    SetAdapter((Adapter)value);\n                }\n\n                if (existing != null)\n                {\n                    existing.ItemsSource = null;\n                }\n            }\n        }\n\n        [MvxSetToNullAfterBinding]\n        public IEnumerable ItemsSource\n        {\n            get { return Adapter.ItemsSource; }\n            set { Adapter.ItemsSource = value; }\n        }\n\n        public int ItemTemplateId\n        {\n            get\n            {\n                var singleItemDefaultTemplateSelector = ItemTemplateSelector as MvxDefaultTemplateSelector;\n\n                if (singleItemDefaultTemplateSelector == null)\n                    throw new InvalidOperationException(\n                        $\"If you wan't to use single item-template RecyclerView Adapter you can't change it's\" +\n                        $\"{nameof(IMvxTemplateSelector)} to anything other than {nameof(MvxDefaultTemplateSelector)}\");\n\n                return singleItemDefaultTemplateSelector.ItemTemplateId;\n            }\n            set\n            {\n                var singleItemDefaultTemplateSelector = ItemTemplateSelector as MvxDefaultTemplateSelector;\n\n                if (singleItemDefaultTemplateSelector == null)\n                    throw new InvalidOperationException(\n                        $\"If you wan't to use single item-template RecyclerView Adapter you can't change it's\" +\n                        $\"{nameof(IMvxTemplateSelector)} to anything other than {nameof(MvxDefaultTemplateSelector)}\");\n\n                singleItemDefaultTemplateSelector.ItemTemplateId = value;\n                Adapter.ItemTemplateSelector = singleItemDefaultTemplateSelector;\n            }\n        }\n\n        public IMvxTemplateSelector ItemTemplateSelector\n        {\n            get { return Adapter.ItemTemplateSelector; }\n            set { Adapter.ItemTemplateSelector = value; }\n        }\n\n        public ICommand ItemClick\n        {\n            get { return Adapter.ItemClick; }\n            set { Adapter.ItemClick = value; }\n        }\n\n        public ICommand ItemLongClick\n        {\n            get { return Adapter.ItemLongClick; }\n            set { Adapter.ItemLongClick = value; }\n        }\n    }\n}\n","lang_cluster":"C#","length":169,"code_uid":"00074eb97df143e191d25d0d1b5e6c46"}
{"diff_hunk":"@@ -53,15 +53,15 @@ namespace MvvmCross.Forms.Platforms.Android.Presenters\n             FormsPagePresenter.RegisterAttributeTypes();\n         }\n \n-        public override void ChangePresentation(MvxPresentationHint hint)\n+        public override async Task<bool> ChangePresentation(MvxPresentationHint hint)\n         {\n-            FormsPagePresenter.ChangePresentation(hint);\n-            base.ChangePresentation(hint);\n+            await FormsPagePresenter.ChangePresentation(hint);\n+            return await base.ChangePresentation(hint);\n         }\n \n-        public override void Close(IMvxViewModel viewModel)\n+        public override Task<bool> Close(IMvxViewModel viewModel)\n         {\n-            FormsPagePresenter.Close(viewModel);\n+            return FormsPagePresenter.Close(viewModel);\n         }\n \n         public virtual bool ShowPlatformHost(Type hostViewModel = null)","old_code":"\ufeff\/\/ Licensed to the .NET Foundation under one or more agreements.\n\/\/ The .NET Foundation licenses this file to you under the MS-PL license.\n\/\/ See the LICENSE file in the project root for more information.\n\nusing MvvmCross.Droid.Support.V7.AppCompat;\nusing MvvmCross.Forms.Presenters;\nusing System;\nusing System.Collections.Generic;\nusing System.Reflection;\nusing MvvmCross.Forms.Core;\nusing MvvmCross.Platforms.Android.Views;\nusing MvvmCross.ViewModels;\nusing MvvmCross.Forms.Platforms.Android.Views;\nusing Xamarin.Forms;\n\nnamespace MvvmCross.Forms.Platforms.Android.Presenters\n{\n    public class MvxFormsAndroidViewPresenter\n        : MvxAppCompatViewPresenter, IMvxFormsViewPresenter\n    {\n        public MvxFormsAndroidViewPresenter(IEnumerable<Assembly> androidViewAssemblies, Application formsApplication) : base(androidViewAssemblies)\n        {\n            FormsApplication = formsApplication ?? throw new ArgumentNullException(nameof(formsApplication), \"MvxFormsApplication cannot be null\");\n        }\n\n        private Application _formsApplication;\n        public Application FormsApplication\n        {\n            get { return _formsApplication; }\n            set { _formsApplication = value; }\n        }\n\n        private IMvxFormsPagePresenter _formsPagePresenter;\n        public virtual IMvxFormsPagePresenter FormsPagePresenter\n        {\n            get\n            {\n                if (_formsPagePresenter == null)\n                    throw new ArgumentNullException(nameof(FormsPagePresenter), \"IMvxFormsPagePresenter cannot be null. Set the value in CreateViewPresenter in the setup.\");\n                return _formsPagePresenter;\n            }\n            set { _formsPagePresenter = value; }\n        }\n\n        public override void Show(MvxViewModelRequest request)\n        {\n            FormsPagePresenter.Show(request);\n        }\n\n        public override void RegisterAttributeTypes()\n        {\n            base.RegisterAttributeTypes();\n            FormsPagePresenter.RegisterAttributeTypes();\n        }\n\n        public override void ChangePresentation(MvxPresentationHint hint)\n        {\n            FormsPagePresenter.ChangePresentation(hint);\n            base.ChangePresentation(hint);\n        }\n\n        public override void Close(IMvxViewModel viewModel)\n        {\n            FormsPagePresenter.Close(viewModel);\n        }\n\n        public virtual bool ShowPlatformHost(Type hostViewModel = null)\n        {\n            \/\/ if there is no Actitivty host associated, assume is the current activity\n            if (hostViewModel == null)\n                hostViewModel = GetCurrentActivityViewModelType();\n\n            var currentHostViewModelType = GetCurrentActivityViewModelType();\n            if (hostViewModel != currentHostViewModelType)\n            {\n                var hostViewModelRequest = MvxViewModelRequest.GetDefaultRequest(hostViewModel);\n                Show(hostViewModelRequest);\n            }\n            return true;\n        }\n\n        public virtual bool ClosePlatformViews()\n        {\n            CloseFragments();\n            if (!(CurrentActivity is MvxFormsAppCompatActivity || CurrentActivity is MvxFormsApplicationActivity) &&\n                !(CurrentActivity is MvxSplashScreenActivity || CurrentActivity is MvxSplashScreenAppCompatActivity))\n                CurrentActivity?.Finish();\n            return true;\n        }\n    }\n}\n","lang_cluster":"C#","length":91,"code_uid":"02b0939912024bf093ec650608854642"}
{"diff_hunk":"@@ -91,5 +91,24 @@ namespace OpenTelemetry.Exporter\n \n             return ExportResult.Success;\n         }\n+\n+        \/\/\/ <inheritdoc \/>\n+        protected override bool OnShutdown(int timeoutMilliseconds)\n+        {\n+            if (this.Channel == null)\n+            {\n+                return true;\n+            }\n+\n+            if (timeoutMilliseconds == -1)\n+            {\n+                this.Channel.ShutdownAsync().Wait();\n+                return true;\n+            }\n+            else\n+            {\n+                return Task.WaitAny(new Task[] { this.Channel.ShutdownAsync(), Task.Delay(timeoutMilliseconds) }) == 0;\n+            }\n+        }\n     }\n }","old_code":"\/\/ <copyright file=\"OtlpMetricsExporter.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing Grpc.Core;\nusing OpenTelemetry.Exporter.OpenTelemetryProtocol;\nusing OpenTelemetry.Exporter.OpenTelemetryProtocol.Implementation;\nusing OpenTelemetry.Metrics;\nusing OtlpCollector = Opentelemetry.Proto.Collector.Metrics.V1;\n\nnamespace OpenTelemetry.Exporter\n{\n    \/\/\/ <summary>\n    \/\/\/ Exporter consuming <see cref=\"Metric\"\/> and exporting the data using\n    \/\/\/ the OpenTelemetry protocol (OTLP).\n    \/\/\/ <\/summary>\n    public class OtlpMetricsExporter : BaseOtlpExporter<Metric>\n    {\n        private readonly OtlpCollector.MetricsService.IMetricsServiceClient metricsClient;\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"OtlpMetricsExporter\"\/> class.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"options\">Configuration options for the exporter.<\/param>\n        public OtlpMetricsExporter(OtlpExporterOptions options)\n            : this(options, null)\n        {\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"OtlpMetricsExporter\"\/> class.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"options\">Configuration options for the exporter.<\/param>\n        \/\/\/ <param name=\"metricsServiceClient\"><see cref=\"OtlpCollector.MetricsService.IMetricsServiceClient\"\/>.<\/param>\n        internal OtlpMetricsExporter(OtlpExporterOptions options, OtlpCollector.MetricsService.IMetricsServiceClient metricsServiceClient = null)\n            : base(options)\n        {\n            if (metricsServiceClient != null)\n            {\n                this.metricsClient = metricsServiceClient;\n            }\n            else\n            {\n                this.Channel = options.CreateChannel();\n                this.metricsClient = new OtlpCollector.MetricsService.MetricsServiceClient(this.Channel);\n            }\n        }\n\n        \/\/\/ <inheritdoc \/>\n        public override ExportResult Export(in Batch<Metric> batch)\n        {\n            \/\/ Prevents the exporter's gRPC and HTTP operations from being instrumented.\n            using var scope = SuppressInstrumentationScope.Begin();\n\n            var request = new OtlpCollector.ExportMetricsServiceRequest();\n\n            request.AddBatch(this.ProcessResource, batch);\n            var deadline = DateTime.UtcNow.AddMilliseconds(this.Options.TimeoutMilliseconds);\n\n            try\n            {\n                this.metricsClient.Export(request, headers: this.Headers, deadline: deadline);\n            }\n            catch (RpcException ex)\n            {\n                OpenTelemetryProtocolExporterEventSource.Log.FailedToReachCollector(ex);\n                return ExportResult.Failure;\n            }\n            catch (Exception ex)\n            {\n                OpenTelemetryProtocolExporterEventSource.Log.ExportMethodException(ex);\n                return ExportResult.Failure;\n            }\n            finally\n            {\n                request.Return();\n            }\n\n            return ExportResult.Success;\n        }\n    }\n}\n","lang_cluster":"C#","length":95,"code_uid":"c5047f884e3b472b8b55102063dfdc9e"}
{"diff_hunk":"@@ -3,6 +3,7 @@\n \n using System;\n using System.Collections.Generic;\n+using System.Linq;\n using System.Reflection;\n using Microsoft.AspNetCore.Hosting;\n using Microsoft.AspNetCore.Hosting.Server;","old_code":"\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Reflection;\nusing Microsoft.AspNetCore.Hosting;\nusing Microsoft.AspNetCore.Hosting.Server;\nusing Microsoft.AspNetCore.Hosting.Server.Features;\nusing Microsoft.AspNetCore.Http.Features;\nusing Microsoft.AspNetCore.Server.Kestrel.Http;\nusing Microsoft.AspNetCore.Server.Kestrel.Infrastructure;\nusing Microsoft.Extensions.Logging;\nusing Microsoft.Extensions.Options;\n\nnamespace Microsoft.AspNetCore.Server.Kestrel\n{\n    public class KestrelServer : IServer\n    {\n        private Stack<IDisposable> _disposables;\n        private readonly IApplicationLifetime _applicationLifetime;\n        private readonly ILogger _logger;\n        private readonly IServerAddressesFeature _serverAddresses;\n\n        public KestrelServer(IOptions<KestrelServerOptions> options, IApplicationLifetime applicationLifetime, ILoggerFactory loggerFactory)\n        {\n            if (options == null)\n            {\n                throw new ArgumentNullException(nameof(options));\n            }\n\n            if (applicationLifetime == null)\n            {\n                throw new ArgumentNullException(nameof(applicationLifetime));\n            }\n\n            if (loggerFactory == null)\n            {\n                throw new ArgumentNullException(nameof(loggerFactory));\n            }\n\n            Options = options.Value ?? new KestrelServerOptions();\n            _applicationLifetime = applicationLifetime;\n            _logger = loggerFactory.CreateLogger(typeof(KestrelServer).GetTypeInfo().Namespace);\n            Features = new FeatureCollection();\n            var componentFactory = new HttpComponentFactory(Options);\n            Features.Set<IHttpComponentFactory>(componentFactory);\n            _serverAddresses = new ServerAddressesFeature();\n            Features.Set<IServerAddressesFeature>(_serverAddresses);\n        }\n\n        public IFeatureCollection Features { get; }\n\n        public KestrelServerOptions Options { get; }\n\n        public void Start<TContext>(IHttpApplication<TContext> application)\n        {\n            if (_disposables != null)\n            {\n                \/\/ The server has already started and\/or has not been cleaned up yet\n                throw new InvalidOperationException(\"Server has already started.\");\n            }\n            _disposables = new Stack<IDisposable>();\n\n            try\n            {\n                var componentFactory = Features.Get<IHttpComponentFactory>();\n                var dateHeaderValueManager = new DateHeaderValueManager();\n                var trace = new KestrelTrace(_logger);\n                var engine = new KestrelEngine(new ServiceContext\n                {\n                    FrameFactory = context =>\n                    {\n                        return new Frame<TContext>(application, context);\n                    },\n                    AppLifetime = _applicationLifetime,\n                    Log = trace,\n                    ThreadPool = new LoggingThreadPool(trace),\n                    DateHeaderValueManager = dateHeaderValueManager,\n                    ServerOptions = Options,\n                    HttpComponentFactory = componentFactory\n                });\n\n                _disposables.Push(engine);\n                _disposables.Push(dateHeaderValueManager);\n\n                var threadCount = Options.ThreadCount;\n\n                if (threadCount <= 0)\n                {\n                    throw new ArgumentOutOfRangeException(nameof(threadCount),\n                        threadCount,\n                        \"ThreadCount must be positive.\");\n                }\n\n                engine.Start(threadCount);\n                var atLeastOneListener = false;\n\n                foreach (var address in _serverAddresses.Addresses)\n                {\n                    var parsedAddress = ServerAddress.FromUrl(address);\n                    if (parsedAddress == null)\n                    {\n                        throw new FormatException(\"Unrecognized listening address: \" + address);\n                    }\n                    else\n                    {\n                        atLeastOneListener = true;\n                        _disposables.Push(engine.CreateServer(\n                            parsedAddress));\n                    }\n                }\n\n                if (!atLeastOneListener)\n                {\n                    throw new InvalidOperationException(\"No recognized listening addresses were configured.\");\n                }\n            }\n            catch\n            {\n                Dispose();\n                throw;\n            }\n        }\n\n        public void Dispose()\n        {\n            if (_disposables != null)\n            {\n                while (_disposables.Count > 0)\n                {\n                    _disposables.Pop().Dispose();\n                }\n                _disposables = null;\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":138,"code_uid":"a554d9ae9cfd4b30a3fe2c0f12882a73"}
{"diff_hunk":"@@ -42,11 +42,15 @@ namespace pwiz.Common.SystemUtil\n         private Exception _readException;\n \n         private readonly List<string> _readLines = new List<string>();\n+        private StringBuilder _errorLines;\n \n-        public ProcessStreamReader(Process process)\n+        public ProcessStreamReader(Process process, bool keepErrorLines = false)\n         {\n-            Thread threadOut = new Thread(() => ReadStream(process.StandardOutput, ref _isOutComplete));\n-            Thread threadErr = new Thread(() => ReadStream(process.StandardError, ref _isErrComplete));\n+            if (keepErrorLines)\n+                _errorLines = new StringBuilder();\n+\n+            Thread threadOut = new Thread(() => ReadStream(process.StandardOutput, ref _isOutComplete, null));\n+            Thread threadErr = new Thread(() => ReadStream(process.StandardError, ref _isErrComplete, _errorLines));\n             threadOut.Start();\n             threadErr.Start();\n         }","old_code":"\ufeff\/*\r\n * Original author: Nicholas Shulman <nicksh .at. u.washington.edu>,\r\n *                  MacCoss Lab, Department of Genome Sciences, UW\r\n *\r\n * Copyright 2009 University of Washington - Seattle, WA\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n *\/\r\nusing System;\r\nusing System.Collections.Generic;\r\nusing System.Diagnostics;\r\nusing System.IO;\r\nusing System.Threading;\r\n\r\nnamespace pwiz.Common.SystemUtil\r\n{\r\n    \/\/\/ <summary>\r\n    \/\/\/ Class for reading both standard out and standard error from a process.\r\n    \/\/\/ <para>\r\n    \/\/\/ This is a tough problem, since TextReader.ReadLine() blocks, until it\r\n    \/\/\/ has a line to return, or the process ends.  It did not seem possible\r\n    \/\/\/ to solve this on a single thread to present real-time feedback to the\r\n    \/\/\/ user based on process output.\r\n    \/\/\/ <\/para><para>\r\n    \/\/\/ One solution presented on the web looked promising, but it did not\r\n    \/\/\/ correctly interleave output from both streams reliably.<\/para>\r\n    \/\/\/ <\/summary>\r\n    public class ProcessStreamReader\r\n    {\r\n        private bool _isOutComplete;\r\n        private bool _isErrComplete;\r\n        private Exception _readException;\r\n\r\n        private readonly List<string> _readLines = new List<string>();\r\n\r\n        public ProcessStreamReader(Process process)\r\n        {\r\n            Thread threadOut = new Thread(() => ReadStream(process.StandardOutput, ref _isOutComplete));\r\n            Thread threadErr = new Thread(() => ReadStream(process.StandardError, ref _isErrComplete));\r\n            threadOut.Start();\r\n            threadErr.Start();\r\n        }\r\n\r\n        \/\/\/ <summary>\r\n        \/\/\/ Public access to read the next line from the interleaved output\r\n        \/\/\/ of both standard out and standard error.\r\n        \/\/\/ <\/summary>\r\n        public string ReadLine(IProgressMonitor progressMonitor)\r\n        {\r\n            int timeout = progressMonitor == null ? Timeout.Infinite : 10000;\r\n            lock (_readLines)\r\n            {\r\n                for (;;)\r\n                {\r\n                    if (progressMonitor != null && progressMonitor.IsCanceled)\r\n                    {\r\n                        return string.Empty;\r\n                    }\r\n                    if (_readLines.Count > 0)\r\n                    {\r\n                        string line = _readLines[0];\r\n                        _readLines.RemoveAt(0);\r\n                        return line;\r\n                    }\r\n                    if (_readException != null)\r\n                        throw _readException;\r\n                    if (_isOutComplete && _isErrComplete)\r\n                        return null;\r\n                    Monitor.Wait(_readLines, timeout);\r\n                }\r\n            }\r\n        }\r\n\r\n        public string ReadLine()\r\n        {\r\n            return ReadLine(null);\r\n        }\r\n\r\n        \/\/\/ <summary>\r\n        \/\/\/ Handles reading from a single stream, and noting its completion\r\n        \/\/\/ on a background thread.\r\n        \/\/\/ <\/summary>\r\n        private void ReadStream(TextReader reader, ref bool isComplete)\r\n        {\r\n            try\r\n            {\r\n                string line;\r\n                while ((line = reader.ReadLine()) != null)\r\n                {\r\n                    lock (_readLines)\r\n                    {\r\n                        _readLines.Add(line);\r\n                        Monitor.Pulse(_readLines);\r\n                    }\r\n                }\r\n\r\n                lock(_readLines)\r\n                {\r\n                    isComplete = true;\r\n                    Monitor.Pulse(_readLines);\r\n                }\r\n            }\r\n            catch (Exception x)\r\n            {\r\n                lock (_readLines)\r\n                {\r\n                    _readException = x;\r\n                    Monitor.Pulse(_readLines);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}","lang_cluster":"C#","length":123,"code_uid":"2df48b872811459597cbe7913d5a72c2"}
{"diff_hunk":"@@ -1,4 +1,4 @@\n-\ufeff\/\/ MvxAppStart.cs\n+\/\/ MvxAppStart.cs\n \n \/\/ MvvmCross is licensed using Microsoft Public License (Ms-PL)\n \/\/ Contributions and inspirations noted in readme.md and license.txt","old_code":"\ufeff\/\/ MvxAppStart.cs\n\n\/\/ MvvmCross is licensed using Microsoft Public License (Ms-PL)\n\/\/ Contributions and inspirations noted in readme.md and license.txt\n\/\/\n\/\/ Project Lead - Stuart Lodge, @slodge, me@slodge.com\n\nusing System;\nusing MvvmCross.Platform.Platform;\n\nnamespace MvvmCross.Core.ViewModels\n{\n    [Obsolete(\"Please use MvxNavigationServiceAppStart instead\")]\n    public class MvxAppStart<TViewModel>\n        : MvxNavigatingObject, IMvxAppStart\n        where TViewModel : IMvxViewModel\n    {\n        public void Start(object hint = null)\n        {\n            if (hint != null)\n            {\n                MvxTrace.Trace(\"Hint ignored in default MvxAppStart\");\n            }\n            ShowViewModel<TViewModel>();\n        }\n    }\n}","lang_cluster":"C#","length":27,"code_uid":"62929ddee7884f37bf7ac0d09d017907"}
{"diff_hunk":"@@ -16,6 +16,7 @@\n \n using FluentAssertions;\n using Nethermind.Blockchain.Validators;\n+using Nethermind.Core;\n using Nethermind.Core.Crypto;\n using Nethermind.Core.Specs;\n using Nethermind.Core.Test.Builders;","old_code":"\ufeff\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing FluentAssertions;\nusing Nethermind.Blockchain.Validators;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Core.Specs;\nusing Nethermind.Core.Test.Builders;\nusing Nethermind.Specs.Forks;\nusing NSubstitute;\nusing NUnit.Framework;\n\nnamespace Nethermind.Blockchain.Test.Validators\n{\n    [TestFixture]\n    public class TxValidatorTests\n    {\n        [SetUp]\n        public void Setup()\n        {\n        }\n\n        [Test]\n        public void Zero_r_is_not_valid()\n        {\n            byte[] sigData = new byte[65];\n            \/\/ r is zero\n            sigData[63] = 1; \/\/ correct s\n\n            Signature signature = new Signature(sigData);\n            var tx = Build.A.Transaction.WithSignature(signature).TestObject;\n            \n            TxValidator txValidator = new TxValidator(1);\n            txValidator.IsWellFormed(tx, MuirGlacier.Instance).Should().BeFalse();\n        }\n        \n        [Test]\n        public void Zero_s_is_not_valid()\n        {\n            byte[] sigData = new byte[65];\n            sigData[31] = 1; \/\/ correct r\n            \/\/ s is zero\n            \n            Signature signature = new Signature(sigData);\n            var tx = Build.A.Transaction.WithSignature(signature).TestObject;\n            \n            TxValidator txValidator = new TxValidator(1);\n            txValidator.IsWellFormed(tx, MuirGlacier.Instance).Should().BeFalse();\n        }\n        \n        [Test]\n        public void Bad_chain_id_is_not_valid()\n        {\n            byte[] sigData = new byte[65];\n            sigData[31] = 1; \/\/ correct r\n            sigData[63] = 1; \/\/ correct s\n            sigData[64] = 39;\n            Signature signature = new Signature(sigData);\n            var tx = Build.A.Transaction.WithSignature(signature).TestObject;\n            \n            TxValidator txValidator = new TxValidator(1);\n            txValidator.IsWellFormed(tx, MuirGlacier.Instance).Should().BeFalse();\n        }\n        \n        [Test]\n        public void No_chain_id_tx_is_valid()\n        {\n            byte[] sigData = new byte[65];\n            sigData[31] = 1; \/\/ correct r\n            sigData[63] = 1; \/\/ correct s\n            Signature signature = new Signature(sigData);\n            var tx = Build.A.Transaction.WithSignature(signature).TestObject;\n            \n            TxValidator txValidator = new TxValidator(1);\n            txValidator.IsWellFormed(tx, MuirGlacier.Instance).Should().BeTrue();\n        }\n        \n        [Test]\n        public void Is_valid_with_valid_chain_id()\n        {\n            byte[] sigData = new byte[65];\n            sigData[31] = 1; \/\/ correct r\n            sigData[63] = 1; \/\/ correct s\n            sigData[64] = 38;\n            Signature signature = new Signature(sigData);\n            var tx = Build.A.Transaction.WithSignature(signature).TestObject;\n            \n            TxValidator txValidator = new TxValidator(1);\n            txValidator.IsWellFormed(tx, MuirGlacier.Instance).Should().BeTrue();\n        }\n        \n        [TestCase(true)]\n        [TestCase(false)]\n        public void Before_eip_155_has_to_have_valid_chain_id_unless_overridden(bool validateChainId)\n        {\n            byte[] sigData = new byte[65];\n            sigData[31] = 1; \/\/ correct r\n            sigData[63] = 1; \/\/ correct s\n            sigData[64] = 41;\n            Signature signature = new Signature(sigData);\n            var tx = Build.A.Transaction.WithSignature(signature).TestObject;\n\n            IReleaseSpec releaseSpec = Substitute.For<IReleaseSpec>();\n            releaseSpec.IsEip155Enabled.Returns(false);\n            releaseSpec.ValidateChainId.Returns(validateChainId);\n            \n            TxValidator txValidator = new TxValidator(1);\n            txValidator.IsWellFormed(tx, releaseSpec).Should().Be(!validateChainId);\n        }\n    }\n}\n","lang_cluster":"C#","length":124,"code_uid":"b6e60c3df38c4a96b9bbfd99e9475d14"}
{"diff_hunk":"@@ -79,6 +79,11 @@ namespace Nethermind.JsonRpc.Data\n         [JsonProperty(NullValueHandling = NullValueHandling.Include)]\n         public byte[]? Input { get; set; }\n \n+        [JsonConverter(typeof(ByteConverter))]\n+        public byte Type { get; set; }\n+        \n+        public AccessListItemForRpc[]? AccessList { get; set; }\n+\n         public UInt256? V { get; set; }\n \n         public UInt256? S { get; set; }","old_code":"\ufeff\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing Nethermind.Core;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Core.Extensions;\nusing Nethermind.Int256;\nusing Newtonsoft.Json;\n\nnamespace Nethermind.JsonRpc.Data\n{\n    public class TransactionForRpc\n    {\n        public TransactionForRpc(Transaction transaction) : this(null, null, null, transaction) { }\n\n        public TransactionForRpc(Keccak? blockHash, long? blockNumber, int? txIndex, Transaction transaction)\n        {\n            Hash = transaction.Hash;\n            Nonce = transaction.Nonce;\n            BlockHash = blockHash;\n            BlockNumber = blockNumber;\n            TransactionIndex = txIndex;\n            From = transaction.SenderAddress;\n            To = transaction.To;\n            Value = transaction.Value;\n            GasPrice = transaction.GasPrice;\n            Gas = transaction.GasLimit;\n            Input = Data = transaction.Data;\n\n            Signature? signature = transaction.Signature;\n            if (signature != null)\n            {\n                R = new UInt256(signature.R, true);\n                S = new UInt256(signature.S, true);\n                V = (UInt256?)signature.V;\n            }\n        }\n\n        \/\/ ReSharper disable once UnusedMember.Global\n        public TransactionForRpc()\n        {\n        }\n\n        public Keccak? Hash { get; set; }\n        public UInt256? Nonce { get; set; }\n\n        [JsonProperty(NullValueHandling = NullValueHandling.Include)]\n        public Keccak? BlockHash { get; set; }\n\n        [JsonProperty(NullValueHandling = NullValueHandling.Include)]\n        public long? BlockNumber { get; set; }\n\n        [JsonProperty(NullValueHandling = NullValueHandling.Include)]\n        public long? TransactionIndex { get; set; }\n\n        public Address? From { get; set; }\n\n        [JsonProperty(NullValueHandling = NullValueHandling.Include)]\n        public Address? To { get; set; }\n\n        public UInt256? Value { get; set; }\n        public UInt256? GasPrice { get; set; }\n        public long? Gas { get; set; }\n        public byte[]? Data { get; set; }\n\n        [JsonProperty(NullValueHandling = NullValueHandling.Include)]\n        public byte[]? Input { get; set; }\n\n        public UInt256? V { get; set; }\n\n        public UInt256? S { get; set; }\n\n        public UInt256? R { get; set; }\n\n        public Transaction ToTransactionWithDefaults()\n        {\n            Transaction tx = new();\n            tx.GasLimit = Gas ?? 90000;\n            tx.GasPrice = GasPrice ?? 20.GWei();\n            tx.Nonce = (ulong)(Nonce ?? 0); \/\/ here pick the last nonce?\n            tx.To = To;\n            tx.SenderAddress = From;\n            tx.Value = Value ?? 0;\n            tx.Data = Data ?? Input;\n\n            return tx;\n        }\n\n        public Transaction ToTransaction()\n        {\n            Transaction tx = new();\n            tx.GasLimit = Gas ?? 0;\n            tx.GasPrice = GasPrice ?? 0;\n            tx.Nonce = (ulong)(Nonce ?? 0); \/\/ here pick the last nonce?\n            tx.To = To;\n            tx.SenderAddress = From;\n            tx.Value = Value ?? 0;\n            tx.Data = Data ?? Input;\n\n            return tx;\n        }\n    }\n}\n","lang_cluster":"C#","length":116,"code_uid":"4f03f7bea1c142fd8d18818422537cca"}
{"diff_hunk":"@@ -6,9 +6,7 @@ using System.Buffers;\n using System.IO.Pipelines;\n using System.Threading;\n using System.Threading.Tasks;\n-using Microsoft.AspNetCore.Http.Features;\n using Microsoft.AspNetCore.Connections;\n-using Microsoft.AspNetCore.Connections.Features;\n using Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Infrastructure;\n using Microsoft.AspNetCore.Server.Kestrel.Transport.Abstractions.Internal;\n using Microsoft.Extensions.Logging;","old_code":"\ufeff\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System;\nusing System.Buffers;\nusing System.IO.Pipelines;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Http.Features;\nusing Microsoft.AspNetCore.Connections;\nusing Microsoft.AspNetCore.Connections.Features;\nusing Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Infrastructure;\nusing Microsoft.AspNetCore.Server.Kestrel.Transport.Abstractions.Internal;\nusing Microsoft.Extensions.Logging;\n\nnamespace Microsoft.AspNetCore.Server.Kestrel.Core.Internal\n{\n    public class ConnectionDispatcher : IConnectionDispatcher\n    {\n        private static long _lastConnectionId = long.MinValue;\n\n        private readonly ServiceContext _serviceContext;\n        private readonly ConnectionDelegate _connectionDelegate;\n\n        public ConnectionDispatcher(ServiceContext serviceContext, ConnectionDelegate connectionDelegate)\n        {\n            _serviceContext = serviceContext;\n            _connectionDelegate = connectionDelegate;\n        }\n\n        private IKestrelTrace Log => _serviceContext.Log;\n\n        public Task OnConnection(TransportConnection connection)\n        {\n            \/\/ REVIEW: Unfortunately, we still need to use the service context to create the pipes since the settings\n            \/\/ for the scheduler and limits are specified here\n            var inputOptions = GetInputPipeOptions(_serviceContext, connection.MemoryPool, connection.InputWriterScheduler);\n            var outputOptions = GetOutputPipeOptions(_serviceContext, connection.MemoryPool, connection.OutputReaderScheduler);\n\n            var pair = DuplexPipe.CreateConnectionPair(inputOptions, outputOptions);\n\n            \/\/ Set the transport and connection id\n            connection.ConnectionId = CorrelationIdGenerator.GetNextId();\n            connection.Transport = pair.Transport;\n\n            \/\/ This *must* be set before returning from OnConnection\n            connection.Application = pair.Application;\n\n            return Execute(new KestrelConnection(connection));\n        }\n\n        private async Task Execute(KestrelConnection connection)\n        {\n            var id = Interlocked.Increment(ref _lastConnectionId);\n            var connectionContext = connection.TransportConnection;\n\n            try\n            {\n                _serviceContext.ConnectionManager.AddConnection(id, connection);\n\n                Log.ConnectionStart(connectionContext.ConnectionId);\n                KestrelEventSource.Log.ConnectionStart(connectionContext);\n\n                using (BeginConnectionScope(connectionContext))\n                {\n                    try\n                    {\n                        await _connectionDelegate(connectionContext);\n                    }\n                    catch (Exception ex)\n                    {\n                        Log.LogCritical(0, ex, $\"{nameof(ConnectionDispatcher)}.{nameof(Execute)}() {connectionContext.ConnectionId}\");\n                    }\n                    finally\n                    {\n                        \/\/ Complete the transport PipeReader and PipeWriter after calling into application code\n                        connectionContext.Transport.Input.Complete();\n                        connectionContext.Transport.Output.Complete();\n                    }\n\n                    \/\/ Wait for the transport to close\n                    await CancellationTokenAsTask(connectionContext.ConnectionClosed);\n                }\n            }\n            finally\n            {\n                Log.ConnectionStop(connectionContext.ConnectionId);\n                KestrelEventSource.Log.ConnectionStop(connectionContext);\n\n                connection.Complete();\n\n                _serviceContext.ConnectionManager.RemoveConnection(id);\n            }\n        }\n\n        private IDisposable BeginConnectionScope(ConnectionContext connectionContext)\n        {\n            if (Log.IsEnabled(LogLevel.Critical))\n            {\n                return Log.BeginScope(new ConnectionLogScope(connectionContext.ConnectionId));\n            }\n\n            return null;\n        }\n\n        private static Task CancellationTokenAsTask(CancellationToken token)\n        {\n            if (token.IsCancellationRequested)\n            {\n                return Task.CompletedTask;\n            }\n\n            \/\/ Transports already dispatch prior to tripping ConnectionClosed\n            \/\/ since application code can register to this token.\n            var tcs = new TaskCompletionSource<object>();\n            token.Register(state => ((TaskCompletionSource<object>)state).SetResult(null), tcs);\n            return tcs.Task;\n        }\n\n        \/\/ Internal for testing\n        internal static PipeOptions GetInputPipeOptions(ServiceContext serviceContext, MemoryPool<byte> memoryPool, PipeScheduler writerScheduler) => new PipeOptions\n        (\n            pool: memoryPool,\n            readerScheduler: serviceContext.Scheduler,\n            writerScheduler: writerScheduler,\n            pauseWriterThreshold: serviceContext.ServerOptions.Limits.MaxRequestBufferSize ?? 0,\n            resumeWriterThreshold: serviceContext.ServerOptions.Limits.MaxRequestBufferSize ?? 0,\n            useSynchronizationContext: false,\n            minimumSegmentSize: KestrelMemoryPool.MinimumSegmentSize\n        );\n\n        internal static PipeOptions GetOutputPipeOptions(ServiceContext serviceContext, MemoryPool<byte> memoryPool, PipeScheduler readerScheduler) => new PipeOptions\n        (\n            pool: memoryPool,\n            readerScheduler: readerScheduler,\n            writerScheduler: serviceContext.Scheduler,\n            pauseWriterThreshold: GetOutputResponseBufferSize(serviceContext),\n            resumeWriterThreshold: GetOutputResponseBufferSize(serviceContext),\n            useSynchronizationContext: false,\n            minimumSegmentSize: KestrelMemoryPool.MinimumSegmentSize\n        );\n\n        private static long GetOutputResponseBufferSize(ServiceContext serviceContext)\n        {\n            var bufferSize = serviceContext.ServerOptions.Limits.MaxResponseBufferSize;\n            if (bufferSize == 0)\n            {\n                \/\/ 0 = no buffering so we need to configure the pipe so the the writer waits on the reader directly\n                return 1;\n            }\n\n            \/\/ null means that we have no back pressure\n            return bufferSize ?? 0;\n        }\n    }\n}\n","lang_cluster":"C#","length":156,"code_uid":"0a0225cbc17a4455a529157f7e8019c6"}
{"diff_hunk":"@@ -11,6 +11,7 @@ namespace Microsoft.VisualStudio.TestPlatform.CommandLine.Processors.Utilities\n     using Microsoft.VisualStudio.TestPlatform.Common.Interfaces;\n     using Microsoft.VisualStudio.TestPlatform.ObjectModel;\n     using Microsoft.VisualStudio.TestPlatform.ObjectModel.Utilities;\n+    using Microsoft.VisualStudio.TestPlatform.Common;\n \n     \/\/\/ <summary>\n     \/\/\/ Utilities to get the run settings from the provider and the commandline options specified.","old_code":"\/\/ Copyright (c) Microsoft Corporation. All rights reserved.\n\/\/ Licensed under the MIT license. See LICENSE file in the project root for full license information.\n\nnamespace Microsoft.VisualStudio.TestPlatform.CommandLine.Processors.Utilities\n{\n    using System.Diagnostics.CodeAnalysis;\n    using System.IO;\n    using System.Xml;\n\n    using Microsoft.VisualStudio.TestPlatform.Utilities;\n    using Microsoft.VisualStudio.TestPlatform.Common.Interfaces;\n    using Microsoft.VisualStudio.TestPlatform.ObjectModel;\n    using Microsoft.VisualStudio.TestPlatform.ObjectModel.Utilities;\n\n    \/\/\/ <summary>\n    \/\/\/ Utilities to get the run settings from the provider and the commandline options specified.\n    \/\/\/ <\/summary>\n    internal class RunSettingsUtilities\n    {\n        private const string EmptyRunSettings = @\"<RunSettings><\/RunSettings>\";\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the run settings to be used for the session.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"runSettingsProvider\"> The current provider of run settings.<\/param>\n        \/\/\/ <param name=\"commandlineOptions\"> The command line options specified. <\/param>\n        \/\/\/ <returns><\/returns>\n        internal static string GetRunSettings(IRunSettingsProvider runSettingsProvider, CommandLineOptions commandlineOptions)\n        {\n            var runSettings = runSettingsProvider?.ActiveRunSettings?.SettingsXml;\n\n            if (string.IsNullOrWhiteSpace(runSettings))\n            {\n                runSettings = EmptyRunSettings;\n            }\n\n            runSettings = GetEffectiveRunSettings(runSettings, commandlineOptions);\n\n            return runSettings;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the effective run settings adding the commandline options to the run settings if not already present.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"runSettings\"> The run settings XML. <\/param>\n        \/\/\/ <param name=\"commandLineOptions\"> The command line options. <\/param>\n        \/\/\/ <returns> Effective run settings. <\/returns>\n        [SuppressMessage(\"Microsoft.Security.Xml\", \"CA3053:UseXmlSecureResolver\",\n            Justification = \"XmlDocument.XmlResolver is not available in core. Suppress until fxcop issue is fixed.\")]\n        private static string GetEffectiveRunSettings(string runSettings, CommandLineOptions commandLineOptions)\n        {\n            var architecture = Constants.DefaultPlatform;\n\n            if (commandLineOptions != null && commandLineOptions.ArchitectureSpecified)\n            {\n                architecture = commandLineOptions.TargetArchitecture;\n            }\n\n            var framework = Framework.DefaultFramework;\n\n            if (commandLineOptions != null && commandLineOptions.FrameworkVersionSpecified)\n            {\n                framework = commandLineOptions.TargetFrameworkVersion;\n            }\n\n            var defaultResultsDirectory = Path.Combine(Directory.GetCurrentDirectory(), Constants.ResultsDirectoryName);\n\n            using (var stream = new StringReader(runSettings))\n            using (var reader = XmlReader.Create(stream, XmlRunSettingsUtilities.ReaderSettings))\n            {\n                var document = new XmlDocument();\n                document.Load(reader);\n\n                var navigator = document.CreateNavigator();\n\n                InferRunSettingsHelper.UpdateRunSettingsWithUserProvidedSwitches(navigator, architecture, framework, defaultResultsDirectory);\n\n                if (commandLineOptions != null && commandLineOptions.Parallel)\n                {\n                    ParallelRunSettingsUtilities.UpdateRunSettingsWithParallelSettingIfNotConfigured(navigator);\n                }\n\n                return navigator.OuterXml;\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":87,"code_uid":"cd8e52cfc43241259b410447ece98b67"}
{"diff_hunk":"@@ -120,6 +120,7 @@ namespace Nethermind.DataMarketplace.Infrastructure\n             EthJsonRpcClientProxy = ethJsonRpcClientProxy;\n             HttpClient = httpClient;\n             MonitoringService = monitoringService;\n+            BloomStorage = bloomStorage;\n         }\n     }\n }","old_code":"\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing Nethermind.Blockchain;\nusing Nethermind.Blockchain.Filters;\nusing Nethermind.Blockchain.Receipts;\nusing Nethermind.Blockchain.TxPools;\nusing Nethermind.Config;\nusing Nethermind.Core;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Core.Specs;\nusing Nethermind.Crypto;\nusing Nethermind.Specs;\nusing Nethermind.DataMarketplace.Channels;\nusing Nethermind.DataMarketplace.Core;\nusing Nethermind.DataMarketplace.Core.Configs;\nusing Nethermind.DataMarketplace.Core.Services;\nusing Nethermind.DataMarketplace.Infrastructure.Persistence.Mongo;\nusing Nethermind.Facade.Proxy;\nusing Nethermind.Grpc;\nusing Nethermind.JsonRpc.Modules;\nusing Nethermind.KeyStore;\nusing Nethermind.Logging;\nusing Nethermind.Monitoring;\nusing Nethermind.Network;\nusing Nethermind.Serialization.Json;\nusing Nethermind.Store;\nusing Nethermind.Wallet;\n\nnamespace Nethermind.DataMarketplace.Infrastructure\n{\n    public class NdmRequiredServices\n    {\n        public IConfigProvider ConfigProvider { get; }\n        public IConfigManager ConfigManager { get; }\n        public INdmConfig NdmConfig { get; }\n        public string BaseDbPath { get; }\n        public IDbProvider RocksProvider { get; }\n        public IMongoProvider MongoProvider { get; }\n        public ILogManager LogManager { get; }\n        public IBlockTree BlockTree { get; }\n        public ITxPool TransactionPool { get; }\n        public ISpecProvider SpecProvider { get; }\n        public IReceiptStorage ReceiptStorage { get; }\n        public IFilterStore FilterStore { get; }\n        public IFilterManager FilterManager { get; }\n        public IWallet Wallet { get; }\n        public ITimestamper Timestamper { get; }\n        public IEthereumEcdsa Ecdsa { get; }\n        public IKeyStore KeyStore { get; }\n        public IRpcModuleProvider RpcModuleProvider { get; }\n        public IJsonSerializer JsonSerializer { get; }\n        public ICryptoRandom CryptoRandom { get; }\n        public IEnode Enode { get; }\n        public INdmConsumerChannelManager NdmConsumerChannelManager { get; }\n        public INdmDataPublisher NdmDataPublisher { get; }\n        public IGrpcServer GrpcServer { get; }\n        public IEthRequestService EthRequestService { get; }\n        public INdmNotifier Notifier { get; }\n        public bool EnableUnsecuredDevWallet { get; }\n        public IBlockProcessor BlockProcessor { get; }\n        public IJsonRpcClientProxy JsonRpcClientProxy { get; }\n        public IEthJsonRpcClientProxy EthJsonRpcClientProxy { get; }\n        public IHttpClient HttpClient { get; }\n        public IMonitoringService MonitoringService { get; }\n\n        public NdmRequiredServices(IConfigProvider configProvider, IConfigManager configManager, INdmConfig ndmConfig,\n            string baseDbPath, IDbProvider rocksProvider, IMongoProvider mongoProvider, ILogManager logManager,\n            IBlockTree blockTree, ITxPool transactionPool, ISpecProvider specProvider, IReceiptStorage receiptStorage,\n            IFilterStore filterStore, IFilterManager filterManager, IWallet wallet, ITimestamper timestamper,\n            IEthereumEcdsa ecdsa, IKeyStore keyStore, IRpcModuleProvider rpcModuleProvider,\n            IJsonSerializer jsonSerializer, ICryptoRandom cryptoRandom, IEnode enode,\n            INdmConsumerChannelManager ndmConsumerChannelManager, INdmDataPublisher ndmDataPublisher,\n            IGrpcServer grpcServer, IEthRequestService ethRequestService, INdmNotifier notifier,\n            bool enableUnsecuredDevWallet, IBlockProcessor blockProcessor, IJsonRpcClientProxy jsonRpcClientProxy,\n            IEthJsonRpcClientProxy ethJsonRpcClientProxy, IHttpClient httpClient, IMonitoringService monitoringService)\n        {\n            ConfigProvider = configProvider;\n            ConfigManager = configManager;\n            NdmConfig = ndmConfig;\n            BaseDbPath = baseDbPath;\n            RocksProvider = rocksProvider;\n            MongoProvider = mongoProvider;\n            LogManager = logManager;\n            BlockTree = blockTree;\n            TransactionPool = transactionPool;\n            SpecProvider = specProvider;\n            ReceiptStorage = receiptStorage;\n            FilterStore = filterStore;\n            FilterManager = filterManager;\n            Wallet = wallet;\n            Timestamper = timestamper;\n            Ecdsa = ecdsa;\n            KeyStore = keyStore;\n            RpcModuleProvider = rpcModuleProvider;\n            JsonSerializer = jsonSerializer;\n            CryptoRandom = cryptoRandom;\n            Enode = enode;\n            NdmConsumerChannelManager = ndmConsumerChannelManager;\n            NdmDataPublisher = ndmDataPublisher;\n            GrpcServer = grpcServer;\n            EthRequestService = ethRequestService;\n            Notifier = notifier;\n            EnableUnsecuredDevWallet = enableUnsecuredDevWallet;\n            BlockProcessor = blockProcessor;\n            JsonRpcClientProxy = jsonRpcClientProxy;\n            EthJsonRpcClientProxy = ethJsonRpcClientProxy;\n            HttpClient = httpClient;\n            MonitoringService = monitoringService;\n        }\n    }\n}","lang_cluster":"C#","length":125,"code_uid":"b1bbc8d8282849ca93e4b4413b4d3f2f"}
{"diff_hunk":"@@ -11,7 +11,6 @@ namespace Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.DataCollection\n     using Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.DataCollection.Interfaces;\n     using Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.Helpers;\n     using Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.Helpers.Interfaces;\n-    using Microsoft.VisualStudio.TestPlatform.ObjectModel;\n \n     \/\/\/ <summary>\n     \/\/\/ The datacollection launcher.","old_code":"\/\/ Copyright (c) Microsoft Corporation. All rights reserved.\n\/\/ Licensed under the MIT license. See LICENSE file in the project root for full license information.\n\nnamespace Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.DataCollection\n{\n    using System.Collections.Generic;\n    using System.Diagnostics;\n    using System.IO;\n    using System.Reflection;\n\n    using Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.DataCollection.Interfaces;\n    using Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.Helpers;\n    using Microsoft.VisualStudio.TestPlatform.CrossPlatEngine.Helpers.Interfaces;\n    using Microsoft.VisualStudio.TestPlatform.ObjectModel;\n\n    \/\/\/ <summary>\n    \/\/\/ The datacollection launcher.\n    \/\/\/ This works for Desktop local scenarios\n    \/\/\/ <\/summary>\n    internal class DataCollectionLauncher : IDataCollectionLauncher\n    {\n        private const string DataCollectorProcessName = \"datacollector.exe\";\n        private const string DotnetProcessName = \"dotnet.exe\";\n        private const string DotnetProcessNameXPlat = \"dotnet\";\n\n        private string dataCollectorProcessName;\n        private Process dataCollectorProcess;\n        private IProcessHelper processHelper;\n        \n        \/\/\/ <summary>\n        \/\/\/ The constructor.\n        \/\/\/ <\/summary>\n        public DataCollectionLauncher()\n            : this(new ProcessHelper())\n        {\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"DataCollectionLauncher\"\/> class.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"processHelper\">\n        \/\/\/ The process helper. \n        \/\/\/ <\/param>\n        internal DataCollectionLauncher(IProcessHelper processHelper)\n        {\n            this.processHelper = processHelper;\n            this.dataCollectorProcess = null;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Initialize with desired architecture for the host\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"architecture\">architecture for the host<\/param>\n        public void Initialize(Architecture architecture)\n        {\n            this.dataCollectorProcessName = DataCollectorProcessName;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Launches the test host for discovery\/execution.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"environmentVariables\">Environment variables for the process.<\/param>\n        \/\/\/ <param name=\"commandLineArguments\">The command line arguments to pass to the process.<\/param>\n        \/\/\/ <returns>ProcessId of launched Process. 0 means not launched.<\/returns>\n        public virtual int LaunchDataCollector(IDictionary<string, string> environmentVariables, IList<string> commandLineArguments)\n        {\n            var currentWorkingDirectory = Path.GetDirectoryName(typeof(DataCollectionLauncher).GetTypeInfo().Assembly.Location);\n            string dataCollectorProcessPath, processWorkingDirectory = null;\n\n            \/\/ TODO: DRY: Move this code to a common place\n            \/\/ If we are running in the dotnet.exe context we do not want to launch dataCollector.exe but dotnet.exe with the dataCollector assembly. \n            \/\/ Since dotnet.exe is already built for multiple platforms this would avoid building dataCollector.exe also in multiple platforms.\n            var currentProcessFileName = this.processHelper.GetCurrentProcessFileName();\n            if (currentProcessFileName.EndsWith(DotnetProcessName) || currentProcessFileName.EndsWith(DotnetProcessNameXPlat))\n            {\n                dataCollectorProcessPath = currentProcessFileName;\n                var dataCollectorAssemblyPath = Path.Combine(currentWorkingDirectory, this.dataCollectorProcessName.Replace(\"exe\", \"dll\"));\n                commandLineArguments.Insert(0, dataCollectorAssemblyPath);\n                processWorkingDirectory = Path.GetDirectoryName(currentProcessFileName);\n            }\n            else\n            {\n                dataCollectorProcessPath = Path.Combine(currentWorkingDirectory, this.dataCollectorProcessName);\n                \/\/ For IDEs and other scenario - Current directory should be the working directory - not the vstest.console.exe location\n                \/\/ For VS - this becomes the solution directory for example\n                \/\/ \"TestResults\" directory will be created at \"current directory\" of test host\n                processWorkingDirectory = Directory.GetCurrentDirectory();\n            }\n\n            var argumentsString = string.Join(\" \", commandLineArguments);\n\n            this.dataCollectorProcess = this.processHelper.LaunchProcess(dataCollectorProcessPath, argumentsString, processWorkingDirectory);\n            return this.dataCollectorProcess.Id;\n        }\n\n    }\n}","lang_cluster":"C#","length":97,"code_uid":"f67939ff30514ca5aa0c9f2920149914"}
{"diff_hunk":"@@ -63,7 +63,7 @@ namespace Microsoft.AspNet.Server.Kestrel.Networking\n             }\n             catch (Exception ex)\n             {\n-                Trace.WriteLine(\"UvConnectRequest \" + ex.ToString());\n+                req._log.LogError(\"UvConnectRequest\", ex);\n             }\n         }\n     }","old_code":"\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.Runtime.InteropServices;\n\nnamespace Microsoft.AspNet.Server.Kestrel.Networking\n{\n    \/\/\/ <summary>\n    \/\/\/ Summary description for UvWriteRequest\n    \/\/\/ <\/summary>\n    public class UvConnectRequest : UvRequest\n    {\n        private readonly static Libuv.uv_connect_cb _uv_connect_cb = UvConnectCb;\n\n        private Action<UvConnectRequest, int, Exception, object> _callback;\n        private object _state;\n\n        public void Init(UvLoopHandle loop)\n        {\n            var requestSize = loop.Libuv.req_size(Libuv.RequestType.CONNECT);\n            CreateMemory(\n                loop.Libuv,\n                loop.ThreadId,\n                requestSize);\n        }\n\n        public void Connect(\n            UvPipeHandle pipe, \n            string name, \n            Action<UvConnectRequest, int, Exception, object> callback, \n            object state)\n        {\n            _callback = callback;\n            _state = state;\n\n            Pin();\n            Libuv.pipe_connect(this, pipe, name, _uv_connect_cb);\n        }\n\n        private static void UvConnectCb(IntPtr ptr, int status)\n        {\n            var req = FromIntPtr<UvConnectRequest>(ptr);\n            req.Unpin();\n\n            var callback = req._callback;\n            req._callback = null;\n\n            var state = req._state;\n            req._state = null;\n\n            Exception error = null;\n            if (status < 0)\n            {\n                req.Libuv.Check(status, out error);\n            }\n\n            try\n            {\n                callback(req, status, error, state);\n            }\n            catch (Exception ex)\n            {\n                Trace.WriteLine(\"UvConnectRequest \" + ex.ToString());\n            }\n        }\n    }\n}","lang_cluster":"C#","length":70,"code_uid":"ec71e2ebfe654a87a4b5f92b33c309ef"}
{"diff_hunk":"@@ -150,6 +150,20 @@ namespace NLog.LayoutRenderers.Wrappers\n             }\n         }\n \n+        bool InvalidateCachedValue(LogEventInfo logEvent)\n+        {\n+            var newCacheKey = CacheKey?.Render(logEvent);\n+            if (_cachedValue == null || _renderedCacheKey != newCacheKey || (_cachedValueTimeout.HasValue && logEvent.TimeStamp > _cachedValueExpires))\n+            {\n+                _renderedCacheKey = newCacheKey;\n+                if (_cachedValueTimeout.HasValue)\n+                    _cachedValueExpires = logEvent.TimeStamp + _cachedValueTimeout.Value;\n+                return true;\n+            }\n+\n+            return false;\n+        }\n+\n         \/\/\/ <inheritdoc\/>\n         string IStringValueRenderer.GetFormattedString(LogEventInfo logEvent) => Cached ? RenderInner(logEvent) : null;\n     }","old_code":"\/\/ \n\/\/ Copyright (c) 2004-2019 Jaroslaw Kowalski <jaak@jkowalski.net>, Kim Christensen, Julian Verdurmen\n\/\/ \n\/\/ All rights reserved.\n\/\/ \n\/\/ Redistribution and use in source and binary forms, with or without \n\/\/ modification, are permitted provided that the following conditions \n\/\/ are met:\n\/\/ \n\/\/ * Redistributions of source code must retain the above copyright notice, \n\/\/   this list of conditions and the following disclaimer. \n\/\/ \n\/\/ * Redistributions in binary form must reproduce the above copyright notice,\n\/\/   this list of conditions and the following disclaimer in the documentation\n\/\/   and\/or other materials provided with the distribution. \n\/\/ \n\/\/ * Neither the name of Jaroslaw Kowalski nor the names of its \n\/\/   contributors may be used to endorse or promote products derived from this\n\/\/   software without specific prior written permission. \n\/\/ \n\/\/ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n\/\/ AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n\/\/ IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n\/\/ ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n\/\/ LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n\/\/ CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n\/\/ SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n\/\/ INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n\/\/ CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n\/\/ ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF \n\/\/ THE POSSIBILITY OF SUCH DAMAGE.\n\/\/ \n\nnamespace NLog.LayoutRenderers.Wrappers\n{\n    using System;\n    using System.ComponentModel;\n    using NLog.Config;\n    using NLog.Internal;\n    using NLog.Layouts;\n\n    \/\/\/ <summary>\n    \/\/\/ Applies caching to another layout output.\n    \/\/\/ <\/summary>\n    \/\/\/ <remarks>\n    \/\/\/ The value of the inner layout will be rendered only once and reused subsequently.\n    \/\/\/ <\/remarks>\n    [LayoutRenderer(\"cached\")]\n    [AmbientProperty(\"Cached\")]\n    [AmbientProperty(\"ClearCache\")]\n    [ThreadAgnostic]\n    public sealed class CachedLayoutRendererWrapper : WrapperLayoutRendererBase, IStringValueRenderer\n    {\n        \/\/\/ <summary>\n        \/\/\/ A value indicating when the cache is cleared.\n        \/\/\/ <\/summary>\n        [Flags]\n        public enum ClearCacheOption \n        { \n            \/\/\/ <summary>Never clear the cache.<\/summary>\n            None = 0,\n            \/\/\/ <summary>Clear the cache whenever the <see cref=\"CachedLayoutRendererWrapper\"\/> is initialized.<\/summary>\n            OnInit = 1,\n            \/\/\/ <summary>Clear the cache whenever the <see cref=\"CachedLayoutRendererWrapper\"\/> is closed.<\/summary>\n            OnClose = 2\n        }\n\n        private string _cachedValue;\n        private string _renderedCacheKey;\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"CachedLayoutRendererWrapper\"\/> class.\n        \/\/\/ <\/summary>\n        public CachedLayoutRendererWrapper()\n        {\n            Cached = true;\n            ClearCache = ClearCacheOption.OnInit | ClearCacheOption.OnClose;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets a value indicating whether this <see cref=\"CachedLayoutRendererWrapper\"\/> is enabled.\n        \/\/\/ <\/summary>\n        \/\/\/ <docgen category='Caching Options' order='10' \/>\n        [DefaultValue(true)]\n        public bool Cached { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets a value indicating when the cache is cleared.\n        \/\/\/ <\/summary>\n        \/\/\/ <docgen category='Caching Options' order='10' \/>\n        public ClearCacheOption ClearCache { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/ Cachekey. If the cachekey changes, resets the value. For example, the cachekey would be the current day.s\n        \/\/\/ <\/summary>\n        \/\/\/ <docgen category='Caching Options' order='10' \/>\n        public Layout CacheKey { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes the layout renderer.\n        \/\/\/ <\/summary>\n        protected override void InitializeLayoutRenderer()\n        {\n            base.InitializeLayoutRenderer();\n            if ((ClearCache & ClearCacheOption.OnInit) == ClearCacheOption.OnInit)\n                _cachedValue = null;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Closes the layout renderer.\n        \/\/\/ <\/summary>\n        protected override void CloseLayoutRenderer()\n        {\n            base.CloseLayoutRenderer();\n            if ((ClearCache & ClearCacheOption.OnClose) == ClearCacheOption.OnClose)\n                _cachedValue = null;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Transforms the output of another layout.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"text\">Output to be transform.<\/param>\n        \/\/\/ <returns>Transformed text.<\/returns>\n        protected override string Transform(string text)\n        {\n            return text;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Renders the inner layout contents.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"logEvent\">The log event.<\/param>\n        \/\/\/ <returns>Contents of inner layout.<\/returns>\n        protected override string RenderInner(LogEventInfo logEvent)\n        {\n            if (Cached)\n            {\n                var newCacheKey = CacheKey?.Render(logEvent);\n                if (_cachedValue == null || _renderedCacheKey != newCacheKey)\n                {\n                    _cachedValue = base.RenderInner(logEvent);\n                    _renderedCacheKey = newCacheKey;\n                }\n\n                return _cachedValue;\n            }\n            else\n            {\n                return base.RenderInner(logEvent);\n            }\n        }\n\n        \/\/\/ <inheritdoc\/>\n        string IStringValueRenderer.GetFormattedString(LogEventInfo logEvent) => Cached ? RenderInner(logEvent) : null;\n    }\n}\n","lang_cluster":"C#","length":156,"code_uid":"70734f3f48f2491ca577ca1f4184ea58"}
{"diff_hunk":"@@ -9,6 +9,7 @@ using Microsoft.AspNetCore.Builder;\n using Microsoft.AspNetCore.Hosting;\n using Microsoft.AspNetCore.Http;\n using Microsoft.AspNetCore.Server.Kestrel.Transport.Abstractions.Internal;\n+using Microsoft.Extensions.Configuration;\n using Microsoft.Extensions.Logging;\n \n namespace SampleApp","old_code":"\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System;\nusing System.IO;\nusing System.Net;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Builder;\nusing Microsoft.AspNetCore.Hosting;\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.AspNetCore.Server.Kestrel.Transport.Abstractions.Internal;\nusing Microsoft.Extensions.Logging;\n\nnamespace SampleApp\n{\n    public class Startup\n    {\n        public void Configure(IApplicationBuilder app, ILoggerFactory loggerFactory)\n        {\n            var logger = loggerFactory.CreateLogger(\"Default\");\n\n            app.Run(async context =>\n            {\n                var connectionFeature = context.Connection;\n                logger.LogDebug($\"Peer: {connectionFeature.RemoteIpAddress?.ToString()}:{connectionFeature.RemotePort}\"\n                    + $\"{Environment.NewLine}\"\n                    + $\"Sock: {connectionFeature.LocalIpAddress?.ToString()}:{connectionFeature.LocalPort}\");\n\n                var response = $\"hello, world{Environment.NewLine}\";\n                context.Response.ContentLength = response.Length;\n                context.Response.ContentType = \"text\/plain\";\n                await context.Response.WriteAsync(response);\n            });\n        }\n\n        public static void Main(string[] args)\n        {\n            TaskScheduler.UnobservedTaskException += (sender, e) =>\n            {\n                Console.WriteLine(\"Unobserved exception: {0}\", e.Exception);\n            };\n\n            var host = new WebHostBuilder()\n                .ConfigureLogging((_, factory) =>\n                {\n                    factory.AddConsole();\n                })\n                .UseKestrel(options =>\n                {\n                    \/\/ Run callbacks on the transport thread\n                    options.ApplicationSchedulingMode = SchedulingMode.Inline;\n\n                    options.Listen(IPAddress.Loopback, 5000, listenOptions =>\n                    {\n                        \/\/ Uncomment the following to enable Nagle's algorithm for this endpoint.\n                        \/\/listenOptions.NoDelay = false;\n\n                        listenOptions.UseConnectionLogging();\n                    });\n\n                    options.Listen(IPAddress.Loopback, 5001, listenOptions =>\n                    {\n                        listenOptions.UseHttps(\"testCert.pfx\", \"testPassword\");\n                        listenOptions.UseConnectionLogging();\n                    });\n\n                    options.UseSystemd();\n\n                    \/\/ The following section should be used to demo sockets\n                    \/\/options.ListenUnixSocket(\"\/tmp\/kestrel-test.sock\");\n                })\n                .UseLibuv(options =>\n                {\n                    \/\/ Uncomment the following line to change the default number of libuv threads for all endpoints.\n                    \/\/ options.ThreadCount = 4;\n                })\n                .UseContentRoot(Directory.GetCurrentDirectory())\n                .UseStartup<Startup>()\n                .Build();\n\n            host.Run();\n        }\n    }\n}","lang_cluster":"C#","length":84,"code_uid":"0a9362e5b7294648a588f8b3b2cb52cf"}
{"diff_hunk":"@@ -61,7 +61,7 @@ namespace OpenTelemetry.Trace\n             if (activity.IsAllDataRequested)\n             {\n                 activity.SetResource(this.resource);\n-                this.activityProcessor.OnStart(activity);\n+                this.activityProcessor?.OnStart(activity);\n             }\n         }\n ","old_code":"\ufeff\/\/ <copyright file=\"ActivitySourceAdapter.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System.Diagnostics;\nusing OpenTelemetry.Resources;\n\nnamespace OpenTelemetry.Trace\n{\n    \/\/\/ <summary>\n    \/\/\/ This class encapsulates the logic for performing ActivitySource actions\n    \/\/\/ on Activities that are created using default ActivitySource.\n    \/\/\/ All activities created without using ActivitySource will have a\n    \/\/\/ default ActivitySource assigned to them with their name as empty string.\n    \/\/\/ This class is to be used by instrumentation adapters which converts\/augments\n    \/\/\/ activies created without ActivitySource, into something which closely\n    \/\/\/ matches the one created using ActivitySource.\n    \/\/\/ <\/summary>\n    \/\/\/ <remarks>\n    \/\/\/ This class is meant to be only used when writing new Instrumentation for\n    \/\/\/ libraries which are already instrumented with DiagnosticSource\/Activity\n    \/\/\/ following this doc:\n    \/\/\/ https:\/\/github.com\/dotnet\/runtime\/blob\/master\/src\/libraries\/System.Diagnostics.DiagnosticSource\/src\/ActivityUserGuide.md.\n    \/\/\/ <\/remarks>\n    public class ActivitySourceAdapter\n    {\n        private readonly Sampler sampler;\n        private readonly Resource resource;\n        private ActivityProcessor activityProcessor;\n\n        internal ActivitySourceAdapter(Sampler sampler, ActivityProcessor activityProcessor, Resource resource)\n        {\n            this.sampler = sampler;\n            this.activityProcessor = activityProcessor;\n            this.resource = resource;\n        }\n\n        private ActivitySourceAdapter()\n        {\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Method that starts an <see cref=\"Activity\"\/>.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"activity\"><see cref=\"Activity\"\/> to be started.<\/param>\n        public void Start(Activity activity)\n        {\n            this.RunGetRequestedData(activity);\n            if (activity.IsAllDataRequested)\n            {\n                activity.SetResource(this.resource);\n                this.activityProcessor.OnStart(activity);\n            }\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Method that stops an <see cref=\"Activity\"\/>.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"activity\"><see cref=\"Activity\"\/> to be stopped.<\/param>\n        public void Stop(Activity activity)\n        {\n            if (activity.IsAllDataRequested)\n            {\n                this.activityProcessor.OnEnd(activity);\n            }\n        }\n\n        internal void UpdateProcessor(ActivityProcessor processor)\n        {\n            this.activityProcessor = processor;\n        }\n\n        private void RunGetRequestedData(Activity activity)\n        {\n            ActivityContext parentContext;\n            if (string.IsNullOrEmpty(activity.ParentId))\n            {\n                parentContext = default;\n            }\n            else if (activity.Parent != null)\n            {\n                parentContext = activity.Parent.Context;\n            }\n            else\n            {\n                parentContext = new ActivityContext(\n                    activity.TraceId,\n                    activity.ParentSpanId,\n                    activity.ActivityTraceFlags,\n                    activity.TraceStateString,\n                    isRemote: true);\n            }\n\n            var samplingParameters = new SamplingParameters(\n                parentContext,\n                activity.TraceId,\n                activity.DisplayName,\n                activity.Kind,\n                activity.TagObjects,\n                activity.Links);\n\n            var samplingResult = this.sampler.ShouldSample(samplingParameters);\n\n            switch (samplingResult.Decision)\n            {\n                case SamplingDecision.NotRecord:\n                    activity.IsAllDataRequested = false;\n                    break;\n                case SamplingDecision.Record:\n                    activity.IsAllDataRequested = true;\n                    break;\n                case SamplingDecision.RecordAndSampled:\n                    activity.IsAllDataRequested = true;\n                    activity.ActivityTraceFlags |= ActivityTraceFlags.Recorded;\n                    break;\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":131,"code_uid":"3e82ad09b7e04905ba59b89df79db5b8"}
{"diff_hunk":"@@ -33,7 +33,7 @@ namespace Nethermind.JsonRpc.Modules\n         \n         private List<string> _modules = new();\n         private List<string> _enabledModules = new();\n-        \n+\n         private Dictionary<string, ResolvedMethodInfo> _methods\n             = new(StringComparer.InvariantCulture);\n         ","old_code":"\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.IO.Abstractions;\nusing System.Linq;\nusing System.Reflection;\nusing System.Threading.Tasks;\nusing Nethermind.Logging;\nusing Newtonsoft.Json;\n\nnamespace Nethermind.JsonRpc.Modules\n{\n    public class RpcModuleProvider : IRpcModuleProvider\n    {\n        private ILogger _logger;\n        private IJsonRpcConfig _jsonRpcConfig;\n        \n        private List<string> _modules = new();\n        private List<string> _enabledModules = new();\n        \n        private Dictionary<string, ResolvedMethodInfo> _methods\n            = new(StringComparer.InvariantCulture);\n        \n        private readonly Dictionary<string, (Func<bool, Task<IRpcModule>> RentModule, Action<IRpcModule> ReturnModule)> _pools\n            = new();\n        \n        private IRpcMethodFilter _filter = NullRpcMethodFilter.Instance;\n\n        public RpcModuleProvider(IFileSystem fileSystem, IJsonRpcConfig jsonRpcConfig, ILogManager logManager)\n        {\n            _logger = logManager?.GetClassLogger() ?? throw new ArgumentNullException(nameof(logManager));\n            _jsonRpcConfig = jsonRpcConfig ?? throw new ArgumentNullException(nameof(jsonRpcConfig));\n            if (fileSystem.File.Exists(_jsonRpcConfig.CallsFilterFilePath))\n            {\n                if(_logger.IsWarn) _logger.Warn(\"Applying JSON RPC filter.\");\n                _filter = new RpcMethodFilter(_jsonRpcConfig.CallsFilterFilePath, fileSystem, _logger);\n            }\n        }\n\n        public IReadOnlyCollection<JsonConverter> Converters { get; } = new List<JsonConverter>();\n\n        public IReadOnlyCollection<string> Enabled => _enabledModules;\n\n        public IReadOnlyCollection<string> All => _modules;\n\n        public void Register<T>(IRpcModulePool<T> pool) where T : IRpcModule\n        {\n            RpcModuleAttribute attribute = typeof(T).GetCustomAttribute<RpcModuleAttribute>();\n            if (attribute == null)\n            {\n                if(_logger.IsWarn) _logger.Warn(\n                    $\"Cannot register {typeof(T).Name} as a JSON RPC module because it does not have a {nameof(RpcModuleAttribute)} applied.\");\n                return;\n            }\n            \n            string moduleType = attribute.ModuleType;\n\n            _pools[moduleType] = (async canBeShared => await pool.GetModule(canBeShared), m => pool.ReturnModule((T) m));\n            _modules.Add(moduleType);\n\n            ((List<JsonConverter>) Converters).AddRange(pool.Factory.GetConverters());\n\n            foreach ((string name, (MethodInfo info, bool readOnly, RpcEndpoint availability)) in GetMethodDict(typeof(T)))\n            {\n                ResolvedMethodInfo resolvedMethodInfo = new(moduleType, info, readOnly, availability);\n                if (_filter.AcceptMethod(resolvedMethodInfo.ToString()))\n                {\n                    _methods[name] = resolvedMethodInfo;\n                }\n            }\n\n            if (_jsonRpcConfig.EnabledModules.Contains(moduleType, StringComparer.InvariantCultureIgnoreCase))\n            {\n                _enabledModules.Add(moduleType);\n            }\n        }\n\n        public ModuleResolution Check(string methodName, RpcEndpoint rpcEndpoint)\n        {\n            if (!_methods.TryGetValue(methodName, out ResolvedMethodInfo result)) return ModuleResolution.Unknown;\n\n            if ((result.Availability & rpcEndpoint) == RpcEndpoint.None) return ModuleResolution.EndpointDisabled;\n            \n            return _enabledModules.Contains(result.ModuleType) ? ModuleResolution.Enabled : ModuleResolution.Disabled;\n        }\n\n        public (MethodInfo, bool) Resolve(string methodName)\n        {\n            if (!_methods.TryGetValue(methodName, out ResolvedMethodInfo result)) return (null, false);\n\n            return (result.MethodInfo, result.ReadOnly);\n        }\n\n        public Task<IRpcModule> Rent(string methodName, bool canBeShared)\n        {\n            if (!_methods.TryGetValue(methodName, out ResolvedMethodInfo result)) return null;\n\n            return _pools[result.ModuleType].RentModule(canBeShared);\n        }\n\n        public void Return(string methodName, IRpcModule rpcModule)\n        {\n            if (!_methods.TryGetValue(methodName, out ResolvedMethodInfo result))\n                throw new InvalidOperationException(\"Not possible to return an unresolved module\");\n\n            _pools[result.ModuleType].ReturnModule(rpcModule);\n        }\n\n        private IDictionary<string, (MethodInfo, bool, RpcEndpoint)> GetMethodDict(Type type)\n        {\n            var methods = type.GetMethods(BindingFlags.Public | BindingFlags.Instance | BindingFlags.DeclaredOnly);\n            return methods.ToDictionary(\n                x => x.Name.Trim(),\n                x =>\n                {\n                    JsonRpcMethodAttribute? jsonRpcMethodAttribute = x.GetCustomAttribute<JsonRpcMethodAttribute>();\n                    return (x, jsonRpcMethodAttribute?.IsSharable ?? true, jsonRpcMethodAttribute?.Availability ?? RpcEndpoint.All);\n                });\n        }\n        \n        private class ResolvedMethodInfo\n        {\n            public ResolvedMethodInfo(\n                string moduleType,\n                MethodInfo methodInfo,\n                bool readOnly,\n                RpcEndpoint availability)\n            {\n                ModuleType = moduleType;\n                MethodInfo = methodInfo;\n                ReadOnly = readOnly;\n                Availability = availability;\n            }\n            \n            public string ModuleType { get; }\n            public MethodInfo MethodInfo { get; }\n            public bool ReadOnly { get; }\n            public RpcEndpoint Availability { get; }\n\n            public override string ToString()\n            {\n                return MethodInfo.Name;\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":162,"code_uid":"da0e7f96779348ce81662e9a3d603eb8"}
{"diff_hunk":"@@ -40,12 +40,12 @@ namespace Microsoft.CodeAnalysis.Sarif\n                 File.WriteAllText(outputFileName, sarif);\n             }\n \n-            if (inputFileName == null) { throw new ArgumentNullException(\"inputFileName\"); };\n-            if (outputFileName == null) { throw new ArgumentNullException(\"outputFileName\"); };\n+            if (inputFileName == null) { throw new ArgumentNullException(nameof(inputFileName)); };\n+            if (outputFileName == null) { throw new ArgumentNullException(nameof(outputFileName)); };\n \n             if (Directory.Exists(outputFileName))\n             {\n-                throw new ArgumentException(\"Specified file output path exists but is a directory.\", \"outputFileName\");\n+                throw new ArgumentException(\"Specified file output path exists but is a directory.\", nameof(outputFileName));\n             }\n \n             if (!conversionOptions.HasFlag(ToolFormatConversionOptions.OverwriteExistingOutputFile) && File.Exists(outputFileName))","old_code":"\ufeff\/\/ Copyright (c) Microsoft. All rights reserved.\n\/\/ Licensed under the MIT license. See LICENSE file in the project root for full license information.\n\nusing System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Threading;\nusing Newtonsoft.Json;\nusing Microsoft.CodeAnalysis.Sarif.Converters;\nusing Microsoft.CodeAnalysis.Sarif.Writers;\nusing System.Runtime.InteropServices;\n\nnamespace Microsoft.CodeAnalysis.Sarif\n{\n    \/\/\/ <summary>\n    \/\/\/ A class that provides helpers for converting a log file produced by \n    \/\/\/ one of a well-known set of tools to the SARIF format.\n    \/\/\/ <\/summary>\n    public class ToolFormatConverter\n    {\n        \/\/\/ <summary>Converts a tool log file into the SARIF format.<\/summary>\n        \/\/\/ <exception cref=\"ArgumentNullException\">Thrown when one or more required arguments are null.<\/exception>\n        \/\/\/ <exception cref=\"ArgumentException\">Thrown when one or more arguments have unsupported or\n        \/\/\/ illegal values.<\/exception>\n        \/\/\/ <exception cref=\"InvalidOperationException\">Thrown when the requested operation is invalid.<\/exception>\n        \/\/\/ <param name=\"toolFormat\">The tool format of the input file.<\/param>\n        \/\/\/ <param name=\"inputFileName\">The input log file name.<\/param>\n        \/\/\/ <param name=\"outputFileName\">The name of the file to which the resulting SARIF log shall be\n        \/\/\/ written. This cannot be a directory.<\/param>\n        \/\/\/ <param name=\"conversionOptions\">Options for controlling the conversion.<\/param>\n        public void ConvertToStandardFormat(\n            ToolFormat toolFormat,\n            string inputFileName,\n            string outputFileName,\n            ToolFormatConversionOptions conversionOptions)\n        {\n            if (toolFormat == ToolFormat.PREfast)\n            {\n                string sarif = ConvertPREfastToStandardFormat(inputFileName);\n                File.WriteAllText(outputFileName, sarif);\n            }\n\n            if (inputFileName == null) { throw new ArgumentNullException(\"inputFileName\"); };\n            if (outputFileName == null) { throw new ArgumentNullException(\"outputFileName\"); };\n\n            if (Directory.Exists(outputFileName))\n            {\n                throw new ArgumentException(\"Specified file output path exists but is a directory.\", \"outputFileName\");\n            }\n\n            if (!conversionOptions.HasFlag(ToolFormatConversionOptions.OverwriteExistingOutputFile) && File.Exists(outputFileName))\n            {\n                throw new InvalidOperationException(\"Output file already exists and option to overwrite was not specified.\");\n            }\n\n            \/\/ FileMode settings here will results in an exception being raised if the input \n            \/\/ file does not exist, and that an existing output file will be overwritten\n            using (var input = File.OpenRead(inputFileName))\n            using (var outputTextStream = File.Create(outputFileName))\n            using (var outputTextWriter = new StreamWriter(outputTextStream))\n            using (var outputJson = new JsonTextWriter(outputTextWriter))\n            {\n                if (conversionOptions.HasFlag(ToolFormatConversionOptions.PrettyPrint))\n                {\n                    outputJson.Formatting = Formatting.Indented;\n                }\n\n                using (var output = new ResultLogJsonWriter(outputJson))\n                {\n                    ConvertToStandardFormat(toolFormat, input, output);\n                }\n            }\n        }\n\n        \/\/\/ <summary>Converts a tool log file to the SARIF format.<\/summary>\n        \/\/\/ <param name=\"toolFormat\">The tool format of the input file.<\/param>\n        \/\/\/ <param name=\"inputFileName\">The input log file name.<\/param>\n        \/\/\/ <param name=\"outputFileName\">The name of the file to which the resulting SARIF log shall be\n        \/\/\/ written. This cannot be a directory.<\/param>\n        public void ConvertToStandardFormat(\n            ToolFormat toolFormat,\n            string inputFileName,\n            string outputFileName)\n        {\n            if (toolFormat == ToolFormat.PREfast)\n            {\n                string sarif = ConvertPREfastToStandardFormat(inputFileName);\n                File.WriteAllText(outputFileName, sarif);\n                return;\n            }\n\n            ConvertToStandardFormat(toolFormat, inputFileName, outputFileName, ToolFormatConversionOptions.None);\n        }\n\n        \/\/\/ <summary>Converts a tool log file represented as a stream into the SARIF format.<\/summary>\n        \/\/\/ <exception cref=\"ArgumentNullException\">Thrown when one or more required arguments are null.<\/exception>\n        \/\/\/ <exception cref=\"ArgumentException\">Thrown when one or more arguments have unsupported or\n        \/\/\/ illegal values.<\/exception>\n        \/\/\/ <param name=\"toolFormat\">The tool format of the input file.<\/param>\n        \/\/\/ <param name=\"inputStream\">A stream that contains tool log contents.<\/param>\n        \/\/\/ <param name=\"outputStream\">A stream to which the converted output should be written.<\/param>\n        public void ConvertToStandardFormat(\n            ToolFormat toolFormat,\n            Stream inputStream,\n            IResultLogWriter outputStream)\n        {\n            if (toolFormat == ToolFormat.PREfast)\n            {\n                throw new ArgumentException(\"Cannot convert PREfast XML from stream. Call ConvertPREfastToStandardFormat helper instead.\");\n            };\n\n            if (inputStream == null) { throw new ArgumentNullException(\"inputStream\"); };\n            if (outputStream == null) { throw new ArgumentNullException(\"outputStream\"); };\n\n            Lazy<IToolFileConverter> converter;\n            if (_converters.TryGetValue(toolFormat, out converter))\n            {\n                converter.Value.Convert(inputStream, outputStream);\n            }\n            else\n            {\n                throw new ArgumentException(\"Unrecognized tool specified: \" + toolFormat.ToString(), \"toolFormat\");\n            }\n        }\n\n        private readonly IDictionary<ToolFormat, Lazy<IToolFileConverter>> _converters = CreateConverterRecords();\n\n        private static Dictionary<ToolFormat, Lazy<IToolFileConverter>> CreateConverterRecords()\n        {\n            var result = new Dictionary<ToolFormat, Lazy<IToolFileConverter>>();\n            CreateConverterRecord<AndroidStudioConverter>(result, ToolFormat.AndroidStudio);\n            CreateConverterRecord<CppCheckConverter>(result, ToolFormat.CppCheck);\n            CreateConverterRecord<ClangAnalyzerConverter>(result, ToolFormat.ClangAnalyzer);\n            CreateConverterRecord<FortifyConverter>(result, ToolFormat.Fortify);\n            CreateConverterRecord<FxCopConverter>(result, ToolFormat.FxCop);\n            return result;\n        }\n\n        private static void CreateConverterRecord<T>(IDictionary<ToolFormat, Lazy<IToolFileConverter>> dict, ToolFormat format)\n            where T : IToolFileConverter, new()\n        {\n            dict.Add(format, new Lazy<IToolFileConverter>(() => new T(), LazyThreadSafetyMode.ExecutionAndPublication));\n        }\n\n        \/\/\/ <summary>Converts a legacy PREfast XML log file into the SARIF format.<\/summary>\n        \/\/\/ <exception cref=\"ArgumentNullException\">Thrown when one or more required arguments are null.<\/exception>\n        \/\/\/ <exception cref=\"ArgumentException\">Thrown when one or more arguments have unsupported or\n        \/\/\/ illegal values.<\/exception>\n        \/\/\/ <exception cref=\"InvalidOperationException\">Thrown when the requested operation is invalid.<\/exception>\n        \/\/\/ <param name=\"toolFormat\">The tool format of the input file.<\/param>\n        \/\/\/ <param name=\"inputFileName\">The input log file name.<\/param>\n        \/\/\/ <returns>The converted PREfast log file in SARIF format.<\/returns>\n        public static string ConvertPREfastToStandardFormat(string inputFileName)\n        {\n            if (inputFileName == null) { throw new ArgumentNullException(\"inputStream\"); };\n\n            return ConvertToSarif(inputFileName);\n        }\n\n        [return: MarshalAs(UnmanagedType.BStr)]\n        [DllImport(\"PREfastXmlSarifConverter\", CallingConvention = CallingConvention.StdCall, CharSet = CharSet.Unicode)]\n        private static extern string ConvertToSarif([MarshalAs(UnmanagedType.BStr)][In]string prefastFilePath);\n    }\n}\n","lang_cluster":"C#","length":164,"code_uid":"2bd14dc34ca04264acf156e452e02402"}
{"diff_hunk":"@@ -17,6 +17,7 @@\n using System;\n using System.Collections.Concurrent;\n using System.Collections.Generic;\n+using System.Runtime.CompilerServices;\n \n namespace OpenTelemetry.Context\n {","old_code":"\ufeff\/\/ <copyright file=\"RuntimeContext.cs\" company=\"OpenTelemetry Authors\">\r\n\/\/ Copyright The OpenTelemetry Authors\r\n\/\/\r\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\r\n\/\/ you may not use this file except in compliance with the License.\r\n\/\/ You may obtain a copy of the License at\r\n\/\/\r\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\r\n\/\/\r\n\/\/ Unless required by applicable law or agreed to in writing, software\r\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\r\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n\/\/ See the License for the specific language governing permissions and\r\n\/\/ limitations under the License.\r\n\/\/ <\/copyright>\r\n\r\nusing System;\r\nusing System.Collections.Concurrent;\r\nusing System.Collections.Generic;\r\n\r\nnamespace OpenTelemetry.Context\r\n{\r\n    \/\/\/ <summary>\r\n    \/\/\/ Generic runtime context management API.\r\n    \/\/\/ <\/summary>\r\n    public sealed class RuntimeContext\r\n    {\r\n        private static readonly ConcurrentDictionary<string, object> Slots = new ConcurrentDictionary<string, object>();\r\n\r\n        \/\/\/ <summary>\r\n        \/\/\/ Gets or sets the actual context carrier implementation.\r\n        \/\/\/ <\/summary>\r\n#if !NET452\r\n        public static Type ContextSlotType { get; set; } = typeof(AsyncLocalRuntimeContextSlot<>);\r\n#else\r\n        public static Type ContextSlotType { get; set; } = typeof(RemotingRuntimeContextSlot<>);\r\n#endif\r\n\r\n        \/\/\/ <summary>\r\n        \/\/\/ Register a named context slot.\r\n        \/\/\/ <\/summary>\r\n        \/\/\/ <param name=\"name\">The name of the context slot.<\/param>\r\n        \/\/\/ <typeparam name=\"T\">The type of the underlying value.<\/typeparam>\r\n        public static void RegisterSlot<T>(string name)\r\n        {\r\n            lock (Slots)\r\n            {\r\n                if (Slots.ContainsKey(name))\r\n                {\r\n                    throw new InvalidOperationException($\"The context slot {name} is already registered.\");\r\n                }\r\n\r\n                var type = ContextSlotType.MakeGenericType(typeof(T));\r\n                var ctor = type.GetConstructor(new Type[] { typeof(string) });\r\n                Slots[name] = ctor.Invoke(new object[] { name });\r\n            }\r\n        }\r\n\r\n        \/*\r\n        public static void Apply(IDictionary<string, object> snapshot)\r\n        {\r\n            foreach (var entry in snapshot)\r\n            {\r\n                \/\/ TODO: revisit this part if we want Snapshot() to be used on critical paths\r\n                dynamic value = entry.Value;\r\n                SetValue(entry.Key, value);\r\n            }\r\n        }\r\n\r\n        public static IDictionary<string, object> Snapshot()\r\n        {\r\n            var retval = new Dictionary<string, object>();\r\n            foreach (var entry in Slots)\r\n            {\r\n                \/\/ TODO: revisit this part if we want Snapshot() to be used on critical paths\r\n                dynamic slot = entry.Value;\r\n                retval[entry.Key] = slot.Get();\r\n            }\r\n            return retval;\r\n        }\r\n        *\/\r\n\r\n        \/\/\/ <summary>\r\n        \/\/\/ Sets the value to a registered slot.\r\n        \/\/\/ <\/summary>\r\n        \/\/\/ <param name=\"name\">The name of the context slot.<\/param>\r\n        \/\/\/ <param name=\"value\">The value to be set.<\/param>\r\n        \/\/\/ <typeparam name=\"T\">The type of the value.<\/typeparam>\r\n        public static void SetValue<T>(string name, T value)\r\n        {\r\n            var slot = (RuntimeContextSlot<T>)Slots[name];\r\n            slot.Set(value);\r\n        }\r\n\r\n        \/\/\/ <summary>\r\n        \/\/\/ Gets the value from a registered slot.\r\n        \/\/\/ <\/summary>\r\n        \/\/\/ <param name=\"name\">The name of the context slot.<\/param>\r\n        \/\/\/ <typeparam name=\"T\">The type of the value.<\/typeparam>\r\n        \/\/\/ <returns>The value retrieved from the context slot.<\/returns>\r\n        public static T GetValue<T>(string name)\r\n        {\r\n            var slot = (RuntimeContextSlot<T>)Slots[name];\r\n            return slot.Get();\r\n        }\r\n\r\n        \/\/ For testing purpose\r\n        \/\/ private static Clear\r\n    }\r\n}\r\n","lang_cluster":"C#","length":110,"code_uid":"77b719eb3d0745118eaa1ccff7bfe442"}
{"diff_hunk":"@@ -47,13 +47,23 @@ def copy_files(use_gpu=False):\n \n \n def clear_path(path):\n-    contents = os.listdir(path)\n-    for file in contents:\n-        file_path = os.path.join(path, file)\n-        if os.path.isfile(file_path):\n-            os.remove(file_path)\n-        else:\n-            shutil.rmtree(file_path)\n+    if os.path.isdir(path):\n+        contents = os.listdir(path)\n+        for file in contents:\n+            file_path = os.path.join(path, file)\n+            if os.path.isfile(file_path):\n+                os.remove(file_path)\n+            else:\n+                shutil.rmtree(file_path)\n+\n+\n+def silent_call(cmd):\n+    try:\n+        with open(os.devnull, \"w\") as shut_up:\n+            subprocess.check_output(cmd, stderr=shut_up)\n+            return 0\n+    except Exception:\n+        return 1\n \n \n def compile_cpp(use_mingw=False, use_gpu=False):","old_code":"# coding: utf-8\n# pylint: disable=invalid-name, exec-used, C0111\n\"\"\"Setup lightgbm package.\"\"\"\nfrom __future__ import absolute_import\n\nimport distutils\nimport os\nimport shutil\nimport struct\nimport sys\n\nfrom setuptools import find_packages, setup\nfrom setuptools.command.install import install\nfrom setuptools.command.install_lib import install_lib\nfrom setuptools.command.sdist import sdist\n\n\ndef find_lib():\n    CURRENT_DIR = os.path.dirname(__file__)\n    libpath_py = os.path.join(CURRENT_DIR, 'lightgbm\/libpath.py')\n    libpath = {'__file__': libpath_py}\n    exec(compile(open(libpath_py, \"rb\").read(), libpath_py, 'exec'), libpath, libpath)\n\n    LIB_PATH = [os.path.relpath(path, CURRENT_DIR) for path in libpath['find_lib_path']()]\n    print(\"Install lib_lightgbm from: %s\" % LIB_PATH)\n    return LIB_PATH\n\n\ndef copy_files(use_gpu=False):\n\n    def copy_files_helper(folder_name):\n        src = os.path.join('..', folder_name)\n        if os.path.exists(src):\n            dst = os.path.join('.\/lightgbm', folder_name)\n            shutil.rmtree(dst, ignore_errors=True)\n            distutils.dir_util.copy_tree(src, dst)\n        else:\n            raise Exception('Cannot copy {} folder'.format(src))\n\n    if not os.path.isfile('.\/_IS_SOURCE_PACKAGE.txt'):\n        copy_files_helper('include')\n        copy_files_helper('src')\n        if use_gpu:\n            copy_files_helper('compute')\n        distutils.file_util.copy_file(\"..\/CMakeLists.txt\", \".\/lightgbm\/\")\n        distutils.file_util.copy_file(\"..\/LICENSE\", \".\/\")\n\n\ndef clear_path(path):\n    contents = os.listdir(path)\n    for file in contents:\n        file_path = os.path.join(path, file)\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n        else:\n            shutil.rmtree(file_path)\n\n\ndef compile_cpp(use_mingw=False, use_gpu=False):\n\n    if os.path.exists(\"build_cpp\"):\n        shutil.rmtree(\"build_cpp\")\n    os.makedirs(\"build_cpp\")\n    os.chdir(\"build_cpp\")\n\n    cmake_cmd = \"cmake \"\n    build_cmd = \"make _lightgbm\"\n    if use_gpu:\n        cmake_cmd += \" -DUSE_GPU=ON \"\n    if os.name == \"nt\":\n        if use_mingw:\n            cmake_cmd += \" -G \\\"MinGW Makefiles\\\" \"\n            os.system(cmake_cmd + \" ..\/lightgbm\/\")\n            build_cmd = \"mingw32-make.exe _lightgbm\"\n        else:\n            vs_versions = [\"Visual Studio 15 2017 Win64\", \"Visual Studio 14 2015 Win64\", \"Visual Studio 12 2013 Win64\"]\n            try_vs = 1\n            for vs in vs_versions:\n                tmp_cmake_cmd = \"%s -G \\\"%s\\\"\" % (cmake_cmd, vs)\n                try_vs = os.system(tmp_cmake_cmd + \" ..\/lightgbm\/\")\n                if try_vs == 0:\n                    cmake_cmd = tmp_cmake_cmd\n                    break\n                else:\n                    clear_path(\".\/\")\n            if try_vs != 0:\n                raise Exception('Please install Visual Studio or MS Build first')\n\n            build_cmd = \"cmake --build . --target _lightgbm  --config Release\"\n    print(\"Start to compile library.\")\n    os.system(cmake_cmd + \" ..\/lightgbm\/\")\n    os.system(build_cmd)\n    os.chdir(\"..\")\n\n\nclass CustomInstallLib(install_lib):\n\n    def install(self):\n        outfiles = install_lib.install(self)\n        src = find_lib()[0]\n        dst = os.path.join(self.install_dir, 'lightgbm')\n        dst, _ = self.copy_file(src, dst)\n        outfiles.append(dst)\n        return outfiles\n\n\nclass CustomInstall(install):\n\n    user_options = install.user_options + [\n        ('mingw', 'm', 'compile with mingw'),\n        ('gpu', 'g', 'compile gpu version'),\n        ('precompile', 'p', 'use precompile library')\n    ]\n\n    def initialize_options(self):\n        install.initialize_options(self)\n        self.mingw = 0\n        self.gpu = 0\n        self.precompile = 0\n\n    def run(self):\n        if not self.precompile:\n            copy_files(use_gpu=self.gpu)\n            compile_cpp(use_mingw=self.mingw, use_gpu=self.gpu)\n        self.distribution.data_files = [('lightgbm', find_lib())]\n        install.run(self)\n\n\nclass CustomSdist(sdist):\n\n    def run(self):\n        copy_files(use_gpu=True)\n        open(\".\/_IS_SOURCE_PACKAGE.txt\", 'w').close()\n        if os.path.exists(\".\/lightgbm\/Release\/\"):\n            shutil.rmtree('.\/lightgbm\/Release\/')\n        if os.path.isfile('.\/lightgbm\/lib_lightgbm.so'):\n            os.remove('.\/lightgbm\/lib_lightgbm.so')\n        sdist.run(self)\n        if os.path.isfile('.\/_IS_SOURCE_PACKAGE.txt'):\n            os.remove('.\/_IS_SOURCE_PACKAGE.txt')\n\n\nif __name__ == \"__main__\":\n    if (8 * struct.calcsize(\"P\")) != 64:\n        raise Exception('Cannot install LightGBM in 32-bit python, please use 64-bit python instead.')\n\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    if os.path.isfile(os.path.join('..', 'VERSION.txt')):\n        distutils.file_util.copy_file(\n            os.path.join('..', 'VERSION.txt'),\n            os.path.join('.', 'lightgbm'))\n    if os.path.isfile(os.path.join(dir_path, 'lightgbm', 'VERSION.txt')):\n        version = open(os.path.join(dir_path, 'lightgbm', 'VERSION.txt')).read().strip()\n\n    sys.path.insert(0, '.')\n\n    setup(name='lightgbm',\n          version=version,\n          description='LightGBM Python Package',\n          long_description=open('README.rst').read(),\n          install_requires=[\n              'numpy',\n              'scipy',\n              'scikit-learn'\n          ],\n          maintainer='Guolin Ke',\n          maintainer_email='guolin.ke@microsoft.com',\n          zip_safe=False,\n          cmdclass={\n              'install': CustomInstall,\n              'install_lib': CustomInstallLib,\n              'sdist': CustomSdist,\n          },\n          packages=find_packages(),\n          include_package_data=True,\n          data_files=[],\n          license='The MIT License (Microsoft)',\n          url='https:\/\/github.com\/Microsoft\/LightGBM')\n","lang_cluster":"C++","length":178,"code_uid":"be05f20eb28d40d69fdd5b27a46b3fee"}
{"diff_hunk":"@@ -48,7 +48,7 @@ static PJ_LP fouc_s_s_inverse (PJ_XY xy, PJ *P) {           \/* Spheroidal, inver\n             lp.phi = xy.y < 0. ? -M_HALFPI : M_HALFPI;\n     } else\n         lp.phi = aasin(P->ctx,xy.y);\n-    V = cos(lp.phi);\n+    const double V = cos(lp.phi);\n     lp.lam = xy.x * (Q->n + Q->n1 * V) \/ V;\n     return lp;\n }","old_code":"#define PJ_LIB__\n\n#include <errno.h>\n#include <math.h>\n\n#include \"proj.h\"\n#include \"proj_internal.h\"\n\nPROJ_HEAD(fouc_s, \"Foucaut Sinusoidal\") \"\\n\\tPCyl, Sph\";\n\n#define MAX_ITER    10\n#define LOOP_TOL    1e-7\n\nnamespace { \/\/ anonymous namespace\nstruct pj_opaque {\n    double n, n1;\n};\n} \/\/ anonymous namespace\n\n\nstatic PJ_XY fouc_s_s_forward (PJ_LP lp, PJ *P) {           \/* Spheroidal, forward *\/\n    PJ_XY xy = {0.0,0.0};\n    struct pj_opaque *Q = static_cast<struct pj_opaque*>(P->opaque);\n    double t;\n\n    t = cos(lp.phi);\n    xy.x = lp.lam * t \/ (Q->n + Q->n1 * t);\n    xy.y = Q->n * lp.phi + Q->n1 * sin(lp.phi);\n    return xy;\n}\n\n\nstatic PJ_LP fouc_s_s_inverse (PJ_XY xy, PJ *P) {           \/* Spheroidal, inverse *\/\n    PJ_LP lp = {0.0,0.0};\n    struct pj_opaque *Q = static_cast<struct pj_opaque*>(P->opaque);\n    double V;\n    int i;\n\n    if (Q->n != 0.0) {\n        lp.phi = xy.y;\n        for (i = MAX_ITER; i ; --i) {\n            lp.phi -= V = (Q->n * lp.phi + Q->n1 * sin(lp.phi) - xy.y ) \/\n                (Q->n + Q->n1 * cos(lp.phi));\n            if (fabs(V) < LOOP_TOL)\n                break;\n        }\n        if (!i)\n            lp.phi = xy.y < 0. ? -M_HALFPI : M_HALFPI;\n    } else\n        lp.phi = aasin(P->ctx,xy.y);\n    V = cos(lp.phi);\n    lp.lam = xy.x * (Q->n + Q->n1 * V) \/ V;\n    return lp;\n}\n\n\nPJ *PROJECTION(fouc_s) {\n    struct pj_opaque *Q = static_cast<struct pj_opaque*>(pj_calloc (1, sizeof (struct pj_opaque)));\n    if (nullptr==Q)\n        return pj_default_destructor (P, ENOMEM);\n    P->opaque = Q;\n\n    Q->n = pj_param(P->ctx, P->params, \"dn\").f;\n    if (Q->n < 0. || Q->n > 1.)\n        return pj_default_destructor (P, PJD_ERR_N_OUT_OF_RANGE);\n\n    Q->n1 = 1. - Q->n;\n    P->es = 0;\n    P->inv = fouc_s_s_inverse;\n    P->fwd = fouc_s_s_forward;\n    return P;\n}\n","lang_cluster":"C++","length":72,"code_uid":"d11c831c4ba44fdf91f2dc7405bffb12"}
{"diff_hunk":"@@ -49,7 +49,7 @@ void DataLayer<Dtype>::DataLayerSetUp(const vector<Blob<Dtype>*>& bottom,\n   int crop_size = this->layer_param_.transform_param().crop_size();\n   if (crop_size > 0) {\n     top[0]->Reshape(this->layer_param_.data_param().batch_size(),\n-                       datum.channels(), crop_size, crop_size);\n+        datum.channels(), crop_size, crop_size);\n     this->prefetch_data_.Reshape(this->layer_param_.data_param().batch_size(),\n         datum.channels(), crop_size, crop_size);\n     this->transformed_data_.Reshape(1, datum.channels(), crop_size, crop_size);","old_code":"#include <opencv2\/core\/core.hpp>\n\n#include <stdint.h>\n\n#include <string>\n#include <vector>\n\n#include \"caffe\/common.hpp\"\n#include \"caffe\/data_layers.hpp\"\n#include \"caffe\/layer.hpp\"\n#include \"caffe\/proto\/caffe.pb.h\"\n#include \"caffe\/util\/benchmark.hpp\"\n#include \"caffe\/util\/io.hpp\"\n#include \"caffe\/util\/math_functions.hpp\"\n#include \"caffe\/util\/rng.hpp\"\n\nnamespace caffe {\n\ntemplate <typename Dtype>\nDataLayer<Dtype>::~DataLayer<Dtype>() {\n  this->JoinPrefetchThread();\n}\n\ntemplate <typename Dtype>\nvoid DataLayer<Dtype>::DataLayerSetUp(const vector<Blob<Dtype>*>& bottom,\n      const vector<Blob<Dtype>*>& top) {\n  \/\/ Initialize DB\n  db_.reset(db::GetDB(this->layer_param_.data_param().backend()));\n  db_->Open(this->layer_param_.data_param().source(), db::READ);\n  cursor_.reset(db_->NewCursor());\n\n  \/\/ Check if we should randomly skip a few data points\n  if (this->layer_param_.data_param().rand_skip()) {\n    unsigned int skip = caffe_rng_rand() %\n                        this->layer_param_.data_param().rand_skip();\n    LOG(INFO) << \"Skipping first \" << skip << \" data points.\";\n    while (skip-- > 0) {\n      cursor_->Next();\n    }\n  }\n  \/\/ Read a data point, and use it to initialize the top blob.\n  Datum datum;\n  datum.ParseFromString(cursor_->value());\n\n  if (DecodeDatum(&datum)) {\n    LOG(INFO) << \"Decoding Datum\";\n  }\n  \/\/ image\n  int crop_size = this->layer_param_.transform_param().crop_size();\n  if (crop_size > 0) {\n    top[0]->Reshape(this->layer_param_.data_param().batch_size(),\n                       datum.channels(), crop_size, crop_size);\n    this->prefetch_data_.Reshape(this->layer_param_.data_param().batch_size(),\n        datum.channels(), crop_size, crop_size);\n    this->transformed_data_.Reshape(1, datum.channels(), crop_size, crop_size);\n  } else {\n    top[0]->Reshape(\n        this->layer_param_.data_param().batch_size(), datum.channels(),\n        datum.height(), datum.width());\n    this->prefetch_data_.Reshape(this->layer_param_.data_param().batch_size(),\n        datum.channels(), datum.height(), datum.width());\n    this->transformed_data_.Reshape(1, datum.channels(),\n      datum.height(), datum.width());\n  }\n  LOG(INFO) << \"output data size: \" << top[0]->num() << \",\"\n      << top[0]->channels() << \",\" << top[0]->height() << \",\"\n      << top[0]->width();\n  \/\/ label\n  if (this->output_labels_) {\n    top[1]->Reshape(this->layer_param_.data_param().batch_size(), 1, 1, 1);\n    this->prefetch_label_.Reshape(this->layer_param_.data_param().batch_size(),\n        1, 1, 1);\n  }\n}\n\n\/\/ This function is used to create a thread that prefetches the data.\ntemplate <typename Dtype>\nvoid DataLayer<Dtype>::InternalThreadEntry() {\n  CPUTimer batch_timer;\n  batch_timer.Start();\n  double read_time = 0;\n  double trans_time = 0;\n  CPUTimer timer;\n  CHECK(this->prefetch_data_.count());\n  CHECK(this->transformed_data_.count());\n  Dtype* top_data = this->prefetch_data_.mutable_cpu_data();\n  Dtype* top_label = NULL;  \/\/ suppress warnings about uninitialized variables\n\n  if (this->output_labels_) {\n    top_label = this->prefetch_label_.mutable_cpu_data();\n  }\n  const int batch_size = this->layer_param_.data_param().batch_size();\n  for (int item_id = 0; item_id < batch_size; ++item_id) {\n    timer.Start();\n    \/\/ get a blob\n    Datum datum;\n    datum.ParseFromString(cursor_->value());\n\n    cv::Mat cv_img;\n    if (datum.encoded()) {\n       cv_img = DecodeDatumToCVMat(datum);\n    }\n    read_time += timer.MicroSeconds();\n    timer.Start();\n\n    \/\/ Apply data transformations (mirror, scale, crop...)\n    int offset = this->prefetch_data_.offset(item_id);\n    this->transformed_data_.set_cpu_data(top_data + offset);\n    if (datum.encoded()) {\n      this->data_transformer_.Transform(cv_img, &(this->transformed_data_));\n    } else {\n      this->data_transformer_.Transform(datum, &(this->transformed_data_));\n    }\n    if (this->output_labels_) {\n      top_label[item_id] = datum.label();\n    }\n    trans_time += timer.MicroSeconds();\n    \/\/ go to the next iter\n    cursor_->Next();\n    if (!cursor_->valid()) {\n      DLOG(INFO) << \"Restarting data prefetching from start.\";\n      cursor_->SeekToFirst();\n    }\n  }\n  batch_timer.Stop();\n  DLOG(INFO) << \"Prefetch batch: \" << batch_timer.MilliSeconds() << \" ms.\";\n  DLOG(INFO) << \"     Read time: \" << read_time \/ 1000 << \" ms.\";\n  DLOG(INFO) << \"Transform time: \" << trans_time \/ 1000 << \" ms.\";\n}\n\nINSTANTIATE_CLASS(DataLayer);\nREGISTER_LAYER_CLASS(Data);\n\n}  \/\/ namespace caffe\n","lang_cluster":"C++","length":134,"code_uid":"cfaab72f125745b1a46dabd7d1edd906"}
{"diff_hunk":"@@ -84,7 +84,26 @@ int main()\n }\n \n \n-\/* Output\n+\/* \n+Input:\n+8\n+14\n+0 1\n+0 2\n+1 2\n+1 4\n+2 0\n+2 3\n+3 3\n+3 6\n+4 0\n+4 5\n+5 6\n+5 7\n+6 2\n+7 3\n+\n+Output:\n \n Breadth First Traversal is : 0 1 2 4 3 5 6 7\n ","old_code":"#include <iostream>\n#include <vector>\n#include <queue>\n\nusing namespace std;\n\nclass Graph\n{\n    int numberVertex;\n    vector <int> *adjacency;\n\n  public:\n    \/\/ Constructor to initialise graph\n    Graph(int numberVertex)\n    {\n        this->numberVertex = numberVertex;\n        adjacency = new vector <int> [numberVertex];\n    }\n\n    \/\/ Function to add edge between source and destination\n    void addEdge(int source, int destination)\n    {\n        adjacency[source].push_back(destination);\n    }\n\n    \/\/ Function to perform Breadth First Search\n    void bfs(int starting);\n};\n\nvoid Graph::bfs(int starting)\n{\n    bool visited[numberVertex];\n\n    for (int i = 0; i < numberVertex; i++)\n        visited[i] = false;\n\n    queue <int> queue_vertex;\n\n    visited[starting] = true;\n    queue_vertex.push(starting);\n\n    while (!queue_vertex.empty())\n    {\n        starting = queue_vertex.front();\n        cout << starting << \" \";\n        queue_vertex.pop();\n\n        for (vector <int> :: iterator it = adjacency[starting].begin(); it != adjacency[starting].end(); ++it)\n        {\n            if(!visited[*it])\n            {\n                visited[*it] = true;\n                queue_vertex.push(*it);\n            }\n        }\n    }\n}\n\nint main()\n{\n    \/\/ Number of vertices is 8\n    Graph graph(8);\n\n    \/\/ Create edges between vertices\n    graph.addEdge(0, 1);\n    graph.addEdge(0, 2);\n    graph.addEdge(1, 2);\n    graph.addEdge(1, 4);\n    graph.addEdge(2, 0);\n    graph.addEdge(2, 3);\n    graph.addEdge(3, 3);\n    graph.addEdge(3, 6);\n    graph.addEdge(4, 0);\n    graph.addEdge(4, 5);\n    graph.addEdge(5, 6);\n    graph.addEdge(5, 7);\n    graph.addEdge(6, 2);\n    graph.addEdge(7, 3);\n\n    cout << \"Breadth First Traversal is : \";\n    graph.bfs(0);\n\n    return 0;\n}\n\n\n\/* Output\n\nBreadth First Traversal is : 0 1 2 4 3 5 6 7\n\n*\/\n","lang_cluster":"C++","length":91,"code_uid":"6ad80a5dab5542f9a70d1f70e37fecf3"}
{"diff_hunk":"@@ -1,6 +1,7 @@\n #include <cstdint>\n \n #include <iostream>\n+#include <numeric>\n #include <stdexcept>\n \n #include <adios2.h>","old_code":"#include <cstdint>\n\n#include <iostream>\n#include <stdexcept>\n\n#include <adios2.h>\n\n#include <gtest\/gtest.h>\n\n#ifdef ADIOS2_HAVE_MPI\n\nTEST(ADIOSInterface, MPICommRemoved)\n{\n    MPI_Comm myComm;\n    MPI_Comm_dup(MPI_COMM_WORLD, &myComm);\n    adios2::ADIOS adios(myComm);\n    adios2::IO io = adios.DeclareIO(\"TestIO\");\n    MPI_Comm_free(&myComm);\n\n    adios2::Engine engine = io.Open(\"test.bp\", adios2::Mode::Write);\n}\n\n#endif\n\nclass ADIOS2_CXX11_API : public ::testing::Test\n{\npublic:\n    ADIOS2_CXX11_API()\n#ifdef ADIOS2_HAVE_MPI\n    : ad(MPI_COMM_WORLD, adios2::DebugON)\n#else\n    : ad(adios2::DebugON)\n#endif\n    {\n#ifdef ADIOS2_HAVE_MPI\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n#endif\n    }\n\n    adios2::ADIOS ad;\n    int rank = 0;\n    int size = 1;\n};\n\nclass ADIOS2_CXX11_API_IO : public ADIOS2_CXX11_API\n{\npublic:\n    ADIOS2_CXX11_API_IO() : io(ad.DeclareIO(\"CXX11_API_TestIO\")) {}\n\n    adios2::IO io;\n};\n\nTEST_F(ADIOS2_CXX11_API_IO, Engine)\n{\n    io.SetEngine(\"bpfile\");\n    EXPECT_EQ(io.EngineType(), \"bpfile\");\n\n    adios2::Engine engine = io.Open(\"types.bp\", adios2::Mode::Write);\n    EXPECT_EQ(engine.Name(), \"types.bp\");\n    EXPECT_EQ(engine.Type(), \"BP3\");\n\n    EXPECT_EQ(io.EngineType(), \"bp\"); \/\/ FIXME? Is it expected that adios2_open\n                                      \/\/ changes the engine_type string?\n}\n\nTEST_F(ADIOS2_CXX11_API_IO, EngineDefault)\n{\n    io.SetEngine(\"\");\n    EXPECT_EQ(io.EngineType(), \"\");\n\n    adios2::Engine engine = io.Open(\"types.bp\", adios2::Mode::Write);\n    EXPECT_EQ(engine.Name(), \"types.bp\");\n    EXPECT_EQ(engine.Type(), \"BP3\");\n\n    EXPECT_EQ(io.EngineType(), \"bp\"); \/\/ FIXME? Is it expected that adios2_open\n                                      \/\/ changes the engine_type string?\n}\n\nint main(int argc, char **argv)\n{\n#ifdef ADIOS2_HAVE_MPI\n    MPI_Init(nullptr, nullptr);\n#endif\n\n    int result;\n    ::testing::InitGoogleTest(&argc, argv);\n    result = RUN_ALL_TESTS();\n\n#ifdef ADIOS2_HAVE_MPI\n    MPI_Finalize();\n#endif\n\n    return result;\n}\n","lang_cluster":"C++","length":95,"code_uid":"dceedce02787467dbb8ddb0bbf01c32d"}
{"diff_hunk":"@@ -29,6 +29,7 @@ THE SOFTWARE.\n #include <thread>\n #include <future>\n #include <functional>\n+#include <vector>\n \n #define NUM_GROUPS 1\n #define GROUP_SIZE 1","old_code":"\/*\nCopyright (c) 2020-present Advanced Micro Devices, Inc. All rights reserved.\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n*\/\n\n#include <stdio.h>\n#include \"hip\/hip_runtime.h\"\n#ifdef __HIP_PLATFORM_HCC__\n#include \"hip\/hip_ext.h\"\n#endif\n#include <iostream>\n#include <chrono>\n#include <algorithm>\n#include <atomic>\n#include <thread>\n#include <future>\n#include <functional>\n\n#define NUM_GROUPS 1\n#define GROUP_SIZE 1\n#define WARMUP_RUN_COUNT 10\n#define TIMING_RUN_COUNT 100\n#define TOTAL_RUN_COUNT WARMUP_RUN_COUNT + TIMING_RUN_COUNT\n\n__global__ void EmptyKernel() {}\n\n\/\/ Helper to print various timing metrics\nvoid print_timing(std::string test, std::array<float, TOTAL_RUN_COUNT> &results, int batch = 1)\n{\n\n    float total_us = 0.0f, mean_us = 0.0f, stddev_us = 0.0f;\n\n    \/\/ remove top outliers due to nature of variability across large number of multi-threaded runs\n    std::sort(results.begin(), results.end(), std::greater<float>());\n    auto start_iter = std::next(results.begin(), WARMUP_RUN_COUNT);\n    auto end_iter = results.end();\n\n    \/\/ mean\n    std::for_each(start_iter, end_iter, [&](const float &run_ms) {\n        total_us += (run_ms * 1000) \/ batch;\n    });\n    mean_us = total_us  \/ TIMING_RUN_COUNT;\n\n   \/\/ stddev\n    total_us = 0;\n    std::for_each(start_iter, end_iter, [&](const float &run_ms) {\n        float dev_us = ((run_ms * 1000) \/ batch) - mean_us;\n        total_us += dev_us * dev_us;\n    });\n    stddev_us = sqrt(total_us \/ TIMING_RUN_COUNT);\n\n    printf(\"\\n %s: %.1f us, std: %.1f us\\n\", test.c_str(), mean_us, stddev_us);\n}\n\n\/\/ Measure time taken to enqueue a kernel on the GPU using hipModuleLaunchKernel\nvoid hipModuleLaunchKernel_enqueue_rate(std::atomic_int* shared, int max_threads)\n{\n    \/\/resources necessary for this thread\n    hipStream_t stream;\n    hipStreamCreate(&stream);\n    hipModule_t module;\n    hipFunction_t function;\n    hipModuleLoad(&module, \"test_kernel.code\");\n    hipModuleGetFunction(&function, module, \"test\");\n    void* kernel_params = nullptr;\n    std::array<float, TOTAL_RUN_COUNT> results;\n\n    \/\/synchronize all threads, before running\n    int tid = shared->fetch_add(1, std::memory_order_release);\n    while (max_threads != shared->load(std::memory_order_acquire)) {}\n\n    for (auto i = 0; i < TOTAL_RUN_COUNT; ++i) {\n        auto start = std::chrono::high_resolution_clock::now();\n        hipModuleLaunchKernel(function, 1, 1, 1, 1, 1, 1, 0, stream, &kernel_params, nullptr);\n        auto stop = std::chrono::high_resolution_clock::now();\n        results[i] = std::chrono::duration<double, std::milli>(stop - start).count();\n    }\n    print_timing(\"Thread ID : \" + std::to_string(tid) + \" , \" + \"hipModuleLaunchKernel enqueue rate\", results);\n}\n\n\/\/ Measure time taken to enqueue a kernel on the GPU using hipLaunchKernelGGL\nvoid hipLaunchKernelGGL_enqueue_rate(std::atomic_int* shared, int max_threads)\n{\n    \/\/resources necessary for this thread\n    hipStream_t stream;\n    hipStreamCreate(&stream);\n    std::array<float, TOTAL_RUN_COUNT> results;\n\n    \/\/synchronize all threads, before running\n    int tid = shared->fetch_add(1, std::memory_order_release);\n    while (max_threads != shared->load(std::memory_order_acquire)) {}\n\n    for (auto i = 0; i < TOTAL_RUN_COUNT; ++i) {\n        auto start = std::chrono::high_resolution_clock::now();\n        hipLaunchKernelGGL((EmptyKernel), dim3(NUM_GROUPS), dim3(GROUP_SIZE), 0, stream);\n        auto stop = std::chrono::high_resolution_clock::now();\n        results[i] = std::chrono::duration<double, std::milli>(stop - start).count();\n    }\n    print_timing(\"Thread ID : \" + std::to_string(tid) + \" , \" + \"hipLaunchKernelGGL enqueue rate\", results);\n}\n\n\/\/ Simple thread pool\nstruct thread_pool {\n    thread_pool(int total_threads) : max_threads(total_threads) {}\n    void start(std::function<void(std::atomic_int*, int)> f) {\n        for (int i = 0; i < max_threads; ++i) {\n            threads.push_back(std::async(std::launch::async, f, &shared, max_threads));\n        }\n    }\n    void finish() {\n        for (auto&&thread : threads) {\n            thread.get();\n        }\n        threads.clear();\n        shared = {0};\n    }\n    ~thread_pool() {\n        finish();\n    }\nprivate:\n    std::atomic_int shared {0};\n    std::vector<std::future<void>> threads;\n    int max_threads = 1;\n};\n\n\nint main(int argc, char* argv[])\n{\n    if (argc != 3) {\n        std::cerr << \"Run test as 'hipDispatchEnqueueRateMT <num_threads> <0-hipModuleLaunchKernel \/1-hipLaunchKernelGGL>'\\n\";\n        return -1;\n    }\n\n    int max_threads = atoi(argv[1]);\n    int run_module_test = atoi(argv[2]);\n    if(max_threads < 1 || run_module_test < 0 || run_module_test > 1) {\n        std::cerr << \"Invalid Input.\\n\";\n        std::cerr << \"Run test as 'hipDispatchEnqueueRateMT <num_threads> <0-hipModuleLaunchKernel \/1-hipLaunchKernelGGL>'\\n\";\n        return -1;\n    }\n    thread_pool task(max_threads);\n\n    if(run_module_test == 0) {\n        task.start(hipModuleLaunchKernel_enqueue_rate);\n        task.finish();\n    } else {\n        task.start(hipLaunchKernelGGL_enqueue_rate);\n        task.finish();\n    }\n\n    return 0;\n}\n\n","lang_cluster":"C++","length":167,"code_uid":"a295977d04434797ab55dd1d9f3c26fc"}
{"diff_hunk":"@@ -95,4 +95,4 @@ PeriodicTable *PeriodicTable::getTable() {\n   return ds_instance.get();\n }\n \n-}  \/\/ end of namespace\n+}  \/\/ namespace RDKit","old_code":"\/\/ $Id$\n\/\/\n\/\/  Copyright (C) 2001-2006 Rational Discovery LLC\n\/\/\n\/\/   @@ All Rights Reserved @@\n\/\/  This file is part of the RDKit.\n\/\/  The contents are covered by the terms of the BSD license\n\/\/  which is included in the file license.txt, found at the root\n\/\/  of the RDKit source tree.\n\/\/\n#include \"PeriodicTable.h\"\n#include <string>\n#include <boost\/tokenizer.hpp>\ntypedef boost::tokenizer<boost::char_separator<char>> tokenizer;\n#include <sstream>\n#include <locale>\n\n#if RDK_BUILD_THREADSAFE_SSS\n#include <mutexp>\n#endif\n\nnamespace RDKit {\n\nclass std::unique_ptr<PeriodicTable> PeriodicTable::ds_instance = nullptr;\n\nPeriodicTable::PeriodicTable() {\n  \/\/ it is assumed that the atomic atomData string constains atoms\n  \/\/ in sequence and no atoms are missing in between\n  byanum.clear();\n  byname.clear();\n\n  boost::char_separator<char> eolSep(\"\\n\");\n  tokenizer tokens(periodicTableAtomData, eolSep);\n  for (tokenizer::iterator token = tokens.begin(); token != tokens.end();\n       ++token) {\n    if (*token != \" \") {\n      atomicData adata(*token);\n      byanum.push_back(adata);\n      std::string enam = adata.Symbol();\n      byname[enam] = adata.AtomicNum();\n    }\n  }\n\n  unsigned int lidx = 0;\n  std::istringstream istr;\n  istr.imbue(std::locale(\"C\"));\n  while (isotopesAtomData[lidx] != \"\" && isotopesAtomData[lidx] != \"EOS\") {\n    tokenizer lines(isotopesAtomData[lidx++], eolSep);\n    boost::char_separator<char> spaceSep(\" \\t\");\n    for (tokenizer::iterator line = lines.begin(); line != lines.end();\n         ++line) {\n      if (*line != \" \") {\n        tokenizer tokens(*line, spaceSep);\n        tokenizer::iterator token = tokens.begin();\n        int anum;\n        istr.clear();\n        istr.str(*token);\n        istr >> anum;\n        atomicData &adata = byanum[anum];\n        ++token;\n        if (token == tokens.end()) continue;\n        ++token;\n        if (token == tokens.end()) continue;\n        unsigned int isotope;\n        istr.clear();\n        istr.str(*token);\n        istr >> isotope;\n        ++token;\n        if (token == tokens.end()) continue;\n        double mass;\n        istr.clear();\n        istr.str(*token);\n        istr >> mass;\n        ++token;\n        if (token == tokens.end()) continue;\n        double abundance;\n        istr.clear();\n        istr.str(*token);\n        istr >> abundance;\n        adata.d_isotopeInfoMap[isotope] = std::make_pair(mass, abundance);\n      }\n    }\n  }\n}\n\nvoid PeriodicTable::initInstance() { ds_instance = std::unique_ptr<PeriodicTable>(new PeriodicTable()); }\n\nPeriodicTable *PeriodicTable::getTable() {\n#if RDK_BUILD_THREADSAFE_SSS\n  static std::once_flag pt_init_once;\n  std::call_once(pt_init_once, initInstance);\n#else\n  if (!ds_instance) initInstance();\n#endif\n  return ds_instance.get();\n}\n\n}  \/\/ end of namespace\n","lang_cluster":"C++","length":98,"code_uid":"de4c4bba93094fb3bbd77b6b24068f7a"}
{"diff_hunk":"@@ -13,30 +13,35 @@ SyncedMemory::~SyncedMemory() {\n   if (cpu_ptr_ && own_cpu_data_) {\n     CaffeFreeHost(cpu_ptr_);\n   }\n-\n-  if (gpu_ptr_) {\n-    CUDA_CHECK(cudaFree(gpu_ptr_));\n+  if (gpu_data_) {\n+    CUDA_CHECK(cudaFree(gpu_data_));\n   }\n }\n \n inline void SyncedMemory::to_cpu() {\n   switch (head_) {\n   case UNINITIALIZED:\n-    CaffeMallocHost(&cpu_ptr_, size_);\n-    memset(cpu_ptr_, 0, size_);\n+    cpu_resize();\n     head_ = HEAD_AT_CPU;\n     own_cpu_data_ = true;\n     break;\n   case HEAD_AT_GPU:\n+    gpu_resize();\n+    cpu_resize();\n+    CUDA_CHECK(cudaMemcpy(cpu_data_, gpu_data_, size_, cudaMemcpyDeviceToHost));\n     if (cpu_ptr_ == NULL) {\n       CaffeMallocHost(&cpu_ptr_, size_);\n       own_cpu_data_ = true;\n     }\n-    CUDA_CHECK(cudaMemcpy(cpu_ptr_, gpu_ptr_, size_, cudaMemcpyDeviceToHost));\n     head_ = SYNCED;\n     break;\n   case HEAD_AT_CPU:\n+    cpu_resize();\n+    break;\n   case SYNCED:\n+    if (cpu_resize()) {\n+      head_ = HEAD_AT_CPU;\n+    }\n     break;\n   }\n }","old_code":"\/\/ Copyright 2014 BVLC and contributors.\n\n#include <cuda_runtime.h>\n\n#include <cstring>\n\n#include \"caffe\/common.hpp\"\n#include \"caffe\/syncedmem.hpp\"\n\nnamespace caffe {\n\nSyncedMemory::~SyncedMemory() {\n  if (cpu_ptr_ && own_cpu_data_) {\n    CaffeFreeHost(cpu_ptr_);\n  }\n\n  if (gpu_ptr_) {\n    CUDA_CHECK(cudaFree(gpu_ptr_));\n  }\n}\n\ninline void SyncedMemory::to_cpu() {\n  switch (head_) {\n  case UNINITIALIZED:\n    CaffeMallocHost(&cpu_ptr_, size_);\n    memset(cpu_ptr_, 0, size_);\n    head_ = HEAD_AT_CPU;\n    own_cpu_data_ = true;\n    break;\n  case HEAD_AT_GPU:\n    if (cpu_ptr_ == NULL) {\n      CaffeMallocHost(&cpu_ptr_, size_);\n      own_cpu_data_ = true;\n    }\n    CUDA_CHECK(cudaMemcpy(cpu_ptr_, gpu_ptr_, size_, cudaMemcpyDeviceToHost));\n    head_ = SYNCED;\n    break;\n  case HEAD_AT_CPU:\n  case SYNCED:\n    break;\n  }\n}\n\ninline void SyncedMemory::to_gpu() {\n  switch (head_) {\n  case UNINITIALIZED:\n    CUDA_CHECK(cudaMalloc(&gpu_ptr_, size_));\n    CUDA_CHECK(cudaMemset(gpu_ptr_, 0, size_));\n    head_ = HEAD_AT_GPU;\n    break;\n  case HEAD_AT_CPU:\n    if (gpu_ptr_ == NULL) {\n      CUDA_CHECK(cudaMalloc(&gpu_ptr_, size_));\n    }\n    CUDA_CHECK(cudaMemcpy(gpu_ptr_, cpu_ptr_, size_, cudaMemcpyHostToDevice));\n    head_ = SYNCED;\n    break;\n  case HEAD_AT_GPU:\n  case SYNCED:\n    break;\n  }\n}\n\nconst void* SyncedMemory::cpu_data() {\n  to_cpu();\n  return (const void*)cpu_ptr_;\n}\n\nvoid SyncedMemory::set_cpu_data(void* data) {\n  CHECK(data);\n  if (own_cpu_data_) {\n    CaffeFreeHost(cpu_ptr_);\n  }\n  cpu_ptr_ = data;\n  head_ = HEAD_AT_CPU;\n  own_cpu_data_ = false;\n}\n\nconst void* SyncedMemory::gpu_data() {\n  to_gpu();\n  return (const void*)gpu_ptr_;\n}\n\nvoid* SyncedMemory::mutable_cpu_data() {\n  to_cpu();\n  head_ = HEAD_AT_CPU;\n  return cpu_ptr_;\n}\n\nvoid* SyncedMemory::mutable_gpu_data() {\n  to_gpu();\n  head_ = HEAD_AT_GPU;\n  return gpu_ptr_;\n}\n\n\n}  \/\/ namespace caffe\n\n","lang_cluster":"C++","length":98,"code_uid":"e876e35bbac849c6b87682f0b8f54e9c"}
{"diff_hunk":"@@ -22,6 +22,7 @@\n #include \"protocol.h\"\n #include \"outputmessage.h\"\n #include \"rsa.h\"\n+#include \"xtea.h\"\n \n extern RSA g_RSA;\n ","old_code":"\/**\n * The Forgotten Server - a free and open-source MMORPG server emulator\n * Copyright (C) 2018  Mark Samman <mark.samman@gmail.com>\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n *\/\n\n#include \"otpch.h\"\n\n#include \"protocol.h\"\n#include \"outputmessage.h\"\n#include \"rsa.h\"\n\nextern RSA g_RSA;\n\nvoid Protocol::onSendMessage(const OutputMessage_ptr& msg) const\n{\n\tif (!rawMessages) {\n\t\tmsg->writeMessageLength();\n\n\t\tif (encryptionEnabled) {\n\t\t\tXTEA_encrypt(*msg);\n\t\t\tmsg->addCryptoHeader(checksumEnabled);\n\t\t}\n\t}\n}\n\nvoid Protocol::onRecvMessage(NetworkMessage& msg)\n{\n\tif (encryptionEnabled && !XTEA_decrypt(msg)) {\n\t\treturn;\n\t}\n\n\tparsePacket(msg);\n}\n\nOutputMessage_ptr Protocol::getOutputBuffer(int32_t size)\n{\n\t\/\/dispatcher thread\n\tif (!outputBuffer) {\n\t\toutputBuffer = OutputMessagePool::getOutputMessage();\n\t} else if ((outputBuffer->getLength() + size) > NetworkMessage::MAX_PROTOCOL_BODY_LENGTH) {\n\t\tsend(outputBuffer);\n\t\toutputBuffer = OutputMessagePool::getOutputMessage();\n\t}\n\treturn outputBuffer;\n}\n\nvoid Protocol::XTEA_encrypt(OutputMessage& msg) const\n{\n\tconst uint32_t delta = 0x61C88647;\n\n\t\/\/ The message must be a multiple of 8\n\tsize_t paddingBytes = msg.getLength() % 8;\n\tif (paddingBytes != 0) {\n\t\tmsg.addPaddingBytes(8 - paddingBytes);\n\t}\n\n\tuint8_t* buffer = msg.getOutputBuffer();\n\tconst size_t messageLength = msg.getLength();\n\tsize_t readPos = 0;\n\tconst uint32_t k[] = {key[0], key[1], key[2], key[3]};\n\twhile (readPos < messageLength) {\n\t\tuint32_t v0;\n\t\tmemcpy(&v0, buffer + readPos, 4);\n\t\tuint32_t v1;\n\t\tmemcpy(&v1, buffer + readPos + 4, 4);\n\n\t\tuint32_t sum = 0;\n\n\t\tfor (int32_t i = 32; --i >= 0;) {\n\t\t\tv0 += ((v1 << 4 ^ v1 >> 5) + v1) ^ (sum + k[sum & 3]);\n\t\t\tsum -= delta;\n\t\t\tv1 += ((v0 << 4 ^ v0 >> 5) + v0) ^ (sum + k[(sum >> 11) & 3]);\n\t\t}\n\n\t\tmemcpy(buffer + readPos, &v0, 4);\n\t\treadPos += 4;\n\t\tmemcpy(buffer + readPos, &v1, 4);\n\t\treadPos += 4;\n\t}\n}\n\nbool Protocol::XTEA_decrypt(NetworkMessage& msg) const\n{\n\tif (((msg.getLength() - 6) & 7) != 0) {\n\t\treturn false;\n\t}\n\n\tconst uint32_t delta = 0x61C88647;\n\n\tuint8_t* buffer = msg.getBuffer() + msg.getBufferPosition();\n\tconst size_t messageLength = (msg.getLength() - 6);\n\tsize_t readPos = 0;\n\tconst uint32_t k[] = {key[0], key[1], key[2], key[3]};\n\twhile (readPos < messageLength) {\n\t\tuint32_t v0;\n\t\tmemcpy(&v0, buffer + readPos, 4);\n\t\tuint32_t v1;\n\t\tmemcpy(&v1, buffer + readPos + 4, 4);\n\n\t\tuint32_t sum = 0xC6EF3720;\n\n\t\tfor (int32_t i = 32; --i >= 0;) {\n\t\t\tv1 -= ((v0 << 4 ^ v0 >> 5) + v0) ^ (sum + k[(sum >> 11) & 3]);\n\t\t\tsum += delta;\n\t\t\tv0 -= ((v1 << 4 ^ v1 >> 5) + v1) ^ (sum + k[sum & 3]);\n\t\t}\n\n\t\tmemcpy(buffer + readPos, &v0, 4);\n\t\treadPos += 4;\n\t\tmemcpy(buffer + readPos, &v1, 4);\n\t\treadPos += 4;\n\t}\n\n\tint innerLength = msg.get<uint16_t>();\n\tif (innerLength > msg.getLength() - 8) {\n\t\treturn false;\n\t}\n\n\tmsg.setLength(innerLength);\n\treturn true;\n}\n\nbool Protocol::RSA_decrypt(NetworkMessage& msg)\n{\n\tif ((msg.getLength() - msg.getBufferPosition()) < 128) {\n\t\treturn false;\n\t}\n\n\tg_RSA.decrypt(reinterpret_cast<char*>(msg.getBuffer()) + msg.getBufferPosition()); \/\/does not break strict aliasing\n\treturn msg.getByte() == 0;\n}\n\nuint32_t Protocol::getIP() const\n{\n\tif (auto connection = getConnection()) {\n\t\treturn connection->getIP();\n\t}\n\n\treturn 0;\n}\n","lang_cluster":"C++","length":154,"code_uid":"0c668fb582a34e2dbd893361c8bdb9b0"}
{"diff_hunk":"@@ -12,6 +12,10 @@\n #include \"storage\/test\/TestUtils.h\"\n #include \"fs\/TempDir.h\"\n \n+DECLARE_int32(load_data_interval_secs);\n+DECLARE_int32(heartbeat_interval_secs);\n+DECLARE_uint32(heartbeat_interval);\n+\n namespace nebula {\n namespace meta {\n ","old_code":"\/* Copyright (c) 2019 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n#include \"base\/Base.h\"\n#include <gtest\/gtest.h>\n#include <folly\/executors\/CPUThreadPoolExecutor.h>\n#include <folly\/synchronization\/Baton.h>\n#include \"meta\/processors\/admin\/Balancer.h\"\n#include \"meta\/test\/TestUtils.h\"\n#include \"storage\/test\/TestUtils.h\"\n#include \"fs\/TempDir.h\"\n\nnamespace nebula {\nnamespace meta {\n\nTEST(BalanceIntegrationTest, SimpleTest) {\n    auto sc = std::make_unique<test::ServerContext>();\n    auto handler = std::make_shared<nebula::storage::StorageServiceHandler>(nullptr, nullptr);\n    sc->mockCommon(\"storage\", 0, handler);\n    LOG(INFO) << \"Start storage server on \" << sc->port_;\n}\n\n}  \/\/ namespace meta\n}  \/\/ namespace nebula\n\nint main(int argc, char** argv) {\n    testing::InitGoogleTest(&argc, argv);\n    folly::init(&argc, &argv, true);\n    google::SetStderrLogging(google::INFO);\n    return RUN_ALL_TESTS();\n}\n\n\n","lang_cluster":"C++","length":35,"code_uid":"3e08c5fdb4d54124b053bd5a413c42c1"}
{"diff_hunk":"@@ -74,7 +74,7 @@ TraverseExecutor::makeTraverseExecutor(Sentence *sentence, ExecutionContext *ect\n     return executor;\n }\n \n-void Collector::collect(VariantType &var, RowWriter *writer) const {\n+Status Collector::collect(VariantType &var, RowWriter *writer) {\n     switch (var.which()) {\n         case VAR_INT64:\n             (*writer) << boost::get<int64_t>(var);","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"base\/Base.h\"\n#include \"graph\/TraverseExecutor.h\"\n#include \"parser\/TraverseSentences.h\"\n#include \"graph\/GoExecutor.h\"\n#include \"graph\/PipeExecutor.h\"\n#include \"graph\/OrderByExecutor.h\"\n#include \"graph\/FetchVerticesExecutor.h\"\n#include \"graph\/FetchEdgesExecutor.h\"\n#include \"dataman\/RowReader.h\"\n#include \"dataman\/RowWriter.h\"\n#include \"graph\/SetExecutor.h\"\n#include \"graph\/FindExecutor.h\"\n#include \"graph\/MatchExecutor.h\"\n#include \"graph\/FindPathExecutor.h\"\n#include \"graph\/LimitExecutor.h\"\n\nnamespace nebula {\nnamespace graph {\n\nstd::unique_ptr<TraverseExecutor> TraverseExecutor::makeTraverseExecutor(Sentence *sentence) {\n    return makeTraverseExecutor(sentence, ectx());\n}\n\n\n\/\/ static\nstd::unique_ptr<TraverseExecutor>\nTraverseExecutor::makeTraverseExecutor(Sentence *sentence, ExecutionContext *ectx) {\n    auto kind = sentence->kind();\n    std::unique_ptr<TraverseExecutor> executor;\n    switch (kind) {\n        case Sentence::Kind::kGo:\n            executor = std::make_unique<GoExecutor>(sentence, ectx);\n            break;\n        case Sentence::Kind::kPipe:\n            executor = std::make_unique<PipeExecutor>(sentence, ectx);\n            break;\n        case Sentence::Kind::kOrderBy:\n            executor = std::make_unique<OrderByExecutor>(sentence, ectx);\n            break;\n        case Sentence::Kind::kFetchVertices:\n            executor = std::make_unique<FetchVerticesExecutor>(sentence, ectx);\n            break;\n        case Sentence::Kind::kFetchEdges:\n            executor = std::make_unique<FetchEdgesExecutor>(sentence, ectx);\n            break;\n        case Sentence::Kind::kSet:\n            executor = std::make_unique<SetExecutor>(sentence, ectx);\n            break;\n        case Sentence::Kind::kMatch:\n            executor = std::make_unique<MatchExecutor>(sentence, ectx);\n            break;\n        case Sentence::Kind::kFind:\n            executor = std::make_unique<FindExecutor>(sentence, ectx);\n            break;\n        case Sentence::Kind::kFindPath:\n            executor = std::make_unique<FindPathExecutor>(sentence, ectx);\n            break;\n        case Sentence::Kind::kLimit:\n            executor = std::make_unique<LimitExecutor>(sentence, ectx);\n            break;\n        case Sentence::Kind::kUnknown:\n            LOG(FATAL) << \"Sentence kind unknown\";\n            break;\n        default:\n            LOG(FATAL) << \"Sentence kind illegal: \" << kind;\n            break;\n    }\n    return executor;\n}\n\nvoid Collector::collect(VariantType &var, RowWriter *writer) const {\n    switch (var.which()) {\n        case VAR_INT64:\n            (*writer) << boost::get<int64_t>(var);\n            break;\n        case VAR_DOUBLE:\n            (*writer) << boost::get<double>(var);\n            break;\n        case VAR_BOOL:\n            (*writer) << boost::get<bool>(var);\n            break;\n        case VAR_STR:\n            (*writer) << boost::get<std::string>(var);\n            break;\n        default:\n            LOG(FATAL) << \"Unknown VariantType: \" << var.which();\n    }\n}\n\nVariantType Collector::getProp(const std::string &prop,\n                               const RowReader *reader) const {\n    DCHECK(reader != nullptr);\n    DCHECK(schema_ != nullptr);\n    using nebula::cpp2::SupportedType;\n    auto type = schema_->getFieldType(prop).type;\n    switch (type) {\n        case SupportedType::BOOL: {\n            bool v;\n            reader->getBool(prop, v);\n            VLOG(3) << \"get prop: \" << prop << \", value: \" << v;\n            return v;\n        }\n        case SupportedType::TIMESTAMP:\n        case SupportedType::INT: {\n            int64_t v;\n            reader->getInt(prop, v);\n            VLOG(3) << \"get prop: \" << prop << \", value: \" << v;\n            return v;\n        }\n        case SupportedType::VID: {\n            VertexID v;\n            reader->getVid(prop, v);\n            VLOG(3) << \"get prop: \" << prop << \", value: \" << v;\n            return v;\n        }\n        case SupportedType::FLOAT: {\n            float v;\n            reader->getFloat(prop, v);\n            VLOG(3) << \"get prop: \" << prop << \", value: \" << v;\n            return static_cast<double>(v);\n        }\n        case SupportedType::DOUBLE: {\n            double v;\n            reader->getDouble(prop, v);\n            VLOG(3) << \"get prop: \" << prop << \", value: \" << v;\n            return v;\n        }\n        case SupportedType::STRING: {\n            folly::StringPiece v;\n            reader->getString(prop, v);\n            VLOG(3) << \"get prop: \" << prop << \", value: \" << v;\n            return v.toString();\n        }\n        default:\n            LOG(FATAL) << \"Unknown type: \" << static_cast<int32_t>(type);\n            return \"\";\n    }\n}\n\n}   \/\/ namespace graph\n}   \/\/ namespace nebula\n","lang_cluster":"C++","length":147,"code_uid":"93ff93d3106a4257801e6a2ba88f66e8"}
{"diff_hunk":"@@ -103,7 +103,7 @@ rtps::Time_t::Time_t(\n \n int64_t rtps::Time_t::to_ns() const\n {\n-    int64_t nano = seconds_ * 1000000000ULL;\n+    int64_t nano = seconds_ * C_SECONDS;\n     nano += nanosec_;\n     return nano;\n }","old_code":"\/\/ Copyright 2019 Proyectos y Sistemas de Mantenimiento SL (eProsima).\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\n\/**\n * @file Time_t.cpp\n *\/\n#include <fastrtps\/rtps\/common\/Time_t.h>\n\nusing namespace eprosima::fastrtps;\n\nTime_t::Time_t()\n{\n    seconds = 0;\n    nanosec = 0;\n}\n\nTime_t::Time_t(\n        int32_t sec,\n        uint32_t nsec)\n{\n    seconds = sec;\n    nanosec = nsec;\n}\n\nTime_t::Time_t(\n        long double sec)\n{\n    seconds = static_cast<int32_t>(sec);\n    nanosec = static_cast<uint32_t>((sec - seconds) * 1000000000ULL);\n}\n\nvoid Time_t::fraction(\n        uint32_t frac)\n{\n    nanosec = (frac == 0xffffffff)\n        ? 0xffffffff\n        : static_cast<uint32_t>(std::lroundl(frac * rtps::FRACTION_TO_NANO));\n}\n\nuint32_t Time_t::fraction() const\n{\n    if (nanosec == 0xffffffff)\n    {\n        return nanosec;\n    }\n\n    uint32_t fraction = static_cast<uint32_t>(std::lroundl(nanosec * rtps::NANO_TO_FRACTION));\n    uint32_t nano_check = static_cast<uint32_t>(std::lroundl(fraction * rtps::FRACTION_TO_NANO));\n    while (nano_check != nanosec)\n    {\n        nano_check = static_cast<uint32_t>(std::lroundl(++fraction * rtps::FRACTION_TO_NANO));\n    }\n\n    return fraction;\n}\n\nint64_t Time_t::to_ns() const\n{\n    int64_t nano = seconds * 1000000000ULL;\n    nano += nanosec;\n    return nano;\n}\n\nrtps::Time_t::Time_t()\n{\n    seconds_ = 0;\n    fraction_ = 0;\n    nanosec_ = 0;\n}\n\nrtps::Time_t::Time_t(\n        int32_t sec,\n        uint32_t frac)\n{\n    seconds_ = sec;\n    set_fraction(frac);\n}\n\nrtps::Time_t::Time_t(\n        long double sec)\n{\n    seconds_ = static_cast<int32_t>(sec);\n    set_fraction(static_cast<uint32_t>((sec - seconds_) * 4294967296ULL));\n}\n\nrtps::Time_t::Time_t(\n        const eprosima::fastrtps::Time_t& time)\n{\n    seconds_ = time.seconds;\n    set_nanosec(time.nanosec);\n}\n\nint64_t rtps::Time_t::to_ns() const\n{\n    int64_t nano = seconds_ * 1000000000ULL;\n    nano += nanosec_;\n    return nano;\n}\n\nint32_t rtps::Time_t::seconds() const\n{\n    return seconds_;\n}\n\nint32_t& rtps::Time_t::seconds()\n{\n    return seconds_;\n}\n\nvoid rtps::Time_t::seconds(\n        int32_t sec)\n{\n    seconds_ = sec;\n}\n\nuint32_t rtps::Time_t::nanosec() const\n{\n    return nanosec_;\n}\n\nvoid rtps::Time_t::nanosec(\n        uint32_t nanos)\n{\n    const uint32_t s_to_nano = 1000000000UL;\n    if (nanos >= s_to_nano)\n    {\n        nanos %= s_to_nano; \/\/ Remove the seconds\n    }\n    set_nanosec(nanos);\n}\n\nuint32_t rtps::Time_t::fraction() const\n{\n    return fraction_;\n}\n\nvoid rtps::Time_t::fraction(\n        uint32_t frac)\n{\n    set_fraction(frac);\n}\n\nDuration_t rtps::Time_t::to_duration_t() const\n{\n    return Duration_t(seconds_, nanosec_);\n}\n\nvoid rtps::Time_t::from_duration_t(const Duration_t& duration)\n{\n    seconds_ = duration.seconds;\n    set_nanosec(duration.nanosec);\n}\n\nvoid rtps::Time_t::set_fraction(\n        uint32_t frac)\n{\n    fraction_ = frac;\n    nanosec_ = (fraction_ == 0xffffffff)\n        ? 0xffffffff\n        : static_cast<uint32_t>(std::lroundl(fraction_ * FRACTION_TO_NANO));\n}\n\nvoid rtps::Time_t::set_nanosec(\n        uint32_t nanos)\n{\n    nanosec_ = nanos;\n    fraction_ = (nanosec_ == 0xffffffff)\n        ? 0xffffffff\n        : static_cast<uint32_t>(std::lroundl(nanosec_ * NANO_TO_FRACTION));\n\n    if (fraction_ != 0xffffffff)\n    {\n        uint32_t nano_check = static_cast<uint32_t>(std::lroundl(fraction_ * FRACTION_TO_NANO));\n        while (nano_check != nanosec_)\n        {\n            nano_check = static_cast<uint32_t>(std::lroundl(++fraction_ * FRACTION_TO_NANO));\n        }\n    }\n}","lang_cluster":"C++","length":190,"code_uid":"2fd1926e1a424515bf8e475fdf8c9349"}
{"diff_hunk":"@@ -38,19 +38,24 @@ int main(int argc, char const *argv[]) {\n                                  .set_max_iteration_count(5)\n                                  .set_accuracy_threshold(0.001);\n \n-    const auto result_train = dal::train(kmeans_desc, x_train, initial_centroids);\n+    #ifdef MPI                                 \n+        oneapi::dal::network::mpi::network net;\n+    #else\n+        oneapi::dal::network::empty_network net;\n+    #endif\n \n-    std::cout << \"Iteration count: \" << result_train.get_iteration_count() << std::endl;\n-    std::cout << \"Objective function value: \" << result_train.get_objective_function_value()\n+    const auto result_train = dal::train(kmeans_desc, x_train, initial_centroids, net);\n+\n+    std::cout << \"[\" << myRank << \"]\" << \"Iteration count: \" << result_train.get_iteration_count() << std::endl;\n+    std::cout << \"[\" << myRank << \"]\" << \"Objective function value: \" << result_train.get_objective_function_value()\n               << std::endl;\n-    std::cout << \"Lables:\" << std::endl << result_train.get_labels() << std::endl;\n-    std::cout << \"Centroids:\" << std::endl << result_train.get_model().get_centroids() << std::endl;\n+    std::cout << \"[\" << myRank << \"]\" << \"Centroids:\" << std::endl << result_train.get_model().get_centroids() << std::endl;\n \n     const auto result_test = dal::infer(kmeans_desc, result_train.get_model(), x_test);\n \n-    std::cout << \"Infer result:\" << std::endl << result_test.get_labels() << std::endl;\n-\n-    std::cout << \"Ground truth:\" << std::endl << y_test << std::endl;\n+    #ifdef MPI\n+        MPI_Finalize();\n+    #endif\n \n     return 0;\n }","old_code":"\/*******************************************************************************\n* Copyright 2020 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n#include \"example_util\/utils.hpp\"\n#include \"oneapi\/dal\/algo\/kmeans.hpp\"\n#include \"oneapi\/dal\/io\/csv.hpp\"\n\nusing namespace oneapi;\n\n\nint main(int argc, char const *argv[]) {\n    const std::string train_data_file_name        = get_data_path(\"kmeans_dense_train_data.csv\");\n    const std::string initial_centroids_file_name = get_data_path(\"kmeans_dense_train_centroids.csv\");\n    const std::string test_data_file_name         = get_data_path(\"kmeans_dense_test_data.csv\");\n    const std::string test_label_file_name        = get_data_path(\"kmeans_dense_test_label.csv\");\n\n    const auto x_train           = dal::read<dal::table>(dal::csv::data_source{train_data_file_name});\n    const auto initial_centroids = dal::read<dal::table>(dal::csv::data_source{initial_centroids_file_name});\n\n    const auto x_test = dal::read<dal::table>(dal::csv::data_source{test_data_file_name});\n    const auto y_test = dal::read<dal::table>(dal::csv::data_source{test_label_file_name});\n\n    const auto kmeans_desc = dal::kmeans::descriptor<>()\n                                 .set_cluster_count(20)\n                                 .set_max_iteration_count(5)\n                                 .set_accuracy_threshold(0.001);\n\n    const auto result_train = dal::train(kmeans_desc, x_train, initial_centroids);\n\n    std::cout << \"Iteration count: \" << result_train.get_iteration_count() << std::endl;\n    std::cout << \"Objective function value: \" << result_train.get_objective_function_value()\n              << std::endl;\n    std::cout << \"Lables:\" << std::endl << result_train.get_labels() << std::endl;\n    std::cout << \"Centroids:\" << std::endl << result_train.get_model().get_centroids() << std::endl;\n\n    const auto result_test = dal::infer(kmeans_desc, result_train.get_model(), x_test);\n\n    std::cout << \"Infer result:\" << std::endl << result_test.get_labels() << std::endl;\n\n    std::cout << \"Ground truth:\" << std::endl << y_test << std::endl;\n\n    return 0;\n}\n","lang_cluster":"C++","length":56,"code_uid":"3d058110f622493487659d77cf4a35f4"}
{"diff_hunk":"@@ -123,17 +123,11 @@ except ImportError:\n     default_client = None\n     wait = None\n \n-    class dask_Array:\n+    class dask_Array: # type: ignore\n         \"\"\"Dummy class for dask.array.Array.\"\"\"\n \n-        pass\n-\n-    class dask_DataFrame:\n+    class dask_DataFrame: # type: ignore\n         \"\"\"Dummy class for dask.dataframe.DataFrame.\"\"\"\n \n-        pass\n-\n-    class dask_Series:\n+    class dask_Series: # type: ignore\n         \"\"\"Dummy class for dask.dataframe.Series.\"\"\"\n-\n-        pass","old_code":"# coding: utf-8\n\"\"\"Compatibility library.\"\"\"\n\n\"\"\"pandas\"\"\"\ntry:\n    from pandas import DataFrame as pd_DataFrame\n    from pandas import Series as pd_Series\n    from pandas import concat\n    from pandas.api.types import is_sparse as is_dtype_sparse\n    PANDAS_INSTALLED = True\nexcept ImportError:\n    PANDAS_INSTALLED = False\n\n    class pd_Series:\n        \"\"\"Dummy class for pandas.Series.\"\"\"\n\n        pass\n\n    class pd_DataFrame:\n        \"\"\"Dummy class for pandas.DataFrame.\"\"\"\n\n        pass\n\n    concat = None\n    is_dtype_sparse = None\n\n\"\"\"matplotlib\"\"\"\ntry:\n    import matplotlib\n    MATPLOTLIB_INSTALLED = True\nexcept ImportError:\n    MATPLOTLIB_INSTALLED = False\n\n\"\"\"graphviz\"\"\"\ntry:\n    import graphviz\n    GRAPHVIZ_INSTALLED = True\nexcept ImportError:\n    GRAPHVIZ_INSTALLED = False\n\n\"\"\"datatable\"\"\"\ntry:\n    import datatable\n    if hasattr(datatable, \"Frame\"):\n        dt_DataTable = datatable.Frame\n    else:\n        dt_DataTable = datatable.DataTable\n    DATATABLE_INSTALLED = True\nexcept ImportError:\n    DATATABLE_INSTALLED = False\n\n    class dt_DataTable:\n        \"\"\"Dummy class for datatable.DataTable.\"\"\"\n\n        pass\n\n\n\"\"\"sklearn\"\"\"\ntry:\n    from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.utils.class_weight import compute_sample_weight\n    from sklearn.utils.multiclass import check_classification_targets\n    from sklearn.utils.validation import assert_all_finite, check_array, check_X_y\n    try:\n        from sklearn.exceptions import NotFittedError\n        from sklearn.model_selection import GroupKFold, StratifiedKFold\n    except ImportError:\n        from sklearn.cross_validation import GroupKFold, StratifiedKFold\n        from sklearn.utils.validation import NotFittedError\n    try:\n        from sklearn.utils.validation import _check_sample_weight\n    except ImportError:\n        from sklearn.utils.validation import check_consistent_length\n\n        # dummy function to support older version of scikit-learn\n        def _check_sample_weight(sample_weight, X, dtype=None):\n            check_consistent_length(sample_weight, X)\n            return sample_weight\n\n    SKLEARN_INSTALLED = True\n    _LGBMModelBase = BaseEstimator\n    _LGBMRegressorBase = RegressorMixin\n    _LGBMClassifierBase = ClassifierMixin\n    _LGBMLabelEncoder = LabelEncoder\n    LGBMNotFittedError = NotFittedError\n    _LGBMStratifiedKFold = StratifiedKFold\n    _LGBMGroupKFold = GroupKFold\n    _LGBMCheckXY = check_X_y\n    _LGBMCheckArray = check_array\n    _LGBMCheckSampleWeight = _check_sample_weight\n    _LGBMAssertAllFinite = assert_all_finite\n    _LGBMCheckClassificationTargets = check_classification_targets\n    _LGBMComputeSampleWeight = compute_sample_weight\nexcept ImportError:\n    SKLEARN_INSTALLED = False\n    _LGBMModelBase = object\n    _LGBMClassifierBase = object\n    _LGBMRegressorBase = object\n    _LGBMLabelEncoder = None\n    LGBMNotFittedError = ValueError\n    _LGBMStratifiedKFold = None\n    _LGBMGroupKFold = None\n    _LGBMCheckXY = None\n    _LGBMCheckArray = None\n    _LGBMCheckSampleWeight = None\n    _LGBMAssertAllFinite = None\n    _LGBMCheckClassificationTargets = None\n    _LGBMComputeSampleWeight = None\n\n\"\"\"dask\"\"\"\ntry:\n    from dask import delayed\n    from dask.array import Array as dask_Array\n    from dask.dataframe import DataFrame as dask_DataFrame\n    from dask.dataframe import Series as dask_Series\n    from dask.distributed import Client, default_client, wait\n    DASK_INSTALLED = True\nexcept ImportError:\n    DASK_INSTALLED = False\n    delayed = None\n    Client = object\n    default_client = None\n    wait = None\n\n    class dask_Array:\n        \"\"\"Dummy class for dask.array.Array.\"\"\"\n\n        pass\n\n    class dask_DataFrame:\n        \"\"\"Dummy class for dask.dataframe.DataFrame.\"\"\"\n\n        pass\n\n    class dask_Series:\n        \"\"\"Dummy class for dask.dataframe.Series.\"\"\"\n\n        pass\n","lang_cluster":"C++","length":139,"code_uid":"19534ee32590435386cfe762ce97bc91"}
{"diff_hunk":"@@ -146,7 +146,7 @@ def _HeightFirstSplit(cluster, n):\n   if len(cluster) == n:\n     return cluster.GetPoints()\n   clusters = [cluster]\n-  for i in range(n - 1):\n+  for _ in range(n - 1):\n     nxtIdx = 0\n     while nxtIdx < len(clusters) and len(clusters[nxtIdx]) == 1:\n       nxtIdx += 1","old_code":"# $Id$\n#\n# Copyright (C) 2001-2008  greg Landrum\n#\n#   @@ All Rights Reserved @@\n#  This file is part of the RDKit.\n#  The contents are covered by the terms of the BSD license\n#  which is included in the file license.txt, found at the root\n#  of the RDKit source tree.\n#\n\"\"\"utility functions for clustering\n\n\"\"\"\n\n\ndef GetNodeList(cluster):\n  \"\"\"returns an ordered list of all nodes below cluster\n\n  the ordering is done using the lengths of the child nodes\n\n   **Arguments**\n\n     - cluster: the cluster in question\n\n   **Returns**\n\n     - a list of the leaves below this cluster\n\n  \"\"\"\n  if len(cluster) == 1:\n    return [cluster]\n  else:\n    children = cluster.GetChildren()\n    children.sort(key=lambda x: len(x), reverse=True)\n    res = []\n    for child in children:\n      res += GetNodeList(child)\n    res += [cluster]\n    return res\n\n\ndef GetNodesDownToCentroids(cluster, above=1):\n  \"\"\"returns an ordered list of all nodes below cluster\n\n\n  \"\"\"\n  if hasattr(cluster, '_isCentroid'):\n    cluster._aboveCentroid = 0\n    above = -1\n  else:\n    cluster._aboveCentroid = above\n  if len(cluster) == 1:\n    return [cluster]\n  else:\n    res = []\n    children = cluster.GetChildren()\n    children.sort(lambda x, y: cmp(len(y), len(x)))\n    for child in children:\n      res = res + GetNodesDownToCentroids(child, above)\n    res = res + [cluster]\n    return res\n\n\ndef FindClusterCentroidFromDists(cluster, dists):\n  \"\"\" find the point in a cluster which has the smallest summed \n     Euclidean distance to all others\n\n   **Arguments**\n\n     - cluster: the cluster to work with\n\n     - dists: the distance matrix to use for the points\n\n   **Returns**\n\n     - the index of the centroid point\n\n  \"\"\"\n  children = cluster.GetPoints()\n  pts = [x.GetData() for x in children]\n\n  best = 1e24\n  bestIdx = -1\n  for pt in pts:\n    dAccum = 0.0\n    # loop over others and add'em up\n    for other in pts:\n      if other != pt:\n        if other > pt:\n          row, col = pt, other\n        else:\n          row, col = other, pt\n        dAccum += dists[col * (col - 1) \/ 2 + row]\n        if dAccum >= best:\n          # minor efficiency hack\n          break\n    if dAccum < best:\n      best = dAccum\n      bestIdx = pt\n  for i in range(len(pts)):\n    pt = pts[i]\n    if pt != bestIdx:\n      if pt > bestIdx:\n        row, col = bestIdx, pt\n      else:\n        row, col = pt, bestIdx\n      children[i]._distToCenter = dists[col * (col - 1) \/ 2 + row]\n    else:\n      children[i]._distToCenter = 0.0\n    children[i]._clustCenter = bestIdx\n  cluster._clustCenter = bestIdx\n  cluster._distToCenter = 0.0\n\n  return bestIdx\n\n\ndef _BreadthFirstSplit(cluster, n):\n  \"\"\"  *Internal Use Only*\n\n  \"\"\"\n  if len(cluster) < n:\n    raise ValueError('Cannot split cluster of length %d into %d pieces' % (len(cluster), n))\n  if len(cluster) == n:\n    return cluster.GetPoints()\n  clusters = [cluster]\n  nxtIdx = 0\n  for i in range(n - 1):\n    while nxtIdx < len(clusters) and len(clusters[nxtIdx]) == 1:\n      nxtIdx += 1\n    assert nxtIdx < len(clusters)\n\n    children = clusters[nxtIdx].GetChildren()\n    children.sort(key=lambda x: x.GetMetric(), reverse=True)\n    for child in children:\n      clusters.append(child)\n    del clusters[nxtIdx]\n  return clusters\n\n\ndef _HeightFirstSplit(cluster, n):\n  \"\"\"  *Internal Use Only*\n\n  \"\"\"\n  if len(cluster) < n:\n    raise ValueError('Cannot split cluster of length %d into %d pieces' % (len(cluster), n))\n  if len(cluster) == n:\n    return cluster.GetPoints()\n  clusters = [cluster]\n  for i in range(n - 1):\n    nxtIdx = 0\n    while nxtIdx < len(clusters) and len(clusters[nxtIdx]) == 1:\n      nxtIdx += 1\n    assert nxtIdx < len(clusters)\n\n    children = clusters[nxtIdx].GetChildren()\n    for child in children:\n      clusters.append(child)\n    del clusters[nxtIdx]\n    clusters.sort(key=lambda x: x.GetMetric(), reverse=True)\n  return clusters\n\n\ndef SplitIntoNClusters(cluster, n, breadthFirst=1):\n  \"\"\"  splits a cluster tree into a set of branches\n\n    **Arguments**\n\n      - cluster: the root of the cluster tree\n\n      - n: the number of clusters to include in the split\n\n      - breadthFirst: toggles breadth first (vs depth first) cleavage\n        of the cluster tree.\n\n    **Returns**\n\n      - a list of sub clusters\n\n  \"\"\"\n  if breadthFirst:\n    return _BreadthFirstSplit(cluster, n)\n  else:\n    return _HeightFirstSplit(cluster, n)\n","lang_cluster":"C++","length":183,"code_uid":"ef4b842150924b7193b0f6b52fbf835c"}
{"diff_hunk":"@@ -147,7 +147,7 @@ int main() {\n         for (int t = 0; t < TEST_ITERS; t++) {\n             hipEventRecord(start);\n             for (int i = 0; i < DISPATCHES_PER_TEST; i++) {\n-                hipLaunchKernelGGL(NearlyNull, dim3(NUM_GROUPS), dim3(GROUP_SIZE), 0, stream, Ad);\n+                hipExtLaunchKernelGGL((EmptyKernel), dim3(NUM_GROUPS), dim3(GROUP_SIZE), 0, stream, start, stop, 0);\n             }\n             stopTest(start, stop, \"StreamASyncDispatchNoWait\", DISPATCHES_PER_TEST);\n         }","old_code":"\/*\nCopyright (c) 2015-present Advanced Micro Devices, Inc. All rights reserved.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n*\/\n\n#include \"hip\/hip_runtime.h\"\n#include <iostream>\n#include <time.h>\n#include \"ResultDatabase.h\"\n\n#define PRINT_PROGRESS 0\n\n#define check(cmd)                                                                                 \\\n    {                                                                                              \\\n        hipError_t status = cmd;                                                                   \\\n        if (status != hipSuccess) {                                                                \\\n            printf(\"error: '%s'(%d) from %s at %s:%d\\n\", hipGetErrorString(status), status, #cmd,  \\\n                   __FILE__, __LINE__);                                                            \\\n            abort();                                                                               \\\n        }                                                                                          \\\n    }\n\n#define LEN 1024 * 1024\n\n#define NUM_GROUPS 1\n#define GROUP_SIZE 64\n#define TEST_ITERS 20\n#define DISPATCHES_PER_TEST 100\n\nconst unsigned p_tests = 0xfffffff;\n\n\n\/\/ HCC optimizes away fully NULL kernel calls, so run one that is nearly null:\n__global__ void NearlyNull(float* Ad) {\n    if (Ad) {\n        Ad[0] = 42;\n    }\n}\n\n\nResultDatabase resultDB;\n\n\nvoid stopTest(hipEvent_t start, hipEvent_t stop, const char* msg, int iters) {\n    float mS = 0;\n    check(hipEventRecord(stop));\n    check(hipDeviceSynchronize());\n    check(hipEventElapsedTime(&mS, start, stop));\n    resultDB.AddResult(std::string(msg), \"\", \"uS\", mS * 1000 \/ iters);\n    if (PRINT_PROGRESS & 0x1) {\n        std::cout << msg << \"\\t\\t\" << mS * 1000 \/ iters << \" uS\" << std::endl;\n    }\n    if (PRINT_PROGRESS & 0x2) {\n        resultDB.DumpSummary(std::cout);\n    }\n}\n\n\nint main() {\n    hipError_t err;\n    float* Ad;\n    check(hipMalloc(&Ad, 4));\n\n\n    hipStream_t stream;\n    check(hipStreamCreate(&stream));\n\n\n    hipEvent_t start, sync, stop;\n    check(hipEventCreate(&start));\n    check(hipEventCreateWithFlags(&sync, hipEventBlockingSync));\n    check(hipEventCreate(&stop));\n\n\n    hipStream_t stream0 = 0;\n\n\n    if (p_tests & 0x1) {\n        hipEventRecord(start);\n        hipLaunchKernelGGL(NearlyNull, dim3(NUM_GROUPS), dim3(GROUP_SIZE), 0, stream0, Ad);\n        stopTest(start, stop, \"FirstKernelLaunch\", 1);\n    }\n\n\n    if (p_tests & 0x2) {\n        hipEventRecord(start);\n        hipLaunchKernelGGL(NearlyNull, dim3(NUM_GROUPS), dim3(GROUP_SIZE), 0, stream0, Ad);\n        stopTest(start, stop, \"SecondKernelLaunch\", 1);\n    }\n\n\n    if (p_tests & 0x4) {\n        for (int t = 0; t < TEST_ITERS; t++) {\n            hipEventRecord(start);\n            for (int i = 0; i < DISPATCHES_PER_TEST; i++) {\n                hipLaunchKernelGGL(NearlyNull, dim3(NUM_GROUPS), dim3(GROUP_SIZE), 0, stream0, Ad);\n                hipEventRecord(sync);\n                hipEventSynchronize(sync);\n            }\n            stopTest(start, stop, \"NullStreamASyncDispatchWait\", DISPATCHES_PER_TEST);\n        }\n    }\n\n\n    if (p_tests & 0x10) {\n        for (int t = 0; t < TEST_ITERS; t++) {\n            hipEventRecord(start);\n            for (int i = 0; i < DISPATCHES_PER_TEST; i++) {\n                hipLaunchKernelGGL(NearlyNull, dim3(NUM_GROUPS), dim3(GROUP_SIZE), 0, stream, Ad);\n                hipEventRecord(sync);\n                hipEventSynchronize(sync);\n            }\n            stopTest(start, stop, \"StreamASyncDispatchWait\", DISPATCHES_PER_TEST);\n        }\n    }\n\n#if 1\n\n    if (p_tests & 0x40) {\n        for (int t = 0; t < TEST_ITERS; t++) {\n            hipEventRecord(start);\n            for (int i = 0; i < DISPATCHES_PER_TEST; i++) {\n                hipLaunchKernelGGL(NearlyNull, dim3(NUM_GROUPS), dim3(GROUP_SIZE), 0, stream0, Ad);\n            }\n            stopTest(start, stop, \"NullStreamASyncDispatchNoWait\", DISPATCHES_PER_TEST);\n        }\n    }\n\n    if (p_tests & 0x80) {\n        for (int t = 0; t < TEST_ITERS; t++) {\n            hipEventRecord(start);\n            for (int i = 0; i < DISPATCHES_PER_TEST; i++) {\n                hipLaunchKernelGGL(NearlyNull, dim3(NUM_GROUPS), dim3(GROUP_SIZE), 0, stream, Ad);\n            }\n            stopTest(start, stop, \"StreamASyncDispatchNoWait\", DISPATCHES_PER_TEST);\n        }\n    }\n#endif\n    resultDB.DumpSummary(std::cout);\n\n\n    check(hipEventDestroy(start));\n    check(hipEventDestroy(sync));\n    check(hipEventDestroy(stop));\n}\n","lang_cluster":"C++","length":162,"code_uid":"714f864f4a644005b3c5a9e6c466fdcd"}
{"diff_hunk":"@@ -163,7 +163,7 @@ class Canvas(CanvasBase):\n \n   def addCanvasPolygon(self, ps, color=(0, 0, 0), fill=True, stroke=False, **kwargs):\n     if not fill and not stroke:\n-      return\n+      return \n     dps = []\n     for p in ps:\n       dps.extend(p)","old_code":"#\n#  Copyright (C) 2008 Greg Landrum\n#\n#   @@ All Rights Reserved @@\n#  This file is part of the RDKit.\n#  The contents are covered by the terms of the BSD license\n#  which is included in the file license.txt, found at the root\n#  of the RDKit source tree.\n#\nimport os\nimport re\n\nfrom aggdraw import Brush, Pen\nfrom aggdraw import Draw\nfrom aggdraw import Font\nfrom rdkit import RDConfig\nfrom rdkit.Chem.Draw.canvasbase import CanvasBase\n\nfaceMap = {'sans': os.path.join(RDConfig.RDCodeDir, 'Chem', 'Draw', 'FreeSans.ttf')}\n\n\ndef convertColor(color):\n  color = (int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n  return color\n\n\nclass Canvas(CanvasBase):\n  # fonts appear smaller in aggdraw than with cairo\n  # fix that here:\n  fontScale = 1.2\n\n  def __init__(self,\n               img=None,\n               imageType=None,  # determines file type\n               fileName=None,  # if set determines output file name\n               size=None, ):\n    if img is None:\n      try:\n        import Image\n      except ImportError:\n        from PIL import Image\n      if size is None:\n        raise ValueError('please provide either an image or a size')\n      img = Image.new('RGBA', size, \"white\")\n    self.image = img\n    self.draw = Draw(img)\n    self.draw.setantialias(True)\n    if size is None:\n      self.size = self.draw.size\n    else:\n      self.size = size\n    if imageType and imageType not in ('png', 'jpg'):\n      raise ValueError('unsupported image type for agg canvas')\n    self.drawType = imageType\n    self.fileName = fileName\n\n  def _doLine(self, p1, p2, pen, **kwargs):\n    if kwargs.get('dash', (0, 0)) == (0, 0):\n      self.draw.line((p1[0], p1[1], p2[0], p2[1]), pen)\n    else:\n      dash = kwargs['dash']\n      pts = self._getLinePoints(p1, p2, dash)\n\n      currDash = 0\n      dashOn = True\n      while currDash < (len(pts) - 1):\n        if dashOn:\n          p1 = pts[currDash]\n          p2 = pts[currDash + 1]\n          self.draw.line((p1[0], p1[1], p2[0], p2[1]), pen)\n        currDash += 1\n        dashOn = not dashOn\n\n  def addCanvasLine(self, p1, p2, color=(0, 0, 0), color2=None, **kwargs):\n    if color2 and color2 != color:\n      mp = (p1[0] + p2[0]) \/ 2., (p1[1] + p2[1]) \/ 2.\n      color = convertColor(color)\n      self._doLine(p1, mp, Pen(color, kwargs.get('linewidth', 1)), **kwargs)\n      color2 = convertColor(color2)\n      self._doLine(mp, p2, Pen(color2, kwargs.get('linewidth', 1)), **kwargs)\n    else:\n      color = convertColor(color)\n      self._doLine(p1, p2, Pen(color, kwargs.get('linewidth', 1)), **kwargs)\n\n  def addCanvasText(self, text, pos, font, color=(0, 0, 0), **kwargs):\n    orientation = kwargs.get('orientation', 'E')\n    color = convertColor(color)\n    aggFont = Font(color, faceMap[font.face], size=font.size * self.fontScale)\n\n    blocks = list(re.finditer(r'\\<(.+?)\\>(.+?)\\<\/\\1\\>', text))\n    w, h = 0, 0\n    supH = 0\n    subH = 0\n    if not len(blocks):\n      w, h = self.draw.textsize(text, aggFont)\n      tw, th = w, h\n      offset = w * pos[2]\n      dPos = pos[0] - w \/ 2. + offset, pos[1] - h \/ 2.\n      self.draw.text(dPos, text, aggFont)\n    else:\n      dblocks = []\n      idx = 0\n      for block in blocks:\n        blockStart, blockEnd = block.span(0)\n        if blockStart != idx:\n          # untagged text:\n          tblock = text[idx:blockStart]\n          tw, th = self.draw.textsize(tblock, aggFont)\n          w += tw\n          h = max(h, th)\n          dblocks.append((tblock, '', tw, th))\n        fmt = block.groups()[0]\n        tblock = block.groups()[1]\n        if fmt in ('sub', 'sup'):\n          lFont = Font(color, faceMap[font.face], size=0.8 * font.size * self.fontScale)\n        else:\n          lFont = aggFont\n        tw, th = self.draw.textsize(tblock, lFont)\n        w += tw\n        if fmt == 'sub':\n          subH = max(subH, th)\n        elif fmt == 'sup':\n          supH = max(supH, th)\n        else:\n          h = max(h, th)\n        dblocks.append((tblock, fmt, tw, th))\n        idx = blockEnd\n      if idx != len(text):\n        # untagged text:\n        tblock = text[idx:]\n        tw, th = self.draw.textsize(tblock, aggFont)\n        w += tw\n        h = max(h, th)\n        dblocks.append((tblock, '', tw, th))\n\n      supH *= 0.5\n      subH *= 0.5\n      h += supH + subH\n      offset = w * pos[2]\n      dPos = [pos[0] - w \/ 2. + offset, pos[1] - h \/ 2.]\n      if orientation == 'W':\n        dPos = [pos[0] - w + offset, pos[1] - h \/ 2.]\n      elif orientation == 'E':\n        dPos = [pos[0] + offset, pos[1] - h \/ 2.]\n      else:\n        dPos = [pos[0] - w \/ 2 + offset, pos[1] - h \/ 2.]\n\n      if supH:\n        dPos[1] += supH\n      for txt, fmt, tw, th in dblocks:\n        tPos = dPos[:]\n        if fmt == 'sub':\n          tPos[1] += subH\n        elif fmt == 'sup':\n          tPos[1] -= supH\n        if fmt in ('sub', 'sup'):\n          lFont = Font(color, faceMap[font.face], size=0.8 * font.size * self.fontScale)\n        else:\n          lFont = aggFont\n        self.draw.text(tPos, txt, lFont)\n        dPos[0] += tw\n    return (tw + th * .4, th + th * .4, offset)\n\n  def addCanvasPolygon(self, ps, color=(0, 0, 0), fill=True, stroke=False, **kwargs):\n    if not fill and not stroke:\n      return\n    dps = []\n    for p in ps:\n      dps.extend(p)\n    color = convertColor(color)\n    brush = None\n    pen = None\n    if fill:\n      brush = Brush(color)\n    if stroke:\n      pen = Pen(color)\n    self.draw.polygon(dps, pen, brush)\n\n  def addCanvasDashedWedge(self, p1, p2, p3, dash=(2, 2), color=(0, 0, 0), color2=None, **kwargs):\n    pen = Pen(color, kwargs.get('linewidth', 1))\n    dash = (3, 3)\n    pts1 = self._getLinePoints(p1, p2, dash)\n    pts2 = self._getLinePoints(p1, p3, dash)\n\n    if len(pts2) < len(pts1):\n      pts2, pts1 = pts1, pts2\n\n    for i in range(len(pts1)):\n      self.draw.line((pts1[i][0], pts1[i][1], pts2[i][0], pts2[i][1]), pen)\n\n  def flush(self):\n    self.draw.flush()\n    if self.fileName:\n      self.image.save(self.fileName)\n","lang_cluster":"C++","length":194,"code_uid":"50e91be155b9408fbf6d84308112e68d"}
{"diff_hunk":"@@ -52,11 +52,18 @@ class EditableMol : boost::noncopyable {\n     PRECONDITION(atom, \"bad atom\");\n     return dp_mol->addAtom(atom, true, false);\n   };\n-  void ReplaceAtom(unsigned int idx, Atom *atom) {\n+  void ReplaceAtom(unsigned int idx, Atom *atom,\n+                   bool updateLabels, bool preserveProps) {\n     PRECONDITION(dp_mol, \"no molecule\");\n     PRECONDITION(atom, \"bad atom\");\n-    dp_mol->replaceAtom(idx, atom);\n+    dp_mol->replaceAtom(idx, atom, updateLabels, preserveProps);\n   };\n+  void ReplaceBond(unsigned int idx, Bond *bond, bool preserveProps) {\n+    PRECONDITION(dp_mol, \"no molecule\");\n+    PRECONDITION(bond, \"bad bond\");\n+    dp_mol->replaceBond(idx, bond, preserveProps);\n+  };\n+  \n   ROMol *GetMol() const {\n     PRECONDITION(dp_mol, \"no molecule\");\n     ROMol *res = new ROMol(*dp_mol);","old_code":"\/\/ $Id$\n\/\/\n\/\/  Copyright (C) 2007 Greg Landrum\n\/\/\n\/\/   @@ All Rights Reserved @@\n\/\/  This file is part of the RDKit.\n\/\/  The contents are covered by the terms of the BSD license\n\/\/  which is included in the file license.txt, found at the root\n\/\/  of the RDKit source tree.\n\/\/\n\/\/ there's a compiler bug in some versions of g++ that causes this file to not\n\/\/ compile unless\n\/\/ we skip the docstrings:\n\/\/#define BOOST_PYTHON_NO_PY_SIGNATURES\n\n#define NO_IMPORT_ARRAY\n#include <RDBoost\/python.h>\n#include <string>\n\n#include \"rdchem.h\"\n\/\/ ours\n#include <GraphMol\/RDKitBase.h>\n\nnamespace python = boost::python;\n\nnamespace RDKit {\n\nnamespace {\nclass EditableMol : boost::noncopyable {\n public:\n  EditableMol(const ROMol &m) { dp_mol = new RWMol(m); };\n  ~EditableMol() {\n    PRECONDITION(dp_mol, \"no molecule\");\n    delete dp_mol;\n  };\n\n  void RemoveAtom(unsigned int idx) {\n    PRECONDITION(dp_mol, \"no molecule\");\n    dp_mol->removeAtom(idx);\n  };\n  void RemoveBond(unsigned int idx1, unsigned int idx2) {\n    PRECONDITION(dp_mol, \"no molecule\");\n    dp_mol->removeBond(idx1, idx2);\n  };\n  int AddBond(unsigned int begAtomIdx, unsigned int endAtomIdx,\n              Bond::BondType order = Bond::UNSPECIFIED) {\n    PRECONDITION(dp_mol, \"no molecule\");\n    return dp_mol->addBond(begAtomIdx, endAtomIdx, order);\n  };\n  int AddAtom(Atom *atom) {\n    PRECONDITION(dp_mol, \"no molecule\");\n    PRECONDITION(atom, \"bad atom\");\n    return dp_mol->addAtom(atom, true, false);\n  };\n  void ReplaceAtom(unsigned int idx, Atom *atom) {\n    PRECONDITION(dp_mol, \"no molecule\");\n    PRECONDITION(atom, \"bad atom\");\n    dp_mol->replaceAtom(idx, atom);\n  };\n  ROMol *GetMol() const {\n    PRECONDITION(dp_mol, \"no molecule\");\n    ROMol *res = new ROMol(*dp_mol);\n    return res;\n  };\n\n private:\n  RWMol *dp_mol;\n};\n}\n\nstruct EditableMol_wrapper {\n  static void wrap() {\n    std::string molClassDoc =\n        \"The EditableMol class.\\n\\n\\\n   This class can be used to add\/remove bonds and atoms to\\n\\\n   a molecule.\\n\\\n   In order to use it, you need to first construct an EditableMol\\n\\\n   from a standard Mol:\\n\\\n   >>> m = Chem.MolFromSmiles('CCC')\\n\\\n   >>> em = Chem.EditableMol(m)\\n\\\n   >>> em.AddAtom(Chem.Atom(8))\\n\\\n   >>> em.AddBond(0,3,Chem.BondType.SINGLE)\\n\\\n   >>> m2 = em.GetMol()\\n\\\n   >>> Chem.SanitizeMol(m2)\\n\\\n   >>> Chem.MolToSmiles(m2)\\n\\\n   'CCCO'\\n\\\n\\n\\\n   *Note*: It is very, very easy to shoot yourself in the foot with\\n\\\n           this class by constructing an unreasonable molecule.\\n\\\n\";\n    python::class_<EditableMol, boost::noncopyable>(\n        \"EditableMol\", \"an editable molecule class\",\n        python::init<const ROMol &>(\"Construct from a Mol\"))\n        .def(\"RemoveAtom\", &EditableMol::RemoveAtom,\n             \"Remove the specified atom from the molecule\")\n        .def(\"RemoveBond\", &EditableMol::RemoveBond,\n             \"Remove the specified bond from the molecule\")\n\n        .def(\"AddBond\", &EditableMol::AddBond,\n             (python::arg(\"mol\"), python::arg(\"beginAtomIdx\"),\n              python::arg(\"endAtomIdx\"),\n              python::arg(\"order\") = Bond::UNSPECIFIED),\n             \"add a bond, returns the index of the newly added bond\")\n\n        .def(\"AddAtom\", &EditableMol::AddAtom,\n             (python::arg(\"mol\"), python::arg(\"atom\")),\n             \"add an atom, returns the index of the newly added atom\")\n        .def(\"ReplaceAtom\", &EditableMol::ReplaceAtom,\n             (python::arg(\"mol\"), python::arg(\"index\"), python::arg(\"newAtom\")),\n             \"replaces the specified atom with the provided one\")\n        .def(\"GetMol\", &EditableMol::GetMol,\n             \"Returns a Mol (a normal molecule)\",\n             python::return_value_policy<python::manage_new_object>());\n  };\n};\n\n}  \/\/ end of namespace\nvoid wrap_EditableMol() { RDKit::EditableMol_wrapper::wrap(); }\n","lang_cluster":"C++","length":118,"code_uid":"97ad2534b6db4df9ab87eccf2ce8cb1c"}
{"diff_hunk":"@@ -167,13 +167,18 @@ void SYCLInternal::initialize(const sycl::queue& q) {\n void SYCLInternal::finalize() {\n   SYCL().fence();\n   was_finalized = true;\n-  if (nullptr != m_scratchSpace || nullptr != m_scratchFlags) {\n-    \/\/ FIXME_SYCL\n-    std::abort();\n-  }\n \n-  using RecordSYCL =\n-      Kokkos::Impl::SharedAllocationRecord<Experimental::SYCLDeviceUSMSpace>;\n+  using RecordSYCL = Kokkos::Impl::SharedAllocationRecord<SYCLDeviceUSMSpace>;\n+  if (nullptr != m_scratchSpace)\n+    RecordSYCL::decrement(RecordSYCL::get_record(m_scratchSpace));\n+  if (nullptr != m_scratchFlags)\n+    RecordSYCL::decrement(RecordSYCL::get_record(m_scratchFlags));\n+  m_syclDev           = -1;\n+  m_scratchSpaceCount = 0;\n+  m_scratchSpace      = nullptr;\n+  m_scratchFlagsCount = 0;\n+  m_scratchFlags      = nullptr;\n+\n   RecordSYCL::decrement(RecordSYCL::get_record(m_scratchConcurrentBitset));\n   m_scratchConcurrentBitset = nullptr;\n ","old_code":"\/*\n\/\/@HEADER\n\/\/ ************************************************************************\n\/\/\n\/\/                        Kokkos v. 3.0\n\/\/       Copyright (2020) National Technology & Engineering\n\/\/               Solutions of Sandia, LLC (NTESS).\n\/\/\n\/\/ Under the terms of Contract DE-NA0003525 with NTESS,\n\/\/ the U.S. Government retains certain rights in this software.\n\/\/\n\/\/ Redistribution and use in source and binary forms, with or without\n\/\/ modification, are permitted provided that the following conditions are\n\/\/ met:\n\/\/\n\/\/ 1. Redistributions of source code must retain the above copyright\n\/\/ notice, this list of conditions and the following disclaimer.\n\/\/\n\/\/ 2. Redistributions in binary form must reproduce the above copyright\n\/\/ notice, this list of conditions and the following disclaimer in the\n\/\/ documentation and\/or other materials provided with the distribution.\n\/\/\n\/\/ 3. Neither the name of the Corporation nor the names of the\n\/\/ contributors may be used to endorse or promote products derived from\n\/\/ this software without specific prior written permission.\n\/\/\n\/\/ THIS SOFTWARE IS PROVIDED BY NTESS \"AS IS\" AND ANY\n\/\/ EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n\/\/ IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n\/\/ PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NTESS OR THE\n\/\/ CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n\/\/ EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n\/\/ PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n\/\/ PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n\/\/ LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n\/\/ NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n\/\/ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\/\/\n\/\/ Questions? Contact Christian R. Trott (crtrott@sandia.gov)\n\/\/\n\/\/ ************************************************************************\n\/\/@HEADER\n*\/\n\n#include <Kokkos_Concepts.hpp>\n#include <SYCL\/Kokkos_SYCL_Instance.hpp>\n#include <KokkosCore_Config_DeclareBackend.hpp>\n#include <Kokkos_SYCL.hpp>\n#include <Kokkos_HostSpace.hpp>\n#include <Kokkos_Serial.hpp>\n#include <impl\/Kokkos_ConcurrentBitset.hpp>\n#include <impl\/Kokkos_Error.hpp>\n\nnamespace Kokkos {\nnamespace Experimental {\nnamespace Impl {\n\nstd::vector<std::optional<sycl::queue>*> SYCLInternal::all_queues;\nstd::mutex SYCLInternal::mutex;\n\nSYCLInternal::~SYCLInternal() {\n  if (!was_finalized || m_scratchSpace || m_scratchFlags ||\n      m_scratchConcurrentBitset) {\n    std::cerr << \"Kokkos::Experimental::SYCL ERROR: Failed to call \"\n                 \"Kokkos::Experimental::SYCL::finalize()\"\n              << std::endl;\n    std::cerr.flush();\n  }\n}\n\nint SYCLInternal::verify_is_initialized(const char* const label) const {\n  if (!is_initialized()) {\n    std::cerr << \"Kokkos::Experimental::SYCL::\" << label\n              << \" : ERROR device not initialized\" << std::endl;\n  }\n  return is_initialized();\n}\nSYCLInternal& SYCLInternal::singleton() {\n  static SYCLInternal self;\n  return self;\n}\n\nvoid SYCLInternal::initialize(const sycl::device& d) {\n  auto exception_handler = [](sycl::exception_list exceptions) {\n    bool asynchronous_error = false;\n    for (std::exception_ptr const& e : exceptions) {\n      try {\n        std::rethrow_exception(e);\n      } catch (sycl::exception const& e) {\n        std::cerr << e.what() << '\\n';\n        asynchronous_error = true;\n      }\n    }\n    if (asynchronous_error)\n      Kokkos::Impl::throw_runtime_exception(\n          \"There was an asynchronous SYCL error!\\n\");\n  };\n  initialize(sycl::queue{d, exception_handler});\n}\n\n\/\/ FIXME_SYCL\nvoid SYCLInternal::initialize(const sycl::queue& q) {\n  if (was_finalized)\n    Kokkos::abort(\"Calling SYCL::initialize after SYCL::finalize is illegal\\n\");\n\n  if (is_initialized()) return;\n\n  if (!HostSpace::execution_space::impl_is_initialized()) {\n    const std::string msg(\n        \"SYCL::initialize ERROR : HostSpace::execution_space is not \"\n        \"initialized\");\n    Kokkos::Impl::throw_runtime_exception(msg);\n  }\n\n  const bool ok_init = nullptr == m_scratchSpace || nullptr == m_scratchFlags;\n  const bool ok_dev  = true;\n  if (ok_init && ok_dev) {\n    m_queue = q;\n    \/\/ guard pushing to all_queues\n    {\n      std::lock_guard<std::mutex> lock(mutex);\n      all_queues.push_back(&m_queue);\n    }\n    const sycl::device& d = m_queue->get_device();\n    std::cout << SYCL::SYCLDevice(d) << '\\n';\n\n    m_maxWorkgroupSize =\n        d.template get_info<sycl::info::device::max_work_group_size>();\n    \/\/ FIXME_SYCL this should give the correct value for NVIDIA GPUs\n    m_maxConcurrency =\n        m_maxWorkgroupSize * 2 *\n        d.template get_info<sycl::info::device::max_compute_units>();\n\n    \/\/ Setup concurent bitset for obtaining unique tokens from within an\n    \/\/ executing kernel.\n    {\n      const int32_t buffer_bound =\n          Kokkos::Impl::concurrent_bitset::buffer_bound(m_maxConcurrency);\n      using Record = Kokkos::Impl::SharedAllocationRecord<\n          Kokkos::Experimental::SYCLDeviceUSMSpace, void>;\n      Record* const r =\n          Record::allocate(Kokkos::Experimental::SYCLDeviceUSMSpace(),\n                           \"Kokkos::SYCL::InternalScratchBitset\",\n                           sizeof(uint32_t) * buffer_bound);\n      Record::increment(r);\n      m_scratchConcurrentBitset = reinterpret_cast<uint32_t*>(r->data());\n      auto event                = m_queue->memset(m_scratchConcurrentBitset, 0,\n                                   sizeof(uint32_t) * buffer_bound);\n      fence(event);\n    }\n\n    m_maxShmemPerBlock =\n        d.template get_info<sycl::info::device::local_mem_size>();\n    m_indirectKernelMem.reset(*m_queue);\n    m_indirectReducerMem.reset(*m_queue);\n  } else {\n    std::ostringstream msg;\n    msg << \"Kokkos::Experimental::SYCL::initialize(...) FAILED\";\n\n    if (!ok_init) {\n      msg << \" : Already initialized\";\n    }\n    Kokkos::Impl::throw_runtime_exception(msg.str());\n  }\n}\n\nvoid SYCLInternal::finalize() {\n  SYCL().fence();\n  was_finalized = true;\n  if (nullptr != m_scratchSpace || nullptr != m_scratchFlags) {\n    \/\/ FIXME_SYCL\n    std::abort();\n  }\n\n  using RecordSYCL =\n      Kokkos::Impl::SharedAllocationRecord<Experimental::SYCLDeviceUSMSpace>;\n  RecordSYCL::decrement(RecordSYCL::get_record(m_scratchConcurrentBitset));\n  m_scratchConcurrentBitset = nullptr;\n\n  m_indirectKernelMem.reset();\n  m_indirectReducerMem.reset();\n  \/\/ guard erasing from all_queues\n  {\n    std::lock_guard<std::mutex> lock(mutex);\n    all_queues.erase(std::find(all_queues.begin(), all_queues.end(), &m_queue));\n  }\n  m_queue.reset();\n}\n\n}  \/\/ namespace Impl\n}  \/\/ namespace Experimental\n}  \/\/ namespace Kokkos\n","lang_cluster":"C++","length":192,"code_uid":"66db9c8d37904221a34e18f98dde0b63"}
{"diff_hunk":"@@ -91,8 +91,21 @@ void StorageHttpIngestHandler::onError(ProxygenError error) noexcept {\n                << proxygen::getErrorString(error);\n }\n \n-bool StorageHttpIngestHandler::ingestSSTFiles(GraphSpaceID space) {\n-    auto code = kvstore_->ingest(space);\n+bool StorageHttpIngestHandler::ingestSSTFiles() {\n+    kvstore::ResultCode code;\n+    if (edge_.has_value()) {\n+        LOG(INFO) << folly::stringPrintf(\n+            \"ingest space %d edge %d\", spaceID_, edge_.value());\n+        code = kvstore_->ingestEdge(spaceID_, edge_.value());\n+    } else if (tag_.has_value()) {\n+        LOG(INFO) << folly::stringPrintf(\n+            \"ingest space %d tag %d\", spaceID_, tag_.value());\n+        code = kvstore_->ingestTag(spaceID_, tag_.value());\n+    } else {\n+        LOG(INFO) << folly::stringPrintf(\n+            \"ingest space %d\", spaceID_);\n+        code = kvstore_->ingest(spaceID_);\n+    }\n     if (code == kvstore::ResultCode::SUCCEEDED) {\n         return true;\n     } else {","old_code":"\/* Copyright (c) 2019 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"storage\/http\/StorageHttpIngestHandler.h\"\n#include <proxygen\/httpserver\/RequestHandler.h>\n#include <proxygen\/lib\/http\/ProxygenErrorEnum.h>\n#include <proxygen\/httpserver\/ResponseBuilder.h>\n\nnamespace nebula {\nnamespace storage {\n\nusing proxygen::HTTPMessage;\nusing proxygen::HTTPMethod;\nusing proxygen::ProxygenError;\nusing proxygen::UpgradeProtocol;\nusing proxygen::ResponseBuilder;\n\nvoid StorageHttpIngestHandler::init(nebula::kvstore::KVStore *kvstore) {\n    kvstore_ = kvstore;\n    CHECK_NOTNULL(kvstore_);\n}\n\nvoid StorageHttpIngestHandler::onRequest(std::unique_ptr<HTTPMessage> headers) noexcept {\n    if (headers->getMethod().value() != HTTPMethod::GET) {\n        \/\/ Unsupported method\n        err_ = HttpCode::E_UNSUPPORTED_METHOD;\n        return;\n    }\n\n    if (!headers->hasQueryParam(\"space\")) {\n        err_ = HttpCode::E_ILLEGAL_ARGUMENT;\n        return;\n    }\n\n    space_ = headers->getIntQueryParam(\"space\");\n}\n\nvoid StorageHttpIngestHandler::onBody(std::unique_ptr<folly::IOBuf>) noexcept {\n    \/\/ Do nothing, we only support GET\n}\n\nvoid StorageHttpIngestHandler::onEOM() noexcept {\n    switch (err_) {\n        case HttpCode::E_UNSUPPORTED_METHOD:\n            ResponseBuilder(downstream_)\n                .status(WebServiceUtils::to(HttpStatusCode::METHOD_NOT_ALLOWED),\n                        WebServiceUtils::toString(HttpStatusCode::METHOD_NOT_ALLOWED))\n                .sendWithEOM();\n            return;\n        case HttpCode::E_ILLEGAL_ARGUMENT:\n            ResponseBuilder(downstream_)\n                .status(WebServiceUtils::to(HttpStatusCode::BAD_REQUEST),\n                        WebServiceUtils::toString(HttpStatusCode::BAD_REQUEST))\n                .sendWithEOM();\n            return;\n        default:\n            break;\n    }\n\n    if (ingestSSTFiles(space_)) {\n        LOG(ERROR) << \"SSTFile ingest successfully \";\n        ResponseBuilder(downstream_)\n            .status(WebServiceUtils::to(HttpStatusCode::OK),\n                    WebServiceUtils::toString(HttpStatusCode::OK))\n            .body(\"SSTFile ingest successfully\")\n            .sendWithEOM();\n    } else {\n        LOG(ERROR) << \"SSTFile ingest failed\";\n        ResponseBuilder(downstream_)\n            .status(WebServiceUtils::to(HttpStatusCode::FORBIDDEN),\n                    WebServiceUtils::toString(HttpStatusCode::FORBIDDEN))\n            .body(\"SSTFile ingest failed\")\n            .sendWithEOM();\n    }\n}\n\nvoid StorageHttpIngestHandler::onUpgrade(UpgradeProtocol) noexcept {\n    \/\/ Do nothing\n}\n\n\nvoid StorageHttpIngestHandler::requestComplete() noexcept {\n    delete this;\n}\n\nvoid StorageHttpIngestHandler::onError(ProxygenError error) noexcept {\n    LOG(ERROR) << \"Web Service MetaHttpIngestHandler Failed: \"\n               << proxygen::getErrorString(error);\n}\n\nbool StorageHttpIngestHandler::ingestSSTFiles(GraphSpaceID space) {\n    auto code = kvstore_->ingest(space);\n    if (code == kvstore::ResultCode::SUCCEEDED) {\n        return true;\n    } else {\n        LOG(ERROR) << \"SSTFile Ingest Failed: \" << code;\n        return false;\n    }\n}\n\n}  \/\/ namespace storage\n}  \/\/ namespace nebula\n","lang_cluster":"C++","length":105,"code_uid":"f1fca3f771974f448425f608cca69b22"}
{"diff_hunk":"@@ -92,7 +92,7 @@ StatusOr<std::vector<cpp2::HostItem>> ListHostsProcessor::allHostsWithStatus(\n         if (kvRet != kvstore::ResultCode::SUCCEEDED) {\n             LOG(ERROR) << \"List Hosts Failed: No partitions\";\n             resp_.set_code(cpp2::ErrorCode::E_NOT_FOUND);\n-            return Status::Error(\"Cant't find any partitions\");\n+            return Status::Error(\"Can't find any partitions\");\n         }\n         while (iter->valid()) {\n             PartitionID partId = MetaServiceUtils::parsePartKeyPartId(iter->key());","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"meta\/processors\/partsMan\/ListHostsProcessor.h\"\n#include \"meta\/ActiveHostsMan.h\"\n#include \"meta\/processors\/admin\/AdminClient.h\"\n\nDECLARE_int32(expired_threshold_sec);\nDEFINE_int32(removed_threshold_sec, 24 * 60 * 60,\n                     \"Hosts will be removed in this time if no heartbeat received\");\n\nnamespace nebula {\nnamespace meta {\n\nvoid ListHostsProcessor::process(const cpp2::ListHostsReq& req) {\n    UNUSED(req);\n    std::unordered_map<GraphSpaceID, std::string> spaceIdNameMap;\n    std::vector<cpp2::HostItem> hostItems;\n    {\n        folly::SharedMutex::ReadHolder rHolder(LockUtils::spaceLock());\n        auto status = allHostsWithStatus(spaceIdNameMap);\n        if (!status.ok()) {\n            onFinished();\n            return;\n        }\n        hostItems = std::move(status.value());\n    }\n    getLeaderDist(hostItems, spaceIdNameMap);\n    resp_.set_hosts(std::move(hostItems));\n    onFinished();\n}\n\nStatusOr<std::vector<cpp2::HostItem>> ListHostsProcessor::allHostsWithStatus(\n                                    std::unordered_map<GraphSpaceID, std::string>& spaceIdNameMap) {\n    std::vector<cpp2::HostItem> hostItems;\n\n    const auto& hostPrefix = MetaServiceUtils::hostPrefix();\n    std::unique_ptr<kvstore::KVIterator> iter;\n    auto kvRet = kvstore_->prefix(kDefaultSpaceId, kDefaultPartId, hostPrefix, &iter);\n    if (kvRet != kvstore::ResultCode::SUCCEEDED) {\n        LOG(ERROR) << \"List Hosts Failed: No hosts\";\n        resp_.set_code(cpp2::ErrorCode::E_NO_HOSTS);\n        return Status::Error(\"Can't access kvstore, ret = %d\", static_cast<int32_t>(kvRet));\n    }\n\n    auto now = time::WallClock::fastNowInMilliSec();\n    std::vector<std::string> removeHostsKey;\n    while (iter->valid()) {\n        cpp2::HostItem item;\n        auto host = MetaServiceUtils::parseHostKey(iter->key());\n        item.set_hostAddr(std::move(host));\n        HostInfo info = HostInfo::decode(iter->val());\n        if (now - info.lastHBTimeInMilliSec_ < FLAGS_removed_threshold_sec * 1000) {\n            if (now - info.lastHBTimeInMilliSec_ < FLAGS_expired_threshold_sec * 1000) {\n                item.set_status(cpp2::HostStatus::ONLINE);\n            } else {\n                item.set_status(cpp2::HostStatus::OFFLINE);\n            }\n            hostItems.emplace_back(item);\n        } else {\n            removeHostsKey.emplace_back(iter->key());\n        }\n        iter->next();\n    }\n\n    \/\/ Get all spaces\n    std::vector<GraphSpaceID> spaces;\n    const auto& spacePrefix = MetaServiceUtils::spacePrefix();\n    kvRet = kvstore_->prefix(kDefaultSpaceId, kDefaultPartId, spacePrefix, &iter);\n    if (kvRet != kvstore::ResultCode::SUCCEEDED) {\n        return hostItems;\n    }\n    while (iter->valid()) {\n        auto spaceId = MetaServiceUtils::spaceId(iter->key());\n        spaces.emplace_back(spaceId);\n        spaceIdNameMap.emplace(spaceId, MetaServiceUtils::spaceName(iter->val()));\n        iter->next();\n    }\n\n    std::unordered_map<HostAddr,\n                       std::unordered_map<std::string, std::vector<PartitionID>>> allParts;\n    for (const auto& spaceId : spaces) {\n        \/\/ get space name by space id\n        auto spaceName = spaceIdNameMap[spaceId];\n\n        std::unordered_map<HostAddr, std::vector<PartitionID>> hostParts;\n        const auto& partPrefix = MetaServiceUtils::partPrefix(spaceId);\n        kvRet = kvstore_->prefix(kDefaultSpaceId, kDefaultPartId, partPrefix, &iter);\n        if (kvRet != kvstore::ResultCode::SUCCEEDED) {\n            LOG(ERROR) << \"List Hosts Failed: No partitions\";\n            resp_.set_code(cpp2::ErrorCode::E_NOT_FOUND);\n            return Status::Error(\"Cant't find any partitions\");\n        }\n        while (iter->valid()) {\n            PartitionID partId = MetaServiceUtils::parsePartKeyPartId(iter->key());\n            auto partHosts = MetaServiceUtils::parsePartVal(iter->val());\n            for (auto& host : partHosts) {\n                hostParts[HostAddr(host.ip, host.port)].emplace_back(partId);\n            }\n            iter->next();\n        }\n\n        for (const auto& hostEntry : hostParts) {\n            allParts[hostEntry.first][spaceName] = std::move(hostEntry.second);\n        }\n    }\n\n    for (const auto& hostEntry : allParts) {\n        auto hostAddr = toThriftHost(hostEntry.first);\n        auto it = std::find_if(hostItems.begin(), hostItems.end(), [&](const auto& item) {\n            return item.get_hostAddr() == hostAddr;\n        });\n        if (it != hostItems.end()) {\n            \/\/ set default leader parts of all space to empty\n            std::unordered_map<std::string, std::vector<PartitionID>> leaderParts;\n            for (auto& spaceEntry : hostEntry.second) {\n                leaderParts[spaceEntry.first] = {};\n            }\n            it->set_leader_parts(std::move(leaderParts));\n            it->set_all_parts(std::move(hostEntry.second));\n        }\n    }\n\n    \/\/ Remove hosts that long time at OFFLINE status\n    if (!removeHostsKey.empty()) {\n        kvstore_->asyncMultiRemove(kDefaultSpaceId,\n                                   kDefaultPartId,\n                                   std::move(removeHostsKey),\n                                   [] (kvstore::ResultCode code) {\n                if (code != kvstore::ResultCode::SUCCEEDED) {\n                    LOG(ERROR) << \"Async remove long time offline hosts failed: \" << code;\n                }\n            });\n    }\n    return hostItems;\n}\n\nvoid ListHostsProcessor::getLeaderDist(\n                                std::vector<cpp2::HostItem>& hostItems,\n                                std::unordered_map<GraphSpaceID, std::string>& spaceIdNameMap) {\n    if (adminClient_ == nullptr) {\n        return;\n    }\n    HostLeaderMap hostLeaderMap;\n    auto ret = adminClient_->getLeaderDist(&hostLeaderMap).get();\n    if (!ret.ok()) {\n        LOG(ERROR) << \"Get leader distribution failed\";\n        return;\n    }\n    for (auto& hostEntry : hostLeaderMap) {\n        auto hostAddr = toThriftHost(hostEntry.first);\n        auto it = std::find_if(hostItems.begin(), hostItems.end(), [&](const auto& item) {\n            return item.get_hostAddr() == hostAddr;\n        });\n\n        if (it != hostItems.end()) {\n            for (auto& leaderEntry : hostEntry.second) {\n                \/\/ get space name by space id\n                auto spaceId = leaderEntry.first;\n                auto spaceIter = spaceIdNameMap.find(spaceId);\n                if (spaceIter == spaceIdNameMap.end()) {\n                    continue;\n                }\n                auto spaceName = spaceIter->second;\n\n                it->leader_parts[spaceName] = std::move(leaderEntry.second);\n            }\n        }\n    }\n}\n\n}  \/\/ namespace meta\n}  \/\/ namespace nebula\n\n","lang_cluster":"C++","length":177,"code_uid":"adeabce530384ffab75f275cd7ae0323"}
{"diff_hunk":"@@ -61,6 +61,10 @@ double convertInt64ToDouble(const Int64 &src)\n   return (double) src;\n }\n \n+double convertUInt64ToDouble(const UInt64 &src)\n+{\n+  return (double) src;\n+}\n \n Int64 uint32ArrayToInt64(const UInt32 array[2])\n {","old_code":"\/**********************************************************************\n\/\/ @@@ START COPYRIGHT @@@\n\/\/\n\/\/ Licensed to the Apache Software Foundation (ASF) under one\n\/\/ or more contributor license agreements.  See the NOTICE file\n\/\/ distributed with this work for additional information\n\/\/ regarding copyright ownership.  The ASF licenses this file\n\/\/ to you under the Apache License, Version 2.0 (the\n\/\/ \"License\"); you may not use this file except in compliance\n\/\/ with the License.  You may obtain a copy of the License at\n\/\/\n\/\/   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing,\n\/\/ software distributed under the License is distributed on an\n\/\/ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\/\/ KIND, either express or implied.  See the License for the\n\/\/ specific language governing permissions and limitations\n\/\/ under the License.\n\/\/\n\/\/ @@@ END COPYRIGHT @@@\n**********************************************************************\/\n\/* -*-C++-*-\n**************************************************************************\n*\n* File:         Int64.C\n* Description:  64-bit integer\n* Created:      3\/5\/96\n* Language:     C++\n*\n*\n*\n*\n**************************************************************************\n*\/\n\n\n#include \"Int64.h\"\n#include \"NABoolean.h\"\n#include \"str.h\"\n#include \"NAStdlib.h\"\n\nInt64 uint32ToInt64(UInt32 value)\n{\n  return (Int64) value;\n}\n\nInt32 int64ToInt32(Int64 value)\n{\n  UInt32 val32u;\n  Int32 val32;\n\n  val32u = (UInt32) value;\n  val32 = (Int32)val32u;\n\n  return val32;\n}\n\ndouble convertInt64ToDouble(const Int64 &src)\n{\n  return (double) src;\n}\n\n\nInt64 uint32ArrayToInt64(const UInt32 array[2])\n{\n  Int64 result = uint32ToInt64(array[0]);\n  Int64 array1 = uint32ToInt64(array[1]);\n  Int64 shift = INT_MAX;\t\/\/ 2^31 - 1\n  shift  += 1;\t\t\t\/\/ 2^31\n  result *= shift;\n  result *= 2;\t\t\t\/\/ 2*32, so result now has array[0] in high word\n  result += array1;\t\t\/\/ and array[1] in low word\n  return result;\n}\n\nInt32 aToInt32(const char* src)\n{\n  NABoolean isNeg = FALSE;\n  if (*src == '-')\n  {\n    isNeg = TRUE;\n    src++;\n  }\n\n  Int32 tgt = 0;\n  while ((*src >= '0') && (*src <= '9')) {\n    tgt = tgt * 10 + (*src - '0');\n    src++;\n  }\n  \n  if (isNeg)\n    return -tgt;\n  else\n    return tgt;\n}\n\nInt64 atoInt64(const char* src)\n{\n  NABoolean isNeg = FALSE;\n  if (*src == '-')\n  {\n    isNeg = TRUE;\n    src++;\n  }\n\n  Int64 tgt = 0;\n  while ((*src >= '0') && (*src <= '9')) {\n    tgt = tgt * 10 + (*src - '0');\n    src++;\n  }\n  \n  if (isNeg)\n    return -tgt;\n  else\n    return tgt;\n\n \n}\n\nvoid convertInt64ToAscii(const Int64 &src, char* tgt)\n{\n  Int64 temp = src;  \/\/ (src >= 0) ? src : - src;\n  char buffer[21];\n  char *s = &buffer[21];\n  *--s = '\\0';\n  do {\n    char c = (char) (temp % 10);\n    if (c < 0)\n      c = -c;\n    *--s = (char)(c + '0');\n    temp \/= 10;\n  } while (temp != 0);\n  if (src < 0)\n    *--s = '-';\n  strcpy(tgt, s);\n}\n\nvoid convertInt64ToUInt32Array(const Int64 &src, UInt32 *tgt)\n{\n  Lng32 *tPtr = (Lng32 *) &src;\n#ifdef NA_LITTLE_ENDIAN\n  tgt[0] = tPtr[1];\n  tgt[1] = tPtr[0];\n#else\n  tgt[0] = tPtr[0];\n  tgt[1] = tPtr[1];\n#endif\n}\n\n\/\/\n\/\/ End of File\n\/\/\n","lang_cluster":"C++","length":153,"code_uid":"76d9ec1042dc4836ba524d583a974734"}
{"diff_hunk":"@@ -114,10 +114,10 @@ template <typename Dtype>\n void im2col_nd_cpu(const Dtype* data_im, const int num_spatial_axes,\n     const int* im_shape, const int* col_shape,\n     const int* kernel_shape, const int* pad, const int* stride,\n-    Dtype* data_col) {\n+    const int* dilation, Dtype* data_col) {\n   const bool kIm2Col = true;\n   im2col_nd_core_cpu(data_im, kIm2Col, num_spatial_axes, im_shape, col_shape,\n-                  kernel_shape, pad, stride, data_col);\n+                  kernel_shape, pad, stride, dilation, data_col);\n }\n \n \/\/ Explicit instantiation","old_code":"#include <vector>\n\n#include \"caffe\/util\/im2col.hpp\"\n#include \"caffe\/util\/math_functions.hpp\"\n\nnamespace caffe {\n\ntemplate <typename Dtype>\nvoid im2col_cpu(const Dtype* data_im, const int channels,\n    const int height, const int width, const int kernel_h, const int kernel_w,\n    const int pad_h, const int pad_w,\n    const int stride_h, const int stride_w,\n    Dtype* data_col) {\n  const int height_col = (height + 2 * pad_h - kernel_h) \/ stride_h + 1;\n  const int width_col = (width + 2 * pad_w - kernel_w) \/ stride_w + 1;\n  const int channels_col = channels * kernel_h * kernel_w;\n  for (int c_col = 0; c_col < channels_col; ++c_col) {\n    int w_offset = c_col % kernel_w;\n    int h_offset = (c_col \/ kernel_w) % kernel_h;\n    int c_im = c_col \/ kernel_h \/ kernel_w;\n    for (int h_col = 0; h_col < height_col; ++h_col) {\n      for (int w_col = 0; w_col < width_col; ++w_col) {\n        int h_im = h_col * stride_h - pad_h + h_offset;\n        int w_im = w_col * stride_w - pad_w + w_offset;\n        data_col[(c_col * height_col + h_col) * width_col + w_col] =\n            (h_im >= 0 && w_im >= 0 && h_im < height && w_im < width) ?\n            data_im[(c_im * height + h_im) * width + w_im] : 0;\n      }\n    }\n  }\n}\n\n\/\/ Explicit instantiation\ntemplate void im2col_cpu<float>(const float* data_im, const int channels,\n    const int height, const int width, const int kernel_h, const int kernel_w,\n    const int pad_h, const int pad_w, const int stride_h,\n    const int stride_w, float* data_col);\ntemplate void im2col_cpu<double>(const double* data_im, const int channels,\n    const int height, const int width, const int kernel_h, const int kernel_w,\n    const int pad_h, const int pad_w, const int stride_h,\n    const int stride_w, double* data_col);\n\ntemplate <typename Dtype>\ninline void im2col_nd_core_cpu(const Dtype* data_input, const bool im2col,\n    const int num_spatial_axes, const int* im_shape, const int* col_shape,\n    const int* kernel_shape, const int* pad, const int* stride,\n    Dtype* data_output) {\n  if (!im2col) {\n    int im_size = im_shape[0];\n    for (int i = 0; i < num_spatial_axes; ++i) {\n      im_size *= im_shape[1 + i];\n    }\n    caffe_set(im_size, Dtype(0), data_output);\n  }\n  int kernel_size = 1;\n  for (int i = 0; i < num_spatial_axes; ++i) {\n    kernel_size *= kernel_shape[i];\n  }\n  const int channels_col = col_shape[0];\n  vector<int> d_offset(num_spatial_axes, 0);\n  vector<int> d_iter(num_spatial_axes, 0);\n  for (int c_col = 0; c_col < channels_col; ++c_col) {\n    \/\/ Loop over spatial axes in reverse order to compute a per-axis offset.\n    int offset = c_col;\n    for (int d_i = num_spatial_axes - 1; d_i >= 0; --d_i) {\n      if (d_i < num_spatial_axes - 1) {\n        offset \/= kernel_shape[d_i + 1];\n      }\n      d_offset[d_i] = offset % kernel_shape[d_i];\n    }\n    for (bool incremented = true; incremented; ) {\n      \/\/ Loop over spatial axes in forward order to compute the indices in the\n      \/\/ image and column, and whether the index lies in the padding.\n      int index_col = c_col;\n      int index_im = c_col \/ kernel_size;\n      bool is_padding = false;\n      for (int d_i = 0; d_i < num_spatial_axes; ++d_i) {\n        const int d = d_iter[d_i];\n        const int d_im = d * stride[d_i] - pad[d_i] + d_offset[d_i];\n        is_padding |= d_im < 0 || d_im >= im_shape[d_i + 1];\n        index_col *= col_shape[d_i + 1];\n        index_col += d;\n        index_im *= im_shape[d_i + 1];\n        index_im += d_im;\n      }\n      if (im2col) {\n        if (is_padding) {\n          data_output[index_col] = 0;\n        } else {\n          data_output[index_col] = data_input[index_im];\n        }\n      } else if (!is_padding) {  \/\/ col2im\n        data_output[index_im] += data_input[index_col];\n      }\n      \/\/ Loop over spatial axes in reverse order to choose an index,\n      \/\/ like counting.\n      incremented = false;\n      for (int d_i = num_spatial_axes - 1; d_i >= 0; --d_i) {\n        const int d_max = col_shape[d_i + 1];\n        DCHECK_LT(d_iter[d_i], d_max);\n        if (d_iter[d_i] == d_max - 1) {\n          d_iter[d_i] = 0;\n        } else {  \/\/ d_iter[d_i] < d_max - 1\n          ++d_iter[d_i];\n          incremented = true;\n          break;\n        }\n      }\n    }  \/\/ while(incremented) {\n  }  \/\/ for (int c = 0; c < channels_col; ++c) {\n}\n\ntemplate <typename Dtype>\nvoid im2col_nd_cpu(const Dtype* data_im, const int num_spatial_axes,\n    const int* im_shape, const int* col_shape,\n    const int* kernel_shape, const int* pad, const int* stride,\n    Dtype* data_col) {\n  const bool kIm2Col = true;\n  im2col_nd_core_cpu(data_im, kIm2Col, num_spatial_axes, im_shape, col_shape,\n                  kernel_shape, pad, stride, data_col);\n}\n\n\/\/ Explicit instantiation\ntemplate void im2col_nd_cpu<float>(const float* data_im,\n    const int num_spatial_axes,\n    const int* im_shape, const int* col_shape,\n    const int* kernel_shape, const int* pad, const int* stride,\n    float* data_col);\ntemplate void im2col_nd_cpu<double>(const double* data_im,\n    const int num_spatial_axes,\n    const int* im_shape, const int* col_shape,\n    const int* kernel_shape, const int* pad, const int* stride,\n    double* data_col);\n\ntemplate <typename Dtype>\nvoid col2im_cpu(const Dtype* data_col, const int channels,\n    const int height, const int width, const int kernel_h, const int kernel_w,\n    const int pad_h, const int pad_w,\n    const int stride_h, const int stride_w,\n    Dtype* data_im) {\n  caffe_set(height * width * channels, Dtype(0), data_im);\n  const int height_col = (height + 2 * pad_h - kernel_h) \/ stride_h + 1;\n  const int width_col = (width + 2 * pad_w - kernel_w) \/ stride_w + 1;\n  const int channels_col = channels * kernel_h * kernel_w;\n  for (int c_col = 0; c_col < channels_col; ++c_col) {\n    int w_offset = c_col % kernel_w;\n    int h_offset = (c_col \/ kernel_w) % kernel_h;\n    int c_im = c_col \/ kernel_h \/ kernel_w;\n    for (int h_col = 0; h_col < height_col; ++h_col) {\n      for (int w_col = 0; w_col < width_col; ++w_col) {\n        int h_im = h_col * stride_h - pad_h + h_offset;\n        int w_im = w_col * stride_w - pad_w + w_offset;\n        if (h_im >= 0 && h_im < height && w_im >= 0 && w_im < width)\n          data_im[(c_im * height + h_im) * width + w_im] +=\n              data_col[(c_col * height_col + h_col) * width_col + w_col];\n      }\n    }\n  }\n}\n\n\/\/ Explicit instantiation\ntemplate void col2im_cpu<float>(const float* data_col, const int channels,\n    const int height, const int width, const int kernel_h, const int kernel_w,\n    const int pad_h, const int pad_w, const int stride_h,\n    const int stride_w, float* data_im);\ntemplate void col2im_cpu<double>(const double* data_col, const int channels,\n    const int height, const int width, const int kernel_h, const int kernel_w,\n    const int pad_h, const int pad_w, const int stride_h,\n    const int stride_w, double* data_im);\n\ntemplate <typename Dtype>\nvoid col2im_nd_cpu(const Dtype* data_col, const int num_spatial_axes,\n    const int* im_shape, const int* col_shape,\n    const int* kernel_shape, const int* pad, const int* stride,\n    Dtype* data_im) {\n  const bool kIm2Col = false;\n  im2col_nd_core_cpu(data_col, kIm2Col, num_spatial_axes, im_shape, col_shape,\n                     kernel_shape, pad, stride, data_im);\n}\n\n\/\/ Explicit instantiation\ntemplate void col2im_nd_cpu<float>(const float* data_col,\n    const int num_spatial_axes,\n    const int* im_shape, const int* col_shape,\n    const int* kernel_shape, const int* pad, const int* stride,\n    float* data_im);\ntemplate void col2im_nd_cpu<double>(const double* data_col,\n    const int num_spatial_axes,\n    const int* im_shape, const int* col_shape,\n    const int* kernel_shape, const int* pad, const int* stride,\n    double* data_im);\n\n\n}  \/\/ namespace caffe\n","lang_cluster":"C++","length":194,"code_uid":"920e501b3d664d7a90f417a7f4c12ea7"}
{"diff_hunk":"@@ -104,7 +104,8 @@ Status TablePlugin::HandleRequest(const RoutingAlgorithmsInterface &algorithms,\n             {\n                 const auto &table_index = row * num_destinations + column;\n                 BOOST_ASSERT(table_index < result_tables_pair.first.size());\n-                if (result_tables_pair.first[table_index] == MAXIMAL_EDGE_DURATION)\n+                if (params.fallback_speed > 0 &&\n+                    result_tables_pair.first[table_index] == MAXIMAL_EDGE_DURATION)\n                 {\n                     const auto &source =\n                         snapped_phantoms[params.sources.empty() ? row : params.sources[row]];","old_code":"#include \"engine\/plugins\/table.hpp\"\n\n#include \"engine\/api\/table_api.hpp\"\n#include \"engine\/api\/table_parameters.hpp\"\n#include \"engine\/routing_algorithms\/many_to_many.hpp\"\n#include \"engine\/search_engine_data.hpp\"\n#include \"util\/coordinate_calculation.hpp\"\n#include \"util\/json_container.hpp\"\n#include \"util\/string_util.hpp\"\n\n#include <cstdlib>\n\n#include <algorithm>\n#include <memory>\n#include <string>\n#include <vector>\n\n#include <boost\/assert.hpp>\n\nnamespace osrm\n{\nnamespace engine\n{\nnamespace plugins\n{\n\nTablePlugin::TablePlugin(const int max_locations_distance_table)\n    : max_locations_distance_table(max_locations_distance_table)\n{\n}\n\nStatus TablePlugin::HandleRequest(const RoutingAlgorithmsInterface &algorithms,\n                                  const api::TableParameters &params,\n                                  util::json::Object &result) const\n{\n    if (!algorithms.HasManyToManySearch())\n    {\n        return Error(\"NotImplemented\",\n                     \"Many to many search is not implemented for the chosen search algorithm.\",\n                     result);\n    }\n\n    BOOST_ASSERT(params.IsValid());\n\n    if (!CheckAllCoordinates(params.coordinates))\n    {\n        return Error(\"InvalidOptions\", \"Coordinates are invalid\", result);\n    }\n\n    if (params.bearings.size() > 0 && params.coordinates.size() != params.bearings.size())\n    {\n        return Error(\n            \"InvalidOptions\", \"Number of bearings does not match number of coordinates\", result);\n    }\n\n    \/\/ Empty sources or destinations means the user wants all of them included, respectively\n    \/\/ The ManyToMany routing algorithm we dispatch to below already handles this perfectly.\n    const auto num_sources =\n        params.sources.empty() ? params.coordinates.size() : params.sources.size();\n    const auto num_destinations =\n        params.destinations.empty() ? params.coordinates.size() : params.destinations.size();\n\n    if (max_locations_distance_table > 0 &&\n        ((num_sources * num_destinations) >\n         static_cast<std::size_t>(max_locations_distance_table * max_locations_distance_table)))\n    {\n        return Error(\"TooBig\", \"Too many table coordinates\", result);\n    }\n\n    if (!CheckAlgorithms(params, algorithms, result))\n        return Status::Error;\n\n    const auto &facade = algorithms.GetFacade();\n    auto phantom_nodes = GetPhantomNodes(facade, params);\n\n    if (phantom_nodes.size() != params.coordinates.size())\n    {\n        return Error(\"NoSegment\",\n                     std::string(\"Could not find a matching segment for coordinate \") +\n                         std::to_string(phantom_nodes.size()),\n                     result);\n    }\n\n    auto snapped_phantoms = SnapPhantomNodes(phantom_nodes);\n\n    bool request_distance = params.annotations & api::TableParameters::AnnotationsType::Distance;\n    bool request_duration = params.annotations & api::TableParameters::AnnotationsType::Duration;\n\n    auto result_tables_pair = algorithms.ManyToManySearch(\n        snapped_phantoms, params.sources, params.destinations, request_distance);\n\n    if ((request_duration && result_tables_pair.first.empty()) ||\n        (request_distance && result_tables_pair.second.empty()))\n    {\n        return Error(\"NoTable\", \"No table found\", result);\n    }\n\n    \/\/ Scan table for null results - if any exist, replace with distance estimates\n    if (params.fallback_speed > 0)\n    {\n        for (std::size_t row = 0; row < num_sources; row++)\n        {\n            for (std::size_t column = 0; column < num_destinations; column++)\n            {\n                const auto &table_index = row * num_destinations + column;\n                BOOST_ASSERT(table_index < result_tables_pair.first.size());\n                if (result_tables_pair.first[table_index] == MAXIMAL_EDGE_DURATION)\n                {\n                    const auto &source =\n                        snapped_phantoms[params.sources.empty() ? row : params.sources[row]];\n                    const auto &destination =\n                        snapped_phantoms[params.destinations.empty() ? column\n                                                                     : params.destinations[column]];\n\n                    auto distance_estimate =\n                        params.fallback_coordinate_type ==\n                                api::TableParameters::FallbackCoordinateType::Input\n                            ? util::coordinate_calculation::fccApproximateDistance(\n                                  source.input_location, destination.input_location)\n                            : util::coordinate_calculation::fccApproximateDistance(\n                                  source.location, destination.location);\n\n                    result_tables_pair.first[table_index] =\n                        distance_estimate \/ (double)params.fallback_speed;\n                    if (!result_tables_pair.second.empty())\n                    {\n                        result_tables_pair.second[table_index] = distance_estimate;\n                    }\n                }\n            }\n        }\n    }\n\n    api::TableAPI table_api{facade, params};\n    table_api.MakeResponse(result_tables_pair, snapped_phantoms, result);\n\n    return Status::Ok;\n}\n}\n}\n}\n","lang_cluster":"C++","length":141,"code_uid":"e13676c29ecd4200931907d42dea111f"}
{"diff_hunk":"@@ -55,6 +55,23 @@ void AddListenerProcessor::process(const cpp2::AddListenerReq& req) {\n     data.emplace_back(MetaKeyUtils::listenerKey(space, parts[i], type),\n                       MetaKeyUtils::serializeHostAddr(hosts[i % hosts.size()]));\n   }\n+\n+  nebula::cpp2::ErrorCode code = nebula::cpp2::ErrorCode::SUCCEEDED;\n+  for (auto& host : hosts) {\n+    auto machineKey = MetaKeyUtils::machineKey(host.host, host.port);\n+    if (machineExist(machineKey) == nebula::cpp2::ErrorCode::SUCCEEDED) {\n+      LOG(ERROR) << \"The host \" << host << \" have existed!\";\n+      code = nebula::cpp2::ErrorCode::E_EXISTED;\n+      break;\n+    }\n+    data.emplace_back(machineKey, \"\");\n+  }\n+\n+  if (code != nebula::cpp2::ErrorCode::SUCCEEDED) {\n+    handleErrorCode(code);\n+    onFinished();\n+    return;\n+  }\n   doSyncPutAndUpdate(std::move(data));\n }\n ","old_code":"\/* Copyright (c) 2020 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License.\n *\/\n\n#include \"meta\/processors\/listener\/ListenerProcessor.h\"\n\n#include \"meta\/ActiveHostsMan.h\"\n\nDECLARE_int32(heartbeat_interval_secs);\nDECLARE_uint32(expired_time_factor);\n\nnamespace nebula {\nnamespace meta {\n\nvoid AddListenerProcessor::process(const cpp2::AddListenerReq& req) {\n  auto space = req.get_space_id();\n  CHECK_SPACE_ID_AND_RETURN(space);\n  auto type = req.get_type();\n  const auto& hosts = req.get_hosts();\n  auto ret = listenerExist(space, type);\n  if (ret != nebula::cpp2::ErrorCode::E_LISTENER_NOT_FOUND) {\n    if (ret == nebula::cpp2::ErrorCode::SUCCEEDED) {\n      LOG(ERROR) << \"Add listener failed, listener already exists.\";\n      ret = nebula::cpp2::ErrorCode::E_EXISTED;\n    } else {\n      LOG(ERROR) << \"Add listener failed, error: \" << apache::thrift::util::enumNameSafe(ret);\n    }\n    handleErrorCode(ret);\n    onFinished();\n    return;\n  }\n\n  \/\/ TODO : (sky) if type is elasticsearch, need check text search service.\n  folly::SharedMutex::WriteHolder wHolder(LockUtils::listenerLock());\n  folly::SharedMutex::ReadHolder rHolder(LockUtils::spaceLock());\n  const auto& prefix = MetaKeyUtils::partPrefix(space);\n  auto iterRet = doPrefix(prefix);\n  if (!nebula::ok(iterRet)) {\n    auto retCode = nebula::error(iterRet);\n    LOG(ERROR) << \"List parts failed, error: \" << apache::thrift::util::enumNameSafe(retCode);\n    handleErrorCode(retCode);\n    onFinished();\n    return;\n  }\n\n  std::vector<PartitionID> parts;\n  auto iter = nebula::value(iterRet).get();\n  while (iter->valid()) {\n    parts.emplace_back(MetaKeyUtils::parsePartKeyPartId(iter->key()));\n    iter->next();\n  }\n  std::vector<kvstore::KV> data;\n  for (size_t i = 0; i < parts.size(); i++) {\n    data.emplace_back(MetaKeyUtils::listenerKey(space, parts[i], type),\n                      MetaKeyUtils::serializeHostAddr(hosts[i % hosts.size()]));\n  }\n  doSyncPutAndUpdate(std::move(data));\n}\n\nvoid RemoveListenerProcessor::process(const cpp2::RemoveListenerReq& req) {\n  auto space = req.get_space_id();\n  CHECK_SPACE_ID_AND_RETURN(space);\n  auto type = req.get_type();\n  auto ret = listenerExist(space, type);\n  if (ret != nebula::cpp2::ErrorCode::SUCCEEDED) {\n    if (ret == nebula::cpp2::ErrorCode::E_LISTENER_NOT_FOUND) {\n      LOG(ERROR) << \"Remove listener failed, listener not exists.\";\n    } else {\n      LOG(ERROR) << \"Remove listener failed, error: \" << apache::thrift::util::enumNameSafe(ret);\n    }\n    handleErrorCode(ret);\n    onFinished();\n    return;\n  }\n\n  folly::SharedMutex::WriteHolder wHolder(LockUtils::listenerLock());\n  std::vector<std::string> keys;\n  const auto& prefix = MetaKeyUtils::listenerPrefix(space, type);\n  auto iterRet = doPrefix(prefix);\n  if (!nebula::ok(iterRet)) {\n    auto retCode = nebula::error(iterRet);\n    LOG(ERROR) << \"Remove listener failed, error: \" << apache::thrift::util::enumNameSafe(retCode);\n    handleErrorCode(retCode);\n    onFinished();\n    return;\n  }\n\n  auto iter = nebula::value(iterRet).get();\n  while (iter->valid()) {\n    keys.emplace_back(iter->key());\n    iter->next();\n  }\n  doSyncMultiRemoveAndUpdate(std::move(keys));\n}\n\nvoid ListListenerProcessor::process(const cpp2::ListListenerReq& req) {\n  auto space = req.get_space_id();\n  CHECK_SPACE_ID_AND_RETURN(space);\n  folly::SharedMutex::ReadHolder rHolder(LockUtils::listenerLock());\n  const auto& prefix = MetaKeyUtils::listenerPrefix(space);\n  auto iterRet = doPrefix(prefix);\n  if (!nebula::ok(iterRet)) {\n    auto retCode = nebula::error(iterRet);\n    LOG(ERROR) << \"List listener failed, error: \" << apache::thrift::util::enumNameSafe(retCode);\n    handleErrorCode(retCode);\n    onFinished();\n    return;\n  }\n\n  auto activeHostsRet =\n      ActiveHostsMan::getActiveHosts(kvstore_,\n                                     FLAGS_heartbeat_interval_secs * FLAGS_expired_time_factor,\n                                     cpp2::HostRole::LISTENER);\n  if (!nebula::ok(activeHostsRet)) {\n    handleErrorCode(nebula::error(activeHostsRet));\n    onFinished();\n    return;\n  }\n\n  std::vector<nebula::meta::cpp2::ListenerInfo> listeners;\n  auto activeHosts = std::move(nebula::value(activeHostsRet));\n  auto iter = nebula::value(iterRet).get();\n  while (iter->valid()) {\n    cpp2::ListenerInfo listener;\n    listener.set_type(MetaKeyUtils::parseListenerType(iter->key()));\n    listener.set_host(MetaKeyUtils::deserializeHostAddr(iter->val()));\n    listener.set_part_id(MetaKeyUtils::parseListenerPart(iter->key()));\n    if (std::find(activeHosts.begin(), activeHosts.end(), *listener.host_ref()) !=\n        activeHosts.end()) {\n      listener.set_status(cpp2::HostStatus::ONLINE);\n    } else {\n      listener.set_status(cpp2::HostStatus::OFFLINE);\n    }\n    listeners.emplace_back(std::move(listener));\n    iter->next();\n  }\n  resp_.set_listeners(std::move(listeners));\n  handleErrorCode(nebula::cpp2::ErrorCode::SUCCEEDED);\n  onFinished();\n}\n\n}  \/\/ namespace meta\n}  \/\/ namespace nebula\n","lang_cluster":"C++","length":144,"code_uid":"1826de56fc6f49f0b89a64ee44e96350"}
{"diff_hunk":"@@ -65,8 +65,7 @@ def EStateIndices(mol, force=True):\n         tmp = (Is[i] - Is[j]) \/ (p * p)\n         accum[i] += tmp\n         accum[j] -= tmp\n-\n-  res = accum + Is\n+  res = numpy.add(accum, Is, dtype='float')\n   mol._eStateIndices = res\n   return res\n ","old_code":"# $Id$\n#\n# Copyright (C) 2002-2006 greg Landrum and Rational Discovery LLC\n#\n#   @@ All Rights Reserved @@\n#  This file is part of the RDKit.\n#  The contents are covered by the terms of the BSD license\n#  which is included in the file license.txt, found at the root\n#  of the RDKit source tree.\n#\n\"\"\" Basic EState definitions\n\n\"\"\"\n\nimport numpy\nfrom rdkit import Chem\n\n\ndef GetPrincipleQuantumNumber(atNum):\n  \"\"\" Get principal quantum number for atom number \"\"\"\n  if atNum <= 2:\n    return 1\n  elif atNum <= 10:\n    return 2\n  elif atNum <= 18:\n    return 3\n  elif atNum <= 36:\n    return 4\n  elif atNum <= 54:\n    return 5\n  elif atNum <= 86:\n    return 6\n  else:\n    return 7\n\n\ndef EStateIndices(mol, force=True):\n  \"\"\" returns a tuple of EState indices for the molecule\n\n    Reference: Hall, Mohney and Kier. JCICS _31_ 76-81 (1991)\n\n  \"\"\"\n  if not force and hasattr(mol, '_eStateIndices'):\n    return mol._eStateIndices\n\n  tbl = Chem.GetPeriodicTable()\n  nAtoms = mol.GetNumAtoms()\n  Is = numpy.zeros(nAtoms, numpy.float)\n  for i in range(nAtoms):\n    at = mol.GetAtomWithIdx(i)\n    atNum = at.GetAtomicNum()\n    d = at.GetDegree()\n    if d > 0:\n      h = at.GetTotalNumHs()\n      dv = tbl.GetNOuterElecs(atNum) - h\n      N = GetPrincipleQuantumNumber(atNum)\n      Is[i] = (4. \/ (N * N) * dv + 1) \/ d\n  dists = Chem.GetDistanceMatrix(mol, useBO=0, useAtomWts=0)\n  dists += 1\n  accum = numpy.zeros(nAtoms, numpy.float)\n  for i in range(nAtoms):\n    for j in range(i + 1, nAtoms):\n      p = dists[i, j]\n      if p < 1e6:\n        tmp = (Is[i] - Is[j]) \/ (p * p)\n        accum[i] += tmp\n        accum[j] -= tmp\n\n  res = accum + Is\n  mol._eStateIndices = res\n  return res\n\n\nEStateIndices.version = '1.0.0'\n\n\ndef MaxEStateIndex(mol, force=1):\n  return max(EStateIndices(mol, force))\n\n\nMaxEStateIndex.version = \"1.0.0\"\n\n\ndef MinEStateIndex(mol, force=1):\n  return min(EStateIndices(mol, force))\n\n\nMinEStateIndex.version = \"1.0.0\"\n\n\ndef MaxAbsEStateIndex(mol, force=1):\n  return max([abs(x) for x in EStateIndices(mol, force)])\n\n\nMaxAbsEStateIndex.version = \"1.0.0\"\n\n\ndef MinAbsEStateIndex(mol, force=1):\n  return min([abs(x) for x in EStateIndices(mol, force)])\n\n\nMinAbsEStateIndex.version = \"1.0.0\"\n\n\ndef _exampleCode():\n  \"\"\" Example code for calculating E-state indices \"\"\"\n  smis = ['CCCC', 'CCCCC', 'CCCCCC', 'CC(N)C(=O)O', 'CC(N)C(=O)[O-].[Na+]']\n  for smi in smis:\n    m = Chem.MolFromSmiles(smi)\n    print(smi)\n    inds = EStateIndices(m)\n    print('\\t', inds)\n\n\nif __name__ == '__main__':  # pragma: nocover\n  _exampleCode()\n","lang_cluster":"C++","length":116,"code_uid":"9060cfa6987e4789b7e2cf1aab3c4f37"}
{"diff_hunk":"@@ -117,16 +117,18 @@ void ShowExecutor::showSpaces() {\n         onFinish_();\n     };\n \n-    auto error = [this] (auto &&e) {\n-        LOG(ERROR) << \"Exception caught: \" << e.what();\n-        DCHECK(onError_);\n-        onError_(Status::Error(\"Internal error\"));\n-        return;\n-    };\n-\n+    LOG_AND_PROCESS_ERROR();\n     std::move(future).via(runner).thenValue(cb).thenError(error);\n }\n \n+void ShowExecutor::showTags() {\n+    \/\/ TODO(darion) support show tags via MetaClient\n+}\n+\n+void ShowExecutor::showEdges() {\n+    \/\/ TODO(darion) support show edges via MetaClient\n+}\n+\n void ShowExecutor::setupResponse(cpp2::ExecutionResponse &resp) {\n     resp = std::move(*resp_);\n }","old_code":"\/* Copyright (c) 2018 - present, VE Software Inc. All rights reserved\n *\n * This source code is licensed under Apache 2.0 License\n *  (found in the LICENSE.Apache file in the root directory)\n *\/\n\n#include \"graph\/ShowExecutor.h\"\n#include \"network\/NetworkUtils.h\"\n\nnamespace nebula {\nnamespace graph {\n\nusing nebula::network::NetworkUtils;\n\nShowExecutor::ShowExecutor(Sentence *sentence,\n                           ExecutionContext *ectx) : Executor(ectx) {\n    sentence_ = static_cast<ShowSentence*>(sentence);\n}\n\n\nStatus ShowExecutor::prepare() {\n    return Status::OK();\n}\n\n\nvoid ShowExecutor::execute() {\n    auto showType = sentence_->showType();\n    switch (showType) {\n        case ShowSentence::ShowType::kShowHosts:\n            showHosts();\n            break;\n        case ShowSentence::ShowType::kShowSpaces:\n            showSpaces();\n            break;\n        case ShowSentence::ShowType::kUnknown:\n            onError_(Status::Error(\"Type unknown\"));\n            break;\n        \/\/ intentionally no `default'\n    }\n}\n\n\nvoid ShowExecutor::showHosts() {\n    auto future = ectx()->getMetaClient()->listHosts();\n    auto *runner = ectx()->rctx()->runner();\n\n    auto cb = [this] (auto &&resp) {\n        if (!resp.ok()) {\n            DCHECK(onError_);\n            onError_(std::move(resp).status());\n            return;\n        }\n\n        auto retShowHosts = std::move(resp).value();\n        std::vector<cpp2::RowValue> rows;\n        std::vector<std::string> header;\n        resp_ = std::make_unique<cpp2::ExecutionResponse>();\n\n        header.push_back(\"Ip\");\n        header.push_back(\"Port\");\n        resp_->set_column_names(std::move(header));\n\n        for (auto &host : retShowHosts) {\n            std::vector<cpp2::ColumnValue> row;\n            row.resize(2);\n            row[0].set_str(NetworkUtils::ipFromHostAddr(host));\n            row[1].set_str(folly::to<std::string>(NetworkUtils::portFromHostAddr(host)));\n            rows.emplace_back();\n            rows.back().set_columns(std::move(row));\n        }\n        resp_->set_rows(std::move(rows));\n\n        DCHECK(onFinish_);\n        onFinish_();\n    };\n\n    auto error = [this] (auto &&e) {\n        LOG(ERROR) << \"Exception caught: \" << e.what();\n        DCHECK(onError_);\n        onError_(Status::Error(\"Internal error\"));\n        return;\n    };\n\n    std::move(future).via(runner).thenValue(cb).thenError(error);\n}\n\n\nvoid ShowExecutor::showSpaces() {\n    auto future = ectx()->getMetaClient()->listSpaces();\n    auto *runner = ectx()->rctx()->runner();\n\n    auto cb = [this] (auto &&resp) {\n        if (!resp.ok()) {\n            DCHECK(onError_);\n            onError_(std::move(resp).status());\n            return;\n        }\n\n        auto retShowSpaces = std::move(resp).value();\n        std::vector<cpp2::RowValue> rows;\n        std::vector<std::string> header;\n        resp_ = std::make_unique<cpp2::ExecutionResponse>();\n\n        header.push_back(\"Name\");\n        resp_->set_column_names(std::move(header));\n\n        for (auto &space : retShowSpaces) {\n            std::vector<cpp2::ColumnValue> row;\n            row.emplace_back();\n            row.back().set_str(std::move(space.second));\n            rows.emplace_back();\n            rows.back().set_columns(std::move(row));\n        }\n        resp_->set_rows(std::move(rows));\n\n        DCHECK(onFinish_);\n        onFinish_();\n    };\n\n    auto error = [this] (auto &&e) {\n        LOG(ERROR) << \"Exception caught: \" << e.what();\n        DCHECK(onError_);\n        onError_(Status::Error(\"Internal error\"));\n        return;\n    };\n\n    std::move(future).via(runner).thenValue(cb).thenError(error);\n}\n\nvoid ShowExecutor::setupResponse(cpp2::ExecutionResponse &resp) {\n    resp = std::move(*resp_);\n}\n\n}   \/\/ namespace graph\n}   \/\/ namespace nebula\n","lang_cluster":"C++","length":135,"code_uid":"4f2c312b64654622a9ad37cd3c3e226e"}
{"diff_hunk":"@@ -19,6 +19,7 @@\n \n #include <fastdds\/rtps\/writer\/StatefulWriter.h>\n #include <fastdds\/rtps\/history\/WriterHistory.h>\n+#include <fastdds\/rtps\/history\/ReaderHistory.h>\n \n #include <fastdds\/dds\/log\/Log.hpp>\n ","old_code":"\/\/ Copyright 2020 Proyectos y Sistemas de Mantenimiento SL (eProsima).\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\n\/**\n * @file EDPServerListener2.cpp\n *\n *\/\n\n#include <fastdds\/rtps\/writer\/StatefulWriter.h>\n#include <fastdds\/rtps\/history\/WriterHistory.h>\n\n#include <fastdds\/dds\/log\/Log.hpp>\n\n#include \".\/EDPServerListeners2.hpp\"\n#include \".\/EDPServer2.hpp\"\n#include \"..\/participant\/PDPServer2.hpp\"\n\nnamespace eprosima {\nnamespace fastdds {\nnamespace rtps {\n\nusing namespace eprosima::fastrtps::rtps;\n\nEDPServerPUBListener2::EDPServerPUBListener2(\n        EDPServer2* sedp)\n    : EDPBasePUBListener(sedp->mp_RTPSParticipant->getAttributes().allocation.locators,\n            sedp->mp_RTPSParticipant->getAttributes().allocation.data_limits)\n    , sedp_(sedp)\n{\n}\n\nvoid EDPServerPUBListener2::onNewCacheChangeAdded(\n        RTPSReader* reader,\n        const CacheChange_t* const change_in)\n{\n    (void)reader;\n    (void)change_in;\n    \/\/ TODO DISCOVERY SERVER VERSION 2\n}\n\nvoid EDPServerPUBListener2::onWriterChangeReceivedByAll(\n        RTPSWriter* writer,\n        CacheChange_t* change)\n{\n    (void)writer;\n\n    if (ChangeKind_t::NOT_ALIVE_DISPOSED_UNREGISTERED == change->kind)\n    {\n        WriterHistory* writer_history =\n                sedp_->publications_writer_.second;\n\n        writer_history->remove_change(change);\n    }\n}\n\nEDPServerSUBListener2::EDPServerSUBListener2(\n        EDPServer2* sedp)\n    : EDPBaseSUBListener(sedp->mp_RTPSParticipant->getAttributes().allocation.locators,\n            sedp->mp_RTPSParticipant->getAttributes().allocation.data_limits)\n    , sedp_(sedp)\n{\n}\n\nvoid EDPServerSUBListener2::onNewCacheChangeAdded(\n        RTPSReader* reader,\n        const CacheChange_t* const change_in)\n{\n    (void)reader;\n    (void)change_in;\n    \/\/ TODO DISCOVERY SERVER VERSION 2\n}\n\nvoid EDPServerSUBListener2::onWriterChangeReceivedByAll(\n        RTPSWriter* writer,\n        CacheChange_t* change)\n{\n    (void)writer;\n\n    if (ChangeKind_t::NOT_ALIVE_DISPOSED_UNREGISTERED == change->kind)\n    {\n        WriterHistory* writer_history =\n                sedp_->subscriptions_writer_.second;\n\n        writer_history->remove_change(change);\n    }\n\n}\n\n} \/* namespace rtps *\/\n} \/\/ namespace fastdds\n} \/* namespace eprosima *\/\n","lang_cluster":"C++","length":102,"code_uid":"433c283762f64e5aacfcdd4bfed440bd"}
{"diff_hunk":"@@ -82,8 +82,8 @@ shaderc_shader_kind MapShadercType(VkShaderStageFlagBits vkShader) {\n \n \/\/ Compile a given string containing GLSL into SPIR-V\n \/\/ Return value of false means an error was encountered\n-bool VkTestFramework::GLSLtoSPV(const VkShaderStageFlagBits shader_type, const char *pshader, std::vector<unsigned int> &spirv,\n-                                bool debug, uint32_t spirv_minor_version) {\n+bool VkTestFramework::GLSLtoSPV(VkPhysicalDeviceLimits const *const device_limits, const VkShaderStageFlagBits shader_type,\n+                                const char *pshader, std::vector<unsigned int> &spirv, bool debug, uint32_t spirv_minor_version) {\n     \/\/ On Android, use shaderc instead.\n     shaderc::Compiler compiler;\n     shaderc::CompileOptions options;","old_code":"\/\/  VK tests\n\/\/\n\/\/  Copyright (c) 2015-2019 The Khronos Group Inc.\n\/\/  Copyright (c) 2015-2019 Valve Corporation\n\/\/  Copyright (c) 2015-2019 LunarG, Inc.\n\/\/  Copyright (c) 2015-2019 Google, Inc.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\n#include \"vktestframeworkandroid.h\"\n#include \"shaderc\/shaderc.hpp\"\n#include <android\/log.h>\n\nVkTestFramework::VkTestFramework() {}\nVkTestFramework::~VkTestFramework() {}\n\n\/\/ Define static elements\nbool VkTestFramework::m_devsim_layer = false;\nANativeWindow *VkTestFramework::window = nullptr;\n\nVkFormat VkTestFramework::GetFormat(VkInstance instance, vk_testing::Device *device) {\n    VkFormatProperties format_props;\n    vk::GetPhysicalDeviceFormatProperties(device->phy().handle(), VK_FORMAT_B8G8R8A8_UNORM, &format_props);\n    if (format_props.linearTilingFeatures & VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT ||\n        format_props.optimalTilingFeatures & VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT) {\n        return VK_FORMAT_B8G8R8A8_UNORM;\n    }\n    vk::GetPhysicalDeviceFormatProperties(device->phy().handle(), VK_FORMAT_R8G8B8A8_UNORM, &format_props);\n    if (format_props.linearTilingFeatures & VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT ||\n        format_props.optimalTilingFeatures & VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT) {\n        return VK_FORMAT_R8G8B8A8_UNORM;\n    }\n    printf(\"Error - device does not support VK_FORMAT_B8G8R8A8_UNORM nor VK_FORMAT_R8G8B8A8_UNORM - exiting\\n\");\n    exit(0);\n}\n\nvoid VkTestFramework::InitArgs(int *argc, char *argv[]) {}\nvoid VkTestFramework::Finish() {}\n\nvoid TestEnvironment::SetUp() {\n    vk_testing::set_error_callback(test_error_callback);\n\n    vk::InitDispatchTable();\n}\n\nvoid TestEnvironment::TearDown() {}\n\n\/\/ Android specific helper functions for shaderc.\nstruct shader_type_mapping {\n    VkShaderStageFlagBits vkshader_type;\n    shaderc_shader_kind shaderc_type;\n};\n\nstatic const shader_type_mapping shader_map_table[] = {\n    {VK_SHADER_STAGE_VERTEX_BIT, shaderc_glsl_vertex_shader},\n    {VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT, shaderc_glsl_tess_control_shader},\n    {VK_SHADER_STAGE_TESSELLATION_EVALUATION_BIT, shaderc_glsl_tess_evaluation_shader},\n    {VK_SHADER_STAGE_GEOMETRY_BIT, shaderc_glsl_geometry_shader},\n    {VK_SHADER_STAGE_FRAGMENT_BIT, shaderc_glsl_fragment_shader},\n    {VK_SHADER_STAGE_COMPUTE_BIT, shaderc_glsl_compute_shader},\n};\n\nshaderc_shader_kind MapShadercType(VkShaderStageFlagBits vkShader) {\n    for (auto shader : shader_map_table) {\n        if (shader.vkshader_type == vkShader) {\n            return shader.shaderc_type;\n        }\n    }\n    assert(false);\n    return shaderc_glsl_infer_from_source;\n}\n\n\/\/ Compile a given string containing GLSL into SPIR-V\n\/\/ Return value of false means an error was encountered\nbool VkTestFramework::GLSLtoSPV(const VkShaderStageFlagBits shader_type, const char *pshader, std::vector<unsigned int> &spirv,\n                                bool debug, uint32_t spirv_minor_version) {\n    \/\/ On Android, use shaderc instead.\n    shaderc::Compiler compiler;\n    shaderc::CompileOptions options;\n    if (debug) {\n        options.SetOptimizationLevel(shaderc_optimization_level_zero);\n        options.SetGenerateDebugInfo();\n    }\n\n    switch (spirv_minor_version) {\n        default:\n        case 0:\n            options.SetTargetSpirv(shaderc_spirv_version_1_0);\n            break;\n        case 1:\n            options.SetTargetSpirv(shaderc_spirv_version_1_1);\n            break;\n        case 2:\n            options.SetTargetSpirv(shaderc_spirv_version_1_2);\n            break;\n        case 3:\n            options.SetTargetSpirv(shaderc_spirv_version_1_3);\n            break;\n        case 4:\n            options.SetTargetSpirv(shaderc_spirv_version_1_4);\n            break;\n    }\n\n    shaderc::SpvCompilationResult result =\n        compiler.CompileGlslToSpv(pshader, strlen(pshader), MapShadercType(shader_type), \"shader\", options);\n    if (result.GetCompilationStatus() != shaderc_compilation_status_success) {\n        __android_log_print(ANDROID_LOG_ERROR, \"VulkanLayerValidationTests\", \"GLSLtoSPV compilation failed: %s\",\n                            result.GetErrorMessage().c_str());\n        return false;\n    }\n\n    for (auto iter = result.begin(); iter != result.end(); iter++) {\n        spirv.push_back(*iter);\n    }\n\n    return true;\n}\n\n\/\/\n\/\/ Compile a given string containing SPIR-V assembly into SPV for use by VK\n\/\/ Return value of false means an error was encountered.\n\/\/\nbool VkTestFramework::ASMtoSPV(const spv_target_env target_env, const uint32_t options, const char *pasm,\n                               std::vector<unsigned int> &spv) {\n    spv_binary binary;\n    spv_diagnostic diagnostic = nullptr;\n    spv_context context = spvContextCreate(target_env);\n    spv_result_t error = spvTextToBinaryWithOptions(context, pasm, strlen(pasm), options, &binary, &diagnostic);\n    spvContextDestroy(context);\n    if (error) {\n        __android_log_print(ANDROID_LOG_ERROR, \"VkLayerValidationTest\", \"ASMtoSPV compilation failed\");\n        spvDiagnosticDestroy(diagnostic);\n        return false;\n    }\n    spv.insert(spv.end(), binary->code, binary->code + binary->wordCount);\n    spvBinaryDestroy(binary);\n\n    return true;\n}\n","lang_cluster":"C++","length":149,"code_uid":"43c9ab53f07e47b5a383036ab18a713c"}
{"diff_hunk":"@@ -120,21 +120,26 @@ void embedding_layer<data_layout::DATA_PARALLEL,El::Device::CPU>::bp_compute() {\n   auto& opt = *m_weights[0]->get_optimizer();\n \n   \/\/ Local data\n-  const auto& local_input = get_local_prev_activations();\n-  auto& local_dict_grad = m_dictionary_gradient.Matrix();\n-  const auto& local_output_grad = get_local_prev_error_signals();\n+  const auto& local_input = dynamic_cast<const CPUMat&>(get_local_prev_activations());\n+  auto& local_dict_grad = dynamic_cast<CPUMat&>(m_dictionary_gradient.Matrix());\n+  const auto& local_output_grad = dynamic_cast<const CPUMat&>(get_local_prev_error_signals());\n   const auto& local_width = local_input.Width();\n   const auto& c = static_cast<const sgd_execution_context&>(this->m_model->get_execution_context());\n   const auto& mini_batch_size = c.get_effective_mini_batch_size();\n \n   \/\/ Update appropriate columns of gradient w.r.t. dictionary\n+  \/\/ Note: Don't update gradient for padding index\n   El::Zero(local_dict_grad);\n   CPUMat dict_grad_v, output_grad_v;\n   for (El::Int col = 0; col < local_width; ++ col) {\n-    const El::Int ind = static_cast<El::Int>(local_input(0, col));\n-    El::View(dict_grad_v, local_dict_grad, El::ALL, El::IR(ind));\n-    El::LockedView(output_grad_v, local_output_grad, El::ALL, El::IR(col));\n-    El::Axpy(DataType{1}, output_grad_v, dict_grad_v);\n+    const El::Int ind = static_cast<El::Int>(std::floor(local_input(0, col)));\n+    if (0 <= ind\n+        && ind < static_cast<El::Int>(m_num_embeddings)\n+        && ind != m_padding_idx) {\n+      El::View(dict_grad_v, local_dict_grad, El::ALL, El::IR(ind));\n+      El::LockedView(output_grad_v, local_output_grad, El::ALL, El::IR(col));\n+      El::Axpy(DataType{1}, output_grad_v, dict_grad_v);\n+    }\n   }\n   opt.add_to_gradient(m_dictionary_gradient,\n                       DataType{1} \/ mini_batch_size,","old_code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/ Copyright (c) 2014-2019, Lawrence Livermore National Security, LLC.\n\/\/ Produced at the Lawrence Livermore National Laboratory.\n\/\/ Written by the LBANN Research Team (B. Van Essen, et al.) listed in\n\/\/ the CONTRIBUTORS file. <lbann-dev@llnl.gov>\n\/\/\n\/\/ LLNL-CODE-697807.\n\/\/ All rights reserved.\n\/\/\n\/\/ This file is part of LBANN: Livermore Big Artificial Neural Network\n\/\/ Toolkit. For details, see http:\/\/software.llnl.gov\/LBANN or\n\/\/ https:\/\/github.com\/LLNL\/LBANN.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"Licensee\"); you\n\/\/ may not use this file except in compliance with the License.  You may\n\/\/ obtain a copy of the License at:\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n\/\/ implied. See the License for the specific language governing\n\/\/ permissions and limitations under the license.\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n#define LBANN_EMBEDDING_LAYER_INSTANTIATE\n#include \"lbann\/layers\/learning\/embedding.hpp\"\n#include \"lbann\/models\/model.hpp\"\n#include \"lbann\/execution_contexts\/sgd_execution_context.hpp\"\n\nnamespace lbann {\n\ntemplate <>\nvoid embedding_layer<data_layout::DATA_PARALLEL,El::Device::CPU>::setup_matrices(const El::Grid& grid) {\n  Layer::setup_matrices(grid);\n  m_dictionary_gradient = StarMat<El::Device::CPU>(grid);\n}\n\ntemplate <>\nvoid embedding_layer<data_layout::DATA_PARALLEL,El::Device::CPU>::setup_dims() {\n  Layer::setup_dims();\n\n  \/\/ Make sure input dimensions are valid\n  if (this->get_input_size() != 1) {\n    const auto& input_dims = this->get_input_dims();\n    std::ostringstream err;\n    err << get_type() << \" layer \\\"\" << get_name() << \"\\\" \"\n        << \"recieved an input tensor with invalid dimensions \"\n        << \"(expected 1, got \";\n    for (size_t i = 0; i < input_dims.size(); ++i) {\n      err << (i > 0 ? \"x\" : \"\") << input_dims[i];\n    }\n    err << \")\";\n    LBANN_ERROR(err.str());\n  }\n\n  \/\/ Output is size of embedding vector\n  this->set_output_dims({static_cast<int>(m_embedding_size)});\n\n}\n\ntemplate <>\nvoid embedding_layer<data_layout::DATA_PARALLEL,El::Device::CPU>::setup_data() {\n  Layer::setup_data();\n\n  \/\/ Make sure layer has weights for dictionary\n  if (this->m_weights.size() != 1) {\n    std::ostringstream err;\n    err << \"attempted to setup \"\n        << this->get_type() << \" layer \\\"\" << this->get_name() << \"\\\" \"\n        << \"with an invalid number of weights \"\n        << \"(expected 1, \"\n        << \"found \" << this->m_weights.size() << \")\";\n    LBANN_ERROR(err.str());\n  }\n\n  \/\/ Initialize dictionary\n  auto& dict = *m_weights[0];\n  auto matrix_dist = get_prev_activations().DistData();\n  matrix_dist.colDist = El::STAR;\n  matrix_dist.rowDist = El::STAR;\n  dict.set_dims({static_cast<int>(m_embedding_size)},\n                {static_cast<int>(m_dictionary_size)});\n  dict.set_matrix_distribution(matrix_dist);\n\n  \/\/ Initialize gradient w.r.t. dictionary\n  m_dictionary_gradient.Resize(m_embedding_size, m_dictionary_size);\n\n}\n\ntemplate <>\nvoid embedding_layer<data_layout::DATA_PARALLEL,El::Device::CPU>::fp_compute() {\n\n  \/\/ Local data\n  const auto& local_dict = m_weights[0]->get_values().LockedMatrix();\n  const auto& local_input = get_local_prev_activations();\n  auto& local_output = get_local_activations();\n  const auto& local_width = local_input.Width();\n\n  \/\/ Populate output matrix with appropriate columns of dictionary\n  CPUMat dict_v, output_v;\n  for (El::Int col = 0; col < local_width; ++ col) {\n    const El::Int ind = static_cast<El::Int>(local_input(0, col));\n    El::LockedView(dict_v, local_dict, El::ALL, El::IR(ind));\n    El::View(output_v, local_output, El::ALL, El::IR(col));\n    El::Copy(dict_v, output_v);\n  }\n\n}\n\ntemplate <>\nvoid embedding_layer<data_layout::DATA_PARALLEL,El::Device::CPU>::bp_compute() {\n\n  \/\/ Embedding layer is not differentiable w.r.t. inputs\n  El::Zero(get_error_signals());\n\n  \/\/ Nothing to be done if dictionary is not being optimized\n  if (m_weights[0]->get_optimizer() == nullptr) { return; }\n  auto& opt = *m_weights[0]->get_optimizer();\n\n  \/\/ Local data\n  const auto& local_input = get_local_prev_activations();\n  auto& local_dict_grad = m_dictionary_gradient.Matrix();\n  const auto& local_output_grad = get_local_prev_error_signals();\n  const auto& local_width = local_input.Width();\n  const auto& c = static_cast<const sgd_execution_context&>(this->m_model->get_execution_context());\n  const auto& mini_batch_size = c.get_effective_mini_batch_size();\n\n  \/\/ Update appropriate columns of gradient w.r.t. dictionary\n  El::Zero(local_dict_grad);\n  CPUMat dict_grad_v, output_grad_v;\n  for (El::Int col = 0; col < local_width; ++ col) {\n    const El::Int ind = static_cast<El::Int>(local_input(0, col));\n    El::View(dict_grad_v, local_dict_grad, El::ALL, El::IR(ind));\n    El::LockedView(output_grad_v, local_output_grad, El::ALL, El::IR(col));\n    El::Axpy(DataType{1}, output_grad_v, dict_grad_v);\n  }\n  opt.add_to_gradient(m_dictionary_gradient,\n                      DataType{1} \/ mini_batch_size,\n                      true);\n\n}\n\n\/\/ Explicit instantiation\ntemplate class embedding_layer<data_layout::DATA_PARALLEL, El::Device::CPU>;\n\n} \/\/ namespace lbann\n","lang_cluster":"C++","length":148,"code_uid":"fc58ae8d51314452a2dea7e52ebe4e05"}
{"diff_hunk":"@@ -141,6 +141,28 @@ struct compute_kernel_gpu<Float, method::dense, task::compute> {\n                         const input_t& input) const {\n         return compute<Float>(ctx, desc, input);\n     }\n+\n+#ifdef ONEDAL_DATA_PARALLEL\n+    void operator()(const context_gpu& ctx,\n+                    const descriptor_t& desc,\n+                    const table& x,\n+                    const table& y,\n+                    homogen_table& res) {\n+        ONEDAL_ASSERT(x.get_row_count() == res.get_row_count());\n+        ONEDAL_ASSERT(y.get_row_count() == res.get_column_count());\n+        ONEDAL_ASSERT(x.get_column_count() == y.get_column_count());\n+\n+        auto& queue = ctx.get_queue();\n+        const auto x_nd = pr::table2ndarray<Float>(queue, x, sycl::usm::alloc::device);\n+        const auto y_nd = pr::table2ndarray<Float>(queue, y, sycl::usm::alloc::device);\n+\n+        auto res_ptr = res.get_data<Float>();\n+        auto res_nd = pr::ndarray<Float, 2>::wrap(const_cast<Float*>(res_ptr),\n+                                                  { res.get_row_count(), res.get_column_count() });\n+\n+        compute_rbf(queue, x_nd, y_nd, res_nd, desc.get_sigma());\n+    }\n+#endif\n };\n \n template struct compute_kernel_gpu<float, method::dense, task::compute>;","old_code":"\/*******************************************************************************\n* Copyright 2020-2021 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n#include \"oneapi\/dal\/algo\/rbf_kernel\/backend\/gpu\/compute_kernel.hpp\"\n#include \"oneapi\/dal\/backend\/primitives\/reduction.hpp\"\n#include \"oneapi\/dal\/backend\/primitives\/blas.hpp\"\n#include \"oneapi\/dal\/backend\/math.hpp\"\n#include \"oneapi\/dal\/backend\/primitives\/utils.hpp\"\n\nnamespace oneapi::dal::rbf_kernel::backend {\n\nusing dal::backend::context_gpu;\nusing input_t = compute_input<task::compute>;\nusing result_t = compute_result<task::compute>;\nusing descriptor_t = detail::descriptor_base<task::compute>;\n\nnamespace pr = dal::backend::primitives;\n\ntemplate <typename Float>\ninline auto compute_exponents(sycl::queue& queue,\n                              const pr::ndview<Float, 1>& sqr_x_nd,\n                              const pr::ndview<Float, 1>& sqr_y_nd,\n                              pr::ndview<Float, 2>& res_nd,\n                              double sigma,\n                              const dal::backend::event_vector& deps = {}) {\n    const std::int64_t x_row_count = sqr_x_nd.get_dimension(0);\n    const std::int64_t y_row_count = sqr_y_nd.get_dimension(0);\n    ONEDAL_ASSERT(res_nd.get_count() == x_row_count * y_row_count);\n\n    const Float coeff = static_cast<Float>(-0.5 \/ (sigma * sigma));\n\n    const Float* sqr_x_ptr = sqr_x_nd.get_data();\n    const Float* sqr_y_ptr = sqr_y_nd.get_data();\n    Float* res_ptr = res_nd.get_mutable_data();\n\n    const Float threshold = dal::backend::exp_low_threshold<Float>();\n\n    const auto wg_size = dal::backend::propose_wg_size(queue);\n    const auto range =\n        dal::backend::make_multiple_nd_range_2d({ x_row_count, y_row_count }, { wg_size, 1 });\n\n    auto compute_rbf_event = queue.submit([&](sycl::handler& cgh) {\n        cgh.depends_on(deps);\n        const std::size_t ld = y_row_count;\n\n        cgh.parallel_for(range, [=](sycl::nd_item<2> item) {\n            const std::size_t i = item.get_global_id(0);\n            const std::size_t j = item.get_global_id(1);\n            const Float sqr_x_i = sqr_x_ptr[i];\n            const Float sqr_y_j = sqr_y_ptr[j];\n            const Float res_rbf_ij = res_ptr[i * ld + j];\n            const Float arg = sycl::fmax((sqr_x_i + sqr_y_j + res_rbf_ij) * coeff, threshold);\n\n            res_ptr[i * ld + j] = sycl::exp(arg);\n        });\n    });\n\n    return compute_rbf_event;\n}\n\ntemplate <typename Float>\ninline auto compute_rbf(sycl::queue& queue,\n                        const pr::ndview<Float, 2>& x_nd,\n                        const pr::ndview<Float, 2>& y_nd,\n                        pr::ndview<Float, 2>& res_nd,\n                        double sigma,\n                        const dal::backend::event_vector& deps = {}) {\n    const std::int64_t x_row_count = x_nd.get_dimension(0);\n    const std::int64_t y_row_count = y_nd.get_dimension(0);\n\n    auto sqr_x_nd = pr::ndarray<Float, 1>::empty(queue, { x_row_count }, sycl::usm::alloc::device);\n    auto sqr_y_nd = pr::ndarray<Float, 1>::empty(queue, { y_row_count }, sycl::usm::alloc::device);\n\n    auto reduce_x_event =\n        pr::reduce_by_rows(queue, x_nd, sqr_x_nd, pr::sum<Float>{}, pr::square<Float>{}, deps);\n    auto reduce_y_event =\n        pr::reduce_by_rows(queue, y_nd, sqr_y_nd, pr::sum<Float>{}, pr::square<Float>{}, deps);\n\n    constexpr Float alpha = -2.0;\n    constexpr Float beta = 0.0;\n    auto gemm_event = pr::gemm(queue, x_nd, y_nd.t(), res_nd, alpha, beta);\n\n    auto compute_exponents_event =\n        compute_exponents(queue,\n                          sqr_x_nd,\n                          sqr_y_nd,\n                          res_nd,\n                          sigma,\n                          { reduce_x_event, reduce_y_event, gemm_event });\n\n    auto smart_event =\n        dal::backend::smart_event{ compute_exponents_event }.attach(sqr_x_nd).attach(sqr_y_nd);\n\n    return smart_event;\n}\n\ntemplate <typename Float>\nstatic result_t compute(const context_gpu& ctx, const descriptor_t& desc, const input_t& input) {\n    const auto x = input.get_x();\n    const auto y = input.get_y();\n\n    auto& queue = ctx.get_queue();\n\n    const std::int64_t x_row_count = x.get_row_count();\n    const std::int64_t y_row_count = y.get_row_count();\n\n    ONEDAL_ASSERT(x.get_column_count() == y.get_column_count());\n    dal::detail::check_mul_overflow(x_row_count, y_row_count);\n\n    const auto x_nd = pr::table2ndarray<Float>(queue, x, sycl::usm::alloc::device);\n    const auto y_nd = pr::table2ndarray<Float>(queue, y, sycl::usm::alloc::device);\n\n    auto res_nd =\n        pr::ndarray<Float, 2>::empty(queue, { x_row_count, y_row_count }, sycl::usm::alloc::device);\n\n    auto compute_rbf_event = compute_rbf(queue, x_nd, y_nd, res_nd, desc.get_sigma());\n\n    const auto res_array = res_nd.flatten(queue, { compute_rbf_event });\n    auto res_table = homogen_table::wrap(res_array, x_row_count, y_row_count);\n\n    return result_t{}.set_values(res_table);\n}\n\ntemplate <typename Float>\nstruct compute_kernel_gpu<Float, method::dense, task::compute> {\n    result_t operator()(const context_gpu& ctx,\n                        const descriptor_t& desc,\n                        const input_t& input) const {\n        return compute<Float>(ctx, desc, input);\n    }\n};\n\ntemplate struct compute_kernel_gpu<float, method::dense, task::compute>;\ntemplate struct compute_kernel_gpu<double, method::dense, task::compute>;\n\n} \/\/ namespace oneapi::dal::rbf_kernel::backend\n","lang_cluster":"C++","length":149,"code_uid":"86029b5c99b240eda439d05c2d809309"}
{"diff_hunk":"@@ -55,7 +55,7 @@ def plot_importance(booster, ax=None, height=0.2,\n     try:\n         import matplotlib.pyplot as plt\n     except ImportError:\n-        raise ImportError('You must install matplotlib for plotting library')\n+        raise ImportError('You must install matplotlib to plot importance.')\n \n     if isinstance(booster, LGBMModel):\n         importance = booster.booster_.feature_importance(importance_type=importance_type)","old_code":"# coding: utf-8\n# pylint: disable = C0103\n\"\"\"Plotting Library.\"\"\"\nfrom __future__ import absolute_import\n\nimport numpy as np\n\nfrom .basic import Booster, is_numpy_1d_array\nfrom .sklearn import LGBMModel\n\n\ndef plot_importance(booster, ax=None, height=0.2,\n                    xlim=None, ylim=None, title='Feature importance',\n                    xlabel='Feature importance', ylabel='Features',\n                    importance_type='split', max_num_features=None,\n                    ignore_zero=True, grid=True, **kwargs):\n    \"\"\"Plot model feature importances.\n\n    Parameters\n    ----------\n    booster : Booster, LGBMModel or array\n        Booster or LGBMModel instance, or array of feature importances\n    ax : matplotlib Axes\n        Target axes instance. If None, new figure and axes will be created.\n    height : float\n        Bar height, passed to ax.barh()\n    xlim : tuple\n        Tuple passed to axes.xlim()\n    ylim : tuple\n        Tuple passed to axes.ylim()\n    title : str\n        Axes title. Pass None to disable.\n    xlabel : str\n        X axis title label. Pass None to disable.\n    ylabel : str\n        Y axis title label. Pass None to disable.\n    importance_type : str\n        How the importance is calculated: \"split\" or \"gain\"\n        \"split\" is the number of times a feature is used in a model\n        \"gain\" is the total gain of splits which use the feature\n    max_num_features : int\n        Max number of top features displayed on plot.\n        If None or smaller than 1, all features will be displayed.\n    ignore_zero : bool\n        Ignore features with zero importance\n    grid : bool\n        Whether add grid for axes\n    **kwargs :\n        Other keywords passed to ax.barh()\n\n    Returns\n    -------\n    ax : matplotlib Axes\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        raise ImportError('You must install matplotlib for plotting library')\n\n    if isinstance(booster, LGBMModel):\n        importance = booster.booster_.feature_importance(importance_type=importance_type)\n    elif isinstance(booster, Booster):\n        importance = booster.feature_importance(importance_type=importance_type)\n    elif is_numpy_1d_array(booster) or isinstance(booster, list):\n        importance = booster\n    else:\n        raise ValueError('booster must be Booster or array instance')\n\n    if not len(importance):\n        raise ValueError('Booster feature_importances are empty')\n\n    tuples = sorted(enumerate(importance), key=lambda x: x[1])\n    if ignore_zero:\n        tuples = [x for x in tuples if x[1] > 0]\n    if max_num_features is not None and max_num_features > 0:\n        tuples = tuples[-max_num_features:]\n    labels, values = zip(*tuples)\n\n    if ax is None:\n        _, ax = plt.subplots(1, 1)\n\n    ylocs = np.arange(len(values))\n    ax.barh(ylocs, values, align='center', height=height, **kwargs)\n\n    for x, y in zip(values, ylocs):\n        ax.text(x + 1, y, x, va='center')\n\n    ax.set_yticks(ylocs)\n    ax.set_yticklabels(labels)\n\n    if xlim is not None:\n        if not isinstance(xlim, tuple) or len(xlim) != 2:\n            raise ValueError('xlim must be a tuple of 2 elements')\n    else:\n        xlim = (0, max(values) * 1.1)\n    ax.set_xlim(xlim)\n\n    if ylim is not None:\n        if not isinstance(ylim, tuple) or len(ylim) != 2:\n            raise ValueError('ylim must be a tuple of 2 elements')\n    else:\n        ylim = (-1, len(values))\n    ax.set_ylim(ylim)\n\n    if title is not None:\n        ax.set_title(title)\n    if xlabel is not None:\n        ax.set_xlabel(xlabel)\n    if ylabel is not None:\n        ax.set_ylabel(ylabel)\n    ax.grid(grid)\n    return ax\n","lang_cluster":"C++","length":112,"code_uid":"124e59e3921a48149c6088060b448454"}
{"diff_hunk":"@@ -21,7 +21,7 @@ void AddEdgesProcessor::process(const cpp2::AddEdgesRequest& req) {\n         std::vector<kvstore::KV> data;\n         std::for_each(partEdges.second.begin(), partEdges.second.end(), [&](auto& edge){\n             auto key = KeyUtils::edgeKey(partId, edge.key.src, edge.key.edge_type,\n-                                         edge.key.ranking, edge.key.dst, now);\n+                                         edge.key.ranking, edge.key.dst, version);\n             data.emplace_back(std::move(key), std::move(edge.get_props()));\n         });\n         doPut(spaceId, partId, std::move(data));","old_code":"\/* Copyright (c) 2018 - present, VE Software Inc. All rights reserved\n *\n * This source code is licensed under Apache 2.0 License\n *  (found in the LICENSE.Apache file in the root directory)\n *\/\n#include \"storage\/AddEdgesProcessor.h\"\n#include <algorithm>\n#include \"time\/TimeUtils.h\"\n#include \"storage\/KeyUtils.h\"\n\nnamespace nebula {\nnamespace storage {\n\nvoid AddEdgesProcessor::process(const cpp2::AddEdgesRequest& req) {\n    auto spaceId = req.get_space_id();\n    auto now = time::TimeUtils::nowInMSeconds();\n    callingNum_ = req.parts.size();\n    CHECK_NOTNULL(kvstore_);\n    std::for_each(req.parts.begin(), req.parts.end(), [&](auto& partEdges){\n        auto partId = partEdges.first;\n        std::vector<kvstore::KV> data;\n        std::for_each(partEdges.second.begin(), partEdges.second.end(), [&](auto& edge){\n            auto key = KeyUtils::edgeKey(partId, edge.key.src, edge.key.edge_type,\n                                         edge.key.ranking, edge.key.dst, now);\n            data.emplace_back(std::move(key), std::move(edge.get_props()));\n        });\n        doPut(spaceId, partId, std::move(data));\n    });\n}\n\n}  \/\/ namespace storage\n}  \/\/ namespace nebula\n","lang_cluster":"C++","length":32,"code_uid":"7f4b0204aee04b22966485899fda123a"}
{"diff_hunk":"@@ -128,6 +128,7 @@ void WANZmq::Write(const char *buffer, size_t size)\n                                          \", in call to WANZmq write\\n\");\n         }\n     }\n+    *\/\n }\n \n void WANZmq::Flush() {}","old_code":"\/*\n * Distributed under the OSI-approved Apache License, Version 2.0.  See\n * accompanying file Copyright.txt for details.\n *\n * WANZmq.cpp\n *\n *  Created on: May 26, 2017\n *      Author: Jason Wang wangr1@ornl.gov\n *\/\n\n#include \"WANZmq.h\"\n\n#include <zmq.h>\n\nnamespace adios2\n{\nnamespace transport\n{\n\nWANZmq::WANZmq(const std::string ipAddress, const std::string port,\n               MPI_Comm mpiComm, const bool debugMode)\n: Transport(\"wan\", \"zmq\", mpiComm, debugMode), m_IPAddress(ipAddress),\n  m_Port(port)\n{\n\n    if (m_DebugMode)\n    {\n        \/\/ TODO verify port is unsigned int\n    }\n}\n\nWANZmq::~WANZmq()\n{\n    if (m_Socket)\n    {\n        zmq_close(m_Socket);\n    }\n}\n\nvoid WANZmq::Open(const std::string &name, const OpenMode openMode)\n{\n    m_Name = name;\n    m_OpenMode = openMode;\n\n    if (m_OpenMode == OpenMode::Write)\n    {\n        if (m_Profiler.IsActive)\n        {\n            m_Profiler.Timers.at(\"open\").Resume();\n        }\n\n        m_Socket = zmq_socket(m_Context, ZMQ_REQ);\n        const std::string fullIP(\"tcp:\/\/\" + m_IPAddress + \":\" + m_Port);\n        zmq_connect(m_Socket, fullIP.c_str());\n\n        if (m_Profiler.IsActive)\n        {\n            m_Profiler.Timers.at(\"open\").Pause();\n        }\n    }\n    else if (m_OpenMode == OpenMode::Append)\n    {\n        if (m_DebugMode)\n        {\n            throw std::invalid_argument(\n                \"ERROR: WAN transport \" + m_Name +\n                \" only supports \"\n                \"OpenMode:w (write\/sender) and \"\n                \"OpenMode:r (read\/receiver), in call to Open\\n\");\n        }\n    }\n    else if (m_OpenMode == OpenMode::Read)\n    {\n        if (m_Profiler.IsActive)\n        {\n            m_Profiler.Timers.at(\"open\").Resume();\n        }\n\n        m_Socket = zmq_socket(m_Context, ZMQ_REP);\n        const std::string fullIP(\"tcp:\/\/\" + m_IPAddress + \":\" + m_Port);\n        zmq_bind(m_Socket, fullIP.c_str());\n\n        if (m_Profiler.IsActive)\n        {\n            m_Profiler.Timers.at(\"open\").Pause();\n        }\n    }\n\n    if (m_DebugMode)\n    {\n        if (m_Socket == NULL) \/\/ something goes wrong\n        {\n            throw std::ios_base::failure(\n                \"ERROR: couldn't open socket for address \" + m_Name +\n                \", in call to WANZmq Open\\n\");\n        }\n    }\n    m_IsOpen = true;\n}\n\nvoid WANZmq::SetBuffer(char *buffer, size_t size) {}\n\nvoid WANZmq::Write(const char *buffer, size_t size)\n{\n\n    if (m_Profiler.IsActive)\n    {\n        m_Profiler.Timers.at(\"write\").Resume();\n    }\n\n    int status = zmq_send(m_Socket, buffer, size, 0);\n    char ret[10];\n    zmq_recv(m_Socket, ret, 10, 0);\n\n    if (m_Profiler.IsActive)\n    {\n        m_Profiler.Timers.at(\"write\").Pause();\n    }\n\n    if (m_DebugMode)\n    {\n        const std::string retString(ret);\n\n        if (status == -1 || retString != \"OK\") \/\/ TODO : verify this\n        {\n            throw std::ios_base::failure(\"ERROR: couldn't send message \" +\n                                         m_Name +\n                                         \", in call to WANZmq write\\n\");\n        }\n    }\n}\n\nvoid WANZmq::Flush() {}\n\nvoid WANZmq::Close()\n{\n    if (m_Socket)\n    {\n        zmq_close(m_Socket);\n    }\n}\n\n} \/\/ end namespace transport\n} \/\/ end namespace adios\n","lang_cluster":"C++","length":144,"code_uid":"8724fab40f104412a40277211eefef86"}
{"diff_hunk":"@@ -68,10 +68,21 @@ bool generateDataStoreOptions(const int argc,\n     \/\/ declare a group of options that will be allowed both on command line\n     \/\/ as well as in a config file\n     boost::program_options::options_description config_options(\"Configuration\");\n-    config_options.add_options()(\"max-wait\",\n-                                 boost::program_options::value<int>(&max_wait)->default_value(-1),\n-                                 \"Maximum number of seconds to wait on a running data update \"\n-                                 \"before aquiring the lock by force.\");\n+    config_options.add_options() \/\/\n+        (\"max-wait\",\n+         boost::program_options::value<int>(&max_wait)->default_value(-1),\n+         \"Maximum number of seconds to wait on a running data update \"\n+         \"before aquiring the lock by force.\") \/\/\n+        (\"dataset-name\",\n+         boost::program_options::value<std::string>(&dataset_name)->default_value(\"\"),\n+         \"Name of the dataset to load into memory. This allows having multiple datasets in memory \"\n+         \"at the same time.\") \/\/\n+        (\"list\",\n+         boost::program_options::value<bool>(&list_datasets)\n+             ->default_value(false)\n+             ->implicit_value(true),\n+         \"Name of the dataset to load into memory. This allows having multiple datasets in memory \"\n+         \"at the same time.\");\n \n     \/\/ hidden options, will be allowed on command line but will not be shown to the user\n     boost::program_options::options_description hidden_options(\"Hidden options\");","old_code":"#include \"storage\/shared_memory.hpp\"\n#include \"storage\/shared_monitor.hpp\"\n#include \"storage\/storage.hpp\"\n#include \"osrm\/exception.hpp\"\n#include \"util\/log.hpp\"\n#include \"util\/meminfo.hpp\"\n#include \"util\/typedefs.hpp\"\n#include \"util\/version.hpp\"\n\n#include <boost\/filesystem.hpp>\n#include <boost\/program_options.hpp>\n\n#include <csignal>\n#include <cstdlib>\n\nusing namespace osrm;\n\nvoid removeLocks() { storage::SharedMonitor<storage::SharedDataTimestamp>::remove(); }\n\nvoid deleteRegion(const storage::SharedDataType region)\n{\n    if (storage::SharedMemory::RegionExists(region) && !storage::SharedMemory::Remove(region))\n    {\n        util::Log(logWARNING) << \"could not delete shared memory region \"\n                              << storage::regionToString(region);\n    }\n}\n\nvoid springClean()\n{\n    osrm::util::Log() << \"Releasing all locks\";\n    osrm::util::Log() << \"ATTENTION! BE CAREFUL!\";\n    osrm::util::Log() << \"----------------------\";\n    osrm::util::Log() << \"This tool may put osrm-routed into an undefined state!\";\n    osrm::util::Log() << \"Type 'Y' to acknowledge that you know what your are doing.\";\n    osrm::util::Log() << \"\\n\\nDo you want to purge all shared memory allocated \"\n                      << \"by osrm-datastore? [type 'Y' to confirm]\";\n\n    const auto letter = getchar();\n    if (letter != 'Y')\n    {\n        osrm::util::Log() << \"aborted.\";\n    }\n    else\n    {\n        deleteRegion(storage::REGION_1);\n        deleteRegion(storage::REGION_2);\n        removeLocks();\n    }\n}\n\n\/\/ generate boost::program_options object for the routing part\nbool generateDataStoreOptions(const int argc,\n                              const char *argv[],\n                              std::string &verbosity,\n                              boost::filesystem::path &base_path,\n                              int &max_wait)\n{\n    \/\/ declare a group of options that will be allowed only on command line\n    boost::program_options::options_description generic_options(\"Options\");\n    generic_options.add_options()(\"version,v\", \"Show version\")(\"help,h\", \"Show this help message\")(\n        \"verbosity,l\",\n        boost::program_options::value<std::string>(&verbosity)->default_value(\"INFO\"),\n        std::string(\"Log verbosity level: \" + util::LogPolicy::GetLevels()).c_str())(\n        \"remove-locks,r\", \"Remove locks\")(\"spring-clean,s\",\n                                          \"Spring-cleaning all shared memory regions\");\n\n    \/\/ declare a group of options that will be allowed both on command line\n    \/\/ as well as in a config file\n    boost::program_options::options_description config_options(\"Configuration\");\n    config_options.add_options()(\"max-wait\",\n                                 boost::program_options::value<int>(&max_wait)->default_value(-1),\n                                 \"Maximum number of seconds to wait on a running data update \"\n                                 \"before aquiring the lock by force.\");\n\n    \/\/ hidden options, will be allowed on command line but will not be shown to the user\n    boost::program_options::options_description hidden_options(\"Hidden options\");\n    hidden_options.add_options()(\"base,b\",\n                                 boost::program_options::value<boost::filesystem::path>(&base_path),\n                                 \"base path to .osrm file\");\n\n    \/\/ positional option\n    boost::program_options::positional_options_description positional_options;\n    positional_options.add(\"base\", 1);\n\n    \/\/ combine above options for parsing\n    boost::program_options::options_description cmdline_options;\n    cmdline_options.add(generic_options).add(config_options).add(hidden_options);\n\n    const auto *executable = argv[0];\n    boost::program_options::options_description visible_options(\n        boost::filesystem::path(executable).filename().string() + \" [<options>] <configuration>\");\n    visible_options.add(generic_options).add(config_options);\n\n    \/\/ print help options if no infile is specified\n    if (argc < 2)\n    {\n        util::Log() << visible_options;\n        return false;\n    }\n\n    \/\/ parse command line options\n    boost::program_options::variables_map option_variables;\n\n    try\n    {\n        boost::program_options::store(boost::program_options::command_line_parser(argc, argv)\n                                          .options(cmdline_options)\n                                          .positional(positional_options)\n                                          .run(),\n                                      option_variables);\n    }\n    catch (const boost::program_options::error &e)\n    {\n        util::Log(logERROR) << e.what();\n        return false;\n    }\n\n    if (option_variables.count(\"version\"))\n    {\n        util::Log() << OSRM_VERSION;\n        return false;\n    }\n\n    if (option_variables.count(\"help\"))\n    {\n        util::Log() << visible_options;\n        return false;\n    }\n\n    if (option_variables.count(\"remove-locks\"))\n    {\n        removeLocks();\n        return false;\n    }\n\n    if (option_variables.count(\"spring-clean\"))\n    {\n        springClean();\n        return false;\n    }\n\n    boost::program_options::notify(option_variables);\n\n    return true;\n}\n\n[[noreturn]] void CleanupSharedBarriers(int signum)\n{ \/\/ Here the lock state of named mutexes is unknown, make a hard cleanup\n    removeLocks();\n    std::_Exit(128 + signum);\n}\n\nint main(const int argc, const char *argv[]) try\n{\n    int signals[] = {SIGTERM, SIGSEGV, SIGINT, SIGILL, SIGABRT, SIGFPE};\n    for (auto sig : signals)\n    {\n        std::signal(sig, CleanupSharedBarriers);\n    }\n\n    util::LogPolicy::GetInstance().Unmute();\n\n    std::string verbosity;\n    boost::filesystem::path base_path;\n    int max_wait = -1;\n    if (!generateDataStoreOptions(argc, argv, verbosity, base_path, max_wait))\n    {\n        return EXIT_SUCCESS;\n    }\n\n    util::LogPolicy::GetInstance().SetLevel(verbosity);\n\n    storage::StorageConfig config(base_path);\n    if (!config.IsValid())\n    {\n        util::Log(logERROR) << \"Config contains invalid file paths. Exiting!\";\n        return EXIT_FAILURE;\n    }\n    storage::Storage storage(std::move(config));\n\n    return storage.Run(max_wait);\n}\ncatch (const osrm::RuntimeError &e)\n{\n    util::Log(logERROR) << e.what();\n    return e.GetCode();\n}\ncatch (const std::bad_alloc &e)\n{\n    util::DumpMemoryStats();\n    util::Log(logERROR) << \"[exception] \" << e.what();\n    util::Log(logERROR) << \"Please provide more memory or disable locking the virtual \"\n                           \"address space (note: this makes OSRM swap, i.e. slow)\";\n    return EXIT_FAILURE;\n}\n","lang_cluster":"C++","length":196,"code_uid":"20b2b3a3fdf148c79bb2edc6f573b8de"}
{"diff_hunk":"@@ -119,7 +119,20 @@ struct train_kernel_gpu<Float, method::thunder, task::classification> {\n     }\n };\n \n+template <typename Float>\n+struct train_kernel_gpu<Float, method::thunder, task::nu_classification> {\n+    train_result<task::nu_classification> operator()(\n+        const dal::backend::context_gpu& ctx,\n+        const detail::descriptor_base<task::nu_classification>& params,\n+        const train_input<task::nu_classification>& input) const {\n+        throw unimplemented(\n+            dal::detail::error_messages::nu_svm_thunder_method_is_not_implemented_for_gpu());\n+    }\n+};\n+\n template struct train_kernel_gpu<float, method::thunder, task::classification>;\n template struct train_kernel_gpu<double, method::thunder, task::classification>;\n+template struct train_kernel_gpu<float, method::thunder, task::nu_classification>;\n+template struct train_kernel_gpu<double, method::thunder, task::nu_classification>;\n \n } \/\/ namespace oneapi::dal::svm::backend","old_code":"\/*******************************************************************************\n* Copyright 2020-2021 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n#include \"oneapi\/dal\/algo\/svm\/backend\/gpu\/train_kernel.hpp\"\n#include \"oneapi\/dal\/algo\/svm\/backend\/model_interop.hpp\"\n#include \"oneapi\/dal\/algo\/svm\/backend\/model_conversion.hpp\"\n#include \"oneapi\/dal\/algo\/svm\/backend\/kernel_function_impl.hpp\"\n#include \"oneapi\/dal\/algo\/svm\/backend\/utils.hpp\"\n\n#include \"oneapi\/dal\/backend\/interop\/common_dpc.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/error_converter.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/table_conversion.hpp\"\n\n#include \"oneapi\/dal\/table\/row_accessor.hpp\"\n\n#include <daal\/src\/algorithms\/svm\/oneapi\/svm_train_thunder_kernel_oneapi.h>\n\nnamespace oneapi::dal::svm::backend {\n\nusing dal::backend::context_gpu;\nusing model_t = model<task::classification>;\nusing input_t = train_input<task::classification>;\nusing result_t = train_result<task::classification>;\nusing descriptor_t = detail::descriptor_base<task::classification>;\n\nnamespace daal_svm = daal::algorithms::svm;\nnamespace daal_kernel_function = daal::algorithms::kernel_function;\nnamespace interop = dal::backend::interop;\n\ntemplate <typename Float>\nusing daal_svm_thunder_kernel_t =\n    daal_svm::training::internal::SVMTrainOneAPI<Float, daal_svm::training::thunder>;\n\ntemplate <typename Float>\nstatic result_t call_daal_kernel(const context_gpu& ctx,\n                                 const descriptor_t& desc,\n                                 const table& data,\n                                 const table& labels) {\n    auto& queue = ctx.get_queue();\n    interop::execution_context_guard guard(queue);\n\n    const std::uint64_t class_count = desc.get_class_count();\n    if (class_count > 2) {\n        throw unimplemented(dal::detail::error_messages::svm_multiclass_not_implemented_for_gpu());\n    }\n\n    const std::int64_t row_count = data.get_row_count();\n    const std::int64_t column_count = data.get_column_count();\n\n    auto arr_label = row_accessor<const Float>{ labels }.pull(queue);\n\n    binary_label_t<Float> unique_label;\n    auto arr_new_label =\n        convert_labels(queue, arr_label, { Float(-1.0), Float(1.0) }, unique_label);\n\n    const auto daal_data = interop::convert_to_daal_table(queue, data);\n    const auto daal_labels = interop::convert_to_daal_table(queue, arr_new_label, row_count, 1);\n\n    auto kernel_impl = detail::get_kernel_function_impl(desc);\n    if (!kernel_impl) {\n        throw internal_error{ dal::detail::error_messages::unknown_kernel_function_type() };\n    }\n    const auto daal_kernel = kernel_impl->get_daal_kernel_function();\n\n    const std::uint64_t cache_megabyte = static_cast<std::uint64_t>(desc.get_cache_size());\n    constexpr std::uint64_t megabyte = 1024 * 1024;\n    dal::detail::check_mul_overflow(cache_megabyte, megabyte);\n    const std::uint64_t cache_byte = cache_megabyte * megabyte;\n\n    daal_svm::training::internal::KernelParameter daal_svm_parameter;\n    daal_svm_parameter.kernel = daal_kernel;\n    daal_svm_parameter.C = desc.get_c();\n    daal_svm_parameter.accuracyThreshold = desc.get_accuracy_threshold();\n    daal_svm_parameter.tau = desc.get_tau();\n    daal_svm_parameter.maxIterations =\n        dal::detail::integral_cast<std::size_t>(desc.get_max_iteration_count());\n    daal_svm_parameter.doShrinking = desc.get_shrinking();\n    daal_svm_parameter.cacheSize = cache_byte;\n\n    auto daal_model = daal_svm::Model::create<Float>(column_count);\n    interop::status_to_exception(daal_svm_thunder_kernel_t<Float>().compute(daal_data,\n                                                                            *daal_labels,\n                                                                            daal_model.get(),\n                                                                            daal_svm_parameter));\n    auto table_support_indices =\n        interop::convert_from_daal_homogen_table<Float>(daal_model->getSupportIndices());\n\n    auto trained_model = convert_from_daal_model<task::classification, Float>(*daal_model)\n                             .set_first_class_label(unique_label.first)\n                             .set_second_class_label(unique_label.second);\n\n    return result_t().set_model(trained_model).set_support_indices(table_support_indices);\n}\n\ntemplate <typename Float>\nstatic result_t train(const context_gpu& ctx, const descriptor_t& desc, const input_t& input) {\n    return call_daal_kernel<Float>(ctx, desc, input.get_data(), input.get_labels());\n}\n\ntemplate <typename Float>\nstruct train_kernel_gpu<Float, method::thunder, task::classification> {\n    result_t operator()(const context_gpu& ctx,\n                        const descriptor_t& desc,\n                        const input_t& input) const {\n        return train<Float>(ctx, desc, input);\n    }\n};\n\ntemplate struct train_kernel_gpu<float, method::thunder, task::classification>;\ntemplate struct train_kernel_gpu<double, method::thunder, task::classification>;\n\n} \/\/ namespace oneapi::dal::svm::backend\n","lang_cluster":"C++","length":125,"code_uid":"d214333ed55a49e48657eb984adc9144"}
{"diff_hunk":"@@ -57,15 +57,10 @@ TEST(StatisticsQosTests, StatisticsDataWriterQosTest)\n  *\/\n TEST(StatisticsQosTests, StatisticsDataReaderQosTest)\n {\n-    \/\/ TODO(jlbueno) Remove this guards after implementation. Here to prevent failures in current CI.\n-#ifdef FASTDDS_STATISTICS\n-    logError(STATISTICS_QOS_TEST, \"This test is going to fail because API is not yet implemented.\")\n-\n     EXPECT_TRUE(STATISTICS_DATAREADER_QOS.reliability().kind == eprosima::fastdds::dds::RELIABLE_RELIABILITY_QOS);\n     EXPECT_TRUE(STATISTICS_DATAREADER_QOS.durability().kind == eprosima::fastdds::dds::TRANSIENT_LOCAL_DURABILITY_QOS);\n     EXPECT_TRUE(STATISTICS_DATAREADER_QOS.history().kind == eprosima::fastdds::dds::KEEP_LAST_HISTORY_QOS);\n     EXPECT_TRUE(STATISTICS_DATAREADER_QOS.history().depth == 100);\n-#endif \/\/ FASTDDS_STATISTICS\n }\n \n } \/\/ namespace dds","old_code":"\/\/ Copyright 2021 Proyectos y Sistemas de Mantenimiento SL (eProsima).\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\n#include <gtest\/gtest.h>\n\n#include <fastdds\/dds\/core\/policy\/QosPolicies.hpp>\n#include <fastdds\/dds\/log\/Log.hpp>\n#include <fastdds\/statistics\/dds\/publisher\/qos\/DataWriterQos.hpp>\n#include <fastdds\/statistics\/dds\/subscriber\/qos\/DataReaderQos.hpp>\n\nnamespace eprosima {\nnamespace fastdds {\nnamespace statistics {\nnamespace dds {\n\n\/*\n * This test checks that STATISTICS_DATAWRITER_QOS correctly sets the expected QoS.\n * 1. Reliability RELIABLE\n * 2. Durability TRANSIENT LOCAL\n * 3. Pull mode enabled\n * 4. Publication mode ASYNCHRONOUS\n * 5. History kind KEEP LAST\n * 6. History depth 100\n *\/\nTEST(StatisticsQosTests, StatisticsDataWriterQosTest)\n{\n    \/\/ TODO(jlbueno) Remove this guards after implementation. Here to prevent failures in current CI.\n#ifdef FASTDDS_STATISTICS\n    logError(STATISTICS_QOS_TEST, \"This test is going to fail because API is not yet implemented.\")\n\n    EXPECT_TRUE(STATISTICS_DATAWRITER_QOS.reliability().kind == eprosima::fastdds::dds::RELIABLE_RELIABILITY_QOS);\n    EXPECT_TRUE(STATISTICS_DATAWRITER_QOS.durability().kind == eprosima::fastdds::dds::TRANSIENT_LOCAL_DURABILITY_QOS);\n    \/\/ TODO(jlbueno) Pull mode is not yet exposed in DDS API\n    EXPECT_TRUE(STATISTICS_DATAWRITER_QOS.publish_mode().kind == eprosima::fastdds::dds::ASYNCHRONOUS_PUBLISH_MODE);\n    EXPECT_TRUE(STATISTICS_DATAWRITER_QOS.history().kind == eprosima::fastdds::dds::KEEP_LAST_HISTORY_QOS);\n    EXPECT_TRUE(STATISTICS_DATAWRITER_QOS.history().depth == 100);\n#endif \/\/ FASTDDS_STATISTICS\n}\n\n\/*\n * This test checks that STATISTICS_DATAREADER_QOS correctly sets the expected QoS.\n * 1. Reliability RELIABLE\n * 2. Durability TRANSIENT LOCAL\n * 3. History kind KEEP LAST\n * 4. History depth 100\n *\/\nTEST(StatisticsQosTests, StatisticsDataReaderQosTest)\n{\n    \/\/ TODO(jlbueno) Remove this guards after implementation. Here to prevent failures in current CI.\n#ifdef FASTDDS_STATISTICS\n    logError(STATISTICS_QOS_TEST, \"This test is going to fail because API is not yet implemented.\")\n\n    EXPECT_TRUE(STATISTICS_DATAREADER_QOS.reliability().kind == eprosima::fastdds::dds::RELIABLE_RELIABILITY_QOS);\n    EXPECT_TRUE(STATISTICS_DATAREADER_QOS.durability().kind == eprosima::fastdds::dds::TRANSIENT_LOCAL_DURABILITY_QOS);\n    EXPECT_TRUE(STATISTICS_DATAREADER_QOS.history().kind == eprosima::fastdds::dds::KEEP_LAST_HISTORY_QOS);\n    EXPECT_TRUE(STATISTICS_DATAREADER_QOS.history().depth == 100);\n#endif \/\/ FASTDDS_STATISTICS\n}\n\n} \/\/ namespace dds\n} \/\/ namespace statistics\n} \/\/ namespace fastdds\n} \/\/ namespace eprosima\n\nint main(\n        int argc,\n        char** argv)\n{\n    eprosima::fastdds::dds::Log::SetVerbosity(eprosima::fastdds::dds::Log::Error);\n\n    testing::InitGoogleTest(&argc, argv);\n    int ret = RUN_ALL_TESTS();\n\n    eprosima::fastdds::dds::Log::KillThread();\n    return ret;\n}\n","lang_cluster":"C++","length":87,"code_uid":"6e7f3fc5499b432d9e68655d23746cac"}
{"diff_hunk":"@@ -80,6 +80,13 @@ void ScriptingEnvironment::init_lua_state(lua_State *lua_state)\n         luabind::def(\"print\", LUA_print<std::string>),\n         luabind::def(\"durationIsValid\", durationIsValid),\n         luabind::def(\"parseDuration\", parseDuration),\n+        luabind::class_<SourceContainer>(\"sources\")\n+            .def(luabind::constructor<>())\n+            .def(\"load\", &SourceContainer::loadRasterSource)\n+            .def(\"query\", &SourceContainer::getRasterDataFromSource)\n+            .def(\"interpolate\", &SourceContainer::getRasterInterpolateFromSource),\n+        luabind::class_<const float>(\"constants\")\n+            .enum_(\"enums\")[luabind::value(\"precision\", COORDINATE_PRECISION)],\n \n         luabind::class_<std::vector<std::string>>(\"vector\")\n             .def(\"Add\", static_cast<void (std::vector<std::string>::*)(const std::string &)>(","old_code":"\/*\n\nCopyright (c) 2015, Project OSRM contributors\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list\nof conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this\nlist of conditions and the following disclaimer in the documentation and\/or\nother materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n*\/\n\n#include \"scripting_environment.hpp\"\n\n#include \"extraction_helper_functions.hpp\"\n#include \"extraction_node.hpp\"\n#include \"extraction_way.hpp\"\n#include \"..\/data_structures\/external_memory_node.hpp\"\n#include \"..\/util\/lua_util.hpp\"\n#include \"..\/util\/osrm_exception.hpp\"\n#include \"..\/util\/simple_logger.hpp\"\n#include \"..\/typedefs.h\"\n\n#include <luabind\/tag_function.hpp>\n\n#include <osmium\/osm.hpp>\n\n#include <sstream>\nnamespace\n{\n\/\/ wrapper method as luabind doesn't automatically overload funcs w\/ default parameters\ntemplate <class T>\nauto get_value_by_key(T const &object, const char *key) -> decltype(object.get_value_by_key(key))\n{\n    return object.get_value_by_key(key, \"\");\n}\n\nint lua_error_callback(lua_State *L) \/\/ This is so I can use my own function as an\n\/\/ exception handler, pcall_log()\n{\n    std::string error_msg = lua_tostring(L, -1);\n    std::ostringstream error_stream;\n    error_stream << error_msg;\n    throw osrm::exception(\"ERROR occured in profile script:\\n\" + error_stream.str());\n}\n}\n\nScriptingEnvironment::ScriptingEnvironment(const std::string &file_name) : file_name(file_name)\n{\n    SimpleLogger().Write() << \"Using script \" << file_name;\n}\n\nvoid ScriptingEnvironment::init_lua_state(lua_State *lua_state)\n{\n    typedef double (osmium::Location::*location_member_ptr_type)() const;\n\n    luabind::open(lua_state);\n    \/\/ open utility libraries string library;\n    luaL_openlibs(lua_state);\n\n    luaAddScriptFolderToLoadPath(lua_state, file_name.c_str());\n\n    \/\/ Add our function to the state's global scope\n    luabind::module(lua_state)[\n        luabind::def(\"print\", LUA_print<std::string>),\n        luabind::def(\"durationIsValid\", durationIsValid),\n        luabind::def(\"parseDuration\", parseDuration),\n\n        luabind::class_<std::vector<std::string>>(\"vector\")\n            .def(\"Add\", static_cast<void (std::vector<std::string>::*)(const std::string &)>(\n                            &std::vector<std::string>::push_back)),\n\n        luabind::class_<osmium::Location>(\"Location\")\n            .def<location_member_ptr_type>(\"lat\", &osmium::Location::lat)\n            .def<location_member_ptr_type>(\"lon\", &osmium::Location::lon),\n\n        luabind::class_<osmium::Node>(\"Node\")\n            \/\/ .def<node_member_ptr_type>(\"tags\", &osmium::Node::tags)\n            .def(\"location\", &osmium::Node::location)\n            .def(\"get_value_by_key\", &osmium::Node::get_value_by_key)\n            .def(\"get_value_by_key\", &get_value_by_key<osmium::Node>)\n            .def(\"id\", &osmium::Node::id),\n\n        luabind::class_<ExtractionNode>(\"ResultNode\")\n            .def_readwrite(\"traffic_lights\", &ExtractionNode::traffic_lights)\n            .def_readwrite(\"barrier\", &ExtractionNode::barrier),\n\n        luabind::class_<ExtractionWay>(\"ResultWay\")\n            \/\/ .def(luabind::constructor<>())\n            .def_readwrite(\"forward_speed\", &ExtractionWay::forward_speed)\n            .def_readwrite(\"backward_speed\", &ExtractionWay::backward_speed)\n            .def_readwrite(\"name\", &ExtractionWay::name)\n            .def_readwrite(\"roundabout\", &ExtractionWay::roundabout)\n            .def_readwrite(\"is_access_restricted\", &ExtractionWay::is_access_restricted)\n            .def_readwrite(\"duration\", &ExtractionWay::duration)\n            .property(\"forward_mode\", &ExtractionWay::get_forward_mode,\n                      &ExtractionWay::set_forward_mode)\n            .property(\"backward_mode\", &ExtractionWay::get_backward_mode,\n                      &ExtractionWay::set_backward_mode)\n            .enum_(\"constants\")[\n                luabind::value(\"notSure\", 0),\n                luabind::value(\"oneway\", 1),\n                luabind::value(\"bidirectional\", 2),\n                luabind::value(\"opposite\", 3)\n            ],\n        luabind::class_<osmium::Way>(\"Way\")\n            .def(\"get_value_by_key\", &osmium::Way::get_value_by_key)\n            .def(\"get_value_by_key\", &get_value_by_key<osmium::Way>)\n            .def(\"id\", &osmium::Way::id)\n    ];\n\n    if (0 != luaL_dofile(lua_state, file_name.c_str()))\n    {\n        luabind::object error_msg(luabind::from_stack(lua_state, -1));\n        std::ostringstream error_stream;\n        error_stream << error_msg;\n        throw osrm::exception(\"ERROR occured in profile script:\\n\" + error_stream.str());\n    }\n}\n\nlua_State *ScriptingEnvironment::get_lua_state()\n{\n    std::lock_guard<std::mutex> lock(init_mutex);\n    bool initialized = false;\n    auto &ref = script_contexts.local(initialized);\n    if (!initialized)\n    {\n        std::shared_ptr<lua_State> state(luaL_newstate(), lua_close);\n        ref = state;\n        init_lua_state(ref.get());\n    }\n    luabind::set_pcall_callback(&lua_error_callback);\n\n    return ref.get();\n}\n","lang_cluster":"C++","length":150,"code_uid":"b576dc9ac3964fe4b80cd92b8c7d0371"}
{"diff_hunk":"@@ -7,6 +7,7 @@\n #include \"base\/Base.h\"\n #include <gtest\/gtest.h>\n #include \"time\/Duration.h\"\n+#include <time.h>\n \n using nebula::time::Duration;\n ","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"base\/Base.h\"\n#include <gtest\/gtest.h>\n#include \"time\/Duration.h\"\n\nusing nebula::time::Duration;\n\nTEST(Duration, elapsedInSeconds) {\n    for (int i = 0; i < 5; i++) {\n        Duration dur;\n        auto start = std::chrono::steady_clock::now();\n        sleep(2);\n        auto diff = std::chrono::steady_clock::now() - start;\n        dur.pause();\n\n        ASSERT_EQ(std::chrono::duration_cast<std::chrono::seconds>(diff).count(),\n                  dur.elapsedInSec()) << \"Inaccuracy in iteration \" << i;\n    }\n}\n\n\nTEST(Duration, elapsedInMilliSeconds) {\n    Duration dur;\n    for (int i = 0; i < 200; i++) {\n        dur.reset();\n        auto start = std::chrono::steady_clock::now();\n        usleep(5000);   \/\/ Sleep for 5 ms\n        auto diff = std::chrono::steady_clock::now() - start;\n        dur.pause();\n\n        \/\/ Allow 1ms difference\n        ASSERT_LE(std::chrono::duration_cast<std::chrono::milliseconds>(diff).count(),\n                  dur.elapsedInMSec()) << \"Inaccuracy in iteration \" << i;\n        ASSERT_GE(std::chrono::duration_cast<std::chrono::milliseconds>(diff).count() + 1,\n                  dur.elapsedInMSec()) << \"Inaccuracy in iteration \" << i;\n    }\n}\n\n\nint main(int argc, char** argv) {\n    testing::InitGoogleTest(&argc, argv);\n    folly::init(&argc, &argv, true);\n    google::SetStderrLogging(google::INFO);\n\n    return RUN_ALL_TESTS();\n}\n\n","lang_cluster":"C++","length":52,"code_uid":"dd02aaea2b3440a985c003c946cc50c2"}
{"diff_hunk":"@@ -17,15 +17,19 @@ namespace nebula {\n namespace meta {\n \n using Status = cpp2::JobStatus;\n+using AdminCmd = nebula::cpp2::AdminCmd;\n+\n+int32_t JobDescription::minDataVer_ = 1;\n+int32_t JobDescription::currDataVer_ = 1;\n \n JobDescription::JobDescription(int32_t id,\n-                               std::string cmd,\n+                               nebula::cpp2::AdminCmd cmd,\n                                std::vector<std::string> paras,\n                                Status status,\n                                int64_t startTime,\n                                int64_t stopTime)\n                                : id_(id),\n-                                 cmd_(std::move(cmd)),\n+                                 cmd_(cmd),\n                                  paras_(std::move(paras)),\n                                  status_(status),\n                                  startTime_(startTime),","old_code":"\/* Copyright (c) 2019 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include <stdexcept>\n#include <string>\n#include <vector>\n#include <folly\/String.h>\n#include <boost\/stacktrace.hpp>\n#include \"meta\/processors\/jobMan\/JobUtils.h\"\n#include \"meta\/processors\/jobMan\/JobDescription.h\"\n\n#include \"kvstore\/KVIterator.h\"\nnamespace nebula {\nnamespace meta {\n\nusing Status = cpp2::JobStatus;\n\nJobDescription::JobDescription(int32_t id,\n                               std::string cmd,\n                               std::vector<std::string> paras,\n                               Status status,\n                               int64_t startTime,\n                               int64_t stopTime)\n                               : id_(id),\n                                 cmd_(std::move(cmd)),\n                                 paras_(std::move(paras)),\n                                 status_(status),\n                                 startTime_(startTime),\n                                 stopTime_(stopTime) {}\n\nfolly::Optional<JobDescription>\nJobDescription::makeJobDescription(folly::StringPiece rawkey,\n                                   folly::StringPiece rawval) {\n    try {\n        if (!isJobKey(rawkey)) {\n            return folly::none;\n        }\n        auto key = parseKey(rawkey);\n        auto tup = parseVal(rawval);\n\n        auto cmd = std::get<0>(tup);\n        auto paras = std::get<1>(tup);\n        for (auto p : paras) {\n            LOG(INFO) << \"p = \" << p;\n        }\n        auto status = std::get<2>(tup);\n        auto startTime = std::get<3>(tup);\n        auto stopTime = std::get<4>(tup);\n        return JobDescription(key, cmd, paras, status, startTime, stopTime);\n    } catch(std::exception& ex) {\n        LOG(ERROR) << ex.what();\n    }\n    return folly::none;\n}\n\nstd::string JobDescription::jobKey() const {\n    return makeJobKey(id_);\n}\n\nstd::string JobDescription::makeJobKey(int32_t iJob) {\n    std::string str;\n    str.reserve(32);\n    str.append(reinterpret_cast<const char*>(JobUtil::jobPrefix().data()),\n                                             JobUtil::jobPrefix().size());\n    str.append(reinterpret_cast<const char*>(&iJob), sizeof(int32_t));\n    return str;\n}\n\nint32_t JobDescription::parseKey(const folly::StringPiece& rawKey) {\n    auto offset = JobUtil::jobPrefix().size();\n    return *reinterpret_cast<const int32_t*>(rawKey.begin() + offset);\n}\n\nstd::string JobDescription::jobVal() const {\n    std::string str;\n    auto cmdLen = cmd_.length();\n    auto paraSize = paras_.size();\n    str.reserve(256);\n    str.append(reinterpret_cast<const char*>(&cmdLen), sizeof(size_t));\n    str.append(reinterpret_cast<const char*>(cmd_.data()), cmd_.length());\n    str.append(reinterpret_cast<const char*>(&paraSize), sizeof(size_t));\n    for (auto& para : paras_) {\n        auto len = para.length();\n        str.append(reinterpret_cast<const char*>(&len), sizeof(len));\n        str.append(reinterpret_cast<const char*>(&para[0]), len);\n    }\n    str.append(reinterpret_cast<const char*>(&status_), sizeof(Status));\n    str.append(reinterpret_cast<const char*>(&startTime_), sizeof(int64_t));\n    str.append(reinterpret_cast<const char*>(&stopTime_), sizeof(int64_t));\n    return str;\n}\n\nstd::tuple<std::string,\n           std::vector<std::string>,\n           Status,\n           int64_t,\n           int64_t>\nJobDescription::parseVal(const folly::StringPiece& rawVal) {\n    size_t offset = 0;\n\n    std::string cmd = JobUtil::parseString(rawVal, offset);\n    offset += sizeof(size_t) + cmd.length();\n\n    std::vector<std::string> paras = JobUtil::parseStrVector(rawVal, &offset);\n\n    auto status = JobUtil::parseFixedVal<Status>(rawVal, offset);\n    offset += sizeof(Status);\n\n    auto tStart = JobUtil::parseFixedVal<int64_t>(rawVal, offset);\n    offset += sizeof(int64_t);\n\n    auto tStop = JobUtil::parseFixedVal<int64_t>(rawVal, offset);\n\n    return std::make_tuple(cmd, paras, status, tStart, tStop);\n}\n\ncpp2::JobDesc JobDescription::toJobDesc() {\n    cpp2::JobDesc ret;\n    ret.set_id(id_);\n    ret.set_cmd(cmd_);\n    ret.set_paras(paras_);\n    ret.set_status(status_);\n    ret.set_start_time(startTime_);\n    ret.set_stop_time(stopTime_);\n    return ret;\n}\n\nstd::string JobDescription::archiveKey() {\n    std::string str;\n    str.reserve(32);\n    str.append(reinterpret_cast<const char*>(JobUtil::archivePrefix().data()),\n                                             JobUtil::archivePrefix().size());\n    str.append(reinterpret_cast<const char*>(&id_), sizeof(id_));\n    return str;\n}\n\nbool JobDescription::setStatus(Status newStatus) {\n    if (JobStatus::laterThan(status_, newStatus)) {\n        return false;\n    }\n    status_ = newStatus;\n    if (newStatus == Status::RUNNING) {\n        startTime_ = std::time(nullptr);\n    }\n    if (JobStatus::laterThan(newStatus, Status::RUNNING)) {\n        stopTime_ = std::time(nullptr);\n    }\n    return true;\n}\n\nbool JobDescription::isJobKey(const folly::StringPiece& rawKey) {\n    return rawKey.size() == JobUtil::jobPrefix().length() + sizeof(int32_t);\n}\n\nfolly::Optional<JobDescription>\nJobDescription::loadJobDescription(int32_t iJob, nebula::kvstore::KVStore* kv) {\n    auto key = makeJobKey(iJob);\n    std::string val;\n    auto rc = kv->get(0, 0, key, &val);\n    if (rc != nebula::kvstore::SUCCEEDED) {\n        return folly::none;\n    }\n    return makeJobDescription(key, val);\n}\n\n}  \/\/ namespace meta\n}  \/\/ namespace nebula\n\n","lang_cluster":"C++","length":171,"code_uid":"7c15a5b2fe9248a9abb1b91597d31a43"}
{"diff_hunk":"@@ -21,10 +21,9 @@ typedef boost::tokenizer<boost::char_separator<char>> tokenizer;\n namespace RDKit {\n namespace {\n \n-MolStandardize::TautomerTransform* getTautomer(const std::string& name,\n-                                               const std::string& smarts,\n-                                               const std::string& bond_str,\n-                                               const std::string& charge_str) {\n+std::unique_ptr<MolStandardize::TautomerTransform> getTautomer(\n+    const std::string& name, const std::string& smarts,\n+    const std::string& bond_str, const std::string& charge_str) {\n   std::vector<Bond::BondType> bond_types =\n       MolStandardize::stringToBondType(bond_str);\n   std::vector<int> charges = MolStandardize::stringToCharge(charge_str);","old_code":"\/\/\n\/\/  Copyright (C) 2018-2021 Susan H. Leung and other RDKit contributors\n\/\/\n\/\/   @@ All Rights Reserved @@\n\/\/  This file is part of the RDKit.\n\/\/  The contents are covered by the terms of the BSD license\n\/\/  which is included in the file license.txt, found at the root\n\/\/  of the RDKit source tree.\n\/\/\n#include \"TautomerCatalogUtils.h\"\n#include <RDGeneral\/BadFileException.h>\n#include <boost\/tokenizer.hpp>\n#include <GraphMol\/SmilesParse\/SmilesParse.h>\n#include <GraphMol\/SmilesParse\/SmilesWrite.h>\n#include <boost\/algorithm\/string.hpp>\n#include <boost\/tokenizer.hpp>\ntypedef boost::tokenizer<boost::char_separator<char>> tokenizer;\n#include <fstream>\n#include <string>\n\nnamespace RDKit {\nnamespace {\n\nMolStandardize::TautomerTransform* getTautomer(const std::string& name,\n                                               const std::string& smarts,\n                                               const std::string& bond_str,\n                                               const std::string& charge_str) {\n  std::vector<Bond::BondType> bond_types =\n      MolStandardize::stringToBondType(bond_str);\n  std::vector<int> charges = MolStandardize::stringToCharge(charge_str);\n\n  ROMol* tautomer = SmartsToMol(smarts);\n  if (!tautomer) {\n    throw ValueErrorException(\"cannot parse tautomer SMARTS: \" + smarts);\n  }\n  tautomer->setProp(common_properties::_Name, name);\n  return new MolStandardize::TautomerTransform(tautomer, bond_types, charges);\n}\n\nMolStandardize::TautomerTransform* getTautomer(const std::string& tmpStr) {\n  if (tmpStr.length() == 0 || tmpStr.substr(0, 2) == \"\/\/\") {\n    \/\/ empty or comment line\n    return nullptr;\n  }\n  boost::char_separator<char> tabSep(\"\\t\");\n  tokenizer tokens(tmpStr, tabSep);\n  std::vector<std::string> result(tokens.begin(), tokens.end());\n\n  \/\/ tautomer information to collect from each line\n  std::string name;\n  std::string smarts;\n  std::string bond_str;\n  std::string charge_str;\n\n  \/\/ line must have at least two tab separated values\n  if (result.size() < 2) {\n    BOOST_LOG(rdWarningLog) << \"Invalid line: \" << tmpStr << std::endl;\n    return nullptr;\n  }\n  \/\/ line only has name and smarts\n  if (result.size() == 2) {\n    name = result[0];\n    smarts = result[1];\n  }\n  \/\/ line has name, smarts, bonds\n  if (result.size() == 3) {\n    name = result[0];\n    smarts = result[1];\n    bond_str = result[2];\n  }\n  \/\/ line has name, smarts, bonds, charges\n  if (result.size() == 4) {\n    name = result[0];\n    smarts = result[1];\n    bond_str = result[2];\n    charge_str = result[3];\n  }\n\n  boost::erase_all(smarts, \" \");\n  boost::erase_all(name, \" \");\n  boost::erase_all(bond_str, \" \");\n  boost::erase_all(charge_str, \" \");\n\n  return getTautomer(name, smarts, bond_str, charge_str);\n}\n}  \/\/ namespace\n\nnamespace MolStandardize {\n\nstd::vector<Bond::BondType> stringToBondType(std::string bond_str) {\n  std::vector<Bond::BondType> bonds;\n  for (const auto& c : bond_str) {\n    switch (c) {\n      case '-':\n        bonds.push_back(Bond::SINGLE);\n        break;\n      case '=':\n        bonds.push_back(Bond::DOUBLE);\n        break;\n      case '#':\n        bonds.push_back(Bond::TRIPLE);\n        break;\n      case ':':\n        bonds.push_back(Bond::AROMATIC);\n        break;\n    }\n  }\n  return bonds;\n}\n\nstd::vector<int> stringToCharge(std::string charge_str) {\n  std::vector<int> charges;\n  for (const auto& c : charge_str) {\n    switch (c) {\n      case '+':\n        charges.push_back(1);\n        break;\n      case '0':\n        charges.push_back(0);\n        break;\n      case '-':\n        charges.push_back(-1);\n        break;\n      default:\n        throw ValueErrorException(\"Charge symbol not recognised.\");\n    }\n  }\n  return charges;\n}\n\nstd::vector<TautomerTransform> readTautomers(std::string fileName) {\n  std::ifstream inStream(fileName.c_str());\n  if ((!inStream) || (inStream.bad())) {\n    std::ostringstream errout;\n    errout << \"Bad input file \" << fileName;\n    throw BadFileException(errout.str());\n  }\n  std::vector<TautomerTransform> tautomers = readTautomers(inStream);\n  return tautomers;\n}\n\nstd::vector<TautomerTransform> readTautomers(std::istream& inStream,\n                                             int nToRead) {\n  if (inStream.bad()) {\n    throw BadFileException(\"Bad stream contents.\");\n  }\n  std::vector<TautomerTransform> tautomers;\n  if (nToRead > 0) {\n    tautomers.reserve(nToRead);\n  }\n  const int MAX_LINE_LEN = 512;\n  char inLine[MAX_LINE_LEN];\n  std::string tmpstr;\n  int nRead = 0;\n  while (!inStream.eof() && !inStream.fail() &&\n         (nToRead < 0 || nRead < nToRead)) {\n    inStream.getline(inLine, MAX_LINE_LEN, '\\n');\n    tmpstr = inLine;\n    \/\/ parse the tautomer on this line (if there is one)\n    TautomerTransform* transform = getTautomer(tmpstr);\n    if (transform) {\n      tautomers.emplace_back(*transform);\n      delete transform;\n      nRead++;\n    }\n  }\n\n  return tautomers;\n}\n\nstd::vector<TautomerTransform> readTautomers(\n    const std::vector<\n        std::tuple<std::string, std::string, std::string, std::string>>& data) {\n  std::vector<TautomerTransform> tautomers;\n  for (const auto& tpl : data) {\n    auto transform = getTautomer(std::get<0>(tpl), std::get<1>(tpl),\n                                 std::get<2>(tpl), std::get<3>(tpl));\n    if (transform) {\n      tautomers.emplace_back(*transform);\n    }\n  }\n  return tautomers;\n}\n\n}  \/\/ namespace MolStandardize\n}  \/\/ namespace RDKit\n","lang_cluster":"C++","length":186,"code_uid":"3e2f3b74d1b547228be43a14c7d2a50d"}
{"diff_hunk":"@@ -108,7 +108,7 @@ int main(int argc, char *argv[]) {\n     }\n \n   } catch (exception& e) {\n-    if (options::get()->has_bool(\"stack_trace_to_file\")) {\n+    if (options::get()->get_bool(\"stack_trace_to_file\")) {\n       std::ostringstream ss(\"stack_trace\");\n       const auto& rank = get_rank_in_world();\n       if (rank >= 0) {","old_code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/ Copyright (c) 2014-2016, Lawrence Livermore National Security, LLC.\n\/\/ Produced at the Lawrence Livermore National Laboratory.\n\/\/ Written by the LBANN Research Team (B. Van Essen, et al.) listed in\n\/\/ the CONTRIBUTORS file. <lbann-dev@llnl.gov>\n\/\/\n\/\/ LLNL-CODE-697807.\n\/\/ All rights reserved.\n\/\/\n\/\/ This file is part of LBANN: Livermore Big Artificial Neural Network\n\/\/ Toolkit. For details, see http:\/\/software.llnl.gov\/LBANN or\n\/\/ https:\/\/github.com\/LLNL\/LBANN.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"Licensee\"); you\n\/\/ may not use this file except in compliance with the License.  You may\n\/\/ obtain a copy of the License at:\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n\/\/ implied. See the License for the specific language governing\n\/\/ permissions and limitations under the license.\n\/\/\n\/\/ lbann_proto.cpp - prototext application\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n#include \"lbann\/lbann.hpp\"\n#include \"lbann\/proto\/proto_common.hpp\"\n#include \"lbann\/utils\/protobuf_utils.hpp\"\n#include \"lbann\/data_store\/generic_data_store.hpp\"\n#include <cstdlib>\n\n\nusing namespace lbann;\n\nint main(int argc, char *argv[]) {\n  int random_seed = lbann_default_random_seed;\n  world_comm_ptr comm = initialize(argc, argv, random_seed);\n  const bool master = comm->am_world_master();\n\n  if (master) {\n    std::cout << \"\\n\\n==============================================================\\n\"\n              << \"STARTING lbann with this command line:\\n\";\n    for (int j=0; j<argc; j++) {\n      std::cout << argv[j] << \" \";\n    }\n    std::cout << std::endl << std::endl;\n  }\n\n  try {\n    \/\/ Initialize options db (this parses the command line)\n    options *opts = options::get();\n    opts->init(argc, argv);\n    if (opts->has_string(\"h\") or opts->has_string(\"help\") or argc == 1) {\n      print_help(*comm);\n      return EXIT_SUCCESS;\n    }\n\n    \/\/this must be called after call to opts->init();\n    if (!opts->has_bool(\"disable_signal_handler\")) {\n      std::string file_base = (opts->has_bool(\"stack_trace_to_file\") ?\n                               \"stack_trace\" : \"\");\n      stack_trace::register_signal_handler(file_base);\n    }\n\n    \/\/to activate, must specify --st_on on cmd line\n    stack_profiler::get()->activate(comm->get_rank_in_world());\n\n    \/\/ Initalize a global I\/O thread pool\n    std::shared_ptr<thread_pool> io_thread_pool = construct_io_thread_pool(comm.get());\n\n    auto pbs = protobuf_utils::load_prototext(master, argc, argv);\n    lbann_data::LbannPB pb = *(pbs[0]);\n\n    lbann_data::Model *pb_model = pb.mutable_model();\n\n    auto model = build_model_from_prototext(argc, argv, pb,\n                                            comm.get(), io_thread_pool, true);\n\n    if (opts->has_string(\"create_tarball\")) {\n      return EXIT_SUCCESS;\n    }\n\n    if (! (opts->has_bool(\"exit_after_setup\") && opts->get_bool(\"exit_after_setup\"))) {\n\n      \/\/ Train model\n      model->train(pb_model->num_epochs());\n\n      \/\/ Evaluate model on test set\n      model->evaluate(execution_mode::testing);\n\n      \/\/has no affect unless option: --st_on was given\n      stack_profiler::get()->print();\n\n    } else {\n      if (comm->am_world_master()) {\n        std::cout <<\n          \"--------------------------------------------------------------------------------\\n\"\n          \"ALERT: model has been setup; we are now exiting due to command\\n\"\n          \"       line option: --exit_after_setup\\n\"\n          \"--------------------------------------------------------------------------------\\n\";\n      }\n\n      \/\/has no affect unless option: --st_on was given\n      stack_profiler::get()->print();\n    }\n\n  } catch (exception& e) {\n    if (options::get()->has_bool(\"stack_trace_to_file\")) {\n      std::ostringstream ss(\"stack_trace\");\n      const auto& rank = get_rank_in_world();\n      if (rank >= 0) {\n        ss << \"_rank\" << rank;\n      }\n      ss << \".txt\";\n      std::ofstream fs(ss.str());\n      e.print_report(fs);\n    }\n    El::ReportException(e);\n    return EXIT_FAILURE;\n  } catch (std::exception& e) {\n    El::ReportException(e);\n    return EXIT_FAILURE;\n  }\n\n  return EXIT_SUCCESS;\n}\n","lang_cluster":"C++","length":129,"code_uid":"26b12a2d975a4496a031528581b023cd"}
{"diff_hunk":"@@ -61,7 +61,8 @@ ReceiverResource::ReceiverResource(ReceiverResource&& rValueResource)\n     max_message_size_ = rValueResource.max_message_size_;\n }\n \n-bool ReceiverResource::SupportsLocator(const Locator_t& localLocator)\n+bool ReceiverResource::SupportsLocator(\n+        const Locator_t& localLocator)\n {\n     if (LocatorMapsToManagedChannel)\n     {","old_code":"\/\/ Copyright 2016 Proyectos y Sistemas de Mantenimiento SL (eProsima).\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\n#include <fastdds\/rtps\/network\/ReceiverResource.h>\n#include <fastdds\/rtps\/messages\/MessageReceiver.h>\n#include <cassert>\n#include <fastdds\/dds\/log\/Log.hpp>\n\n#define IDSTRING \"(ID:\" << std::this_thread::get_id() <<\") \"<<\n\nusing namespace std;\nusing namespace eprosima::fastdds::rtps;\n\nnamespace eprosima{\nnamespace fastrtps{\nnamespace rtps{\n\nReceiverResource::ReceiverResource(\n\t\t\tTransportInterface& transport,\n\t\t\tconst Locator_t& locator,\n\t\t\tuint32_t max_recv_buffer_size)\n        : Cleanup(nullptr)\n        , LocatorMapsToManagedChannel(nullptr)\n        , mValid(false)\n        , mtx()\n        , receiver(nullptr)\n        , max_message_size_(max_recv_buffer_size)\n{\n    \/\/ Internal channel is opened and assigned to this resource.\n    mValid = transport.OpenInputChannel(locator, this, max_message_size_);\n    if (!mValid)\n    {\n        return; \/\/ Invalid resource to be discarded by the factory.\n    }\n\n    \/\/ Implementation functions are bound to the right transport parameters\n    Cleanup = [&transport, locator]() { transport.CloseInputChannel(locator); };\n    LocatorMapsToManagedChannel = [&transport, locator](const Locator_t& locatorToCheck) -> bool\n    { return transport.DoInputLocatorsMatch(locator, locatorToCheck); };\n}\n\nReceiverResource::ReceiverResource(ReceiverResource&& rValueResource)\n{\n    Cleanup.swap(rValueResource.Cleanup);\n    LocatorMapsToManagedChannel.swap(rValueResource.LocatorMapsToManagedChannel);\n    receiver = rValueResource.receiver;\n    rValueResource.receiver = nullptr;\n    mValid = rValueResource.mValid;\n    rValueResource.mValid = false;\n    max_message_size_ = rValueResource.max_message_size_;\n}\n\nbool ReceiverResource::SupportsLocator(const Locator_t& localLocator)\n{\n    if (LocatorMapsToManagedChannel)\n    {\n        return LocatorMapsToManagedChannel(localLocator);\n    }\n    return false;\n}\n\nvoid ReceiverResource::RegisterReceiver(MessageReceiver* rcv)\n{\n    std::unique_lock<std::mutex> lock(mtx);\n    if (receiver == nullptr)\n        receiver = rcv;\n}\n\nvoid ReceiverResource::UnregisterReceiver(MessageReceiver* rcv)\n{\n    std::unique_lock<std::mutex> lock(mtx);\n    if (receiver == rcv)\n        receiver = nullptr;\n}\n\nvoid ReceiverResource::OnDataReceived(const octet * data, const uint32_t size,\n    const Locator_t & localLocator, const Locator_t & remoteLocator)\n{\n    (void)localLocator;\n\n    std::unique_lock<std::mutex> lock(mtx);\n    MessageReceiver* rcv = receiver;\n\n    if (rcv != nullptr)\n    {\n        CDRMessage_t msg(0);\n        msg.wraps = true;\n        msg.buffer = const_cast<octet*>(data);\n        msg.length = size;\n        msg.max_size = size;\n        msg.reserved_size = size;\n\n        \/\/ TODO: Should we unlock in case UnregisterReceiver is called from callback ?\n        rcv->processCDRMsg(remoteLocator, &msg);\n    }\n\n}\n\nvoid ReceiverResource::disable()\n{\n    if (Cleanup)\n    {\n        Cleanup();\n    }\n}\n\nReceiverResource::~ReceiverResource()\n{\n}\n\n} \/\/ namespace rtps\n} \/\/ namespace fastrtps\n} \/\/ namespace eprosima\n","lang_cluster":"C++","length":124,"code_uid":"2b14f84f6063460296355c519ffd9448"}
{"diff_hunk":"@@ -23,6 +23,8 @@ DEFINE_string(peers, \"\", \"It is a list of IPs split by comma,\"\n                          \"the ips number equals replica number.\"\n                          \"If empty, it means replica is 1\");\n DEFINE_string(local_ip, \"\", \"Local ip speicified for NetworkUtils::getLocalIP\");\n+DEFINE_int32(num_workers, 4, \"Number of worker threads\");\n+DEFINE_int32(num_io_threads, 16, \"Number of IO threads\");\n DECLARE_string(part_man_type);\n \n DEFINE_string(pid_file, \"pids\/nebula-metad.pid\", \"File to hold the process id\");","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"base\/Base.h\"\n#include <thrift\/lib\/cpp2\/server\/ThriftServer.h>\n#include \"meta\/MetaServiceHandler.h\"\n#include \"meta\/MetaHttpHandler.h\"\n#include \"webservice\/WebService.h\"\n#include \"network\/NetworkUtils.h\"\n#include \"process\/ProcessUtils.h\"\n#include \"kvstore\/PartManager.h\"\n\nusing nebula::ProcessUtils;\nusing nebula::Status;\n\nDEFINE_int32(port, 45500, \"Meta daemon listening port\");\nDEFINE_bool(reuse_port, true, \"Whether to turn on the SO_REUSEPORT option\");\nDEFINE_string(data_path, \"\", \"Root data path\");\nDEFINE_string(peers, \"\", \"It is a list of IPs split by comma,\"\n                         \"the ips number equals replica number.\"\n                         \"If empty, it means replica is 1\");\nDEFINE_string(local_ip, \"\", \"Local ip speicified for NetworkUtils::getLocalIP\");\nDECLARE_string(part_man_type);\n\nDEFINE_string(pid_file, \"pids\/nebula-metad.pid\", \"File to hold the process id\");\nDEFINE_bool(daemonize, true, \"Whether run as a daemon process\");\n\nstatic std::unique_ptr<apache::thrift::ThriftServer> gServer;\n\nstatic void signalHandler(int sig);\nstatic Status setupSignalHandler();\n\nint main(int argc, char *argv[]) {\n    folly::init(&argc, &argv, true);\n    if (FLAGS_data_path.empty()) {\n        LOG(ERROR) << \"Meta Data Path should not empty\";\n        return EXIT_FAILURE;\n    }\n\n    if (FLAGS_daemonize) {\n        google::SetStderrLogging(google::FATAL);\n    } else {\n        google::SetStderrLogging(google::INFO);\n    }\n\n    \/\/ Detect if the server has already been started\n    auto pidPath = FLAGS_pid_file;\n    auto status = ProcessUtils::isPidAvailable(pidPath);\n    if (!status.ok()) {\n        LOG(ERROR) << status;\n        return EXIT_FAILURE;\n    }\n\n    if (FLAGS_daemonize) {\n        status = ProcessUtils::daemonize(pidPath);\n        if (!status.ok()) {\n            LOG(ERROR) << status;\n            return EXIT_FAILURE;\n        }\n    } else {\n        status = ProcessUtils::makePidFile(pidPath);\n        if (!status.ok()) {\n            LOG(ERROR) << status;\n            return EXIT_FAILURE;\n        }\n    }\n\n    LOG(INFO) << \"Starting Meta HTTP Service\";\n    nebula::WebService::registerHandler(\"\/status\", [] {\n        return new nebula::meta::MetaHttpHandler();\n    });\n    status = nebula::WebService::start();\n    if (!status.ok()) {\n        LOG(ERROR) << \"Failed to start web service: \" << status;\n        return EXIT_FAILURE;\n    }\n\n    auto result = nebula::network::NetworkUtils::getLocalIP(FLAGS_local_ip);\n    if (!result.ok()) {\n        LOG(ERROR) << \"Get local ip failed! status:\" << result.status();\n        return EXIT_FAILURE;\n    }\n    auto hostAddrRet = nebula::network::NetworkUtils::toHostAddr(result.value(), FLAGS_port);\n    if (!hostAddrRet.ok()) {\n        LOG(ERROR) << \"Bad local host addr, status:\" << hostAddrRet.status();\n        return EXIT_FAILURE;\n    }\n    auto& localHost = hostAddrRet.value();\n\n    auto peersRet = nebula::network::NetworkUtils::toHosts(FLAGS_peers);\n    if (!peersRet.ok()) {\n        LOG(ERROR) << \"Can't get peers address, status:\" << peersRet.status();\n        return EXIT_FAILURE;\n    }\n    \/\/ Setup the signal handlers\n    status = setupSignalHandler();\n    if (!status.ok()) {\n        LOG(ERROR) << status;\n        return EXIT_FAILURE;\n    }\n\n    auto partMan\n        = std::make_unique<nebula::kvstore::MemPartManager>();\n    \/\/ The meta server has only one space, one part.\n    partMan->addPart(0, 0, std::move(peersRet.value()));\n\n    nebula::kvstore::KVOptions options;\n    options.local_ = localHost;\n    options.dataPaths_ = {FLAGS_data_path};\n    options.partMan_ = std::move(partMan);\n    std::unique_ptr<nebula::kvstore::KVStore> kvstore(\n            nebula::kvstore::KVStore::instance(std::move(options)));\n\n    auto handler = std::make_shared<nebula::meta::MetaServiceHandler>(kvstore.get());\n\n    nebula::operator<<(operator<<(LOG(INFO), \"The meta deamon start on \"), localHost);\n    try {\n        gServer = std::make_unique<apache::thrift::ThriftServer>();\n        gServer->setInterface(std::move(handler));\n        gServer->setPort(FLAGS_port);\n        gServer->setReusePort(FLAGS_reuse_port);\n        gServer->setIdleTimeout(std::chrono::seconds(0));  \/\/ No idle timeout on client connection\n        gServer->serve();  \/\/ Will wait until the server shuts down\n    } catch (const std::exception &e) {\n        LOG(ERROR) << \"Exception thrown: \" << e.what();\n        return EXIT_FAILURE;\n    }\n\n    LOG(INFO) << \"The meta Daemon stopped\";\n}\n\n\nStatus setupSignalHandler() {\n    ::signal(SIGPIPE, SIG_IGN);\n    ::signal(SIGINT, signalHandler);\n    ::signal(SIGTERM, signalHandler);\n    return Status::OK();\n}\n\n\nvoid signalHandler(int sig) {\n    switch (sig) {\n        case SIGINT:\n        case SIGTERM:\n            FLOG_INFO(\"Signal %d(%s) received, stopping this server\", sig, ::strsignal(sig));\n            nebula::WebService::stop();\n            gServer->stop();\n            break;\n        default:\n            FLOG_ERROR(\"Signal %d(%s) received but ignored\", sig, ::strsignal(sig));\n    }\n}\n","lang_cluster":"C++","length":155,"code_uid":"c6b25412b4ec4f2e9c9633eb758612eb"}
{"diff_hunk":"@@ -112,6 +112,26 @@ std::string NebulaKeyUtils::edgePrefix(PartitionID partId, VertexID srcId, EdgeT\n     return key;\n }\n \n+\/\/ static\n+std::string NebulaKeyUtils::edgePrefix(PartitionID partId,\n+                                       VertexID srcId,\n+                                       EdgeType type,\n+                                       EdgeRanking rank,\n+                                       VertexID dstId) {\n+    type |= kEdgeMaskSet;\n+    int32_t item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);\n+    std::string key;\n+    key.reserve(sizeof(PartitionID) + sizeof(VertexID)\n+                + sizeof(EdgeType) + sizeof(VertexID)\n+                + sizeof(EdgeRanking));\n+    key.append(reinterpret_cast<const char*>(&item), sizeof(PartitionID))\n+            .append(reinterpret_cast<const char*>(&srcId), sizeof(VertexID))\n+            .append(reinterpret_cast<const char*>(&type), sizeof(EdgeType))\n+            .append(reinterpret_cast<const char*>(&rank), sizeof(EdgeRanking))\n+            .append(reinterpret_cast<const char*>(&dstId), sizeof(VertexID));\n+    return key;\n+}\n+\n \/\/ static\n std::string NebulaKeyUtils::prefix(PartitionID partId) {\n     PartitionID item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"base\/NebulaKeyUtils.h\"\n\nnamespace nebula {\n\n\/\/ static\nstd::string NebulaKeyUtils::vertexKey(PartitionID partId, VertexID vId,\n                                      TagID tagId, TagVersion tv) {\n    tagId &= kTagMaskSet;\n    int32_t item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);\n\n    std::string key;\n    key.reserve(kVertexLen);\n    key.append(reinterpret_cast<const char*>(&item), sizeof(int32_t))\n       .append(reinterpret_cast<const char*>(&vId), sizeof(VertexID))\n       .append(reinterpret_cast<const char*>(&tagId), sizeof(TagID))\n       .append(reinterpret_cast<const char*>(&tv), sizeof(TagVersion));\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::edgeKey(PartitionID partId,\n                                    VertexID srcId,\n                                    EdgeType type,\n                                    EdgeRanking rank,\n                                    VertexID dstId,\n                                    EdgeVersion ev) {\n    type |= kEdgeMaskSet;\n    int32_t item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);\n\n    std::string key;\n    key.reserve(kEdgeLen);\n    key.append(reinterpret_cast<const char*>(&item), sizeof(PartitionID))\n       .append(reinterpret_cast<const char*>(&srcId), sizeof(VertexID))\n       .append(reinterpret_cast<const char*>(&type), sizeof(EdgeType))\n       .append(reinterpret_cast<const char*>(&rank), sizeof(EdgeRanking))\n       .append(reinterpret_cast<const char*>(&dstId), sizeof(VertexID))\n       .append(reinterpret_cast<const char*>(&ev), sizeof(EdgeVersion));\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::systemCommitKey(PartitionID partId) {\n    int32_t item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kSystem);\n    uint32_t type = static_cast<uint32_t>(NebulaSystemKeyType::kSystemCommit);\n    std::string key;\n    key.reserve(kSystemLen);\n    key.append(reinterpret_cast<const char*>(&item), sizeof(PartitionID))\n       .append(reinterpret_cast<const char*>(&type), sizeof(NebulaSystemKeyType));\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::systemPartKey(PartitionID partId) {\n    uint32_t item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kSystem);\n    uint32_t type = static_cast<uint32_t>(NebulaSystemKeyType::kSystemPart);\n    std::string key;\n    key.reserve(kSystemLen);\n    key.append(reinterpret_cast<const char*>(&item), sizeof(PartitionID))\n       .append(reinterpret_cast<const char*>(&type), sizeof(NebulaSystemKeyType));\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::uuidKey(PartitionID partId, const folly::StringPiece& name) {\n    std::string key;\n    key.reserve(sizeof(PartitionID) + name.size());\n    int32_t item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kUUID);\n    key.append(reinterpret_cast<const char*>(&item), sizeof(int32_t))\n       .append(name.data(), name.size());\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::kvKey(PartitionID partId, const folly::StringPiece& name) {\n    std::string key;\n    key.reserve(sizeof(PartitionID) + name.size());\n    int32_t item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);\n    key.append(reinterpret_cast<const char*>(&item), sizeof(int32_t))\n       .append(name.data(), name.size());\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::vertexPrefix(PartitionID partId, VertexID vId, TagID tagId) {\n    tagId &= kTagMaskSet;\n    PartitionID item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);\n\n    std::string key;\n    key.reserve(kVertexLen);\n    key.append(reinterpret_cast<const char*>(&item), sizeof(PartitionID))\n       .append(reinterpret_cast<const char*>(&vId), sizeof(VertexID))\n       .append(reinterpret_cast<const char*>(&tagId), sizeof(TagID));\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::edgePrefix(PartitionID partId, VertexID srcId, EdgeType type) {\n    type |= kEdgeMaskSet;\n    PartitionID item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);\n\n    std::string key;\n    key.reserve(sizeof(PartitionID) + sizeof(VertexID) + sizeof(EdgeType));\n    key.append(reinterpret_cast<const char*>(&item), sizeof(PartitionID))\n       .append(reinterpret_cast<const char*>(&srcId), sizeof(VertexID))\n       .append(reinterpret_cast<const char*>(&type), sizeof(EdgeType));\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::prefix(PartitionID partId) {\n    PartitionID item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);\n    std::string key;\n    key.reserve(sizeof(PartitionID));\n    key.append(reinterpret_cast<const char*>(&item), sizeof(PartitionID));\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::vertexPrefix(PartitionID partId, VertexID vId) {\n    PartitionID item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);\n    std::string key;\n    key.reserve(sizeof(PartitionID) + sizeof(VertexID));\n    key.append(reinterpret_cast<const char*>(&item), sizeof(PartitionID))\n       .append(reinterpret_cast<const char*>(&vId), sizeof(VertexID));\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::edgePrefix(PartitionID partId, VertexID vId) {\n    PartitionID item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);\n    std::string key;\n    key.reserve(sizeof(PartitionID) + sizeof(VertexID));\n    key.append(reinterpret_cast<const char*>(&item), sizeof(PartitionID))\n       .append(reinterpret_cast<const char*>(&vId), sizeof(VertexID));\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::systemPrefix() {\n    int8_t type = static_cast<uint32_t>(NebulaKeyType::kSystem);\n    std::string key;\n    key.reserve(sizeof(int8_t));\n    key.append(reinterpret_cast<const char*>(&type), sizeof(int8_t));\n    return key;\n}\n\n\/\/ static\nstd::string NebulaKeyUtils::prefix(PartitionID partId, VertexID src, EdgeType type,\n                                   EdgeRanking ranking, VertexID dst) {\n    type |= kEdgeMaskSet;\n    PartitionID item = (partId << kPartitionOffset) | static_cast<uint32_t>(NebulaKeyType::kData);\n\n    std::string key;\n    key.reserve(sizeof(PartitionID) + sizeof(VertexID) + sizeof(EdgeType)\n                + sizeof(VertexID) + sizeof(EdgeRanking));\n    key.append(reinterpret_cast<const char*>(&item), sizeof(PartitionID))\n       .append(reinterpret_cast<const char*>(&src), sizeof(VertexID))\n       .append(reinterpret_cast<const char*>(&type), sizeof(EdgeType))\n       .append(reinterpret_cast<const char*>(&ranking), sizeof(EdgeRanking))\n       .append(reinterpret_cast<const char*>(&dst), sizeof(VertexID));\n    return key;\n}\n\n}  \/\/ namespace nebula\n\n","lang_cluster":"C++","length":171,"code_uid":"277ef8ef30bc44c782451a2570131e63"}
{"diff_hunk":"@@ -137,8 +137,7 @@ TopologicalTorsionEnvGenerator<OutputType>::getEnvironments(\n           code = getTopologicalTorsionCode(\n               pathCodes, topologicalTorsionArguments->df_includeChirality);\n         }\n-\n-        result.push_back(new TopologicalTorsionAtomEnv<OutputType>(code));\n+        result.push_back(new TopologicalTorsionAtomEnv<OutputType>(code, path));\n       }\n     }\n   }","old_code":"\/\/\n\/\/  Copyright (C) 2018 Boran Adas, Google Summer of Code\n\/\/\n\/\/   @@ All Rights Reserved @@\n\/\/  This file is part of the RDKit.\n\/\/  The contents are covered by the terms of the BSD license\n\/\/  which is included in the file license.txt, found at the root\n\/\/  of the RDKit source tree.\n\/\/\n\n#include <GraphMol\/Fingerprints\/TopologicalTorsionGenerator.h>\n#include <GraphMol\/Fingerprints\/FingerprintUtil.h>\n#include <GraphMol\/Fingerprints\/AtomPairGenerator.h>\n\nnamespace RDKit {\nnamespace TopologicalTorsion {\n\nusing namespace AtomPairs;\n\ntemplate <typename OutputType>\nTopologicalTorsionArguments<OutputType>::TopologicalTorsionArguments(\n    const bool includeChirality, const uint32_t torsionAtomCount,\n    const bool countSimulation, const std::vector<std::uint32_t> countBounds,\n    const std::uint32_t fpSize)\n    : FingerprintArguments<OutputType>(countSimulation, countBounds, fpSize),\n      df_includeChirality(includeChirality),\n      d_torsionAtomCount(torsionAtomCount){};\n\ntemplate <typename OutputType>\nOutputType TopologicalTorsionArguments<OutputType>::getResultSize() const {\n  OutputType result = 1;\n  return (result << (d_torsionAtomCount *\n                     (codeSize + (df_includeChirality ? numChiralBits : 0))));\n};\n\ntemplate <typename OutputType>\nstd::string TopologicalTorsionArguments<OutputType>::infoString() const {\n  return \"TopologicalTorsionArguments includeChirality=\" +\n         std::to_string(df_includeChirality) +\n         \" torsionAtomCount=\" + std::to_string(d_torsionAtomCount);\n};\ntemplate <typename OutputType>\nOutputType TopologicalTorsionAtomEnv<OutputType>::getBitId(\n    FingerprintArguments<OutputType> *,  \/\/ arguments\n    const std::vector<std::uint32_t> *,  \/\/ atomInvariants\n    const std::vector<std::uint32_t> *,  \/\/ bondInvariants\n    const AdditionalOutput *,            \/\/ additionalOutput\n    const bool                           \/\/ hashResults\n) const {\n  return d_bitId;\n};\n\ntemplate <typename OutputType>\nTopologicalTorsionAtomEnv<OutputType>::TopologicalTorsionAtomEnv(\n    OutputType bitId)\n    : d_bitId(bitId){};\n\ntemplate <typename OutputType>\nstd::vector<AtomEnvironment<OutputType> *>\nTopologicalTorsionEnvGenerator<OutputType>::getEnvironments(\n    const ROMol &mol, FingerprintArguments<OutputType> *arguments,\n    const std::vector<std::uint32_t> *fromAtoms,\n    const std::vector<std::uint32_t> *ignoreAtoms,\n    const int,                 \/\/ confId\n    const AdditionalOutput *,  \/\/ additionalOutput\n    const std::vector<std::uint32_t> *atomInvariants,\n    const std::vector<std::uint32_t> *,  \/\/ bondInvariants\n    const bool hashResults) const {\n  auto *topologicalTorsionArguments =\n      dynamic_cast<TopologicalTorsionArguments<OutputType> *>(arguments);\n\n  std::vector<AtomEnvironment<OutputType> *> result =\n      std::vector<AtomEnvironment<OutputType> *>();\n\n  boost::dynamic_bitset<> *fromAtomsBV = nullptr;\n  if (fromAtoms) {\n    fromAtomsBV = new boost::dynamic_bitset<>(mol.getNumAtoms());\n    for (auto fAt : *fromAtoms) {\n      fromAtomsBV->set(fAt);\n    }\n  }\n  boost::dynamic_bitset<> *ignoreAtomsBV = nullptr;\n  if (ignoreAtoms) {\n    ignoreAtomsBV = new boost::dynamic_bitset<>(mol.getNumAtoms());\n    for (auto fAt : *ignoreAtoms) {\n      ignoreAtomsBV->set(fAt);\n    }\n  }\n  boost::dynamic_bitset<> pAtoms(mol.getNumAtoms());\n  PATH_LIST paths = findAllPathsOfLengthN(\n      mol, topologicalTorsionArguments->d_torsionAtomCount, false);\n  for (PATH_LIST::const_iterator pathIt = paths.begin(); pathIt != paths.end();\n       ++pathIt) {\n    bool keepIt = true;\n    if (fromAtomsBV) {\n      keepIt = false;\n    }\n    std::vector<std::uint32_t> pathCodes;\n    const PATH_TYPE &path = *pathIt;\n    if (fromAtomsBV) {\n      if (fromAtomsBV->test(static_cast<std::uint32_t>(path.front())) ||\n          fromAtomsBV->test(static_cast<std::uint32_t>(path.back()))) {\n        keepIt = true;\n      }\n    }\n    if (keepIt && ignoreAtomsBV) {\n      for (int pElem : path) {\n        if (ignoreAtomsBV->test(pElem)) {\n          keepIt = false;\n          break;\n        }\n      }\n    }\n    if (keepIt) {\n      pAtoms.reset();\n      for (auto pIt = path.begin(); pIt < path.end(); ++pIt) {\n        \/\/ look for a cycle that doesn't start at the first atom\n        \/\/ we can't effectively canonicalize these at the moment\n        \/\/ (was github #811)\n        if (pIt != path.begin() && *pIt != *(path.begin()) && pAtoms[*pIt]) {\n          pathCodes.clear();\n          break;\n        }\n        pAtoms.set(*pIt);\n        unsigned int code = (*atomInvariants)[*pIt] % ((1 << codeSize) - 1) + 1;\n        \/\/ subtract off the branching number:\n        if (pIt != path.begin() && pIt + 1 != path.end()) {\n          --code;\n        }\n        pathCodes.push_back(code);\n      }\n      if (pathCodes.size()) {\n        OutputType code;\n        if (hashResults) {\n          code = getTopologicalTorsionHash(pathCodes);\n        } else {\n          code = getTopologicalTorsionCode(\n              pathCodes, topologicalTorsionArguments->df_includeChirality);\n        }\n\n        result.push_back(new TopologicalTorsionAtomEnv<OutputType>(code));\n      }\n    }\n  }\n  delete fromAtomsBV;\n  delete ignoreAtomsBV;\n\n  return result;\n};\n\ntemplate <typename OutputType>\nstd::string TopologicalTorsionEnvGenerator<OutputType>::infoString() const {\n  return \"TopologicalTorsionEnvGenerator\";\n};\n\ntemplate <typename OutputType>\nFingerprintGenerator<OutputType> *getTopologicalTorsionGenerator(\n    const bool includeChirality, const uint32_t torsionAtomCount,\n    AtomInvariantsGenerator *atomInvariantsGenerator,\n    const bool countSimulation, const std::vector<std::uint32_t> countBounds,\n    const std::uint32_t fpSize, const bool ownsAtomInvGen) {\n  auto *envGenerator = new TopologicalTorsionEnvGenerator<OutputType>();\n\n  auto *arguments = new TopologicalTorsionArguments<OutputType>(\n      includeChirality, torsionAtomCount, countSimulation, countBounds, fpSize);\n\n  bool ownsAtomInvGenerator = ownsAtomInvGen;\n  if (!atomInvariantsGenerator) {\n    atomInvariantsGenerator =\n        new AtomPair::AtomPairAtomInvGenerator(includeChirality, true);\n    ownsAtomInvGenerator = true;\n  }\n\n  return new FingerprintGenerator<OutputType>(envGenerator, arguments,\n                                              atomInvariantsGenerator, nullptr,\n                                              ownsAtomInvGenerator, false);\n};\n\n\/\/ Topological torsion fingerprint does not support 32 bit output yet\n\ntemplate RDKIT_FINGERPRINTS_EXPORT FingerprintGenerator<std::uint64_t> *\ngetTopologicalTorsionGenerator(const bool includeChirality,\n                               const uint32_t torsionAtomCount,\n                               AtomInvariantsGenerator *atomInvariantsGenerator,\n                               const bool countSimulation,\n                               const std::vector<std::uint32_t> countBounds,\n                               const std::uint32_t fpSize,\n                               const bool ownsAtomInvGen);\n\n}  \/\/ namespace TopologicalTorsion\n}  \/\/ namespace RDKit\n","lang_cluster":"C++","length":191,"code_uid":"1bee5920c12c4f6486c32da27c00cba5"}
{"diff_hunk":"@@ -187,13 +187,19 @@ void init_io_random(int seed) {\n \n   ::io_generator_seed_base = seed;\n   ::io_generator_seed_inited = true;\n-  \/\/\/ Reset the init flag so that generator will reinitialize\n-  ::io_generator_inited = false;\n-\n   ::fast_io_generator_seed_base = seed;\n   ::fast_io_generator_seed_inited = true;\n-  \/\/\/ Reset the init flag so that generator will reinitialize\n-  ::fast_io_generator_inited = false;\n+\n+  ::io_generator.resize(num_io_RNGs);\n+  ::fast_io_generator.resize(num_io_RNGs);\n+  ::io_generator_inited.resize(num_io_RNGs);\n+  ::fast_io_generator_inited.resize(num_io_RNGs);\n+  for(int i = 0; i < num_io_RNGs; i++) {\n+    \/\/\/ Reset the init flag so that I\/O generator will reinitialize\n+    ::io_generator_inited[i] = false;\n+    \/\/\/ Reset the init flag so that fast I\/O generator will reinitialize\n+    ::fast_io_generator_inited[i] = false;\n+  }\n }\n \n }  \/\/ namespace lbann","old_code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/ Copyright (c) 2014-2019, Lawrence Livermore National Security, LLC.\n\/\/ Produced at the Lawrence Livermore National Laboratory.\n\/\/ Written by the LBANN Research Team (B. Van Essen, et al.) listed in\n\/\/ the CONTRIBUTORS file. <lbann-dev@llnl.gov>\n\/\/\n\/\/ LLNL-CODE-697807.\n\/\/ All rights reserved.\n\/\/\n\/\/ This file is part of LBANN: Livermore Big Artificial Neural Network\n\/\/ Toolkit. For details, see http:\/\/software.llnl.gov\/LBANN or\n\/\/ https:\/\/github.com\/LLNL\/LBANN.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"Licensee\"); you\n\/\/ may not use this file except in compliance with the License.  You may\n\/\/ obtain a copy of the License at:\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n\/\/ implied. See the License for the specific language governing\n\/\/ permissions and limitations under the license.\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n#include <omp.h>\n#include \"lbann\/utils\/random_number_generators.hpp\"\n#include \"lbann\/utils\/hash.hpp\"\n#include \"lbann\/utils\/exception.hpp\"\n#include <thread>\n\nnamespace {\n#ifdef __ICC\nlbann::rng_gen generator;\n#pragma omp threadprivate(generator)\n\nlbann::fast_rng_gen fast_generator;\n#pragma omp threadprivate(fast_generator)\n#else\n\/\/ Random number generator, file-visible only.\n\/\/ Defined like this to work around a GCC problem with threadprivate objects:\n\/\/ https:\/\/stackoverflow.com\/questions\/23552077\/how-to-define-a-object-or-struct-as-threadprivate-in-openmp\/\nextern lbann::rng_gen generator;\n#pragma omp threadprivate(generator)\nlbann::rng_gen generator;\n\nextern lbann::fast_rng_gen fast_generator;\n#pragma omp threadprivate(fast_generator)\nlbann::fast_rng_gen fast_generator;\n#endif\n\nbool generator_inited = false;\nbool fast_generator_inited = false;\n\nthread_local lbann::rng_gen data_seq_generator;\nthread_local bool data_seq_generator_inited = false;\nint data_seq_generator_seed_base = 0;\nbool data_seq_generator_seed_inited = false;\n\nthread_local lbann::rng_gen io_generator;\nthread_local bool io_generator_inited = false;\nint io_generator_seed_base = 0;\nbool io_generator_seed_inited = false;\n\nthread_local lbann::fast_rng_gen fast_io_generator;\nthread_local bool fast_io_generator_inited = false;\nint fast_io_generator_seed_base = 0;\nbool fast_io_generator_seed_inited = false;\n}\n\nnamespace lbann {\n\nrng_gen& get_generator() {\n  if (!::generator_inited) { LBANN_ERROR(\"RNG seed not set\"); }\n  return ::generator;\n}\n\nfast_rng_gen& get_fast_generator() {\n  if (!::fast_generator_inited) { LBANN_ERROR(\"Fast RNG seed not set\"); }\n  return ::fast_generator;\n}\n\nrng_gen& get_data_seq_generator() {\n  if (!::data_seq_generator_inited) {\n    if (!::data_seq_generator_seed_inited) { LBANN_ERROR(\"data sequence RNG seed not set\"); }\n    ::data_seq_generator.seed(::data_seq_generator_seed_base);\n    ::data_seq_generator_inited = true;\n  }\n  return ::data_seq_generator;\n}\n\nrng_gen& get_io_generator() {\n  if (!::io_generator_inited) {\n    if (!::io_generator_seed_inited) { LBANN_ERROR(\"I\/O RNG seed not set\"); }\n    ::io_generator.seed(hash_combine(::io_generator_seed_base,\n                                     std::this_thread::get_id()));\n    ::io_generator_inited = true;\n  }\n  return ::io_generator;\n}\n\nfast_rng_gen& get_fast_io_generator() {\n  if (!::fast_io_generator_inited) {\n    if (!::fast_io_generator_seed_inited) { LBANN_ERROR(\"Fast I\/O RNG seed not set\"); }\n    ::fast_io_generator.seed(hash_combine(::fast_io_generator_seed_base,\n                                          std::this_thread::get_id()));\n    ::fast_io_generator_inited = true;\n  }\n  return ::fast_io_generator;\n}\nvoid init_random(int seed, lbann_comm *comm) {\n  generator_inited = true;\n  fast_generator_inited = true;\n  if (seed != -1) {\n    \/\/ Seed every OpenMP thread, if present.\n    \/\/ Note: Threadprivate OMP variables don't work with dynamic threads.\n#ifdef _OPENMP\n    #pragma omp parallel\n    {\n      get_generator().seed(hash_combine(seed, omp_get_thread_num()));\n      get_fast_generator().seed(hash_combine(seed, omp_get_thread_num()));\n    }\n#else\n    get_generator().seed(seed);\n    get_fast_generator().seed(seed);\n#endif\n\n#ifdef LBANN_SET_EL_RNG\n    \/\/ Set Elemental's RNG seed\n    auto elemental_seed = hash_combine(seed, 104729); \/\/ 10000th prime\n    int mpi_initialized = 0;\n    MPI_Initialized(&mpi_initialized);\n    if(mpi_initialized) {\n      \/\/ If MPI is initialized mix in the rank to ensure that Hydrogen\n      \/\/ has good RNGs.  Note that under some configurations LBANN\n      \/\/ will not do this, so it is good to ensure that Hydrogen is\n      \/\/ well seeded.\n      elemental_seed = (comm == nullptr\n                        ? hash_combine(elemental_seed, El::mpi::Rank(El::mpi::COMM_WORLD))\n                        : hash_combine(elemental_seed, comm->get_rank_in_trainer()));\n    }\n    El::Generator().seed(elemental_seed);\n#endif\n\n  } else {\n    \/\/ Seed with a random value.\n    std::random_device rd;\n    unsigned rand_val = rd();\n#ifdef _OPENMP\n    #pragma omp parallel\n    {\n      get_generator().seed(hash_combine(rand_val, omp_get_thread_num()));\n      get_fast_generator().seed(hash_combine(rand_val, omp_get_thread_num()));\n    }\n#else\n    get_generator().seed(rand_val);\n    get_fast_generator().seed(rand_val);\n#endif\n#ifdef LBANN_SET_EL_RNG\n    El::Generator().seed(rand_val);\n#endif\n  }\n\n  init_io_random(seed);\n}\n\nvoid init_data_seq_random(int seed) {\n  if (seed == -1) {\n    \/\/ Seed with a random value.\n    std::random_device rd;\n    seed = rd();\n  }\n\n  ::data_seq_generator_seed_base = seed;\n  ::data_seq_generator_seed_inited = true;\n  \/\/\/ Reset the init flag so that generator will reinitialize\n  ::data_seq_generator_inited = false;\n}\n\nvoid init_io_random(int seed) {\n  if (seed == -1) {\n    \/\/ Seed with a random value.\n    std::random_device rd;\n    seed = rd();\n  }\n\n  ::io_generator_seed_base = seed;\n  ::io_generator_seed_inited = true;\n  \/\/\/ Reset the init flag so that generator will reinitialize\n  ::io_generator_inited = false;\n\n  ::fast_io_generator_seed_base = seed;\n  ::fast_io_generator_seed_inited = true;\n  \/\/\/ Reset the init flag so that generator will reinitialize\n  ::fast_io_generator_inited = false;\n}\n\n}  \/\/ namespace lbann\n","lang_cluster":"C++","length":199,"code_uid":"e5245fefabb140ae803afcdfb9cc910a"}
{"diff_hunk":"@@ -74,11 +74,8 @@ void LXQtMountPlugin::settingsChanged()\n         delete mDeviceAction;\n         mDeviceAction = DeviceAction::create(actionId, this);\n \n-        connect(Solid::DeviceNotifier::instance(), &Solid::DeviceNotifier::deviceAdded,\n-                mDeviceAction, &DeviceAction::onDeviceAdded);\n-\n-        connect(Solid::DeviceNotifier::instance(), &Solid::DeviceNotifier::deviceRemoved,\n-                mDeviceAction, &DeviceAction::onDeviceRemoved);\n+        connect(mPopup, &Popup::deviceAdded, mDeviceAction, &DeviceAction::onDeviceAdded);\n+        connect(mPopup, &Popup::deviceRemoved, mDeviceAction, &DeviceAction::onDeviceRemoved);\n     }\n \n }","old_code":"\/* BEGIN_COMMON_COPYRIGHT_HEADER\n * (c)LGPL2+\n *\n * LXDE-Qt - a lightweight, Qt based, desktop toolset\n * http:\/\/razor-qt.org\n *\n * Copyright: 2010-2011 Razor team\n * Authors:\n *   Petr Vanek <petr@scribus.info>\n *\n * This program or library is free software; you can redistribute it\n * and\/or modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * This library is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n\n * You should have received a copy of the GNU Lesser General\n * Public License along with this library; if not, write to the\n * Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,\n * Boston, MA 02110-1301 USA\n *\n * END_COMMON_COPYRIGHT_HEADER *\/\n\n#include \"lxqtmountplugin.h\"\n#include \"configuration.h\"\n\n#include <Solid\/DeviceNotifier>\n\nLXQtMountPlugin::LXQtMountPlugin(const ILXQtPanelPluginStartupInfo &startupInfo):\n    QObject(),\n    ILXQtPanelPlugin(startupInfo),\n    mPopup(nullptr),\n    mDeviceAction(nullptr)\n{\n    mButton = new Button;\n    mPopup = new Popup(this);\n\n    connect(mButton, &QToolButton::clicked, mPopup, &Popup::showHide);\n    connect(mPopup, &Popup::visibilityChanged, mButton, &QToolButton::setDown);\n}\n\nLXQtMountPlugin::~LXQtMountPlugin()\n{\n    delete mButton;\n    delete mPopup;\n}\n\nQDialog *LXQtMountPlugin::configureDialog()\n{\n    if (mPopup)\n        mPopup->hide();\n\n    Configuration *configWindow = new Configuration(settings());\n    configWindow->setAttribute(Qt::WA_DeleteOnClose, true);\n    return configWindow;\n}\n\nvoid LXQtMountPlugin::realign()\n{\n    \/\/nothing to do\n}\n\nvoid LXQtMountPlugin::settingsChanged()\n{\n    QString s = settings()->value(QLatin1String(CFG_KEY_ACTION)).toString();\n    DeviceAction::ActionId actionId = DeviceAction::stringToActionId(s, DeviceAction::ActionMenu);\n\n    if (mDeviceAction == nullptr || mDeviceAction->Type() != actionId)\n    {\n        delete mDeviceAction;\n        mDeviceAction = DeviceAction::create(actionId, this);\n\n        connect(Solid::DeviceNotifier::instance(), &Solid::DeviceNotifier::deviceAdded,\n                mDeviceAction, &DeviceAction::onDeviceAdded);\n\n        connect(Solid::DeviceNotifier::instance(), &Solid::DeviceNotifier::deviceRemoved,\n                mDeviceAction, &DeviceAction::onDeviceRemoved);\n    }\n\n}\n","lang_cluster":"C++","length":84,"code_uid":"ed0beb786cf44773b36a1fba716e917a"}
{"diff_hunk":"@@ -94,7 +94,8 @@ uint16_t  Adafruit_MPR121::filteredData(uint8_t t) {\n }\n \n uint16_t  Adafruit_MPR121::baselineData(uint8_t t) {\n-  if (t > 12) return 0;\n+  if ((_sensitivity == MPR212_EXTRA_SENSITIVITY) && (t > 3)) return 0;\n+  if (t > 12) return 0; \/\/ MPR212_NORMAL_SENSITIVITY\n   uint16_t bl = readRegister8(MPR121_BASELINE_0 + t);\n   return (bl << 2);\n }","old_code":"\/***************************************************\n  This is a library for the MPR121 I2C 12-chan Capacitive Sensor\n\n  Designed specifically to work with the MPR121 sensor from Adafruit\n  ----> https:\/\/www.adafruit.com\/products\/1982\n\n  These sensors use I2C to communicate, 2+ pins are required to\n  interface\n  Adafruit invests time and resources providing this open source code,\n  please support Adafruit and open-source hardware by purchasing\n  products from Adafruit!\n\n  Written by Limor Fried\/Ladyada for Adafruit Industries.\n  BSD license, all text above must be included in any redistribution\n ****************************************************\/\n\n#include \"Adafruit_MPR121.h\"\n\nAdafruit_MPR121::Adafruit_MPR121() {\n}\n\nboolean Adafruit_MPR121::begin(uint8_t i2caddr) {\n  \/\/Wire.begin();   called in ESPEasy framework\n\n  _i2caddr = i2caddr;\n\n  \/\/ soft reset\n  writeRegister(MPR121_SOFTRESET, 0x63);\n  delay(1);\n  for (uint8_t i=0; i<0x7F; i++) {\n  \/\/  Serial.print(\"$\"); Serial.print(i, HEX);\n  \/\/  Serial.print(\": 0x\"); Serial.println(readRegister8(i));\n  }\n\n\n  writeRegister(MPR121_ECR, 0x0);\n\n  uint8_t c = readRegister8(MPR121_CONFIG2);\n\n  if (c != 0x24) return false;\n\n\n  setThresholds(12, 6);\n  writeRegister(MPR121_MHDR, 0x01);\n  writeRegister(MPR121_NHDR, 0x01);\n  writeRegister(MPR121_NCLR, 0x0E);\n  writeRegister(MPR121_FDLR, 0x00);\n\n  writeRegister(MPR121_MHDF, 0x01);\n  writeRegister(MPR121_NHDF, 0x05);\n  writeRegister(MPR121_NCLF, 0x01);\n  writeRegister(MPR121_FDLF, 0x00);\n\n  writeRegister(MPR121_NHDT, 0x00);\n  writeRegister(MPR121_NCLT, 0x00);\n  writeRegister(MPR121_FDLT, 0x00);\n\n  writeRegister(MPR121_DEBOUNCE, 0);\n  writeRegister(MPR121_CONFIG1, 0x10); \/\/ default, 16uA charge current\n  writeRegister(MPR121_CONFIG2, 0x20); \/\/ 0.5uS encoding, 1ms period\n\n\/\/  writeRegister(MPR121_AUTOCONFIG0, 0x8F);\n\n\/\/  writeRegister(MPR121_UPLIMIT, 150);\n\/\/  writeRegister(MPR121_TARGETLIMIT, 100); \/\/ should be ~400 (100 shifted)\n\/\/  writeRegister(MPR121_LOWLIMIT, 50);\n  \/\/ enable all electrodes\n  writeRegister(MPR121_ECR, 0x8F);  \/\/ start with first 5 bits of baseline tracking\n\n  return true;\n}\n\nvoid Adafruit_MPR121::setThreshholds(uint8_t touch, uint8_t release) {\n\n  setThresholds(touch, release);\n  }\n\nvoid Adafruit_MPR121::setThresholds(uint8_t touch, uint8_t release) {\n  for (uint8_t i=0; i<12; i++) {\n    writeRegister(MPR121_TOUCHTH_0 + 2*i, touch);\n    writeRegister(MPR121_RELEASETH_0 + 2*i, release);\n  }\n}\n\nvoid Adafruit_MPR121::setThreshold(uint8_t t, uint8_t touch, uint8_t release) {\n  if (t > 12) return;\n  writeRegister(MPR121_TOUCHTH_0 + 2 * t, touch);\n  writeRegister(MPR121_RELEASETH_0 + 2 * t, release);\n}\n\nuint16_t  Adafruit_MPR121::filteredData(uint8_t t) {\n  if (t > 12) return 0;\n  return readRegister16(MPR121_FILTDATA_0L + t*2);\n}\n\nuint16_t  Adafruit_MPR121::baselineData(uint8_t t) {\n  if (t > 12) return 0;\n  uint16_t bl = readRegister8(MPR121_BASELINE_0 + t);\n  return (bl << 2);\n}\n\nuint16_t  Adafruit_MPR121::touched(void) {\n  uint16_t t = readRegister16(MPR121_TOUCHSTATUS_L);\n  return t & 0x0FFF;\n}\n\n\/*********************************************************************\/\n\n\nuint8_t Adafruit_MPR121::readRegister8(uint8_t reg) {\n    Wire.beginTransmission(_i2caddr);\n    Wire.write(reg);\n    Wire.endTransmission(false);\n    while (Wire.requestFrom(_i2caddr, 1) != 1);\n    return ( Wire.read());\n}\n\nuint16_t Adafruit_MPR121::readRegister16(uint8_t reg) {\n    Wire.beginTransmission(_i2caddr);\n    Wire.write(reg);\n    Wire.endTransmission(false);\n    while (Wire.requestFrom(_i2caddr, 2) != 2);\n    uint16_t v = Wire.read();\n    v |=  ((uint16_t) Wire.read()) << 8;\n    return v;\n}\n\n\/**************************************************************************\/\n\/*!\n    @brief  Writes 8-bits to the specified destination register\n*\/\n\/**************************************************************************\/\nvoid Adafruit_MPR121::writeRegister(uint8_t reg, uint8_t value) {\n    Wire.beginTransmission(_i2caddr);\n    Wire.write((uint8_t)reg);\n    Wire.write((uint8_t)(value));\n    Wire.endTransmission();\n}\n","lang_cluster":"C++","length":138,"code_uid":"bd4d59f5739e4fb496a8eb4e84eb4b3c"}
{"diff_hunk":"@@ -67,20 +67,20 @@ struct WaitListInfo\n \tWaitList priorityWaitList;\n \tWaitList waitList;\n \n-\tstd::pair<WaitList::iterator, WaitList::size_type> findClient(const Player *player) {\n+\tstd::tuple<WaitList&, WaitList::iterator, WaitList::size_type> findClient(const Player *player) {\n \t\tstd::size_t slot = 1;\n \t\tfor (auto it = priorityWaitList.begin(), end = priorityWaitList.end(); it != end; ++it, ++slot) {\n \t\t\tif (it->playerGUID == player->getGUID()) {\n-\t\t\t\treturn {it, slot};\n+\t\t\t\treturn std::make_tuple(std::ref(priorityWaitList), it, slot);\n \t\t\t}\n \t\t}\n \n \t\tfor (auto it = waitList.begin(), end = waitList.end(); it != end; ++it, ++slot) {\n \t\t\tif (it->playerGUID == player->getGUID()) {\n-\t\t\t\treturn {it, slot};\n+\t\t\t\treturn std::make_tuple(std::ref(waitList), it, slot);\n \t\t\t}\n \t\t}\n-\t\treturn {waitList.end(), slot};\n+\t\treturn std::make_tuple(std::ref(waitList), waitList.end(), slot);\n \t}\n };\n ","old_code":"\/**\n * The Forgotten Server - a free and open-source MMORPG server emulator\n * Copyright (C) 2019  Mark Samman <mark.samman@gmail.com>\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n *\/\n\n#include \"otpch.h\"\n\n#include \"configmanager.h\"\n#include \"game.h\"\n#include \"waitlist.h\"\n\nextern ConfigManager g_config;\nextern Game g_game;\n\n\nnamespace {\n\nstruct Wait\n{\n\tconstexpr Wait(std::size_t timeout, uint32_t playerGUID) :\n\t\t\ttimeout(timeout), playerGUID(playerGUID) {}\n\n\tstd::size_t timeout;\n\tuint32_t playerGUID;\n};\n\nusing WaitList = std::list<Wait>;\n\nvoid cleanupList(WaitList& list)\n{\n\tint64_t time = OTSYS_TIME();\n\n\tauto it = list.begin(), end = list.end();\n\twhile (it != end) {\n\t\tif ((it->timeout - time) <= 0) {\n\t\t\tit = list.erase(it);\n\t\t} else {\n\t\t\t++it;\n\t\t}\n\t}\n}\n\nstd::size_t getTimeout(std::size_t slot)\n{\n\t\/\/timeout is set to 15 seconds longer than expected retry attempt\n\treturn WaitingList::getTime(slot) + 15;\n}\n\n} \/\/ namespace\n\nstruct WaitListInfo\n{\n\tWaitList priorityWaitList;\n\tWaitList waitList;\n\n\tstd::pair<WaitList::iterator, WaitList::size_type> findClient(const Player *player) {\n\t\tstd::size_t slot = 1;\n\t\tfor (auto it = priorityWaitList.begin(), end = priorityWaitList.end(); it != end; ++it, ++slot) {\n\t\t\tif (it->playerGUID == player->getGUID()) {\n\t\t\t\treturn {it, slot};\n\t\t\t}\n\t\t}\n\n\t\tfor (auto it = waitList.begin(), end = waitList.end(); it != end; ++it, ++slot) {\n\t\t\tif (it->playerGUID == player->getGUID()) {\n\t\t\t\treturn {it, slot};\n\t\t\t}\n\t\t}\n\t\treturn {waitList.end(), slot};\n\t}\n};\n\nWaitingList& WaitingList::getInstance()\n{\n\tstatic WaitingList waitingList;\n\treturn waitingList;\n}\n\nstd::size_t WaitingList::getTime(std::size_t slot)\n{\n\tif (slot < 5) {\n\t\treturn 5;\n\t} else if (slot < 10) {\n\t\treturn 10;\n\t} else if (slot < 20) {\n\t\treturn 20;\n\t} else if (slot < 50) {\n\t\treturn 60;\n\t} else {\n\t\treturn 120;\n\t}\n}\n\nbool WaitingList::clientLogin(const Player* player)\n{\n\tif (player->hasFlag(PlayerFlag_CanAlwaysLogin) || player->getAccountType() >= ACCOUNT_TYPE_GAMEMASTER) {\n\t\treturn true;\n\t}\n\n\tauto maxPlayers = static_cast<uint32_t>(g_config.getNumber(ConfigManager::MAX_PLAYERS));\n\tif (maxPlayers == 0 || (info->priorityWaitList.empty() && info->waitList.empty() && g_game.getPlayersOnline() < maxPlayers)) {\n\t\treturn true;\n\t}\n\n\tcleanupList(info->priorityWaitList);\n\tcleanupList(info->waitList);\n\n\tWaitList::iterator it;\n\tWaitList::size_type slot;\n\tstd::tie(it, slot) = info->findClient(player);\n\tif (it != info->waitList.end()) {\n\t\tif ((g_game.getPlayersOnline() + slot) <= maxPlayers) {\n\t\t\t\/\/should be able to login now\n\t\t\tinfo->waitList.erase(it);\n\t\t\treturn true;\n\t\t}\n\n\t\t\/\/let them wait a bit longer\n\t\tit->timeout = OTSYS_TIME() + (getTimeout(slot) * 1000);\n\t\treturn false;\n\t}\n\n\tslot = info->priorityWaitList.size();\n\tif (player->isPremium()) {\n\t\tinfo->priorityWaitList.emplace_back(OTSYS_TIME() + (getTimeout(slot + 1) * 1000), player->getGUID());\n\t} else {\n\t\tslot += info->waitList.size();\n\t\tinfo->waitList.emplace_back(OTSYS_TIME() + (getTimeout(slot + 1) * 1000), player->getGUID());\n\t}\n\treturn false;\n}\n\nstd::size_t WaitingList::getClientSlot(const Player* player)\n{\n\tWaitList::iterator it;\n\tWaitList::size_type slot;\n\tstd::tie(it, slot) = info->findClient(player);\n\tif (it == info->waitList.end()) {\n\t\treturn 0;\n\t}\n\treturn slot;\n}\n\nWaitingList::WaitingList() : info(new WaitListInfo) {}\n","lang_cluster":"C++","length":158,"code_uid":"d3f5c584da8241c1b0bf906c004847e1"}
{"diff_hunk":"@@ -10,6 +10,7 @@ import (\n \t\"strconv\"\n \t\"strings\"\n \n+\t\"github.com\/opencontainers\/runc\/libcontainer\"\n \t\"github.com\/urfave\/cli\"\n )\n ","old_code":"\/\/ +build linux\n\npackage main\n\nimport (\n\t\"encoding\/json\"\n\t\"fmt\"\n\t\"os\"\n\t\"os\/exec\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com\/urfave\/cli\"\n)\n\nvar psCommand = cli.Command{\n\tName:      \"ps\",\n\tUsage:     \"ps displays the processes running inside a container\",\n\tArgsUsage: `<container-id> [ps options]`,\n\tFlags: []cli.Flag{\n\t\tcli.StringFlag{\n\t\t\tName:  \"format, f\",\n\t\t\tValue: \"table\",\n\t\t\tUsage: `select one of: ` + formatOptions,\n\t\t},\n\t},\n\tAction: func(context *cli.Context) error {\n\t\tif err := checkArgs(context, 1, minArgs); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcontainer, err := getContainer(context)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tpids, err := container.Processes()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tswitch context.String(\"format\") {\n\t\tcase \"table\":\n\t\tcase \"json\":\n\t\t\treturn json.NewEncoder(os.Stdout).Encode(pids)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"invalid format option\")\n\t\t}\n\n\t\t\/\/ [1:] is to remove command name, ex:\n\t\t\/\/ context.Args(): [containet_id ps_arg1 ps_arg2 ...]\n\t\t\/\/ psArgs:         [ps_arg1 ps_arg2 ...]\n\t\t\/\/\n\t\tpsArgs := context.Args()[1:]\n\t\tif len(psArgs) == 0 {\n\t\t\tpsArgs = []string{\"-ef\"}\n\t\t}\n\n\t\tcmd := exec.Command(\"ps\", psArgs...)\n\t\toutput, err := cmd.CombinedOutput()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"%s: %s\", err, output)\n\t\t}\n\n\t\tlines := strings.Split(string(output), \"\\n\")\n\t\tpidIndex, err := getPidIndex(lines[0])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfmt.Println(lines[0])\n\t\tfor _, line := range lines[1:] {\n\t\t\tif len(line) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfields := strings.Fields(line)\n\t\t\tp, err := strconv.Atoi(fields[pidIndex])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"unexpected pid '%s': %s\", fields[pidIndex], err)\n\t\t\t}\n\n\t\t\tfor _, pid := range pids {\n\t\t\t\tif pid == p {\n\t\t\t\t\tfmt.Println(line)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t},\n\tSkipArgReorder: true,\n}\n\nfunc getPidIndex(title string) (int, error) {\n\ttitles := strings.Fields(title)\n\n\tpidIndex := -1\n\tfor i, name := range titles {\n\t\tif name == \"PID\" {\n\t\t\treturn i, nil\n\t\t}\n\t}\n\n\treturn pidIndex, fmt.Errorf(\"couldn't find PID field in ps output\")\n}\n","lang_cluster":"Go","length":104,"code_uid":"facf1047e20b4c60851144da70172570"}
{"diff_hunk":"@@ -23,6 +23,7 @@ package thrift\n import (\n \t\"bytes\"\n \t\"fmt\"\n+\t\"io\"\n \t\"math\/rand\"\n \t\"reflect\"\n \t\"testing\"","old_code":"\/\/ Copyright (c) 2021 Uber Technologies, Inc.\n\/\/\n\/\/ Permission is hereby granted, free of charge, to any person obtaining a copy\n\/\/ of this software and associated documentation files (the \"Software\"), to deal\n\/\/ in the Software without restriction, including without limitation the rights\n\/\/ to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n\/\/ copies of the Software, and to permit persons to whom the Software is\n\/\/ furnished to do so, subject to the following conditions:\n\/\/\n\/\/ The above copyright notice and this permission notice shall be included in\n\/\/ all copies or substantial portions of the Software.\n\/\/\n\/\/ THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\/\/ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\/\/ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\/\/ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\/\/ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\/\/ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n\/\/ THE SOFTWARE.\n\npackage thrift\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math\/rand\"\n\t\"reflect\"\n\t\"testing\"\n\t\"testing\/quick\"\n\t\"time\"\n\n\t\"github.com\/stretchr\/testify\/assert\"\n\t\"go.uber.org\/thriftrw\/protocol\/binary\"\n\t\"go.uber.org\/thriftrw\/wire\"\n)\n\nfunc TestDisableEnveloperEncode(t *testing.T) {\n\trand := rand.New(rand.NewSource(time.Now().Unix()))\n\n\ttests := []struct {\n\t\tvalue wire.Value\n\t\twant  []byte\n\t}{\n\t\t{\n\t\t\twire.NewValueStruct(wire.Struct{Fields: []wire.Field{}}),\n\t\t\t[]byte{0x00},\n\t\t},\n\t\t{\n\t\t\twire.NewValueStruct(wire.Struct{Fields: []wire.Field{\n\t\t\t\t{ID: 1, Value: wire.NewValueI32(42)},\n\t\t\t}}),\n\t\t\t[]byte{\n\t\t\t\t0x08, 0x00, 0x01,\n\t\t\t\t0x00, 0x00, 0x00, 0x2a,\n\t\t\t\t0x00,\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\te := wire.Envelope{Value: tt.value, Type: wire.Call}\n\t\tgenerate(&e.Name, rand)\n\t\tgenerate(&e.SeqID, rand)\n\n\t\tvar buffer bytes.Buffer\n\t\tproto := disableEnvelopingProtocol{binary.Default, wire.Reply}\n\t\tif !assert.NoError(t, proto.EncodeEnveloped(e, &buffer)) {\n\t\t\tcontinue\n\t\t}\n\n\t\tassert.Equal(t, tt.want, buffer.Bytes())\n\n\t\tgotE, err := proto.DecodeEnveloped(bytes.NewReader(tt.want))\n\t\tif !assert.NoError(t, err) {\n\t\t\tcontinue\n\t\t}\n\n\t\tassert.Equal(t, wire.Reply, gotE.Type)\n\t\tassert.True(t, wire.ValuesAreEqual(tt.value, gotE.Value))\n\t}\n}\n\n\/\/ generate generates a random value into the given pointer.\n\/\/\n\/\/ \tvar i int\n\/\/ \tgenerate(&i, rand)\n\/\/\n\/\/ If the type implements the quick.Generator interface, that is used.\nfunc generate(v interface{}, r *rand.Rand) {\n\tt := reflect.TypeOf(v)\n\tif t.Kind() != reflect.Ptr {\n\t\tpanic(fmt.Sprintf(\"%v is not a pointer type\", t))\n\t}\n\n\tout, ok := quick.Value(t.Elem(), r)\n\tif !ok {\n\t\tpanic(fmt.Sprintf(\"could not generate a value for %v\", t))\n\t}\n\n\treflect.ValueOf(v).Elem().Set(out)\n}\n","lang_cluster":"Go","length":101,"code_uid":"fbbf4313d44448f2acc37eb76e2e83aa"}
{"diff_hunk":"@@ -15,6 +15,14 @@ import (\n \/\/ Compile-time variable\n var existingServer = \"False\"\n \n+const lockFile = \"\/var\/lock\/k3s-test.lock\"\n+\n+type K3sServer struct {\n+\tcmd     *exec.Cmd\n+\tscanner *bufio.Scanner\n+\tlock    int\n+}\n+\n func findK3sExecutable() string {\n \t\/\/ if running on an existing cluster, it maybe installed via k3s.service\n \t\/\/ or run manually from dist\/artifacts\/k3s","old_code":"package util\n\nimport (\n\t\"bufio\"\n\t\"encoding\/json\"\n\t\"os\"\n\t\"os\/exec\"\n\t\"os\/user\"\n\t\"strings\"\n\n\t\"github.com\/rancher\/k3s\/pkg\/flock\"\n\t\"github.com\/sirupsen\/logrus\"\n)\n\n\/\/ Compile-time variable\nvar existingServer = \"False\"\n\nfunc findK3sExecutable() string {\n\t\/\/ if running on an existing cluster, it maybe installed via k3s.service\n\t\/\/ or run manually from dist\/artifacts\/k3s\n\tif IsExistingServer() {\n\t\tk3sBin, err := exec.LookPath(\"k3s\")\n\t\tif err == nil {\n\t\t\treturn k3sBin\n\t\t}\n\t}\n\tk3sBin := \"dist\/artifacts\/k3s\"\n\tfor {\n\t\t_, err := os.Stat(k3sBin)\n\t\tif err != nil {\n\t\t\tk3sBin = \"..\/\" + k3sBin\n\t\t\tcontinue\n\t\t}\n\t\tbreak\n\t}\n\treturn k3sBin\n}\n\n\/\/ IsRoot return true if the user is root (UID 0)\nfunc IsRoot() bool {\n\tcurrentUser, err := user.Current()\n\tif err != nil {\n\t\treturn false\n\t}\n\treturn currentUser.Uid == \"0\"\n}\n\nfunc IsExistingServer() bool {\n\treturn existingServer == \"True\"\n}\n\n\/\/ K3sCmd launches the provided K3s command via exec. Command blocks until finished.\n\/\/ Command output from both Stderr and Stdout is provided via string.\n\/\/   cmdEx1, err := K3sCmd(\"etcd-snapshot\", \"ls\")\n\/\/   cmdEx2, err := K3sCmd(\"kubectl\", \"get\", \"pods\", \"-A\")\nfunc K3sCmd(cmdName string, cmdArgs ...string) (string, error) {\n\tk3sBin := findK3sExecutable()\n\t\/\/ Only run sudo if not root\n\tvar cmd *exec.Cmd\n\tif IsRoot() {\n\t\tk3sCmd := append([]string{cmdName}, cmdArgs...)\n\t\tcmd = exec.Command(k3sBin, k3sCmd...)\n\t} else {\n\t\tk3sCmd := append([]string{k3sBin, cmdName}, cmdArgs...)\n\t\tcmd = exec.Command(\"sudo\", k3sCmd...)\n\t}\n\tbyteOut, err := cmd.CombinedOutput()\n\treturn string(byteOut), err\n}\n\nfunc contains(source []string, target string) bool {\n\tfor _, s := range source {\n\t\tif s == target {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n\/\/ ServerArgsPresent checks if the given arguments are found in the running k3s server\nfunc ServerArgsPresent(neededArgs []string) bool {\n\tcurrentArgs := K3sServerArgs()\n\tfor _, arg := range neededArgs {\n\t\tif !contains(currentArgs, arg) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n\/\/ K3sServerArgs returns the list of arguments that the k3s server launched with\nfunc K3sServerArgs() []string {\n\tresults, err := K3sCmd(\"kubectl\", \"get\", \"nodes\", \"-o\", `jsonpath='{.items[0].metadata.annotations.k3s\\.io\/node-args}'`)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tres := strings.ReplaceAll(results, \"'\", \"\")\n\tvar args []string\n\tif err := json.Unmarshal([]byte(res), &args); err != nil {\n\t\tlogrus.Error(err)\n\t\treturn nil\n\t}\n\treturn args\n}\n\nfunc FindStringInCmdAsync(scanner *bufio.Scanner, target string) bool {\n\tfor scanner.Scan() {\n\t\tif strings.Contains(scanner.Text(), target) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\ntype K3sServer struct {\n\tcmd     *exec.Cmd\n\tscanner *bufio.Scanner\n\tlock    int\n}\n\n\/\/ K3sStartServer acquires an exclusive lock on a temporary file, then launches a k3s cluster\n\/\/ with the provided arguments. Subsequent\/parallel calls to this function will block until\n\/\/ the original lock is cleared using K3sKillServer\nfunc K3sStartServer(cmdArgs ...string) (*K3sServer, error) {\n\tlogrus.Info(\"waiting to get server lock\")\n\tk3sLock, err := flock.Acquire(\"\/var\/lock\/k3s-test.lock\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tk3sBin := findK3sExecutable()\n\tvar cmd *exec.Cmd\n\tif IsRoot() {\n\t\tk3sCmd := append([]string{\"server\"}, cmdArgs...)\n\t\tcmd = exec.Command(k3sBin, k3sCmd...)\n\t} else {\n\t\tk3sCmd := append([]string{k3sBin, \"server\"}, cmdArgs...)\n\t\tcmd = exec.Command(\"sudo\", k3sCmd...)\n\t}\n\tcmdOut, _ := cmd.StderrPipe()\n\tcmd.Stderr = os.Stderr\n\terr = cmd.Start()\n\treturn &K3sServer{cmd, bufio.NewScanner(cmdOut), k3sLock}, err\n}\n\n\/\/ K3sKillServer terminates the running K3s server and unlocks the file for\n\/\/ other tests\nfunc K3sKillServer(server *K3sServer) error {\n\tif IsRoot() {\n\t\tif err := server.cmd.Process.Kill(); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t\/\/ Since k3s was launched as sudo, we can't just kill the process\n\t\tkillCmd := exec.Command(\"sudo\", \"pkill\", \"k3s\")\n\t\tif err := killCmd.Run(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn flock.Release(server.lock)\n}\n","lang_cluster":"Go","length":161,"code_uid":"499d405ab9c745e4a94ef54e15437925"}
{"diff_hunk":"@@ -72,7 +72,8 @@ func Example_withTLS() {\n \t\tlog.Fatalf(\"failed to create gRPC client TLS credentials: %v\", err)\n \t}\n \n-\texp, err := otlp.NewExporter(otlp.WithTLSCredentials(creds))\n+\texp, err := otlp.NewExporter(otlp.EmptyConfiguration,\n+\t\totlp.NewConnectionConfig(otlp.WithTLSCredentials(creds)))\n \tif err != nil {\n \t\tlog.Fatalf(\"failed to create the collector exporter: %v\", err)\n \t}","old_code":"\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage otlp_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"google.golang.org\/grpc\/credentials\"\n\n\t\"go.opentelemetry.io\/otel\/api\/global\"\n\t\"go.opentelemetry.io\/otel\/exporters\/otlp\"\n\tsdktrace \"go.opentelemetry.io\/otel\/sdk\/trace\"\n)\n\nfunc Example_insecure() {\n\texp, err := otlp.NewExporter(otlp.WithInsecure())\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create the collector exporter: %v\", err)\n\t}\n\tdefer func() {\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\tif err := exp.Shutdown(ctx); err != nil {\n\t\t\tglobal.Handle(err)\n\t\t}\n\t}()\n\n\ttp := sdktrace.NewProvider(\n\t\tsdktrace.WithConfig(sdktrace.Config{DefaultSampler: sdktrace.AlwaysSample()}),\n\t\tsdktrace.WithBatcher(\n\t\t\texp,\n\t\t\t\/\/ add following two options to ensure flush\n\t\t\tsdktrace.WithBatchTimeout(5),\n\t\t\tsdktrace.WithMaxExportBatchSize(10),\n\t\t),\n\t)\n\tglobal.SetTracerProvider(tp)\n\n\ttracer := global.Tracer(\"test-tracer\")\n\n\t\/\/ Then use the OpenTelemetry tracing library, like we normally would.\n\tctx, span := tracer.Start(context.Background(), \"CollectorExporter-Example\")\n\tdefer span.End()\n\n\tfor i := 0; i < 10; i++ {\n\t\t_, iSpan := tracer.Start(ctx, fmt.Sprintf(\"Sample-%d\", i))\n\t\t<-time.After(6 * time.Millisecond)\n\t\tiSpan.End()\n\t}\n}\n\nfunc Example_withTLS() {\n\t\/\/ Please take at look at https:\/\/pkg.go.dev\/google.golang.org\/grpc\/credentials#TransportCredentials\n\t\/\/ for ways on how to initialize gRPC TransportCredentials.\n\tcreds, err := credentials.NewClientTLSFromFile(\"my-cert.pem\", \"\")\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to create gRPC client TLS credentials: %v\", err)\n\t}\n\n\texp, err := otlp.NewExporter(otlp.WithTLSCredentials(creds))\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to create the collector exporter: %v\", err)\n\t}\n\tdefer func() {\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\tif err := exp.Shutdown(ctx); err != nil {\n\t\t\tglobal.Handle(err)\n\t\t}\n\t}()\n\n\ttp := sdktrace.NewProvider(\n\t\tsdktrace.WithConfig(sdktrace.Config{DefaultSampler: sdktrace.AlwaysSample()}),\n\t\tsdktrace.WithBatcher(\n\t\t\texp,\n\t\t\t\/\/ add following two options to ensure flush\n\t\t\tsdktrace.WithBatchTimeout(5),\n\t\t\tsdktrace.WithMaxExportBatchSize(10),\n\t\t),\n\t)\n\tglobal.SetTracerProvider(tp)\n\n\ttracer := global.Tracer(\"test-tracer\")\n\n\t\/\/ Then use the OpenTelemetry tracing library, like we normally would.\n\tctx, span := tracer.Start(context.Background(), \"Securely-Talking-To-Collector-Span\")\n\tdefer span.End()\n\n\tfor i := 0; i < 10; i++ {\n\t\t_, iSpan := tracer.Start(ctx, fmt.Sprintf(\"Sample-%d\", i))\n\t\t<-time.After(6 * time.Millisecond)\n\t\tiSpan.End()\n\t}\n}\n","lang_cluster":"Go","length":109,"code_uid":"490588a6184a47ef96f103ff92ef1065"}
{"diff_hunk":"@@ -84,20 +84,20 @@ func TestChunkUploadDownload(t *testing.T) {\n \t\t)\n \n \t\t\/\/ try to fetch the same chunk\n-\t\tresp := request(t, client, http.MethodGet, resource(validHash), nil, http.StatusOK)\n+\t\tresp := request(t, client, http.MethodGet, resource(chunk.Address()), nil, http.StatusOK)\n \t\tdata, err := ioutil.ReadAll(resp.Body)\n \t\tif err != nil {\n \t\t\tt.Fatal(err)\n \t\t}\n \n-\t\tif !bytes.Equal(validContent, data) {\n+\t\tif !bytes.Equal(chunk.Data(), data) {\n \t\t\tt.Fatal(\"data retrieved doesnt match uploaded content\")\n \t\t}\n \t})\n \n \tt.Run(\"pin-invalid-value\", func(t *testing.T) {\n-\t\tjsonhttptest.Request(t, client, http.MethodPost, resource(validHash), http.StatusOK,\n-\t\t\tjsonhttptest.WithRequestBody(bytes.NewReader(validContent)),\n+\t\tjsonhttptest.Request(t, client, http.MethodPost, resource(chunk.Address()), http.StatusOK,\n+\t\t\tjsonhttptest.WithRequestBody(bytes.NewReader(chunk.Data())),\n \t\t\tjsonhttptest.WithExpectedJSONResponse(jsonhttp.StatusResponse{\n \t\t\t\tMessage: http.StatusText(http.StatusOK),\n \t\t\t\tCode:    http.StatusOK,","old_code":"\/\/ Copyright 2020 The Swarm Authors. All rights reserved.\n\/\/ Use of this source code is governed by a BSD-style\n\/\/ license that can be found in the LICENSE file.\n\npackage api_test\n\nimport (\n\t\"bytes\"\n\t\"io\"\n\t\"io\/ioutil\"\n\t\"net\/http\"\n\t\"testing\"\n\n\t\"github.com\/ethersphere\/bee\/pkg\/logging\"\n\tstatestore \"github.com\/ethersphere\/bee\/pkg\/statestore\/mock\"\n\n\t\"github.com\/ethersphere\/bee\/pkg\/tags\"\n\n\t\"github.com\/ethersphere\/bee\/pkg\/api\"\n\t\"github.com\/ethersphere\/bee\/pkg\/jsonhttp\"\n\t\"github.com\/ethersphere\/bee\/pkg\/jsonhttp\/jsonhttptest\"\n\t\"github.com\/ethersphere\/bee\/pkg\/storage\"\n\t\"github.com\/ethersphere\/bee\/pkg\/storage\/mock\"\n\t\"github.com\/ethersphere\/bee\/pkg\/storage\/mock\/validator\"\n\t\"github.com\/ethersphere\/bee\/pkg\/swarm\"\n)\n\n\/\/ TestChunkUploadDownload uploads a chunk to an API that verifies the chunk according\n\/\/ to a given validator, then tries to download the uploaded data.\nfunc TestChunkUploadDownload(t *testing.T) {\n\n\tvar (\n\t\ttargets              = \"0x222\"\n\t\tresource             = func(addr swarm.Address) string { return \"\/chunks\/\" + addr.String() }\n\t\tresourceTargets      = func(addr swarm.Address) string { return \"\/chunks\/\" + addr.String() + \"?targets=\" + targets }\n\t\tvalidHash            = swarm.MustParseHexAddress(\"aabbcc\")\n\t\tinvalidHash          = swarm.MustParseHexAddress(\"bbccdd\")\n\t\tvalidContent         = []byte(\"bbaatt\")\n\t\tinvalidContent       = []byte(\"bbaattss\")\n\t\tmockValidator        = validator.NewMockValidator(validHash, validContent)\n\t\tmockStatestore       = statestore.NewStateStore()\n\t\tlogger               = logging.New(ioutil.Discard, 0)\n\t\ttag                  = tags.NewTags(mockStatestore, logger)\n\t\tmockValidatingStorer = mock.NewStorer(mock.WithValidator(mockValidator))\n\t\tclient, _, _         = newTestServer(t, testServerOptions{\n\t\t\tStorer: mockValidatingStorer,\n\t\t\tTags:   tag,\n\t\t})\n\t)\n\n\tt.Run(\"invalid hash\", func(t *testing.T) {\n\t\tjsonhttptest.Request(t, client, http.MethodPost, resource(invalidHash), http.StatusBadRequest,\n\t\t\tjsonhttptest.WithRequestBody(bytes.NewReader(validContent)),\n\t\t\tjsonhttptest.WithExpectedJSONResponse(jsonhttp.StatusResponse{\n\t\t\t\tMessage: \"chunk write error\",\n\t\t\t\tCode:    http.StatusBadRequest,\n\t\t\t}),\n\t\t)\n\n\t\t\/\/ make sure chunk is not retrievable\n\t\t_ = request(t, client, http.MethodGet, resource(invalidHash), nil, http.StatusNotFound)\n\t})\n\n\tt.Run(\"invalid content\", func(t *testing.T) {\n\t\tjsonhttptest.Request(t, client, http.MethodPost, resource(invalidHash), http.StatusBadRequest,\n\t\t\tjsonhttptest.WithRequestBody(bytes.NewReader(invalidContent)),\n\t\t\tjsonhttptest.WithExpectedJSONResponse(jsonhttp.StatusResponse{\n\t\t\t\tMessage: \"chunk write error\",\n\t\t\t\tCode:    http.StatusBadRequest,\n\t\t\t}),\n\t\t)\n\n\t\t\/\/ make sure not retrievable\n\t\t_ = request(t, client, http.MethodGet, resource(validHash), nil, http.StatusNotFound)\n\t})\n\n\tt.Run(\"ok\", func(t *testing.T) {\n\t\tjsonhttptest.Request(t, client, http.MethodPost, resource(validHash), http.StatusOK,\n\t\t\tjsonhttptest.WithRequestBody(bytes.NewReader(validContent)),\n\t\t\tjsonhttptest.WithExpectedJSONResponse(jsonhttp.StatusResponse{\n\t\t\t\tMessage: http.StatusText(http.StatusOK),\n\t\t\t\tCode:    http.StatusOK,\n\t\t\t}),\n\t\t)\n\n\t\t\/\/ try to fetch the same chunk\n\t\tresp := request(t, client, http.MethodGet, resource(validHash), nil, http.StatusOK)\n\t\tdata, err := ioutil.ReadAll(resp.Body)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif !bytes.Equal(validContent, data) {\n\t\t\tt.Fatal(\"data retrieved doesnt match uploaded content\")\n\t\t}\n\t})\n\n\tt.Run(\"pin-invalid-value\", func(t *testing.T) {\n\t\tjsonhttptest.Request(t, client, http.MethodPost, resource(validHash), http.StatusOK,\n\t\t\tjsonhttptest.WithRequestBody(bytes.NewReader(validContent)),\n\t\t\tjsonhttptest.WithExpectedJSONResponse(jsonhttp.StatusResponse{\n\t\t\t\tMessage: http.StatusText(http.StatusOK),\n\t\t\t\tCode:    http.StatusOK,\n\t\t\t}),\n\t\t\tjsonhttptest.WithRequestHeader(api.SwarmPinHeader, \"invalid-pin\"),\n\t\t)\n\n\t\t\/\/ Also check if the chunk is NOT pinned\n\t\tif mockValidatingStorer.GetModeSet(validHash) == storage.ModeSetPin {\n\t\t\tt.Fatal(\"chunk should not be pinned\")\n\t\t}\n\t})\n\tt.Run(\"pin-header-missing\", func(t *testing.T) {\n\t\tjsonhttptest.Request(t, client, http.MethodPost, resource(validHash), http.StatusOK,\n\t\t\tjsonhttptest.WithRequestBody(bytes.NewReader(validContent)),\n\t\t\tjsonhttptest.WithExpectedJSONResponse(jsonhttp.StatusResponse{\n\t\t\t\tMessage: http.StatusText(http.StatusOK),\n\t\t\t\tCode:    http.StatusOK,\n\t\t\t}),\n\t\t)\n\n\t\t\/\/ Also check if the chunk is NOT pinned\n\t\tif mockValidatingStorer.GetModeSet(validHash) == storage.ModeSetPin {\n\t\t\tt.Fatal(\"chunk should not be pinned\")\n\t\t}\n\t})\n\tt.Run(\"pin-ok\", func(t *testing.T) {\n\t\tjsonhttptest.Request(t, client, http.MethodPost, resource(validHash), http.StatusOK,\n\t\t\tjsonhttptest.WithRequestBody(bytes.NewReader(validContent)),\n\t\t\tjsonhttptest.WithExpectedJSONResponse(jsonhttp.StatusResponse{\n\t\t\t\tMessage: http.StatusText(http.StatusOK),\n\t\t\t\tCode:    http.StatusOK,\n\t\t\t}),\n\t\t\tjsonhttptest.WithRequestHeader(api.SwarmPinHeader, \"True\"),\n\t\t)\n\n\t\t\/\/ Also check if the chunk is pinned\n\t\tif mockValidatingStorer.GetModePut(validHash) != storage.ModePutUploadPin {\n\t\t\tt.Fatal(\"chunk is not pinned\")\n\t\t}\n\n\t})\n\tt.Run(\"retrieve-targets\", func(t *testing.T) {\n\t\tresp := request(t, client, http.MethodGet, resourceTargets(validHash), nil, http.StatusOK)\n\n\t\t\/\/ Check if the target is obtained correctly\n\t\tif resp.Header.Get(api.TargetsRecoveryHeader) != targets {\n\t\t\tt.Fatalf(\"targets mismatch. got %s, want %s\", resp.Header.Get(api.TargetsRecoveryHeader), targets)\n\t\t}\n\t})\n}\n\nfunc request(t *testing.T, client *http.Client, method, resource string, body io.Reader, responseCode int) *http.Response {\n\tt.Helper()\n\n\treq, err := http.NewRequest(method, resource, body)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tresp, err := client.Do(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif resp.StatusCode != responseCode {\n\t\tt.Fatalf(\"got response status %s, want %v %s\", resp.Status, responseCode, http.StatusText(responseCode))\n\t}\n\treturn resp\n}\n","lang_cluster":"Go","length":168,"code_uid":"56f4ef3918a543abbfecec8a490307bb"}
{"diff_hunk":"@@ -58,6 +58,13 @@ func Register(r *gin.RouterGroup, s *Service) {\n \tendpoint.GET(\"\", s.listExperiments)\n \tendpoint.GET(\"\/detail\/search\", s.experimentDetailSearch)\n \tendpoint.GET(\"\/detail\", s.experimentDetail)\n+\tendpoint.GET(\"\/report\", s.experimentReport)\n+}\n+\n+\/\/ ArchiveExperimentDetail represents an experiment instance.\n+type ArchiveExperimentDetail struct {\n+\tcore.ArchiveExperimentMeta\n+\tExperimentInfo core.ExperimentInfo `json:\"experiment_info\"`\n }\n \n \/\/ @Summary Get archived chaos experiments.","old_code":"\/\/ Copyright 2020 Chaos Mesh Authors.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage archive\n\nimport (\n\t\"context\"\n\t\"net\/http\"\n\n\t\"github.com\/gin-gonic\/gin\"\n\t\"github.com\/jinzhu\/gorm\"\n\n\t\"github.com\/chaos-mesh\/chaos-mesh\/pkg\/apiserver\/utils\"\n\t\"github.com\/chaos-mesh\/chaos-mesh\/pkg\/config\"\n\t\"github.com\/chaos-mesh\/chaos-mesh\/pkg\/core\"\n\n\t\"sigs.k8s.io\/controller-runtime\/pkg\/client\"\n)\n\n\/\/ Service defines a handler service for archive experiments.\ntype Service struct {\n\tconf    *config.ChaosDashboardConfig\n\tkubeCli client.Client\n\tarchive core.ExperimentStore\n\tevent   core.EventStore\n}\n\n\/\/ NewService returns an archive experiment service instance.\nfunc NewService(\n\tconf *config.ChaosDashboardConfig,\n\tcli client.Client,\n\tarchive core.ExperimentStore,\n\tevent core.EventStore,\n) *Service {\n\treturn &Service{\n\t\tconf:    conf,\n\t\tkubeCli: cli,\n\t\tarchive: archive,\n\t\tevent:   event,\n\t}\n}\n\n\/\/ Register mounts our HTTP handler on the mux.\nfunc Register(r *gin.RouterGroup, s *Service) {\n\tendpoint := r.Group(\"\/archives\")\n\n\t\/\/ TODO: add more api handlers\n\tendpoint.GET(\"\", s.listExperiments)\n\tendpoint.GET(\"\/detail\/search\", s.experimentDetailSearch)\n\tendpoint.GET(\"\/detail\", s.experimentDetail)\n}\n\n\/\/ @Summary Get archived chaos experiments.\n\/\/ @Description Get archived chaos experiments.\n\/\/ @Tags archives\n\/\/ @Produce json\n\/\/ @Param namespace query string false \"namespace\"\n\/\/ @Param name query string false \"name\"\n\/\/ @Param kind query string false \"kind\" Enums(PodChaos, IoChaos, NetworkChaos, TimeChaos, KernelChaos, StressChaos)\n\/\/ @Success 200 {array} core.ArchiveExperimentMeta\n\/\/ @Router \/api\/archives [get]\n\/\/ @Failure 500 {object} utils.APIError\nfunc (s *Service) listExperiments(c *gin.Context) {\n\tkind := c.Query(\"kind\")\n\tname := c.Query(\"name\")\n\tns := c.Query(\"namespace\")\n\n\tdata, err := s.archive.ListMeta(context.TODO(), kind, ns, name)\n\tif err != nil {\n\t\tc.Status(http.StatusInternalServerError)\n\t\t_ = c.Error(utils.ErrInternalServer.NewWithNoMessage())\n\t\treturn\n\t}\n\n\tc.JSON(http.StatusOK, data)\n}\n\n\/\/ @Summary Get the details of chaos experiment.\n\/\/ @Description Get the details of chaos experiment.\n\/\/ @Tags archives\n\/\/ @Produce json\n\/\/ @Param namespace query string false \"namespace\"\n\/\/ @Param name query string false \"name\"\n\/\/ @Param kind query string false \"kind\" Enums(PodChaos, IoChaos, NetworkChaos, TimeChaos, KernelChaos, StressChaos)\n\/\/ @Param uid query string false \"uid\"\n\/\/ @Success 200 {array} core.ArchiveExperiment\n\/\/ @Router \/api\/archives\/detail\/search [get]\n\/\/ @Failure 500 {object} utils.APIError\nfunc (s *Service) experimentDetailSearch(c *gin.Context) {\n\tkind := c.Query(\"kind\")\n\tname := c.Query(\"name\")\n\tns := c.Query(\"namespace\")\n\tuid := c.Query(\"uid\")\n\n\tdata, err := s.archive.DetailList(context.TODO(), kind, ns, name, uid)\n\tif err != nil {\n\t\tif !gorm.IsRecordNotFoundError(err) {\n\t\t\tc.Status(http.StatusInternalServerError)\n\t\t\t_ = c.Error(utils.ErrInternalServer.NewWithNoMessage())\n\t\t} else {\n\t\t\tc.Status(http.StatusInternalServerError)\n\t\t\t_ = c.Error(utils.ErrInvalidRequest.New(\"the archive is not found\"))\n\t\t}\n\t\treturn\n\t}\n\n\tc.JSON(http.StatusOK, data)\n}\n\n\/\/ @Summary Get the details of chaos experiment.\n\/\/ @Description Get the details of chaos experiment.\n\/\/ @Tags archives\n\/\/ @Produce json\n\/\/ @Param uid query string true \"uid\"\n\/\/ @Success 200 {array} core.ArchiveExperiment\n\/\/ @Router \/api\/archives\/detail [get]\n\/\/ @Failure 500 {object} utils.APIError\nfunc (s *Service) experimentDetail(c *gin.Context) {\n\n\tuid := c.Query(\"uid\")\n\n\tif uid == \"\" {\n\t\tc.Status(http.StatusBadRequest)\n\t\t_ = c.Error(utils.ErrInvalidRequest.New(\"uid cannot be empty\"))\n\t\treturn\n\t}\n\n\tdata, err := s.archive.FindByUID(context.TODO(), uid)\n\tif err != nil {\n\t\tif !gorm.IsRecordNotFoundError(err) {\n\t\t\tc.Status(http.StatusInternalServerError)\n\t\t\t_ = c.Error(utils.ErrInternalServer.NewWithNoMessage())\n\t\t} else {\n\t\t\tc.Status(http.StatusInternalServerError)\n\t\t\t_ = c.Error(utils.ErrInvalidRequest.New(\"the archive is not found\"))\n\t\t}\n\t\treturn\n\t}\n\n\tc.JSON(http.StatusOK, data)\n}\n","lang_cluster":"Go","length":151,"code_uid":"70274b3c2dd44e57908cd57c842c024c"}
{"diff_hunk":"@@ -60,7 +60,7 @@ func DefaultIpcPath() (string, error) {\n \n \/\/ NewSigner creates a new connection to the signer at endpoint\n \/\/ As clef does not expose public keys it signs a test message to recover the public key\n-func NewSigner(clef ExternalSignerInterface, recoverFunc crypto.RecoverFunc) (signer crypto.Signer, err error) {\n+func NewSigner(clef ExternalSignerInterface, client RpcClient, recoverFunc crypto.RecoverFunc) (signer crypto.Signer, err error) {\n \t\/\/ get the list of available ethereum accounts\n \tclefAccounts := clef.Accounts()\n \tif len(clefAccounts) == 0 {","old_code":"\/\/ Copyright 2020 The Swarm Authors. All rights reserved.\n\/\/ Use of this source code is governed by a BSD-style\n\/\/ license that can be found in the LICENSE file.\n\npackage clef\n\nimport (\n\t\"crypto\/ecdsa\"\n\t\"errors\"\n\t\"math\/big\"\n\t\"os\"\n\t\"path\/filepath\"\n\t\"runtime\"\n\n\t\"github.com\/ethereum\/go-ethereum\/accounts\"\n\t\"github.com\/ethereum\/go-ethereum\/common\"\n\t\"github.com\/ethereum\/go-ethereum\/core\/types\"\n\t\"github.com\/ethersphere\/bee\/pkg\/crypto\"\n)\n\nvar (\n\tErrNoAccounts       = errors.New(\"no accounts found in clef\")\n\tclefRecoveryMessage = []byte(\"public key recovery message\")\n)\n\n\/\/ ExternalSignerInterface is the interface for the clef client from go-ethereum\ntype ExternalSignerInterface interface {\n\tSignData(account accounts.Account, mimeType string, data []byte) ([]byte, error)\n\tSignTx(account accounts.Account, tx *types.Transaction, chainID *big.Int) (*types.Transaction, error)\n\tAccounts() []accounts.Account\n}\n\ntype clefSigner struct {\n\tclef    ExternalSignerInterface\n\taccount accounts.Account \/\/ the account this signer will use\n\tpubKey  *ecdsa.PublicKey \/\/ the public key for the account\n}\n\n\/\/ DefaultIpcPath returns the os-dependent default ipc path for clef\nfunc DefaultIpcPath() (string, error) {\n\tsocket := \"clef.ipc\"\n\t\/\/ on windows clef uses top level pipes\n\tif runtime.GOOS == \"windows\" {\n\t\treturn `\\\\.\\pipe\\` + socket, nil\n\t}\n\n\thome, err := os.UserHomeDir()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t\/\/ on mac os clef defaults to ~\/Library\/Signer\/clef.ipc\n\tif runtime.GOOS == \"darwin\" {\n\t\treturn filepath.Join(home, \"Library\", \"Signer\", socket), nil\n\t}\n\n\t\/\/ on unix clef defaults to ~\/.clef\/clef.ipc\n\treturn filepath.Join(home, \".clef\", socket), nil\n}\n\n\/\/ NewSigner creates a new connection to the signer at endpoint\n\/\/ As clef does not expose public keys it signs a test message to recover the public key\nfunc NewSigner(clef ExternalSignerInterface, recoverFunc crypto.RecoverFunc) (signer crypto.Signer, err error) {\n\t\/\/ get the list of available ethereum accounts\n\tclefAccounts := clef.Accounts()\n\tif len(clefAccounts) == 0 {\n\t\treturn nil, ErrNoAccounts\n\t}\n\n\t\/\/ pick the first account as the one we use\n\taccount := clefAccounts[0]\n\n\t\/\/ clef currently does not expose the public key\n\t\/\/ sign some data so we can recover it\n\tsig, err := clef.SignData(account, accounts.MimetypeTextPlain, clefRecoveryMessage)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpubKey, err := recoverFunc(sig, clefRecoveryMessage)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &clefSigner{\n\t\tclef:    clef,\n\t\taccount: account,\n\t\tpubKey:  pubKey,\n\t}, nil\n}\n\n\/\/ PublicKey returns the public key recovered during creation\nfunc (c *clefSigner) PublicKey() (*ecdsa.PublicKey, error) {\n\treturn c.pubKey, nil\n}\n\n\/\/ SignData signs with the text\/plain type which is the standard Ethereum prefix method\nfunc (c *clefSigner) Sign(data []byte) ([]byte, error) {\n\treturn c.clef.SignData(c.account, accounts.MimetypeTextPlain, data)\n}\n\n\/\/ SignTx signs an ethereum transaction\nfunc (c *clefSigner) SignTx(transaction *types.Transaction) (*types.Transaction, error) {\n\t\/\/ chainId is nil here because it is set on the clef side\n\treturn c.clef.SignTx(c.account, transaction, nil)\n}\n\n\/\/ EthereumAddress returns the ethereum address this signer uses\nfunc (c *clefSigner) EthereumAddress() (common.Address, error) {\n\treturn c.account.Address, nil\n}\n","lang_cluster":"Go","length":111,"code_uid":"70b5cd210e9646e4932982e06ba6df36"}
{"diff_hunk":"@@ -19,6 +19,7 @@ package validation\n import (\n \t\"fmt\"\n \t\"net\"\n+\t\"net\/mail\"\n \n \t\"k8s.io\/apimachinery\/pkg\/runtime\"\n \t\"k8s.io\/apimachinery\/pkg\/util\/validation\/field\"","old_code":"\/*\nCopyright 2019 The Jetstack cert-manager contributors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage validation\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\n\t\"k8s.io\/apimachinery\/pkg\/runtime\"\n\t\"k8s.io\/apimachinery\/pkg\/util\/validation\/field\"\n\n\t\"github.com\/jetstack\/cert-manager\/pkg\/api\/util\"\n\tcmapiv1alpha2 \"github.com\/jetstack\/cert-manager\/pkg\/apis\/certmanager\/v1alpha2\"\n\tcmapi \"github.com\/jetstack\/cert-manager\/pkg\/internal\/apis\/certmanager\"\n\tcmmeta \"github.com\/jetstack\/cert-manager\/pkg\/internal\/apis\/meta\"\n)\n\n\/\/ Validation functions for cert-manager Certificate types\n\nfunc ValidateCertificateSpec(crt *cmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {\n\tel := field.ErrorList{}\n\tif crt.SecretName == \"\" {\n\t\tel = append(el, field.Required(fldPath.Child(\"secretName\"), \"must be specified\"))\n\t}\n\n\tel = append(el, validateIssuerRef(crt.IssuerRef, fldPath)...)\n\n\tif len(crt.CommonName) == 0 && len(crt.DNSNames) == 0 && len(crt.URISANs) == 0 {\n\t\tel = append(el, field.Required(fldPath.Child(\"commonName\", \"dnsNames\", \"uriSANs\"),\n\t\t\t\"at least one of commonName, dnsNames, or uriSANs must be set\"))\n\t}\n\n\t\/\/ if a common name has been specified, ensure it is no longer than 64 chars\n\tif len(crt.CommonName) > 64 {\n\t\tel = append(el, field.TooLong(fldPath.Child(\"commonName\"), crt.CommonName, 64))\n\t}\n\n\tif len(crt.IPAddresses) > 0 {\n\t\tel = append(el, validateIPAddresses(crt, fldPath)...)\n\t}\n\tswitch crt.KeyAlgorithm {\n\tcase cmapi.KeyAlgorithm(\"\"):\n\tcase cmapi.RSAKeyAlgorithm:\n\t\tif crt.KeySize > 0 && (crt.KeySize < 2048 || crt.KeySize > 8192) {\n\t\t\tel = append(el, field.Invalid(fldPath.Child(\"keySize\"), crt.KeySize, \"must be between 2048 & 8192 for rsa keyAlgorithm\"))\n\t\t}\n\tcase cmapi.ECDSAKeyAlgorithm:\n\t\tif crt.KeySize > 0 && crt.KeySize != 256 && crt.KeySize != 384 && crt.KeySize != 521 {\n\t\t\tel = append(el, field.NotSupported(fldPath.Child(\"keySize\"), crt.KeySize, []string{\"256\", \"384\", \"521\"}))\n\t\t}\n\tdefault:\n\t\tel = append(el, field.Invalid(fldPath.Child(\"keyAlgorithm\"), crt.KeyAlgorithm, \"must be either empty or one of rsa or ecdsa\"))\n\t}\n\n\tif crt.Duration != nil || crt.RenewBefore != nil {\n\t\tel = append(el, ValidateDuration(crt, fldPath)...)\n\t}\n\tif len(crt.Usages) > 0 {\n\t\tel = append(el, validateUsages(crt, fldPath)...)\n\t}\n\treturn el\n}\n\nfunc ValidateCertificate(obj runtime.Object) field.ErrorList {\n\tcrt := obj.(*cmapi.Certificate)\n\tallErrs := ValidateCertificateSpec(&crt.Spec, field.NewPath(\"spec\"))\n\treturn allErrs\n}\n\nfunc validateIssuerRef(issuerRef cmmeta.ObjectReference, fldPath *field.Path) field.ErrorList {\n\tel := field.ErrorList{}\n\n\tissuerRefPath := fldPath.Child(\"issuerRef\")\n\tif issuerRef.Name == \"\" {\n\t\tel = append(el, field.Required(issuerRefPath.Child(\"name\"), \"must be specified\"))\n\t}\n\tif issuerRef.Group == \"\" || issuerRef.Group == cmapi.SchemeGroupVersion.Group {\n\t\tswitch issuerRef.Kind {\n\t\tcase \"\":\n\t\tcase \"Issuer\", \"ClusterIssuer\":\n\t\tdefault:\n\t\t\tel = append(el, field.Invalid(issuerRefPath.Child(\"kind\"), issuerRef.Kind, \"must be one of Issuer or ClusterIssuer\"))\n\t\t}\n\t}\n\treturn el\n}\n\nfunc validateIPAddresses(a *cmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {\n\tif len(a.IPAddresses) <= 0 {\n\t\treturn nil\n\t}\n\tel := field.ErrorList{}\n\tfor i, d := range a.IPAddresses {\n\t\tip := net.ParseIP(d)\n\t\tif ip == nil {\n\t\t\tel = append(el, field.Invalid(fldPath.Child(\"ipAddresses\").Index(i), d, \"invalid IP address\"))\n\t\t}\n\t}\n\treturn el\n}\n\nfunc validateUsages(a *cmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {\n\tel := field.ErrorList{}\n\tfor i, u := range a.Usages {\n\t\t_, kok := util.KeyUsageType(cmapiv1alpha2.KeyUsage(u))\n\t\t_, ekok := util.ExtKeyUsageType(cmapiv1alpha2.KeyUsage(u))\n\t\tif !kok && !ekok {\n\t\t\tel = append(el, field.Invalid(fldPath.Child(\"usages\").Index(i), u, \"unknown keyusage\"))\n\t\t}\n\t}\n\treturn el\n}\n\nfunc ValidateDuration(crt *cmapi.CertificateSpec, fldPath *field.Path) field.ErrorList {\n\tel := field.ErrorList{}\n\n\tduration := util.DefaultCertDuration(crt.Duration)\n\trenewBefore := cmapiv1alpha2.DefaultRenewBefore\n\tif crt.RenewBefore != nil {\n\t\trenewBefore = crt.RenewBefore.Duration\n\t}\n\tif duration < cmapiv1alpha2.MinimumCertificateDuration {\n\t\tel = append(el, field.Invalid(fldPath.Child(\"duration\"), duration, fmt.Sprintf(\"certificate duration must be greater than %s\", cmapiv1alpha2.MinimumCertificateDuration)))\n\t}\n\tif renewBefore < cmapiv1alpha2.MinimumRenewBefore {\n\t\tel = append(el, field.Invalid(fldPath.Child(\"renewBefore\"), renewBefore, fmt.Sprintf(\"certificate renewBefore must be greater than %s\", cmapiv1alpha2.MinimumRenewBefore)))\n\t}\n\tif duration <= renewBefore {\n\t\tel = append(el, field.Invalid(fldPath.Child(\"renewBefore\"), renewBefore, fmt.Sprintf(\"certificate duration %s must be greater than renewBefore %s\", duration, renewBefore)))\n\t}\n\treturn el\n}\n","lang_cluster":"Go","length":146,"code_uid":"25f36661a18e4b4db1ab2e6a829bfe60"}
{"diff_hunk":"@@ -95,37 +95,9 @@ func buildAndCreateCVR() {\n \t\tTargetIP:   cvrObj.Spec.TargetIP,\n \t\tPhase:      \"Recreate\",\n \t\tCapacity:   cvrObj.Spec.Capacity,\n+\t\tReplicaID:  replicaID,\n \t}\n \tops.Config = cvrConfig\n \tnewCVRObj = ops.BuildAndCreateCVR()\n-\n-\tcvrName := pvcObj.Spec.VolumeName + \"-\" + cspObj.Name\n-\thashUID, err := hash.Hash(newCVRObj.UID)\n-\tExpect(err).To(BeNil())\n-\tReplicaID = strings.ToUpper(hashUID)\n-\tfor i := 0; i < retryUpdate; i++ {\n-\t\tnewCVRObj.Spec.ReplicaID = ReplicaID\n-\t\tnewCVRObj, err = ops.CVRClient.\n-\t\t\tWithNamespace(openebsNamespace).\n-\t\t\tUpdate(newCVRObj)\n-\t\tif err == nil {\n-\t\t\tbreak\n-\t\t}\n-\t\ttime.Sleep(time.Second * 5)\n-\t\tnewCVRObj, getErr = ops.CVRClient.Get(cvrName, metav1.GetOptions{})\n-\t\tExpect(getErr).To(BeNil())\n-\t}\n-\tExpect(err).To(BeNil())\n-\t\/\/TODO: Need to fix bug in cvr during creation time\n-\tpodLabel := cspLabel + cspObj.Name\n-\tpodObjList, err := ops.PodClient.\n-\t\tWithNamespace(openebsNamespace).\n-\t\tList(metav1.ListOptions{LabelSelector: podLabel})\n-\tExpect(err).To(BeNil())\n-\terr = ops.PodClient.Delete(podObjList.Items[0].Name, &metav1.DeleteOptions{})\n-\tExpect(err).To(BeNil())\n-\tisPodDeleted := ops.IsPodDeletedEventually(\n-\t\tpodObjList.Items[0].Namespace,\n-\t\tpodObjList.Items[0].Name)\n-\tExpect(isPodDeleted).To(Equal(true))\n+\tReplicaID = replicaID\n }","old_code":"\/*\nCopyright 2019 The OpenEBS Authors\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage replicascaleup\n\nimport (\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t. \"github.com\/onsi\/gomega\"\n\tapis \"github.com\/openebs\/maya\/pkg\/apis\/openebs.io\/v1alpha1\"\n\thash \"github.com\/openebs\/maya\/pkg\/hash\"\n\t\"github.com\/openebs\/maya\/tests\"\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n\n\t\/\/ auth plugins\n\t_ \"k8s.io\/client-go\/plugin\/pkg\/client\/auth\/gcp\"\n)\n\nfunc verifyVolumeConfigurationEventually() {\n\tvar err error\n\tconsistencyFactor := (ReplicaCount \/ 2) + 1\n\tfor i := 0; i < MaxRetry; i++ {\n\t\tcvObj, err = ops.CVClient.WithNamespace(openebsNamespace).\n\t\t\tGet(pvcObj.Spec.VolumeName, metav1.GetOptions{})\n\t\tExpect(err).To(BeNil())\n\t\tif cvObj.Spec.ReplicationFactor == ReplicaCount {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(5 * time.Second)\n\t}\n\tExpect(cvObj.Spec.ConsistencyFactor).To(Equal(consistencyFactor), \"mismatch of consistencyFactor\")\n\t_, isReplicaIDExist := cvObj.Status.ReplicaDetails.KnownReplicas[apis.ReplicaID(ReplicaID)]\n\tExpect(isReplicaIDExist).To(Equal(true), \"replicaId should exist in known replicas of cstorvolume\")\n\tExpect(cvObj.Status.Phase).To(Equal(apis.CStorVolumePhase(\"Healthy\")))\n}\n\nfunc buildAndCreateSC() {\n\tcasConfig := strings.Replace(\n\t\topenebsCASConfigValue, \"$spcName\", spcObj.Name, 1)\n\tcasConfig = strings.Replace(\n\t\tcasConfig, \"$count\", strconv.Itoa(ReplicaCount), 1)\n\tannotations[string(apis.CASTypeKey)] = string(apis.CstorVolume)\n\tannotations[string(apis.CASConfigKey)] = casConfig\n\tscConfig := &tests.SCConfig{\n\t\tName:        scName,\n\t\tAnnotations: annotations,\n\t\tProvisioner: openebsProvisioner,\n\t}\n\tops.Config = scConfig\n\tscObj = ops.CreateStorageClass()\n}\n\nfunc updateDesiredReplicationFactor() {\n\tvar err error\n\tcvObj, err = ops.CVClient.WithNamespace(openebsNamespace).\n\t\tGet(pvcObj.Spec.VolumeName, metav1.GetOptions{})\n\tExpect(err).To(BeNil())\n\tcvObj.Spec.DesiredReplicationFactor = cvObj.Spec.DesiredReplicationFactor + 1\n\t\/\/ Namespace is already set to CVClient in above step\n\tcvObj, err = ops.CVClient.Update(cvObj)\n\tExpect(err).To(BeNil())\n}\n\nfunc buildAndCreateCVR() {\n\tvar err, getErr error\n\tretryUpdate := 3\n\tvolumeLabel := pvLabel + pvcObj.Spec.VolumeName\n\tcvrObjList, err := ops.CVRClient.\n\t\tWithNamespace(openebsNamespace).\n\t\tList(metav1.ListOptions{LabelSelector: volumeLabel})\n\tExpect(err).To(BeNil())\n\n\tcvrObj = &cvrObjList.Items[0]\n\tpoolLabel := string(apis.StoragePoolClaimCPK) + \"=\" + spcObj.Name\n\tcspObj = ops.GetUnUsedCStorPool(cvrObjList, poolLabel)\n\tcvrConfig := &tests.CVRConfig{\n\t\tVolumeName: pvcObj.Spec.VolumeName,\n\t\tPoolObj:    cspObj,\n\t\tNamespace:  openebsNamespace,\n\t\tTargetIP:   cvrObj.Spec.TargetIP,\n\t\tPhase:      \"Recreate\",\n\t\tCapacity:   cvrObj.Spec.Capacity,\n\t}\n\tops.Config = cvrConfig\n\tnewCVRObj = ops.BuildAndCreateCVR()\n\n\tcvrName := pvcObj.Spec.VolumeName + \"-\" + cspObj.Name\n\thashUID, err := hash.Hash(newCVRObj.UID)\n\tExpect(err).To(BeNil())\n\tReplicaID = strings.ToUpper(hashUID)\n\tfor i := 0; i < retryUpdate; i++ {\n\t\tnewCVRObj.Spec.ReplicaID = ReplicaID\n\t\tnewCVRObj, err = ops.CVRClient.\n\t\t\tWithNamespace(openebsNamespace).\n\t\t\tUpdate(newCVRObj)\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(time.Second * 5)\n\t\tnewCVRObj, getErr = ops.CVRClient.Get(cvrName, metav1.GetOptions{})\n\t\tExpect(getErr).To(BeNil())\n\t}\n\tExpect(err).To(BeNil())\n\t\/\/TODO: Need to fix bug in cvr during creation time\n\tpodLabel := cspLabel + cspObj.Name\n\tpodObjList, err := ops.PodClient.\n\t\tWithNamespace(openebsNamespace).\n\t\tList(metav1.ListOptions{LabelSelector: podLabel})\n\tExpect(err).To(BeNil())\n\terr = ops.PodClient.Delete(podObjList.Items[0].Name, &metav1.DeleteOptions{})\n\tExpect(err).To(BeNil())\n\tisPodDeleted := ops.IsPodDeletedEventually(\n\t\tpodObjList.Items[0].Namespace,\n\t\tpodObjList.Items[0].Name)\n\tExpect(isPodDeleted).To(Equal(true))\n}\n","lang_cluster":"Go","length":131,"code_uid":"c0865017d8564973854c87d3c2b5abde"}
{"diff_hunk":"@@ -57,7 +57,7 @@ type balanceState string\n type PromiseProcessor struct {\n \tdialog  communication.Dialog\n \tbalance identity.Balance\n-\tstorage storage.Storage\n+\tstorage Storer\n \n \tbalanceInterval   time.Duration\n \tbalanceState      balanceState","old_code":"\/*\n * Copyright (C) 2018 The \"MysteriumNetwork\/node\" Authors.\n *\n * This program is free software: you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n *\/\n\npackage noop\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\tlog \"github.com\/cihub\/seelog\"\n\t\"github.com\/mysteriumnetwork\/node\/communication\"\n\t\"github.com\/mysteriumnetwork\/node\/core\/promise\"\n\t\"github.com\/mysteriumnetwork\/node\/core\/storage\"\n\t\"github.com\/mysteriumnetwork\/node\/identity\"\n\t\"github.com\/mysteriumnetwork\/node\/money\"\n\t\"github.com\/mysteriumnetwork\/node\/service_discovery\/dto\"\n)\n\nconst (\n\tprocessorLogPrefix = \"[promise-processor] \"\n\n\tbalanceNotifying = balanceState(\"Notifying\")\n\tbalanceStopped   = balanceState(\"Stopped\")\n)\n\n\/\/ NewPromiseProcessor creates instance of PromiseProcessor\nfunc NewPromiseProcessor(dialog communication.Dialog, balance identity.Balance, storage storage.Storage) *PromiseProcessor {\n\treturn &PromiseProcessor{\n\t\tdialog:  dialog,\n\t\tbalance: balance,\n\t\tstorage: storage,\n\n\t\tbalanceInterval: 5 * time.Second,\n\t\tbalanceState:    balanceStopped,\n\t\tbalanceShutdown: make(chan bool, 1),\n\t}\n}\n\ntype balanceState string\n\n\/\/ PromiseProcessor process promises in such way, what no actual money is deducted from promise\ntype PromiseProcessor struct {\n\tdialog  communication.Dialog\n\tbalance identity.Balance\n\tstorage storage.Storage\n\n\tbalanceInterval   time.Duration\n\tbalanceState      balanceState\n\tbalanceStateMutex sync.RWMutex\n\tbalanceShutdown   chan bool\n\n\t\/\/ these are populated later at runtime\n\tlastPromise promise.Promise\n}\n\n\/\/ Start processing promises for given service proposal\nfunc (processor *PromiseProcessor) Start(proposal dto.ServiceProposal) error {\n\t\/\/ TODO: replace static value with some real data\n\tprocessor.lastPromise = promise.Promise{\n\t\tAmount: money.NewMoney(10, money.CURRENCY_MYST),\n\t}\n\n\tconsumer := promise.NewConsumer(proposal, processor.balance, processor.storage)\n\tif err := processor.dialog.Respond(consumer); err != nil {\n\t\treturn err\n\t}\n\n\tprocessor.balanceShutdown = make(chan bool, 1)\n\tgo processor.balanceLoop()\n\n\treturn nil\n}\n\n\/\/ Stop stops processing promises\nfunc (processor *PromiseProcessor) Stop() error {\n\tprocessor.balanceShutdown <- true\n\treturn nil\n}\n\nfunc (processor *PromiseProcessor) balanceLoop() {\n\tprocessor.setBalanceState(balanceNotifying)\n\nbalanceLoop:\n\tfor {\n\t\tselect {\n\t\tcase <-processor.balanceShutdown:\n\t\t\tbreak balanceLoop\n\n\t\tcase <-time.After(processor.balanceInterval):\n\t\t\t\/\/ TODO: replace static value with some real data\n\t\t\tprocessor.balanceSend(\n\t\t\t\tpromise.BalanceMessage{1, true, processor.lastPromise.Amount},\n\t\t\t)\n\t\t}\n\t}\n\n\tprocessor.setBalanceState(balanceStopped)\n}\n\nfunc (processor *PromiseProcessor) setBalanceState(state balanceState) {\n\tprocessor.balanceStateMutex.Lock()\n\tdefer processor.balanceStateMutex.Unlock()\n\n\tprocessor.balanceState = state\n}\n\nfunc (processor *PromiseProcessor) getBalanceState() balanceState {\n\tprocessor.balanceStateMutex.RLock()\n\tdefer processor.balanceStateMutex.RUnlock()\n\n\treturn processor.balanceState\n}\n\nfunc (processor *PromiseProcessor) balanceSend(message promise.BalanceMessage) error {\n\tlog.Info(processorLogPrefix, fmt.Sprintf(\"Notifying balance %s\", message.Balance.String()))\n\treturn processor.dialog.Send(&promise.BalanceMessageProducer{\n\t\tMessage: message,\n\t})\n}\n","lang_cluster":"Go","length":134,"code_uid":"c9816236c0024df5a352e49624a7b5d5"}
{"diff_hunk":"@@ -114,11 +114,22 @@ func (it *InvoiceTracker) getNotReceivedExchangeMessageCount() uint64 {\n }\n \n func (it *InvoiceTracker) sendInvoiceExpectExchangeMessage() error {\n-\terr := it.sendInvoice()\n+\t\/\/ TODO: this should be calculated according to the passed in payment period\n+\tshouldBe := uint64(math.Trunc(it.timeTracker.Elapsed().Minutes() * float64(it.paymentInfo.GetPrice().Amount) * 100000000))\n+\n+\t\/\/ TODO: fill in the fee\n+\tinvoice := crypto.CreateInvoice(it.lastInvoice.invoice.AgreementID, shouldBe, 0, it.lastInvoice.r)\n+\tinvoice.Provider = it.providerID.Address\n+\terr := it.peerInvoiceSender.Send(invoice)\n \tif err != nil {\n \t\treturn err\n \t}\n \n+\terr = it.invoiceStorage.Store(it.peer, invoice)\n+\tif err != nil {\n+\t\treturn errors.Wrap(err, \"could not store invoice\")\n+\t}\n+\n \terr = it.receiveExchangeMessageOrTimeout()\n \tif err != nil {\n \t\thandlerErr := it.handleExchangeMessageReceiveError(err)","old_code":"\/*\n * Copyright (C) 2019 The \"MysteriumNetwork\/node\" Authors.\n *\n * This program is free software: you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n *\/\n\npackage pingpong\n\nimport (\n\t\"math\"\n\t\"sync\"\n\t\"sync\/atomic\"\n\t\"time\"\n\n\t\"github.com\/ethereum\/go-ethereum\/common\"\n\t\"github.com\/mysteriumnetwork\/node\/identity\"\n\t\"github.com\/mysteriumnetwork\/payments\/crypto\"\n\t\"github.com\/pkg\/errors\"\n\t\"github.com\/rs\/zerolog\/log\"\n)\n\n\/\/ PeerInvoiceSender allows to send invoices\ntype PeerInvoiceSender interface {\n\tSend(crypto.Invoice) error\n}\n\n\/\/ ErrExchangeWaitTimeout indicates that we did not get an exchange message in time\nvar ErrExchangeWaitTimeout = errors.New(\"did not get a new exchange message\")\n\n\/\/ ErrExchangeValidationFailed indicates that there was an error with the exchange signature\nvar ErrExchangeValidationFailed = errors.New(\"exchange validation failed\")\n\nconst chargePeriodLeeway = time.Hour * 2\n\n\/\/ InvoiceTracker keeps tab of invoices and sends them to the consumer\ntype InvoiceTracker struct {\n\tpeer                            identity.Identity\n\tstop                            chan struct{}\n\tpeerInvoiceSender               PeerInvoiceSender\n\texchangeMessageChan             chan crypto.ExchangeMessage\n\tchargePeriod                    time.Duration\n\texchangeMessageWaitTimeout      time.Duration\n\tnotReceivedExchangeMessageCount uint64\n\tmaxNotReceivedExchangeMessages  uint64\n\tonce                            sync.Once\n}\n\n\/\/ NewInvoiceTracker creates a new instance of invoice tracker\nfunc NewInvoiceTracker(\n\tpeer identity.Identity,\n\tpeerInvoiceSender PeerInvoiceSender,\n\tchargePeriod time.Duration,\n\texchangeMessageChan chan crypto.ExchangeMessage,\n\texchangeMessageWaitTimeout time.Duration) *InvoiceTracker {\n\treturn &InvoiceTracker{\n\t\tpeer:                           peer,\n\t\tstop:                           make(chan struct{}),\n\t\tpeerInvoiceSender:              peerInvoiceSender,\n\t\texchangeMessageChan:            exchangeMessageChan,\n\t\texchangeMessageWaitTimeout:     exchangeMessageWaitTimeout,\n\t\tchargePeriod:                   chargePeriod,\n\t\tmaxNotReceivedExchangeMessages: calculateMaxNotReceivedExchangeMessageCount(chargePeriodLeeway, chargePeriod),\n\t}\n}\n\nfunc calculateMaxNotReceivedExchangeMessageCount(chargeLeeway, chargePeriod time.Duration) uint64 {\n\treturn uint64(math.Round(float64(chargeLeeway) \/ float64(chargePeriod)))\n}\n\n\/\/ Start stars the invoice tracker\nfunc (it *InvoiceTracker) Start() error {\n\tlog.Debug().Msg(\"Starting...\")\n\t\/\/ give the consumer a second to start up his payments before sending the first request\n\tfirstSend := time.After(time.Second)\n\tfor {\n\t\tselect {\n\t\tcase <-firstSend:\n\t\t\terr := it.sendInvoiceExpectExchangeMessage()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\tcase <-it.stop:\n\t\t\treturn nil\n\t\tcase <-time.After(it.chargePeriod):\n\t\t\terr := it.sendInvoiceExpectExchangeMessage()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (it *InvoiceTracker) markExchangeMessageNotReceived() {\n\tatomic.AddUint64(&it.notReceivedExchangeMessageCount, 1)\n}\n\nfunc (it *InvoiceTracker) resetNotReceivedExchangeMessageCount() {\n\tatomic.SwapUint64(&it.notReceivedExchangeMessageCount, 0)\n}\n\nfunc (it *InvoiceTracker) getNotReceivedExchangeMessageCount() uint64 {\n\treturn atomic.LoadUint64(&it.notReceivedExchangeMessageCount)\n}\n\nfunc (it *InvoiceTracker) sendInvoiceExpectExchangeMessage() error {\n\terr := it.sendInvoice()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = it.receiveExchangeMessageOrTimeout()\n\tif err != nil {\n\t\thandlerErr := it.handleExchangeMessageReceiveError(err)\n\t\tif handlerErr != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tit.resetNotReceivedExchangeMessageCount()\n\t}\n\treturn nil\n}\n\nfunc (it *InvoiceTracker) handleExchangeMessageReceiveError(err error) error {\n\t\/\/ if it's a timeout, we'll want to ignore it if we're not exceeding maxNotReceivedexchangeMessages\n\tif err == ErrExchangeWaitTimeout {\n\t\tit.markExchangeMessageNotReceived()\n\t\tif it.getNotReceivedExchangeMessageCount() >= it.maxNotReceivedExchangeMessages {\n\t\t\treturn err\n\t\t}\n\t\tlog.Warn().Err(err).Msg(\"Failed to receive exchangeMessage\")\n\t\treturn nil\n\t}\n\treturn err\n}\n\nfunc (it *InvoiceTracker) sendInvoice() error {\n\t\/\/ TODO: a ton of actions should go here\n\n\t\/\/ TODO: fill the fields\n\treturn it.peerInvoiceSender.Send(crypto.Invoice{AgreementID: 1234})\n}\n\nfunc (it *InvoiceTracker) receiveExchangeMessageOrTimeout() error {\n\tselect {\n\tcase pm := <-it.exchangeMessageChan:\n\t\tif res := pm.ValidateExchangeMessage(common.HexToAddress(it.peer.Address)); !res {\n\t\t\treturn ErrExchangeValidationFailed\n\t\t}\n\tcase <-time.After(it.exchangeMessageWaitTimeout):\n\t\treturn ErrExchangeWaitTimeout\n\tcase <-it.stop:\n\t\treturn nil\n\t}\n\treturn nil\n}\n\n\/\/ Stop stops the invoice tracker\nfunc (it *InvoiceTracker) Stop() {\n\tit.once.Do(func() {\n\t\tlog.Debug().Msg(\"Stopping...\")\n\t\tclose(it.stop)\n\t})\n}\n","lang_cluster":"Go","length":174,"code_uid":"a03a9458b01a4043909e48326510d802"}
{"diff_hunk":"@@ -6,6 +6,7 @@ package full\n \n import (\n \t\"context\"\n+\t\"errors\"\n \t\"math\/rand\"\n \t\"sync\"\n \t\"time\"","old_code":"\/\/ Copyright 2020 The Swarm Authors. All rights reserved.\n\/\/ Use of this source code is governed by a BSD-style\n\/\/ license that can be found in the LICENSE file.\n\npackage full\n\nimport (\n\t\"context\"\n\t\"math\/rand\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com\/ethersphere\/bee\/pkg\/addressbook\"\n\t\"github.com\/ethersphere\/bee\/pkg\/discovery\"\n\t\"github.com\/ethersphere\/bee\/pkg\/logging\"\n\t\"github.com\/ethersphere\/bee\/pkg\/p2p\"\n\t\"github.com\/ethersphere\/bee\/pkg\/swarm\"\n\t\"github.com\/ethersphere\/bee\/pkg\/topology\"\n)\n\nfunc init() {\n\trand.Seed(time.Now().UnixNano())\n}\n\nvar _ topology.Driver = (*Driver)(nil)\n\n\/\/ Driver drives the connectivity between nodes. It is a basic implementation of a connectivity Driver.\n\/\/ that enabled full connectivity in the sense that:\n\/\/ - Every peer which is added to the Driver gets broadcasted to every other peer regardless of its address.\n\/\/ - A random peer is picked when asking for a peer to retrieve an arbitrary chunk (Peerer interface).\ntype Driver struct {\n\tdiscovery     discovery.Driver\n\taddressBook   addressbook.GetPutter\n\tp2pService    p2p.Service\n\treceivedPeers map[string]struct{} \/\/ track already received peers. Note: implement cleanup or expiration if needed to stop infinite grow\n\tmtx           sync.Mutex          \/\/ guards received peers\n\tlogger        logging.Logger\n}\n\nfunc New(disc discovery.Driver, addressBook addressbook.GetPutter, p2pService p2p.Service, logger logging.Logger) *Driver {\n\treturn &Driver{\n\t\tdiscovery:     disc,\n\t\taddressBook:   addressBook,\n\t\tp2pService:    p2pService,\n\t\treceivedPeers: make(map[string]struct{}),\n\t\tlogger:        logger,\n\t}\n}\n\n\/\/ AddPeer adds a new peer to the topology driver.\n\/\/ The peer would be subsequently broadcasted to all connected peers.\n\/\/ All conneceted peers are also broadcasted to the new peer.\nfunc (d *Driver) AddPeer(ctx context.Context, addr swarm.Address) error {\n\td.mtx.Lock()\n\tif _, ok := d.receivedPeers[addr.ByteString()]; ok {\n\t\td.mtx.Unlock()\n\t\treturn nil\n\t}\n\n\td.receivedPeers[addr.ByteString()] = struct{}{}\n\td.mtx.Unlock()\n\n\tconnectedPeers := d.p2pService.Peers()\n\tma, exists := d.addressBook.Get(addr)\n\tif !exists {\n\t\treturn topology.ErrNotFound\n\t}\n\n\tif !isConnected(addr, connectedPeers) {\n\t\tpeerAddr, err := d.p2pService.Connect(ctx, ma)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t\/\/ update addr if it is wrong or it has been changed\n\t\tif !addr.Equal(peerAddr) {\n\t\t\taddr = peerAddr\n\t\t\td.addressBook.Put(peerAddr, ma)\n\t\t}\n\t}\n\n\tconnectedAddrs := []swarm.Address{}\n\tfor _, addressee := range connectedPeers {\n\t\t\/\/ skip newly added peer\n\t\tif addressee.Address.Equal(addr) {\n\t\t\tcontinue\n\t\t}\n\n\t\tconnectedAddrs = append(connectedAddrs, addressee.Address)\n\t\tif err := d.discovery.BroadcastPeers(context.Background(), addressee.Address, addr); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif len(connectedAddrs) == 0 {\n\t\treturn nil\n\t}\n\n\tif err := d.discovery.BroadcastPeers(context.Background(), addr, connectedAddrs...); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n\/\/ ChunkPeer is used to suggest a peer to ask a certain chunk from.\nfunc (d *Driver) ChunkPeer(addr swarm.Address) (peerAddr swarm.Address, err error) {\n\tconnectedPeers := d.p2pService.Peers()\n\tif len(connectedPeers) == 0 {\n\t\treturn swarm.Address{}, topology.ErrNotFound\n\t}\n\n\titemIdx := rand.Intn(len(connectedPeers))\n\ti := 0\n\tfor _, v := range connectedPeers {\n\t\tif i == itemIdx {\n\t\t\treturn v.Address, nil\n\t\t}\n\t\ti++\n\t}\n\n\treturn swarm.Address{}, topology.ErrNotFound\n}\n\nfunc isConnected(addr swarm.Address, connectedPeers []p2p.Peer) bool {\n\tfor _, p := range connectedPeers {\n\t\tif p.Address.Equal(addr) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n","lang_cluster":"Go","length":133,"code_uid":"5ca7ee747c04401abf796ba468c33358"}
{"diff_hunk":"@@ -96,17 +96,17 @@ func (api *API) ConfigGet(dottedPath string) (interface{}, error) {\n \n \/\/ ChainHead returns the head tipset\n func (api *API) ChainHead(ctx context.Context) types.TipSet {\n-\treturn api.chain.Head(ctx)\n+\treturn api.chain.Head()\n }\n \n \/\/ ChainLs returns a channel of tipsets from head to genesis\n func (api *API) ChainLs(ctx context.Context) <-chan interface{} {\n-\treturn api.chain.Ls(ctx)\n+\treturn api.chain.BlockHistory(ctx, api.chain.Head())\n }\n \n \/\/ BlockGet gets a block by CID\n func (api *API) BlockGet(ctx context.Context, id cid.Cid) (*types.Block, error) {\n-\treturn api.chain.BlockGet(ctx, id)\n+\treturn api.chain.GetBlock(ctx, id)\n }\n \n \/\/ MessagePoolRemove removes a message from the message pool","old_code":"package plumbing\n\nimport (\n\t\"context\"\n\n\t\"gx\/ipfs\/QmR8BauakNcBa3RbE4nbQu76PDiJgoQgz8AJdhJuiU4TAw\/go-cid\"\n\t\"gx\/ipfs\/QmTu65MVbemtUxJEWgsTtzv9Zv9P8rvmqNA4eG9TrTRGYc\/go-libp2p-peer\"\n\tlogging \"gx\/ipfs\/QmbkT7eMTyXfpeyB3ZMxxcxg7XH8t6uXp49jqzz4HB7BGF\/go-log\"\n\n\t\"github.com\/filecoin-project\/go-filecoin\/address\"\n\t\"github.com\/filecoin-project\/go-filecoin\/core\"\n\t\"github.com\/filecoin-project\/go-filecoin\/exec\"\n\t\"github.com\/filecoin-project\/go-filecoin\/plumbing\/cfg\"\n\t\"github.com\/filecoin-project\/go-filecoin\/plumbing\/chn\"\n\t\"github.com\/filecoin-project\/go-filecoin\/plumbing\/msg\"\n\t\"github.com\/filecoin-project\/go-filecoin\/plumbing\/mthdsig\"\n\t\"github.com\/filecoin-project\/go-filecoin\/plumbing\/ntwk\"\n\t\"github.com\/filecoin-project\/go-filecoin\/types\"\n\t\"github.com\/filecoin-project\/go-filecoin\/wallet\"\n)\n\n\/\/ API is the plumbing implementation, the irreducible set of calls required\n\/\/ to implement protocols and user\/network-facing features. You probably should\n\/\/ depend on the higher level porcelain.API instead of this api, as it includes\n\/\/ these calls in addition to higher level convenience calls to make them more\n\/\/ ergonomic.\ntype API struct {\n\tlogger logging.EventLogger\n\n\tchain        *chn.Reader\n\tconfig       *cfg.Config\n\tmessagePool  *core.MessagePool\n\tmsgPreviewer *msg.Previewer\n\tmsgQueryer   *msg.Queryer\n\tmsgSender    *msg.Sender\n\tmsgWaiter    *msg.Waiter\n\tnetwork      *ntwk.Network\n\tsigGetter    *mthdsig.Getter\n\twallet       *wallet.Wallet\n}\n\n\/\/ APIDeps contains all the API's dependencies\ntype APIDeps struct {\n\tChain        *chn.Reader\n\tConfig       *cfg.Config\n\tMessagePool  *core.MessagePool\n\tMsgPreviewer *msg.Previewer\n\tMsgQueryer   *msg.Queryer\n\tMsgSender    *msg.Sender\n\tMsgWaiter    *msg.Waiter\n\tNetwork      *ntwk.Network\n\tSigGetter    *mthdsig.Getter\n\tWallet       *wallet.Wallet\n}\n\n\/\/ New constructs a new instance of the API.\nfunc New(deps *APIDeps) *API {\n\treturn &API{\n\t\tlogger: logging.Logger(\"porcelain\"),\n\n\t\tchain:        deps.Chain,\n\t\tconfig:       deps.Config,\n\t\tmessagePool:  deps.MessagePool,\n\t\tmsgPreviewer: deps.MsgPreviewer,\n\t\tmsgQueryer:   deps.MsgQueryer,\n\t\tmsgSender:    deps.MsgSender,\n\t\tmsgWaiter:    deps.MsgWaiter,\n\t\tnetwork:      deps.Network,\n\t\tsigGetter:    deps.SigGetter,\n\t\twallet:       deps.Wallet,\n\t}\n}\n\n\/\/ ActorGetSignature returns the signature of the given actor's given method.\n\/\/ The function signature is typically used to enable a caller to decode the\n\/\/ output of an actor method call (message).\nfunc (api *API) ActorGetSignature(ctx context.Context, actorAddr address.Address, method string) (_ *exec.FunctionSignature, err error) {\n\treturn api.sigGetter.Get(ctx, actorAddr, method)\n}\n\n\/\/ ConfigSet sets the given parameters at the given path in the local config.\n\/\/ The given path may be either a single field name, or a dotted path to a field.\n\/\/ The JSON value may be either a single value or a whole data structure to be replace.\n\/\/ For example:\n\/\/ ConfigSet(\"datastore.path\", \"dev\/null\") and ConfigSet(\"datastore\", \"{\\\"path\\\":\\\"dev\/null\\\"}\")\n\/\/ are the same operation.\nfunc (api *API) ConfigSet(dottedPath string, paramJSON string) error {\n\treturn api.config.Set(dottedPath, paramJSON)\n}\n\n\/\/ ConfigGet gets config parameters from the given path.\n\/\/ The path may be either a single field name, or a dotted path to a field.\nfunc (api *API) ConfigGet(dottedPath string) (interface{}, error) {\n\treturn api.config.Get(dottedPath)\n}\n\n\/\/ ChainHead returns the head tipset\nfunc (api *API) ChainHead(ctx context.Context) types.TipSet {\n\treturn api.chain.Head(ctx)\n}\n\n\/\/ ChainLs returns a channel of tipsets from head to genesis\nfunc (api *API) ChainLs(ctx context.Context) <-chan interface{} {\n\treturn api.chain.Ls(ctx)\n}\n\n\/\/ BlockGet gets a block by CID\nfunc (api *API) BlockGet(ctx context.Context, id cid.Cid) (*types.Block, error) {\n\treturn api.chain.BlockGet(ctx, id)\n}\n\n\/\/ MessagePoolRemove removes a message from the message pool\nfunc (api *API) MessagePoolRemove(cid cid.Cid) {\n\tapi.messagePool.Remove(cid)\n}\n\n\/\/ MessagePreview previews the Gas cost of a message by running it locally on the client and\n\/\/ recording the amount of Gas used.\nfunc (api *API) MessagePreview(ctx context.Context, from, to address.Address, method string, params ...interface{}) (types.GasUnits, error) {\n\treturn api.msgPreviewer.Preview(ctx, from, to, method, params...)\n}\n\n\/\/ MessageQuery calls an actor's method using the most recent chain state. It is read-only,\n\/\/ it does not change any state. It is use to interrogate actor state. The from address\n\/\/ is optional; if not provided, an address will be chosen from the node's wallet.\nfunc (api *API) MessageQuery(ctx context.Context, optFrom, to address.Address, method string, params ...interface{}) ([][]byte, *exec.FunctionSignature, error) {\n\treturn api.msgQueryer.Query(ctx, optFrom, to, method, params...)\n}\n\n\/\/ MessageSend sends a message. It uses the default from address if none is given and signs the\n\/\/ message using the wallet. This call \"sends\" in the sense that it enqueues the\n\/\/ message in the msg pool and broadcasts it to the network; it does not wait for the\n\/\/ message to go on chain. Note that no default from address is provided. If you need\n\/\/ a default address, use MessageSendWithDefaultAddress instead.\nfunc (api *API) MessageSend(ctx context.Context, from, to address.Address, value *types.AttoFIL, gasPrice types.AttoFIL, gasLimit types.GasUnits, method string, params ...interface{}) (cid.Cid, error) {\n\treturn api.msgSender.Send(ctx, from, to, value, gasPrice, gasLimit, method, params...)\n}\n\n\/\/ MessageWait invokes the callback when a message with the given cid appears on chain.\n\/\/ It will find the message in both the case that it is already on chain and\n\/\/ the case that it appears in a newly mined block. An error is returned if one is\n\/\/ encountered or if the context is canceled. Otherwise, it waits forever for the message\n\/\/ to appear on chain.\nfunc (api *API) MessageWait(ctx context.Context, msgCid cid.Cid, cb func(*types.Block, *types.SignedMessage, *types.MessageReceipt) error) error {\n\treturn api.msgWaiter.Wait(ctx, msgCid, cb)\n}\n\n\/\/ NetworkGetPeerID gets the current peer id from Util\nfunc (api *API) NetworkGetPeerID() peer.ID {\n\treturn api.network.GetPeerID()\n}\n\n\/\/ SignBytes uses private key information associated with the given address to sign the given bytes.\nfunc (api *API) SignBytes(data []byte, addr address.Address) (types.Signature, error) {\n\treturn api.wallet.SignBytes(data, addr)\n}\n\n\/\/ WalletAddresses gets addresses from the wallet\nfunc (api *API) WalletAddresses() []address.Address {\n\treturn api.wallet.Addresses()\n}\n\n\/\/ WalletFind finds addresses on the wallet\nfunc (api *API) WalletFind(address address.Address) (wallet.Backend, error) {\n\treturn api.wallet.Find(address)\n}\n\n\/\/ WalletNewAddress generates a new wallet address\nfunc (api *API) WalletNewAddress() (address.Address, error) {\n\treturn wallet.NewAddress(api.wallet)\n}\n","lang_cluster":"Go","length":171,"code_uid":"c45dc673da2c4f7ca145181981f9fa4b"}
{"diff_hunk":"@@ -26,6 +26,10 @@ import (\n )\n \n const cloneStepID = \"CLONE\"\n+const checkoutStepID = \"CHECKOUT\"\n+const javaArtifactsID = \"JAVA-ARTIFACTS\"\n+const preStepID = \"PREPROCESS\"\n+const extractStepID = \"EXTRACT\"\n \n \/\/ commonSteps returns cloudbuild BuildSteps for copying a repo and creating\n \/\/ an output directory.","old_code":"\/*\n * Copyright 2018 The Kythe Authors. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage config\n\nimport (\n\t\"fmt\"\n\t\"path\"\n\n\t\"kythe.io\/kythe\/go\/extractors\/constants\"\n\n\t\"google.golang.org\/api\/cloudbuild\/v1\"\n)\n\nconst cloneStepID = \"CLONE\"\n\n\/\/ commonSteps returns cloudbuild BuildSteps for copying a repo and creating\n\/\/ an output directory.\n\/\/\n\/\/ The BuildStep for the repo copy uses id cloneStepID, as described in\n\/\/ https:\/\/cloud.google.com\/cloud-build\/docs\/build-config#id, for any future\n\/\/ steps that need to depend on the repo clone step.  The repo copy step puts\n\/\/ the code into \/workspace\/code.\n\/\/\n\/\/ The output directory is \/workspace\/out.\nfunc commonSteps() []*cloudbuild.BuildStep {\n\treturn []*cloudbuild.BuildStep{\n\t\t&cloudbuild.BuildStep{\n\t\t\tName:    constants.GCRGitImage, \/\/ This triggers with command 'git'.\n\t\t\tArgs:    []string{\"clone\", repoName, \"\/workspace\/code\"},\n\t\t\tId:      cloneStepID,\n\t\t\tWaitFor: []string{\"-\"},\n\t\t},\n\t\t&cloudbuild.BuildStep{\n\t\t\tName:    \"ubuntu\", \/\/ This, however, has no entrypoint command.\n\t\t\tArgs:    []string{\"mkdir\", \"\/workspace\/out\"},\n\t\t\tWaitFor: []string{\"-\"},\n\t\t},\n\t}\n}\n\nfunc preprocessorStep(build string) *cloudbuild.BuildStep {\n\treturn &cloudbuild.BuildStep{\n\t\tName:    constants.KytheBuildPreprocessorImage,\n\t\tArgs:    []string{build},\n\t\tWaitFor: []string{cloneStepID},\n\t}\n}\n\n\/\/ TODO(#3095): This step needs to be configurable by the java version used for\n\/\/ a given BuildTarget.\nfunc javaExtractorsStep() *cloudbuild.BuildStep {\n\treturn &cloudbuild.BuildStep{\n\t\tName: constants.KytheJavacExtractorArtifactsImage,\n\t\tVolumes: []*cloudbuild.Volume{\n\t\t\t&cloudbuild.Volume{\n\t\t\t\tName: javaVolumeName,\n\t\t\t\tPath: constants.DefaultExtractorsDir,\n\t\t\t},\n\t\t},\n\t\tWaitFor: []string{\"-\"},\n\t}\n}\n\nfunc zipMergeStep() *cloudbuild.BuildStep {\n\treturn &cloudbuild.BuildStep{\n\t\tName:       constants.KytheKzipToolsImage,\n\t\tEntrypoint: \"bash\",\n\t\tArgs: []string{\n\t\t\t\"-c\",\n\t\t\tfmt.Sprintf(\n\t\t\t\t\"%s merge --output %s %s\/*.kzip\",\n\t\t\t\tconstants.DefaultKzipToolLocation,\n\t\t\t\tpath.Join(outputDirectory, outputFilePattern),\n\t\t\t\toutputDirectory),\n\t\t},\n\t}\n}\n","lang_cluster":"Go","length":91,"code_uid":"b785dc8830bc4814b5642858ae8f1e67"}
{"diff_hunk":"@@ -74,6 +74,20 @@ var configSetCmd = &cobra.Command{\n \t},\n }\n \n+\/\/ configResetCmd represents the config reset command\n+var configResetCmd = &cobra.Command{\n+\tUse:   \"reset\",\n+\tShort: \"Reset config to default\",\n+\tRunE: func(cmd *cobra.Command, args []string) error {\n+\t\tcmd.SilenceUsage = true\n+\t\toutput, err := reset()\n+\t\tif err == nil {\n+\t\t\tfmt.Println(output)\n+\t\t}\n+\t\treturn err\n+\t},\n+}\n+\n func init() {\n \tconfigSetCmd.Flags().BoolVar(&Insecure, \"insecure\", false,\n \t\t\"set insecure connection as default\")","old_code":"\/\/ Copyright (c) 2019 IoTeX\n\/\/ This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n\/\/ warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n\/\/ permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n\/\/ License 2.0 that can be found in the LICENSE file.\n\npackage config\n\nimport (\n\t\"fmt\"\n\t\"io\/ioutil\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com\/spf13\/cobra\"\n\t\"gopkg.in\/yaml.v2\"\n\n\t\"github.com\/iotexproject\/iotex-core\/ioctl\/validator\"\n)\n\nconst (\n\tipPattern       = `((25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)`\n\tdomainPattern   = `[a-zA-Z0-9][a-zA-Z0-9_-]{0,62}(\\.[a-zA-Z0-9][a-zA-Z0-9_-]{0,62})*(\\.[a-zA-Z][a-zA-Z0-9]{0,10}){1}`\n\tlocalPattern    = \"localhost\"\n\tendpointPattern = \"(\" + ipPattern + \"|(\" + domainPattern + \")\" + \"|(\" + localPattern + \"))\" + `(:\\d{1,5})?`\n)\n\nvar (\n\tvalidArgs       = []string{\"endpoint\", \"wallet\", \"currentcontext\"}\n\tendpointCompile = regexp.MustCompile(\"^\" + endpointPattern + \"$\")\n)\n\n\/\/ configGetCmd represents the config get command\nvar configGetCmd = &cobra.Command{\n\tUse:       \"get VARIABLE\",\n\tShort:     \"Get config from ioctl\",\n\tValidArgs: validArgs,\n\tArgs: func(cmd *cobra.Command, args []string) error {\n\t\tif len(args) != 1 {\n\t\t\treturn fmt.Errorf(\"accepts 1 arg(s), received %d,\"+\n\t\t\t\t\" valid arg(s): %s\", len(args), validArgs)\n\t\t}\n\t\treturn cobra.OnlyValidArgs(cmd, args)\n\t},\n\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\tcmd.SilenceUsage = true\n\t\toutput, err := Get(args[0])\n\t\tif err == nil {\n\t\t\tfmt.Println(output)\n\t\t}\n\t\treturn err\n\t},\n}\n\n\/\/ configSetCmd represents the config set command\nvar configSetCmd = &cobra.Command{\n\tUse:       \"set VARIABLE VALUE\",\n\tShort:     \"Set config for ioctl\",\n\tValidArgs: validArgs,\n\tArgs: func(cmd *cobra.Command, args []string) error {\n\t\tif len(args) != 2 {\n\t\t\treturn fmt.Errorf(\"accepts 2 arg(s), received %d,\"+\n\t\t\t\t\" valid arg(s): %s\", len(args), validArgs)\n\t\t}\n\t\treturn cobra.OnlyValidArgs(cmd, args[:1])\n\t},\n\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\tcmd.SilenceUsage = true\n\t\toutput, err := set(args)\n\t\tif err == nil {\n\t\t\tfmt.Println(output)\n\t\t}\n\t\treturn err\n\t},\n}\n\nfunc init() {\n\tconfigSetCmd.Flags().BoolVar(&Insecure, \"insecure\", false,\n\t\t\"set insecure connection as default\")\n}\n\n\/\/ Get gets config variable\nfunc Get(arg string) (string, error) {\n\tswitch arg {\n\tdefault:\n\t\treturn \"\", ErrConfigNotMatch\n\tcase \"endpoint\":\n\t\tif ReadConfig.Endpoint == \"\" {\n\t\t\treturn \"\", ErrEmptyEndpoint\n\t\t}\n\t\treturn fmt.Sprint(ReadConfig.Endpoint, \"    secure connect(TLS):\",\n\t\t\tReadConfig.SecureConnect), nil\n\tcase \"wallet\":\n\t\treturn ReadConfig.Wallet, nil\n\tcase \"currentcontext\":\n\t\treturn fmt.Sprint(ReadConfig.CurrentContext), nil\n\t}\n}\n\n\/\/ GetContextAddressOrAlias gets current context\nfunc GetContextAddressOrAlias() (string, error) {\n\tcurrentcontext := ReadConfig.CurrentContext\n\tif strings.EqualFold(currentcontext.AddressOrAlias, \"\") {\n\t\treturn \"\", fmt.Errorf(`use \"ioctl config set currentcontext address or alias\" to config current account first`)\n\t}\n\treturn currentcontext.AddressOrAlias, nil\n}\n\n\/\/ GetAddressOrAlias gets address from args or context\nfunc GetAddressOrAlias(args []string) (address string, err error) {\n\tif len(args) == 1 && !strings.EqualFold(args[0], \"\") {\n\t\taddress = args[0]\n\t} else {\n\t\taddress, err = GetContextAddressOrAlias()\n\t}\n\treturn\n}\n\n\/\/ make sure endpoint match pattern\nfunc isMatch(endpoint string) bool {\n\treturn endpointCompile.MatchString(endpoint)\n}\n\n\/\/ set sets config variable\nfunc set(args []string) (string, error) {\n\tswitch args[0] {\n\tdefault:\n\t\treturn \"\", ErrConfigNotMatch\n\tcase \"endpoint\":\n\t\tif !isMatch(args[1]) {\n\t\t\treturn \"\", fmt.Errorf(\"Endpoint %s is not valid\", args[1])\n\t\t}\n\t\tReadConfig.Endpoint = args[1]\n\t\tReadConfig.SecureConnect = !Insecure\n\tcase \"wallet\":\n\t\tReadConfig.Wallet = args[1]\n\tcase \"currentcontext\":\n\t\terr1 := validator.ValidateAlias(args[1])\n\t\terr2 := validator.ValidateAddress(args[1])\n\t\tif err1 != nil && err2 != nil {\n\t\t\treturn \"\", fmt.Errorf(\"failed to validate alias or address:%s %s\", err1, err2)\n\t\t}\n\t\tReadConfig.CurrentContext.AddressOrAlias = args[1]\n\t}\n\tout, err := yaml.Marshal(&ReadConfig)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tif err := ioutil.WriteFile(DefaultConfigFile, out, 0600); err != nil {\n\t\treturn \"\", fmt.Errorf(\"failed to write to config file %s\", DefaultConfigFile)\n\t}\n\treturn args[0] + \" is set to \" + args[1], nil\n}\n","lang_cluster":"Go","length":153,"code_uid":"371d92e6268b4d8795931e4906ae7139"}
{"diff_hunk":"@@ -112,11 +112,10 @@ func (s *Store) GetObject(ctx context.Context, path string) (object filestore.Ob\n \n \tobject.Path = path\n \tobject.Content = content\n-\tobject.Size = int64(len(content))\n \treturn\n }\n \n-func (s *Store) PutObject(ctx context.Context, path string, content []byte) error {\n+func (s *Store) Put(ctx context.Context, path string, content []byte) error {\n \twc := s.client.Bucket(s.bucket).Object(path).NewWriter(ctx)\n \tif _, err := wc.Write(content); err != nil {\n \t\twc.Close()","old_code":"\/\/ Copyright 2020 The PipeCD Authors.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage gcs\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"io\/ioutil\"\n\t\"net\/http\"\n\n\t\"cloud.google.com\/go\/storage\"\n\t\"go.uber.org\/zap\"\n\t\"google.golang.org\/api\/iterator\"\n\t\"google.golang.org\/api\/option\"\n\n\t\"github.com\/pipe-cd\/pipe\/pkg\/filestore\"\n)\n\ntype Store struct {\n\tclient          *storage.Client\n\tbucket          string\n\tcredentialsFile string\n\thttpClient      *http.Client\n\tlogger          *zap.Logger\n}\n\ntype Option func(*Store)\n\nfunc WithCredentialsFile(path string) Option {\n\treturn func(s *Store) {\n\t\ts.credentialsFile = path\n\t}\n}\n\nfunc WithHTTPClient(client *http.Client) Option {\n\treturn func(s *Store) {\n\t\ts.httpClient = client\n\t}\n}\n\nfunc WithLogger(logger *zap.Logger) Option {\n\treturn func(s *Store) {\n\t\ts.logger = logger.Named(\"gcs\")\n\t}\n}\n\nfunc NewStore(ctx context.Context, bucket string, opts ...Option) (*Store, error) {\n\ts := &Store{\n\t\tbucket: bucket,\n\t\tlogger: zap.NewNop(),\n\t}\n\tfor _, opt := range opts {\n\t\topt(s)\n\t}\n\n\tvar options []option.ClientOption\n\tif s.credentialsFile != \"\" {\n\t\toptions = append(options, option.WithCredentialsFile(s.credentialsFile))\n\t}\n\tif s.httpClient != nil {\n\t\toptions = append(options, option.WithHTTPClient(s.httpClient))\n\t}\n\tclient, err := storage.NewClient(ctx, options...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.client = client\n\treturn s, nil\n}\n\nfunc (s *Store) NewReader(ctx context.Context, path string) (rc io.ReadCloser, err error) {\n\trc, err = s.client.Bucket(s.bucket).Object(path).NewReader(ctx)\n\tswitch err {\n\tcase nil:\n\tcase storage.ErrObjectNotExist:\n\t\terr = filestore.ErrNotFound\n\t\treturn\n\tdefault:\n\t\ts.logger.Error(\"failed to create GCS object reader\", zap.String(\"path\", path), zap.Error(err))\n\t\treturn\n\t}\n\treturn\n}\n\nfunc (s *Store) GetObject(ctx context.Context, path string) (object filestore.Object, err error) {\n\trc, err := s.NewReader(ctx, path)\n\tif err != nil {\n\t\treturn\n\t}\n\tdefer func() {\n\t\tif err := rc.Close(); err != nil {\n\t\t\ts.logger.Error(\"failed to close object reader\")\n\t\t}\n\t}()\n\n\tcontent, err := ioutil.ReadAll(rc)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tobject.Path = path\n\tobject.Content = content\n\tobject.Size = int64(len(content))\n\treturn\n}\n\nfunc (s *Store) PutObject(ctx context.Context, path string, content []byte) error {\n\twc := s.client.Bucket(s.bucket).Object(path).NewWriter(ctx)\n\tif _, err := wc.Write(content); err != nil {\n\t\twc.Close()\n\t\treturn err\n\t}\n\tif err := wc.Close(); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (s *Store) ListObjects(ctx context.Context, prefix string) ([]filestore.Object, error) {\n\tvar objects []filestore.Object\n\tquery := &storage.Query{\n\t\tPrefix: prefix,\n\t}\n\tit := s.client.Bucket(s.bucket).Objects(ctx, query)\n\tfor {\n\t\tattrs, err := it.Next()\n\t\tif err == iterator.Done {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\ts.logger.Error(\"failed to iterate to the next object\",\n\t\t\t\tzap.String(\"prefix\", prefix),\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t\treturn nil, err\n\t\t}\n\t\tobject := filestore.Object{\n\t\t\tPath:    attrs.Name,\n\t\t\tSize:    attrs.Size,\n\t\t\tContent: []byte{},\n\t\t}\n\t\tobjects = append(objects, object)\n\t}\n\treturn objects, nil\n}\n\nfunc (s *Store) Close() error {\n\treturn s.client.Close()\n}\n","lang_cluster":"Go","length":161,"code_uid":"3b5230c25c6f4c448058f610c9c4dad8"}
{"diff_hunk":"@@ -1,3 +1,4 @@\n+\/\/go:build cgo\n \/\/ +build cgo\n \n package sqlstore","old_code":"\/\/ +build cgo\n\npackage sqlstore\n\nimport (\n\t\"errors\"\n\t\"net\/url\"\n\n\t\"github.com\/jinzhu\/gorm\"\n\t\"github.com\/mattn\/go-sqlite3\"\n\t\"github.com\/sirupsen\/logrus\"\n\n\t\/\/ gorm sqlite dialect init registration\n\t_ \"github.com\/jinzhu\/gorm\/dialects\/sqlite\"\n)\n\ntype sqliteDB struct {\n\tlog logrus.FieldLogger\n}\n\nfunc (s sqliteDB) connect(cfg *configuration, isReadOnly bool) (db *gorm.DB, version string, supportsCTE bool, err error) {\n\tif isReadOnly {\n\t\ts.log.Warn(\"Read-only connection is not applicable for sqlite3. Falling back to primary connection\")\n\t}\n\n\tdb, err = openSQLite3(cfg.ConnectionString)\n\tif err != nil {\n\t\treturn nil, \"\", false, err\n\t}\n\n\tversion, err = queryVersion(db, \"SELECT sqlite_version()\")\n\tif err != nil {\n\t\treturn nil, \"\", false, err\n\t}\n\n\t\/\/ The embedded version of SQLite3 unconditionally supports CTE.\n\treturn db, version, true, nil\n}\n\nfunc (s sqliteDB) isConstraintViolation(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tvar e sqlite3.Error\n\tok := errors.As(err, &e)\n\treturn ok && e.Code == sqlite3.ErrConstraint\n}\n\nfunc openSQLite3(connString string) (*gorm.DB, error) {\n\tembellished, err := embellishSQLite3ConnString(connString)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdb, err := gorm.Open(\"sqlite3\", embellished)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\treturn db, nil\n}\n\n\/\/ embellishSQLite3ConnString adds query values supported by\n\/\/ github.com\/mattn\/go-sqlite3 to enable journal mode and foreign key support.\n\/\/ These query values MUST be part of the connection string in order to be\n\/\/ enabled for *each* connection opened by db\/sql. If the connection string is\n\/\/ not already a file: URI, it is converted first.\nfunc embellishSQLite3ConnString(connectionString string) (string, error) {\n\tu, err := url.Parse(connectionString)\n\tif err != nil {\n\t\treturn \"\", sqlError.Wrap(err)\n\t}\n\n\tswitch {\n\tcase u.Scheme == \"\":\n\t\t\/\/ connection string is a path. move the path section into the\n\t\t\/\/ opaque section so it renders property for sqlite3, for example:\n\t\t\/\/ data.db = file:data.db\n\t\t\/\/ .\/data.db = file:.\/data.db\n\t\t\/\/ \/data.db = file:\/data.db\n\t\tu.Scheme = \"file\"\n\t\tu.Opaque, u.Path = u.Path, \"\"\n\tcase u.Scheme != \"file\":\n\t\t\/\/ only no scheme (i.e. file path) or file scheme is supported\n\t\treturn \"\", sqlError.New(\"unsupported scheme %q\", u.Scheme)\n\t}\n\n\tq := u.Query()\n\tq.Set(\"_foreign_keys\", \"ON\")\n\tq.Set(\"_journal_mode\", \"WAL\")\n\tu.RawQuery = q.Encode()\n\treturn u.String(), nil\n}\n","lang_cluster":"Go","length":91,"code_uid":"a894edea788c4ad9b6bebbb3f172df5b"}
{"diff_hunk":"@@ -5,6 +5,14 @@\n \/\/ This file defines service deployment resources.\n package deploy\n \n+import (\n+\t\"fmt\"\n+\t\"strings\"\n+)\n+\n+\/\/ FmtTaskECRRepoName is the pattern used to generate the ECR repository's name\n+const FmtTaskECRRepoName = \"copilot-%s\"\n+\n \/\/ CreateTaskResourcesInput holds the fields required to create a task stack.\n type CreateTaskResourcesInput struct {\n \tName   string","old_code":"\/\/ Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\/\/ SPDX-License-Identifier: Apache-2.0\n\n\/\/ Package deploy holds the structures to deploy infrastructure resources.\n\/\/ This file defines service deployment resources.\npackage deploy\n\n\/\/ CreateTaskResourcesInput holds the fields required to create a task stack.\ntype CreateTaskResourcesInput struct {\n\tName   string\n\tCPU    int\n\tMemory int\n\n\tImage         string\n\tTaskRole      string\n\tExecutionRole string\n\tCommand       []string\n\tEnvVars       map[string]string\n\n\tApp string\n\tEnv string\n\n\tAdditionalTags map[string]string\n}\n","lang_cluster":"Go","length":24,"code_uid":"0abc7e00bab64632805ec0e62c07a974"}
{"diff_hunk":"@@ -1,12 +1,14 @@\n package action\n \n import (\n+\t\"math\/big\"\n \t\"testing\"\n \n \t\"github.com\/iotexproject\/iotex-proto\/golang\/iotextypes\"\n \t\"github.com\/stretchr\/testify\/require\"\n \n \t\"github.com\/iotexproject\/iotex-core\/pkg\/unit\"\n+\t\"github.com\/iotexproject\/iotex-core\/state\"\n \t\"github.com\/iotexproject\/iotex-core\/test\/identityset\"\n )\n ","old_code":"package action\n\nimport (\n\t\"testing\"\n\n\t\"github.com\/iotexproject\/iotex-proto\/golang\/iotextypes\"\n\t\"github.com\/stretchr\/testify\/require\"\n\n\t\"github.com\/iotexproject\/iotex-core\/pkg\/unit\"\n\t\"github.com\/iotexproject\/iotex-core\/test\/identityset\"\n)\n\nfunc TestEnvelope_Basic(t *testing.T) {\n\treq := require.New(t)\n\tevlp, tsf := createEnvelope()\n\treq.Equal(uint32(1), evlp.Version())\n\treq.Equal(uint64(10), evlp.Nonce())\n\treq.Equal(uint64(20010), evlp.GasLimit())\n\treq.Equal(\"11000000000000000000\", evlp.GasPrice().String())\n\tc, err := evlp.Cost()\n\treq.NoError(err)\n\treq.Equal(\"111010000000000000000000\", c.String())\n\tg, err := evlp.IntrinsicGas()\n\treq.NoError(err)\n\treq.Equal(uint64(10000), g)\n\td, ok := evlp.Destination()\n\treq.True(ok)\n\treq.Equal(\"io1jh0ekmccywfkmj7e8qsuzsupnlk3w5337hjjg2\", d)\n\ttsf2, ok := evlp.Action().(*Transfer)\n\treq.True(ok)\n\treq.Equal(tsf, tsf2)\n}\nfunc TestEnvelope_Proto(t *testing.T) {\n\treq := require.New(t)\n\teb, tsf := createEnvelope()\n\tevlp, ok := eb.(*envelope)\n\treq.True(ok)\n\n\tproto := evlp.Proto()\n\tactCore := &iotextypes.ActionCore{\n\t\tVersion:  evlp.version,\n\t\tNonce:    evlp.nonce,\n\t\tGasLimit: evlp.gasLimit,\n\t}\n\tactCore.GasPrice = evlp.gasPrice.String()\n\tactCore.Action = &iotextypes.ActionCore_Transfer{Transfer: tsf.Proto()}\n\treq.Equal(actCore, proto)\n\n\treq.NoError(evlp.LoadProto(proto))\n\ttsf2, ok := evlp.Action().(*Transfer)\n\treq.True(ok)\n\treq.Equal(tsf.amount, tsf2.amount)\n\treq.Equal(tsf.recipient, tsf2.recipient)\n\treq.Equal(tsf.payload, tsf2.payload)\n}\n\nfunc createEnvelope() (Envelope, *Transfer) {\n\ttsf, _ := NewTransfer(\n\t\tuint64(10),\n\t\tunit.ConvertIotxToRau(1000+int64(10)),\n\t\tidentityset.Address(10%identityset.Size()).String(),\n\t\tnil,\n\t\t20000+uint64(10),\n\t\tunit.ConvertIotxToRau(1+int64(10)),\n\t)\n\teb := EnvelopeBuilder{}\n\tevlp := eb.\n\t\tSetAction(tsf).\n\t\tSetGasLimit(tsf.GasLimit()).\n\t\tSetGasPrice(tsf.GasPrice()).\n\t\tSetNonce(tsf.Nonce()).\n\t\tSetVersion(1).\n\t\tBuild()\n\treturn evlp, tsf\n}\n","lang_cluster":"Go","length":75,"code_uid":"d2981ff079574fca985405f99b4654bf"}
{"diff_hunk":"@@ -35,7 +35,7 @@ func (b *builder) createGoInstruction(funcPtr llvm.Value, params []llvm.Value, p\n \t\t\t\/\/ The stack size is fixed at compile time. By emitting it here as a\n \t\t\t\/\/ constant, it can be optimized.\n \t\t\tif b.Scheduler == \"tasks\" && b.DefaultStackSize == 0 {\n-\t\t\t\tb.addError(pos, \"default stack size for goroutines is not set\")\n+\t\t\t\tb.addError(instr.Pos(), \"default stack size for goroutines is not set\")\n \t\t\t}\n \t\t\tstackSize = llvm.ConstInt(b.uintptrType, b.DefaultStackSize, false)\n \t\t}","old_code":"package compiler\n\n\/\/ This file implements the 'go' keyword to start a new goroutine. See\n\/\/ goroutine-lowering.go for more details.\n\nimport (\n\t\"go\/token\"\n\n\t\"github.com\/tinygo-org\/tinygo\/compiler\/llvmutil\"\n\t\"golang.org\/x\/tools\/go\/ssa\"\n\t\"tinygo.org\/x\/go-llvm\"\n)\n\n\/\/ createGoInstruction starts a new goroutine with the provided function pointer\n\/\/ and parameters.\n\/\/ In general, you should pass all regular parameters plus the context parameter.\n\/\/ There is one exception: the task-based scheduler needs to have the function\n\/\/ pointer passed in as a parameter too in addition to the context.\n\/\/\n\/\/ Because a go statement doesn't return anything, return undef.\nfunc (b *builder) createGoInstruction(funcPtr llvm.Value, params []llvm.Value, prefix string, pos token.Pos) llvm.Value {\n\tparamBundle := b.emitPointerPack(params)\n\tvar callee, stackSize llvm.Value\n\tswitch b.Scheduler {\n\tcase \"none\", \"tasks\":\n\t\tcallee = b.createGoroutineStartWrapper(funcPtr, prefix, pos)\n\t\tif b.AutomaticStackSize {\n\t\t\t\/\/ The stack size is not known until after linking. Call a dummy\n\t\t\t\/\/ function that will be replaced with a load from a special ELF\n\t\t\t\/\/ section that contains the stack size (and is modified after\n\t\t\t\/\/ linking).\n\t\t\tstackSizeFn := b.getFunction(b.program.ImportedPackage(\"internal\/task\").Members[\"getGoroutineStackSize\"].(*ssa.Function))\n\t\t\tstackSize = b.createCall(stackSizeFn, []llvm.Value{callee, llvm.Undef(b.i8ptrType), llvm.Undef(b.i8ptrType)}, \"stacksize\")\n\t\t} else {\n\t\t\t\/\/ The stack size is fixed at compile time. By emitting it here as a\n\t\t\t\/\/ constant, it can be optimized.\n\t\t\tif b.Scheduler == \"tasks\" && b.DefaultStackSize == 0 {\n\t\t\t\tb.addError(pos, \"default stack size for goroutines is not set\")\n\t\t\t}\n\t\t\tstackSize = llvm.ConstInt(b.uintptrType, b.DefaultStackSize, false)\n\t\t}\n\tcase \"coroutines\":\n\t\tcallee = b.CreatePtrToInt(funcPtr, b.uintptrType, \"\")\n\t\t\/\/ There is no goroutine stack size: coroutines are used instead of\n\t\t\/\/ stacks.\n\t\tstackSize = llvm.Undef(b.uintptrType)\n\tdefault:\n\t\tpanic(\"unreachable\")\n\t}\n\tstart := b.getFunction(b.program.ImportedPackage(\"internal\/task\").Members[\"start\"].(*ssa.Function))\n\tb.createCall(start, []llvm.Value{callee, paramBundle, stackSize, llvm.Undef(b.i8ptrType), llvm.ConstPointerNull(b.i8ptrType)}, \"\")\n\treturn llvm.Undef(funcPtr.Type().ElementType().ReturnType())\n}\n\n\/\/ createGoroutineStartWrapper creates a wrapper for the task-based\n\/\/ implementation of goroutines. For example, to call a function like this:\n\/\/\n\/\/     func add(x, y int) int { ... }\n\/\/\n\/\/ It creates a wrapper like this:\n\/\/\n\/\/     func add$gowrapper(ptr *unsafe.Pointer) {\n\/\/         args := (*struct{\n\/\/             x, y int\n\/\/         })(ptr)\n\/\/         add(args.x, args.y)\n\/\/     }\n\/\/\n\/\/ This is useful because the task-based goroutine start implementation only\n\/\/ allows a single (pointer) argument to the newly started goroutine. Also, it\n\/\/ ignores the return value because newly started goroutines do not have a\n\/\/ return value.\nfunc (c *compilerContext) createGoroutineStartWrapper(fn llvm.Value, prefix string, pos token.Pos) llvm.Value {\n\tvar wrapper llvm.Value\n\n\tbuilder := c.ctx.NewBuilder()\n\tdefer builder.Dispose()\n\n\tif !fn.IsAFunction().IsNil() {\n\t\t\/\/ See whether this wrapper has already been created. If so, return it.\n\t\tname := fn.Name()\n\t\twrapper = c.mod.NamedFunction(name + \"$gowrapper\")\n\t\tif !wrapper.IsNil() {\n\t\t\treturn llvm.ConstPtrToInt(wrapper, c.uintptrType)\n\t\t}\n\n\t\t\/\/ Create the wrapper.\n\t\twrapperType := llvm.FunctionType(c.ctx.VoidType(), []llvm.Type{c.i8ptrType}, false)\n\t\twrapper = llvm.AddFunction(c.mod, name+\"$gowrapper\", wrapperType)\n\t\twrapper.SetLinkage(llvm.LinkOnceODRLinkage)\n\t\twrapper.SetUnnamedAddr(true)\n\t\twrapper.AddAttributeAtIndex(-1, c.ctx.CreateStringAttribute(\"tinygo-gowrapper\", name))\n\t\tentry := c.ctx.AddBasicBlock(wrapper, \"entry\")\n\t\tbuilder.SetInsertPointAtEnd(entry)\n\n\t\tif c.Debug {\n\t\t\tpos := c.program.Fset.Position(pos)\n\t\t\tdiFuncType := c.dibuilder.CreateSubroutineType(llvm.DISubroutineType{\n\t\t\t\tFile:       c.getDIFile(pos.Filename),\n\t\t\t\tParameters: nil, \/\/ do not show parameters in debugger\n\t\t\t\tFlags:      0,   \/\/ ?\n\t\t\t})\n\t\t\tdifunc := c.dibuilder.CreateFunction(c.getDIFile(pos.Filename), llvm.DIFunction{\n\t\t\t\tName:         \"<goroutine wrapper>\",\n\t\t\t\tFile:         c.getDIFile(pos.Filename),\n\t\t\t\tLine:         pos.Line,\n\t\t\t\tType:         diFuncType,\n\t\t\t\tLocalToUnit:  true,\n\t\t\t\tIsDefinition: true,\n\t\t\t\tScopeLine:    0,\n\t\t\t\tFlags:        llvm.FlagPrototyped,\n\t\t\t\tOptimized:    true,\n\t\t\t})\n\t\t\twrapper.SetSubprogram(difunc)\n\t\t\tbuilder.SetCurrentDebugLocation(uint(pos.Line), uint(pos.Column), difunc, llvm.Metadata{})\n\t\t}\n\n\t\t\/\/ Create the list of params for the call.\n\t\tparamTypes := fn.Type().ElementType().ParamTypes()\n\t\tparams := llvmutil.EmitPointerUnpack(builder, c.mod, wrapper.Param(0), paramTypes[:len(paramTypes)-1])\n\t\tparams = append(params, llvm.Undef(c.i8ptrType))\n\n\t\t\/\/ Create the call.\n\t\tbuilder.CreateCall(fn, params, \"\")\n\n\t} else {\n\t\t\/\/ For a function pointer like this:\n\t\t\/\/\n\t\t\/\/     var funcPtr func(x, y int) int\n\t\t\/\/\n\t\t\/\/ A wrapper like the following is created:\n\t\t\/\/\n\t\t\/\/     func .gowrapper(ptr *unsafe.Pointer) {\n\t\t\/\/         args := (*struct{\n\t\t\/\/             x, y int\n\t\t\/\/             fn   func(x, y int) int\n\t\t\/\/         })(ptr)\n\t\t\/\/         args.fn(x, y)\n\t\t\/\/     }\n\t\t\/\/\n\t\t\/\/ With a bit of luck, identical wrapper functions like these can be\n\t\t\/\/ merged into one.\n\n\t\t\/\/ Create the wrapper.\n\t\twrapperType := llvm.FunctionType(c.ctx.VoidType(), []llvm.Type{c.i8ptrType}, false)\n\t\twrapper = llvm.AddFunction(c.mod, prefix+\".gowrapper\", wrapperType)\n\t\twrapper.SetLinkage(llvm.LinkOnceODRLinkage)\n\t\twrapper.SetUnnamedAddr(true)\n\t\twrapper.AddAttributeAtIndex(-1, c.ctx.CreateStringAttribute(\"tinygo-gowrapper\", \"\"))\n\t\tentry := c.ctx.AddBasicBlock(wrapper, \"entry\")\n\t\tbuilder.SetInsertPointAtEnd(entry)\n\n\t\tif c.Debug {\n\t\t\tpos := c.program.Fset.Position(pos)\n\t\t\tdiFuncType := c.dibuilder.CreateSubroutineType(llvm.DISubroutineType{\n\t\t\t\tFile:       c.getDIFile(pos.Filename),\n\t\t\t\tParameters: nil, \/\/ do not show parameters in debugger\n\t\t\t\tFlags:      0,   \/\/ ?\n\t\t\t})\n\t\t\tdifunc := c.dibuilder.CreateFunction(c.getDIFile(pos.Filename), llvm.DIFunction{\n\t\t\t\tName:         \"<goroutine wrapper>\",\n\t\t\t\tFile:         c.getDIFile(pos.Filename),\n\t\t\t\tLine:         pos.Line,\n\t\t\t\tType:         diFuncType,\n\t\t\t\tLocalToUnit:  true,\n\t\t\t\tIsDefinition: true,\n\t\t\t\tScopeLine:    0,\n\t\t\t\tFlags:        llvm.FlagPrototyped,\n\t\t\t\tOptimized:    true,\n\t\t\t})\n\t\t\twrapper.SetSubprogram(difunc)\n\t\t\tbuilder.SetCurrentDebugLocation(uint(pos.Line), uint(pos.Column), difunc, llvm.Metadata{})\n\t\t}\n\n\t\t\/\/ Get the list of parameters, with the extra parameters at the end.\n\t\tparamTypes := fn.Type().ElementType().ParamTypes()\n\t\tparamTypes[len(paramTypes)-1] = fn.Type() \/\/ the last element is the function pointer\n\t\tparams := llvmutil.EmitPointerUnpack(builder, c.mod, wrapper.Param(0), paramTypes)\n\n\t\t\/\/ Get the function pointer.\n\t\tfnPtr := params[len(params)-1]\n\n\t\t\/\/ Ignore the last param, which isn't used anymore.\n\t\t\/\/ TODO: avoid this extra \"parent handle\" parameter in most functions.\n\t\tparams[len(params)-1] = llvm.Undef(c.i8ptrType)\n\n\t\t\/\/ Create the call.\n\t\tbuilder.CreateCall(fnPtr, params, \"\")\n\t}\n\n\t\/\/ Finish the function. Every basic block must end in a terminator, and\n\t\/\/ because goroutines never return a value we can simply return void.\n\tbuilder.CreateRetVoid()\n\n\t\/\/ Return a ptrtoint of the wrapper, not the function itself.\n\treturn builder.CreatePtrToInt(wrapper, c.uintptrType, \"\")\n}\n","lang_cluster":"Go","length":197,"code_uid":"e55f7bff0c594a75a89e7e1c5b9e7cfe"}
{"diff_hunk":"@@ -50,6 +50,7 @@ var (\n \t\tName:  \"agreed-terms-and-conditions\",\n \t\tUsage: \"Agree with terms & conditions\",\n \t}\n+\n \t\/\/ FlagAccessPolicyAddress Trust oracle URL for retrieving access policies.\n \tFlagAccessPolicyAddress = cli.StringFlag{\n \t\tName:  \"access-policy.address\",","old_code":"\/*\n * Copyright (C) 2019 The \"MysteriumNetwork\/node\" Authors.\n *\n * This program is free software: you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n *\/\n\npackage config\n\nimport (\n\t\"time\"\n\n\t\"github.com\/mysteriumnetwork\/node\/metadata\"\n\t\"github.com\/urfave\/cli\/v2\"\n)\n\n\/\/ ServicesOptions describes options shared among multiple services\ntype ServicesOptions struct {\n\tAccessPolicyAddress       string\n\tAccessPolicyList          []string\n\tAccessPolicyFetchInterval time.Duration\n\tShaperEnabled             bool\n}\n\nvar (\n\t\/\/ FlagIdentity keystore's identity.\n\tFlagIdentity = cli.StringFlag{\n\t\tName:  \"identity\",\n\t\tUsage: \"Keystore's identity used to provide service. If not given identity will be created automatically\",\n\t\tValue: \"\",\n\t}\n\t\/\/ FlagIdentityPassphrase passphrase to unlock the identity.\n\tFlagIdentityPassphrase = cli.StringFlag{\n\t\tName:  \"identity.passphrase\",\n\t\tUsage: \"Used to unlock keystore's identity\",\n\t\tValue: \"\",\n\t}\n\t\/\/ FlagAgreedTermsConditions agree with terms & conditions.\n\tFlagAgreedTermsConditions = cli.BoolFlag{\n\t\tName:  \"agreed-terms-and-conditions\",\n\t\tUsage: \"Agree with terms & conditions\",\n\t}\n\t\/\/ FlagAccessPolicyAddress Trust oracle URL for retrieving access policies.\n\tFlagAccessPolicyAddress = cli.StringFlag{\n\t\tName:  \"access-policy.address\",\n\t\tUsage: \"URL of trust oracle endpoint for retrieving lists of access policies\",\n\t\tValue: metadata.DefaultNetwork.AccessPolicyOracleAddress,\n\t}\n\t\/\/ FlagAccessPolicyList a comma-separated list of access policies that determines allowed identities to use the service.\n\tFlagAccessPolicyList = cli.StringFlag{\n\t\tName:  \"access-policy.list\",\n\t\tUsage: \"Comma separated list that determines the access policies applied to provide service.\",\n\t\tValue: \"\",\n\t}\n\t\/\/ FlagAccessPolicyFetchInterval policy list fetch interval.\n\tFlagAccessPolicyFetchInterval = cli.DurationFlag{\n\t\tName:  \"access-policy.fetch\",\n\t\tUsage: `Proposal fetch interval { \"30s\", \"3m\", \"1h20m30s\" }`,\n\t\tValue: 10 * time.Minute,\n\t}\n\t\/\/ FlagShaperEnabled enables bandwidth limitation.\n\tFlagShaperEnabled = cli.BoolFlag{\n\t\tName:  \"shaper.enabled\",\n\t\tUsage: \"Limit service bandwidth\",\n\t}\n\t\/\/ FlagNoopPriceMinute sets the price per minute for provided noop service.\n\tFlagNoopPriceMinute = cli.Float64Flag{\n\t\tName:   \"noop.price-minute\",\n\t\tUsage:  \"Sets the price of the noop service per minute.\",\n\t\tValue:  0.0001,\n\t\tHidden: true,\n\t}\n)\n\n\/\/ RegisterFlagsServiceShared registers shared service CLI flags\nfunc RegisterFlagsServiceShared(flags *[]cli.Flag) {\n\t*flags = append(*flags,\n\t\t&FlagIdentity,\n\t\t&FlagIdentityPassphrase,\n\t\t&FlagAgreedTermsConditions,\n\t\t&FlagAccessPolicyAddress,\n\t\t&FlagAccessPolicyList,\n\t\t&FlagAccessPolicyFetchInterval,\n\t\t&FlagShaperEnabled,\n\t\t&FlagNoopPriceMinute,\n\t)\n}\n\n\/\/ ParseFlagsServiceShared parses shared service CLI flags and registers values to the configuration\nfunc ParseFlagsServiceShared(ctx *cli.Context) {\n\tCurrent.ParseStringFlag(ctx, FlagIdentity)\n\tCurrent.ParseStringFlag(ctx, FlagIdentityPassphrase)\n\tCurrent.ParseBoolFlag(ctx, FlagAgreedTermsConditions)\n\tCurrent.ParseStringFlag(ctx, FlagAccessPolicyAddress)\n\tCurrent.ParseStringFlag(ctx, FlagAccessPolicyList)\n\tCurrent.ParseDurationFlag(ctx, FlagAccessPolicyFetchInterval)\n\tCurrent.ParseBoolFlag(ctx, FlagShaperEnabled)\n\tCurrent.ParseFloat64Flag(ctx, FlagNoopPriceMinute)\n}\n","lang_cluster":"Go","length":109,"code_uid":"b202f6391cb949fd8be4457503e95c8c"}
{"diff_hunk":"@@ -22,6 +22,14 @@ type Bounds struct {\n \tindex int\n }\n \n+\/\/ NewBounds create a new Bounds given start and stop values\n+func NewBounds(start, stop values.Time) Bounds {\n+\treturn Bounds{\n+\t\tstart: start,\n+\t\tstop:  stop,\n+\t}\n+}\n+\n func (b Bounds) Start() values.Time {\n \treturn b.start\n }","old_code":"package interval\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\n\t\"github.com\/influxdata\/flux\/values\"\n)\n\nconst (\n\tMaxTime = math.MaxInt64\n\tMinTime = math.MinInt64\n)\n\ntype Bounds struct {\n\tstart values.Time\n\tstop  values.Time\n\t\/\/ index keeps track of how many windows have been added or subtracted as additional\n\t\/\/ windows are added to or subtracted from the initial bounds. In essence, it tracks the\n\t\/\/ offset from the original bounds in order to keep operations more straightforward.\n\t\/\/ See the Window struct and the window tests for additional info.\n\tindex int\n}\n\nfunc (b Bounds) Start() values.Time {\n\treturn b.start\n}\n\nfunc (b Bounds) Stop() values.Time {\n\treturn b.stop\n}\n\nfunc (b Bounds) IsEmpty() bool {\n\treturn b.start >= b.stop\n}\n\nfunc (b Bounds) String() string {\n\treturn fmt.Sprintf(\"[%v, %v)\", b.start, b.stop)\n}\n\nfunc (b Bounds) Contains(t values.Time) bool {\n\treturn t >= b.start && t < b.stop\n}\n\nfunc (b Bounds) Overlaps(o Bounds) bool {\n\treturn b.Contains(o.start) || (b.Contains(o.stop) && o.stop > b.start) || o.Contains(b.start)\n}\n\nfunc (b Bounds) Equal(o Bounds) bool {\n\treturn b == o\n}\n\nfunc (b Bounds) Length() values.Duration {\n\tif b.IsEmpty() {\n\t\treturn values.ConvertDurationNsecs(0)\n\t}\n\treturn b.stop.Sub(b.start)\n}\n","lang_cluster":"Go","length":58,"code_uid":"d0e166628ccb4d05a4e6c34c895a4256"}
{"diff_hunk":"@@ -5,7 +5,6 @@ import (\n \t\"crypto\/tls\"\n \t\"fmt\"\n \t\"net\"\n-\t\"sync\"\n \t\"time\"\n \n \t\"github.com\/ethereum\/go-ethereum\/common\"","old_code":"package locator\n\nimport (\n\t\"crypto\/ecdsa\"\n\t\"crypto\/tls\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com\/ethereum\/go-ethereum\/common\"\n\tlog \"github.com\/noxiouz\/zapctx\/ctxlog\"\n\t\"github.com\/pkg\/errors\"\n\tpb \"github.com\/sonm-io\/core\/proto\"\n\t\"github.com\/sonm-io\/core\/util\"\n\t\"go.uber.org\/zap\"\n\t\"golang.org\/x\/net\/context\"\n\t\"google.golang.org\/grpc\"\n\t\"google.golang.org\/grpc\/codes\"\n\t\"google.golang.org\/grpc\/credentials\"\n\t\"google.golang.org\/grpc\/peer\"\n\t\"google.golang.org\/grpc\/status\"\n)\n\nvar errNodeNotFound = errors.New(\"node with given Eth address cannot be found\")\n\ntype node struct {\n\tethAddr common.Address\n\tipAddr  []string\n\tts      time.Time\n}\n\ntype Locator struct {\n\tmx sync.Mutex\n\n\tconf        *LocatorConfig\n\tdb          map[common.Address]*node\n\tctx         context.Context\n\tethKey      *ecdsa.PrivateKey\n\tgrpc        *grpc.Server\n\tcertRotator util.HitlessCertRotator\n\tcreds       credentials.TransportCredentials\n}\n\nfunc (l *Locator) Announce(ctx context.Context, req *pb.AnnounceRequest) (*pb.Empty, error) {\n\tethAddr, err := l.extractEthAddr(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.G(l.ctx).Info(\"handling Announce request\",\n\t\tzap.Stringer(\"eth\", ethAddr), zap.Strings(\"ips\", req.IpAddr))\n\n\tl.putAnnounce(&node{\n\t\tethAddr: ethAddr,\n\t\tipAddr:  req.IpAddr,\n\t})\n\n\treturn &pb.Empty{}, nil\n}\n\nfunc (l *Locator) Resolve(ctx context.Context, req *pb.ResolveRequest) (*pb.ResolveReply, error) {\n\tlog.G(l.ctx).Info(\"handling Resolve request\", zap.String(\"eth\", req.EthAddr))\n\n\tif !common.IsHexAddress(req.EthAddr) {\n\t\treturn nil, fmt.Errorf(\"invalid ethaddress %s\", req.EthAddr)\n\t}\n\n\tn, err := l.getResolve(common.HexToAddress(req.EthAddr))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &pb.ResolveReply{IpAddr: n.ipAddr}, nil\n}\n\nfunc (l *Locator) Serve() error {\n\tlis, err := net.Listen(\"tcp\", l.conf.ListenAddr)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn l.grpc.Serve(lis)\n}\n\nfunc (l *Locator) extractEthAddr(ctx context.Context) (common.Address, error) {\n\tpr, ok := peer.FromContext(ctx)\n\tif !ok {\n\t\treturn common.Address{}, status.Error(codes.DataLoss, \"failed to get peer from ctx\")\n\t}\n\n\tswitch info := pr.AuthInfo.(type) {\n\tcase util.EthAuthInfo:\n\t\treturn info.Wallet, nil\n\tdefault:\n\t\treturn common.Address{}, status.Error(codes.Unauthenticated, \"wrong AuthInfo type\")\n\t}\n}\n\nfunc (l *Locator) putAnnounce(n *node) {\n\tl.mx.Lock()\n\tdefer l.mx.Unlock()\n\n\tn.ts = time.Now()\n\tl.db[n.ethAddr] = n\n}\n\nfunc (l *Locator) getResolve(ethAddr common.Address) (*node, error) {\n\tl.mx.Lock()\n\tdefer l.mx.Unlock()\n\n\tn, ok := l.db[ethAddr]\n\tif !ok {\n\t\treturn nil, errNodeNotFound\n\t}\n\n\treturn n, nil\n}\n\nfunc (l *Locator) cleanExpiredNodes() {\n\tt := time.NewTicker(l.conf.CleanupPeriod)\n\tdefer t.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-t.C:\n\t\t\tl.traverseAndClean()\n\t\t}\n\t}\n}\n\nfunc (l *Locator) traverseAndClean() {\n\tdeadline := time.Now().Add(-1 * l.conf.NodeTTL)\n\n\tl.mx.Lock()\n\tdefer l.mx.Unlock()\n\n\tvar (\n\t\ttotal = len(l.db)\n\t\tdel   uint64\n\t\tkeep  uint64\n\t)\n\tfor addr, node := range l.db {\n\t\tif node.ts.Before(deadline) {\n\t\t\tdelete(l.db, addr)\n\t\t\tdel++\n\t\t} else {\n\t\t\tkeep++\n\t\t}\n\t}\n\n\tlog.G(l.ctx).Debug(\"expired nodes cleaned\",\n\t\tzap.Int(\"total\", total), zap.Uint64(\"keep\", keep), zap.Uint64(\"del\", del))\n}\n\nfunc NewLocator(ctx context.Context, conf *LocatorConfig, key *ecdsa.PrivateKey) (l *Locator, err error) {\n\tif key == nil {\n\t\treturn nil, errors.Wrap(err, \"private key should be provided\")\n\t}\n\n\tl = &Locator{\n\t\tdb:     make(map[common.Address]*node),\n\t\tconf:   conf,\n\t\tctx:    ctx,\n\t\tethKey: key,\n\t}\n\n\tvar TLSConfig *tls.Config\n\tl.certRotator, TLSConfig, err = util.NewHitlessCertRotator(ctx, l.ethKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tl.creds = util.NewTLS(TLSConfig)\n\tsrv := util.MakeGrpcServer(l.creds)\n\tl.grpc = srv\n\n\tgo l.cleanExpiredNodes()\n\n\tpb.RegisterLocatorServer(srv, l)\n\n\treturn l, nil\n}\n","lang_cluster":"Go","length":183,"code_uid":"7b4205b54c604db185349d2ac5c3f474"}
{"diff_hunk":"@@ -5,6 +5,7 @@ import (\n \n \t\"google.golang.org\/grpc\/codes\"\n \t\"google.golang.org\/grpc\/status\"\n+\t\"google.golang.org\/protobuf\/types\/known\/structpb\"\n \tv1 \"k8s.io\/api\/core\/v1\"\n \tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n ","old_code":"package k8s\n\nimport (\n\t\"context\"\n\n\t\"google.golang.org\/grpc\/codes\"\n\t\"google.golang.org\/grpc\/status\"\n\tv1 \"k8s.io\/api\/core\/v1\"\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n\n\tk8sapiv1 \"github.com\/lyft\/clutch\/backend\/api\/k8s\/v1\"\n)\n\nfunc (s *svc) DescribeConfigMap(ctx context.Context, clientset, cluster, namespace, name string) (*k8sapiv1.ConfigMap, error) {\n\tcs, err := s.manager.GetK8sClientset(ctx, clientset, cluster, namespace)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tconfigMapList, err := cs.CoreV1().ConfigMaps(cs.Namespace()).List(ctx, metav1.ListOptions{\n\t\tFieldSelector: \"metadata.name=\" + name,\n\t})\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif len(configMapList.Items) == 1 {\n\t\treturn protoForConfigMap(cs.Cluster(), &configMapList.Items[0]), nil\n\t} else if len(configMapList.Items) > 1 {\n\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"located multiple config maps with name '%s'\", name)\n\t}\n\treturn nil, status.Error(codes.NotFound, \"unable to locate specified config map\")\n}\n\nfunc (s *svc) DeleteConfigMap(ctx context.Context, clientset, cluster, namespace, name string) error {\n\tcs, err := s.manager.GetK8sClientset(ctx, clientset, cluster, namespace)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\topts := metav1.DeleteOptions{}\n\n\treturn cs.CoreV1().ConfigMaps(cs.Namespace()).Delete(ctx, name, opts)\n}\n\nfunc (s *svc) ListConfigMaps(ctx context.Context, clientset, cluster, namespace string, listOptions *k8sapiv1.ListOptions) ([]*k8sapiv1.ConfigMap, error) {\n\tcs, err := s.manager.GetK8sClientset(ctx, clientset, cluster, namespace)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\topts := ApplyListOptions(listOptions)\n\n\tconfigMapList, err := cs.CoreV1().ConfigMaps(cs.Namespace()).List(ctx, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar configMaps []*k8sapiv1.ConfigMap\n\tfor _, cm := range configMapList.Items {\n\t\tconfigMap := cm\n\t\tconfigMaps = append(configMaps, protoForConfigMap(cs.Cluster(), &configMap))\n\t}\n\n\treturn configMaps, nil\n}\n\nfunc protoForConfigMap(cluster string, k8sconfigMap *v1.ConfigMap) *k8sapiv1.ConfigMap {\n\tclusterName := k8sconfigMap.ClusterName\n\tif clusterName == \"\" {\n\t\tclusterName = cluster\n\t}\n\n\treturn &k8sapiv1.ConfigMap{\n\t\tCluster:     clusterName,\n\t\tNamespace:   k8sconfigMap.Namespace,\n\t\tName:        k8sconfigMap.Name,\n\t\tLabels:      k8sconfigMap.Labels,\n\t\tAnnotations: k8sconfigMap.Annotations,\n\t}\n}\n","lang_cluster":"Go","length":82,"code_uid":"428465548d3b4fb0828182f14989f813"}
{"diff_hunk":"@@ -2,6 +2,7 @@ package cfg\n \n import (\n \t\"github.com\/filecoin-project\/go-filecoin\/repo\"\n+\t\"github.com\/pkg\/errors\"\n \t\"sync\"\n )\n ","old_code":"package cfg\n\nimport (\n\t\"github.com\/filecoin-project\/go-filecoin\/repo\"\n\t\"sync\"\n)\n\n\/\/ Config is plumbing implementation for setting and retrieving values from local config.\ntype Config struct {\n\trepo repo.Repo\n\tlock sync.Mutex\n}\n\n\/\/ NewConfig returns a new Config.\nfunc NewConfig(repo repo.Repo) *Config {\n\treturn &Config{repo: repo}\n}\n\n\/\/ Set sets a value in config\nfunc (s *Config) Set(dottedKey string, jsonString string) error {\n\ts.lock.Lock()\n\tdefer s.lock.Unlock()\n\n\tcfg := s.repo.Config()\n\tif err := cfg.Set(dottedKey, jsonString); err != nil {\n\t\treturn err\n\t}\n\n\treturn s.repo.ReplaceConfig(cfg)\n}\n\n\/\/ Get gets a value from config\nfunc (s *Config) Get(dottedKey string) (interface{}, error) {\n\treturn s.repo.Config().Get(dottedKey)\n}\n","lang_cluster":"Go","length":35,"code_uid":"cb0ccdb26f904d549c10f72b9ae52785"}
{"diff_hunk":"@@ -130,7 +130,7 @@ func ListBuilderForYamls(yamls ...string) *ListBuilder {\n func ListBuilderForObjects(objs ...*unstructured.Unstructured) *ListBuilder {\n \tlb := &ListBuilder{list: &UnstructList{}}\n \tfor _, obj := range objs {\n-\t\tlb.list.items = append(lb.list.items, &Unstruct{obj})\n+\t\tlb.list.Items = append(lb.list.Items, &Unstruct{obj})\n \t}\n \treturn lb\n }","old_code":"\/*\nCopyright 2019 The OpenEBS Authors\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage v1alpha2\n\nimport (\n\t\"strings\"\n\n\t\"github.com\/ghodss\/yaml\"\n\t\"github.com\/pkg\/errors\"\n\t\"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\/unstructured\"\n)\n\n\/\/ Unstruct holds an object of Unstructured\ntype Unstruct struct {\n\tobject *unstructured.Unstructured\n}\n\n\/\/ GetUnstructured converts Unstruct object\n\/\/ to API's Unstructured\nfunc (u *Unstruct) GetUnstructured() *unstructured.Unstructured {\n\treturn u.object\n}\n\n\/\/ Builder enables building of an\n\/\/ Unstructured instance\ntype Builder struct {\n\tunstruct *Unstruct\n\terrs     []error\n}\n\n\/\/ NewBuilder returns a new instance of\n\/\/ empty Builder\nfunc NewBuilder() *Builder {\n\treturn &Builder{\n\t\tunstruct: &Unstruct{\n\t\t\t&unstructured.Unstructured{\n\t\t\t\tObject: map[string]interface{}{},\n\t\t\t},\n\t\t},\n\t}\n}\n\n\/\/ BuilderForYaml returns a new instance of\n\/\/ Unstruct Builder by making use of the provided\n\/\/ YAML\nfunc BuilderForYaml(doc string) *Builder {\n\tb := NewBuilder()\n\terr := yaml.Unmarshal([]byte(doc), &b.unstruct.object)\n\tif err != nil {\n\t\tb.errs = append(b.errs, err)\n\t}\n\treturn b\n}\n\n\/\/ BuilderForObject returns a new instance of\n\/\/ Unstruct Builder by making use of the provided object\nfunc BuilderForObject(obj *unstructured.Unstructured) *Builder {\n\tb := NewBuilder()\n\tb.unstruct.object = obj\n\treturn b\n}\n\n\/\/ Build returns the Unstruct object created by\n\/\/ the Builder\nfunc (b *Builder) Build() (*Unstruct, error) {\n\tif len(b.errs) != 0 {\n\t\treturn nil, errors.Errorf(\"errors {%+v}\", b.errs)\n\t}\n\treturn b.unstruct, nil\n}\n\n\/\/ BuildAPIUnstructured returns the Unstruct object created by\n\/\/ the Builder\nfunc (b *Builder) BuildAPIUnstructured() (*unstructured.Unstructured, error) {\n\tif len(b.errs) != 0 {\n\t\treturn nil, errors.Errorf(\"errors {%+v}\", b.errs)\n\t}\n\treturn b.unstruct.object, nil\n}\n\n\/\/ UnstructList contains a list of Unstructured\n\/\/ items\ntype UnstructList struct {\n\titems []*Unstruct\n}\n\n\/\/ ListBuilder enables building a list\n\/\/ of an Unstruct instance\ntype ListBuilder struct {\n\tlist *UnstructList\n\terrs []error\n}\n\n\/\/ ListBuilderForYamls returns a new instance of\n\/\/ list Unstruct Builder by making use of the provided YAMLs\nfunc ListBuilderForYamls(yamls ...string) *ListBuilder {\n\tlb := &ListBuilder{list: &UnstructList{}}\n\tfor _, yaml := range yamls {\n\t\ty := strings.Split(strings.Trim(yaml, \"---\"), \"---\")\n\t\tfor _, f := range y {\n\t\t\tf = strings.TrimSpace(f)\n\t\t\ta, err := BuilderForYaml(f).Build()\n\t\t\tif err != nil {\n\t\t\t\tlb.errs = append(lb.errs, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tlb.list.items = append(lb.list.items, a)\n\t\t}\n\t}\n\treturn lb\n}\n\n\/\/ ListBuilderForObjects returns a mew instance of\n\/\/ list Unstruct Builder by making use of the provided\n\/\/ Unstructured object\nfunc ListBuilderForObjects(objs ...*unstructured.Unstructured) *ListBuilder {\n\tlb := &ListBuilder{list: &UnstructList{}}\n\tfor _, obj := range objs {\n\t\tlb.list.items = append(lb.list.items, &Unstruct{obj})\n\t}\n\treturn lb\n}\n\n\/\/ Build returns the list of Unstruct objects created by\n\/\/ the Builder\nfunc (l *ListBuilder) Build() ([]*Unstruct, error) {\n\tif len(l.errs) > 0 {\n\t\treturn nil, errors.Errorf(\"errors {%+v}\", l.errs)\n\t}\n\treturn l.list.items, nil\n}\n","lang_cluster":"Go","length":145,"code_uid":"af63737485294d98ba95066844e15a3b"}
{"diff_hunk":"@@ -39,7 +39,8 @@ type ArtifactStore cp.ArtifactStore\n \/\/ New returns a CodePipeline client configured against the input session.\n func New(s *session.Session) *CodePipeline {\n \treturn &CodePipeline{\n-\t\tclient: cp.New(s),\n+\t\tclient:   cp.New(s),\n+\t\trgClient: rg.New(s),\n \t}\n }\n ","old_code":"\/\/ Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\/\/ SPDX-License-Identifier: Apache-2.0\n\n\/\/ Package codepipeline provides a client to make API requests to Amazon Elastic Container Service.\npackage codepipeline\n\nimport (\n\t\"fmt\"\n\n\t\"github.com\/aws\/aws-sdk-go\/aws\"\n\t\"github.com\/aws\/aws-sdk-go\/aws\/session\"\n\tcp \"github.com\/aws\/aws-sdk-go\/service\/codepipeline\"\n)\n\ntype api interface {\n\tGetPipeline(*cp.GetPipelineInput) (*cp.GetPipelineOutput, error)\n\tListPipelines(*cp.ListPipelinesInput) (*cp.ListPipelinesOutput, error)\n}\n\n\/\/ CodePipeline wraps the AWS CodePipeline client.\ntype CodePipeline struct {\n\tclient api\n}\n\n\/\/ Pipeline contains information about the pipeline\n\/\/ TODO wrap nested resources or just use what the SDK provides?\ntype Pipeline struct {\n\tName string `json:\"name\"`\n\t\/\/ Stages        []Stage       `json:\"stages\"`\n\t\/\/ ArtifactStore ArtifactStore `json:\"artifactStore\"`\n}\n\n\/\/ Stage wraps the codepipeline pipeline stage\ntype Stage cp.StageDeclaration\n\n\/\/ ArtifactStore wraps the artifact store for the pipeline\ntype ArtifactStore cp.ArtifactStore\n\n\/\/ New returns a CodePipeline client configured against the input session.\nfunc New(s *session.Session) *CodePipeline {\n\treturn &CodePipeline{\n\t\tclient: cp.New(s),\n\t}\n}\n\n\/\/ GetPipeline retrieves information from a given pipeline\nfunc (c *CodePipeline) GetPipeline(pipelineName string) (*Pipeline, error) {\n\tinput := &cp.GetPipelineInput{\n\t\tName: aws.String(pipelineName),\n\t}\n\tresp, err := c.client.GetPipeline(input)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"get pipeline %s: %w\", pipelineName, err)\n\t}\n\tpipeline := &Pipeline{\n\t\tName: aws.StringValue(resp.Pipeline.Name),\n\t}\n\n\treturn pipeline, nil\n}\n\n\/\/ ListPipelines retrieves summaries of all pipelines for a project\nfunc (c *CodePipeline) ListPipelines() ([]string, error) {\n\tinput := &cp.ListPipelinesInput{}\n\tresp, err := c.client.ListPipelines(input)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"list pipelines: %w\", err)\n\t}\n\n\tvar pipelines []string\n\n\tfor _, ps := range resp.Pipelines {\n\t\tp := aws.StringValue(ps.Name)\n\t\tpipelines = append(pipelines, p)\n\t}\n\n\treturn pipelines, nil\n}\n","lang_cluster":"Go","length":79,"code_uid":"89a0a444954d4a1085e4c396aeed45d9"}
{"diff_hunk":"@@ -183,8 +183,13 @@ func (d decoder) DecodeMap(f func(key string, d2 driver.Decoder) bool) {\n }\n \n func (d decoder) AsSpecial(v reflect.Value) (bool, interface{}, error) {\n-\tif bin, ok := d.val.(primitive.Binary); ok {\n-\t\treturn true, bin.Data, nil\n+\tswitch v := d.val.(type) {\n+\tcase primitive.Binary:\n+\t\treturn true, v.Data, nil\n+\tcase primitive.DateTime:\n+\t\t\/\/ A DateTime represents milliseconds since the Unix epoch.\n+\t\treturn true, time.Unix(int64(v)\/1000, int64(v)%1000*1e6), nil\n+\tdefault:\n+\t\treturn false, nil, nil\n \t}\n-\treturn false, nil, nil\n }","old_code":"\/\/ Copyright 2019 The Go Cloud Development Kit Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage mongodocstore\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\n\t\"go.mongodb.org\/mongo-driver\/bson\/primitive\"\n\t\"gocloud.dev\/internal\/docstore\/driver\"\n)\n\n\/\/ Encode and decode to map[string]interface{}.\n\/\/ This isn't ideal, because the mongo client encodes\/decodes a second time.\n\/\/ TODO(jba): find a way do only one encode\/decode.\n\n\/\/ This code is copied from memdocstore\/codec.go, with some changes:\n\/\/ - special treatment for primitive.Binary\n\nfunc encodeDoc(doc driver.Document) (map[string]interface{}, error) {\n\tvar e encoder\n\tif err := doc.Encode(&e); err != nil {\n\t\treturn nil, err\n\t}\n\treturn e.val.(map[string]interface{}), nil\n}\n\nfunc encodeValue(x interface{}) (interface{}, error) {\n\tvar e encoder\n\tif err := driver.Encode(reflect.ValueOf(x), &e); err != nil {\n\t\treturn nil, err\n\t}\n\treturn e.val, nil\n}\n\ntype encoder struct {\n\tval interface{}\n}\n\nfunc (e *encoder) EncodeNil()                                { e.val = nil }\nfunc (e *encoder) EncodeBool(x bool)                         { e.val = x }\nfunc (e *encoder) EncodeInt(x int64)                         { e.val = x }\nfunc (e *encoder) EncodeUint(x uint64)                       { e.val = int64(x) }\nfunc (e *encoder) EncodeBytes(x []byte)                      { e.val = x }\nfunc (e *encoder) EncodeFloat(x float64)                     { e.val = x }\nfunc (e *encoder) EncodeComplex(x complex128)                { e.val = x }\nfunc (e *encoder) EncodeString(x string)                     { e.val = x }\nfunc (e *encoder) ListIndex(int)                             { panic(\"impossible\") }\nfunc (e *encoder) MapKey(string)                             { panic(\"impossible\") }\nfunc (e *encoder) EncodeSpecial(reflect.Value) (bool, error) { return false, nil } \/\/ no special handling\n\nfunc (e *encoder) EncodeList(n int) driver.Encoder {\n\t\/\/ All slices and arrays are encoded as []interface{}\n\ts := make([]interface{}, n)\n\te.val = s\n\treturn &listEncoder{s: s}\n}\n\ntype listEncoder struct {\n\ts []interface{}\n\tencoder\n}\n\nfunc (e *listEncoder) ListIndex(i int) { e.s[i] = e.val }\n\ntype mapEncoder struct {\n\tm map[string]interface{}\n\tencoder\n}\n\nfunc (e *encoder) EncodeMap(n int) driver.Encoder {\n\tm := make(map[string]interface{}, n)\n\te.val = m\n\treturn &mapEncoder{m: m}\n}\n\nfunc (e *mapEncoder) MapKey(k string) { e.m[k] = e.val }\n\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n\/\/ decodeDoc decodes m into ddoc.\nfunc decodeDoc(m map[string]interface{}, ddoc driver.Document) error {\n\treturn ddoc.Decode(decoder{m})\n}\n\ntype decoder struct {\n\tval interface{}\n}\n\nfunc (d decoder) String() string {\n\treturn fmt.Sprint(d.val)\n}\n\nfunc (d decoder) AsNull() bool {\n\treturn d.val == nil\n}\n\nfunc (d decoder) AsBool() (bool, bool) {\n\tb, ok := d.val.(bool)\n\treturn b, ok\n}\n\nfunc (d decoder) AsString() (string, bool) {\n\ts, ok := d.val.(string)\n\treturn s, ok\n}\n\nfunc (d decoder) AsInt() (int64, bool) {\n\ti, ok := d.val.(int64)\n\treturn i, ok\n}\n\nfunc (d decoder) AsUint() (uint64, bool) {\n\ti, ok := d.val.(int64)\n\treturn uint64(i), ok\n}\n\nfunc (d decoder) AsFloat() (float64, bool) {\n\tf, ok := d.val.(float64)\n\treturn f, ok\n}\n\nfunc (d decoder) AsComplex() (complex128, bool) {\n\tc, ok := d.val.(complex128)\n\treturn c, ok\n}\n\nfunc (d decoder) AsBytes() ([]byte, bool) {\n\tswitch v := d.val.(type) {\n\tcase []byte:\n\t\treturn v, true\n\tcase primitive.Binary:\n\t\treturn v.Data, true\n\tdefault:\n\t\treturn nil, false\n\t}\n}\n\nfunc (d decoder) AsInterface() (interface{}, error) {\n\treturn d.val, nil\n}\n\nfunc (d decoder) ListLen() (int, bool) {\n\tif s, ok := d.val.([]interface{}); ok {\n\t\treturn len(s), true\n\t}\n\treturn 0, false\n}\n\nfunc (d decoder) DecodeList(f func(i int, d2 driver.Decoder) bool) {\n\tfor i, e := range d.val.([]interface{}) {\n\t\tif !f(i, decoder{e}) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (d decoder) MapLen() (int, bool) {\n\tif m, ok := d.val.(map[string]interface{}); ok {\n\t\treturn len(m), true\n\t}\n\treturn 0, false\n}\n\nfunc (d decoder) DecodeMap(f func(key string, d2 driver.Decoder) bool) {\n\tfor k, v := range d.val.(map[string]interface{}) {\n\t\tif !f(k, decoder{v}) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (d decoder) AsSpecial(v reflect.Value) (bool, interface{}, error) {\n\tif bin, ok := d.val.(primitive.Binary); ok {\n\t\treturn true, bin.Data, nil\n\t}\n\treturn false, nil, nil\n}\n","lang_cluster":"Go","length":190,"code_uid":"9c19f8ed31334e86b5e853bb9ac09c34"}
{"diff_hunk":"@@ -8,6 +8,7 @@ import (\n \t\"time\"\n \n \t\"crypto\/ecdsa\"\n+\t\"fmt\"\n \t\"github.com\/spiffe\/spire\/pkg\/common\/util\"\n \t\"github.com\/spiffe\/spire\/proto\/api\/node\"\n \t\"github.com\/spiffe\/spire\/proto\/common\"","old_code":"package cache\n\nimport (\n\t\"crypto\/sha256\"\n\t\"hash\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"crypto\/ecdsa\"\n\t\"github.com\/spiffe\/spire\/pkg\/common\/util\"\n\t\"github.com\/spiffe\/spire\/proto\/api\/node\"\n\t\"github.com\/spiffe\/spire\/proto\/common\"\n)\n\ntype selectors []*common.Selector\n\ntype CacheEntry struct {\n\tRegistrationEntry *common.RegistrationEntry\n\tSVID              *node.Svid\n\tPrivateKey        *ecdsa.PrivateKey\n\tExpiry            time.Time\n\n\t\/\/ Bundles stores the ID => Bundle map for\n\t\/\/ federated bundles. The registration entry\n\t\/\/ only stores references to the keys here.\n\tBundles map[string][]byte\n}\n\ntype Cache interface {\n\tEntry([]*common.Selector) (entry []CacheEntry)\n\tSetEntry(cacheEntry CacheEntry)\n\tDeleteEntry([]*common.Selector) (deleted bool)\n}\n\ntype cacheImpl struct {\n\tcache map[string][]CacheEntry\n\tm     sync.Mutex\n}\n\nfunc NewCache() *cacheImpl {\n\treturn &cacheImpl{cache: make(map[string][]CacheEntry)}\n}\n\nfunc (c *cacheImpl) Entry(selectors []*common.Selector) (entry []CacheEntry) {\n\tkey := deriveCacheKey(selectors)\n\tc.m.Lock()\n\tdefer c.m.Unlock()\n\treturn c.cache[key]\n}\n\nfunc (c *cacheImpl) SetEntry(cacheEntry CacheEntry) {\n\tc.m.Lock()\n\tdefer c.m.Unlock()\n\tkey := deriveCacheKey(cacheEntry.RegistrationEntry.Selectors)\n\tc.cache[key] = append(c.cache[key], cacheEntry)\n\treturn\n\n}\n\nfunc (c *cacheImpl) DeleteEntry(selectors []*common.Selector) (deleted bool) {\n\tc.m.Lock()\n\tdefer c.m.Unlock()\n\tkey := deriveCacheKey(selectors)\n\tif _, exists := c.cache[key]; exists == true {\n\t\tdelete(c.cache, key)\n\t\tdeleted = true\n\t}\n\treturn\n}\n\nfunc deriveCacheKey(s selectors) (key string) {\n\tvar concatSelectors string\n\tsort.Slice(s, util.SelectorsSortFunction(s))\n\n\tfor _, selector := range s {\n\t\tconcatSelectors = concatSelectors + \"::\" + selector.Type + \":\" + selector.Value\n\t}\n\thashedSelectors := hash.Hash.Sum(sha256.New(), []byte(concatSelectors))\n\n\treturn string(hashedSelectors)\n}\n","lang_cluster":"Go","length":82,"code_uid":"efa0d59fda3c47febac0f947f141157f"}
{"diff_hunk":"@@ -18,6 +18,8 @@ package deployment\n \n import (\n \t\"net\/http\"\n+\t\"os\/exec\"\n+\t\"path\"\n \t\"time\"\n \n \t. \"github.com\/onsi\/ginkgo\"","old_code":"\/*\nCopyright 2019 The KubeEdge Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage deployment\n\nimport (\n\t\"net\/http\"\n\t\"time\"\n\n\t. \"github.com\/onsi\/ginkgo\"\n\t. \"github.com\/onsi\/gomega\"\n\tappsv1 \"k8s.io\/api\/apps\/v1\"\n\tcorev1 \"k8s.io\/api\/core\/v1\"\n\n\t\"github.com\/kubeedge\/kubeedge\/tests\/e2e\/constants\"\n\t. \"github.com\/kubeedge\/kubeedge\/tests\/e2e\/testsuite\"\n\t\"github.com\/kubeedge\/kubeedge\/tests\/e2e\/utils\"\n)\n\nvar DeploymentTestTimerGroup *utils.TestTimerGroup = utils.NewTestTimerGroup()\n\n\/\/Run Test cases\nvar _ = Describe(\"Application deployment test in E2E scenario\", func() {\n\tvar UID string\n\tvar testTimer *utils.TestTimer\n\tvar testDescription GinkgoTestDescription\n\tContext(\"Test application deployment and delete deployment using deployment spec\", func() {\n\t\tBeforeEach(func() {\n\t\t\t\/\/ Get current test description\n\t\t\ttestDescription = CurrentGinkgoTestDescription()\n\t\t\t\/\/ Start test timer\n\t\t\ttestTimer = DeploymentTestTimerGroup.NewTestTimer(testDescription.TestText)\n\t\t})\n\t\tAfterEach(func() {\n\t\t\t\/\/ End test timer\n\t\t\ttestTimer.End()\n\t\t\t\/\/ Print result\n\t\t\ttestTimer.PrintResult()\n\t\t\tvar podlist corev1.PodList\n\t\t\tvar deploymentList appsv1.DeploymentList\n\t\t\terr := utils.GetDeployments(&deploymentList, ctx.Cfg.K8SMasterForKubeEdge+constants.DeploymentHandler)\n\t\t\tExpect(err).To(BeNil())\n\t\t\tfor _, deployment := range deploymentList.Items {\n\t\t\t\tif deployment.Name == UID {\n\t\t\t\t\tlabel := nodeName\n\t\t\t\t\tpodlist, err = utils.GetPods(ctx.Cfg.K8SMasterForKubeEdge+constants.AppHandler, label)\n\t\t\t\t\tExpect(err).To(BeNil())\n\t\t\t\t\tStatusCode := utils.DeleteDeployment(ctx.Cfg.K8SMasterForKubeEdge+constants.DeploymentHandler, deployment.Name)\n\t\t\t\t\tExpect(StatusCode).Should(Equal(http.StatusOK))\n\t\t\t\t}\n\t\t\t}\n\t\t\tutils.CheckPodDeleteState(ctx.Cfg.K8SMasterForKubeEdge+constants.AppHandler, podlist)\n\t\t\tutils.PrintTestcaseNameandStatus()\n\t\t})\n\n\t\tIt(\"E2E_APP_DEPLOYMENT_1: Create deployment and check the pods are coming up correctly\", func() {\n\t\t\treplica := 1\n\t\t\t\/\/Generate the random string and assign as a UID\n\t\t\tUID = \"edgecore-depl-app-\" + utils.GetRandomString(5)\n\t\t\tCreateDeploymentTest(replica, UID, nodeName, nodeSelector, ctx)\n\t\t})\n\t\tIt(\"E2E_APP_DEPLOYMENT_2: Create deployment with replicas and check the pods are coming up correctly\", func() {\n\t\t\treplica := 3\n\t\t\t\/\/Generate the random string and assign as a UID\n\t\t\tUID = \"edgecore-depl-app-\" + utils.GetRandomString(5)\n\t\t\tCreateDeploymentTest(replica, UID, nodeName, nodeSelector, ctx)\n\t\t})\n\n\t\tIt(\"E2E_APP_DEPLOYMENT_3: Create deployment and check deployment ctrler re-creating pods when user deletes the pods manually\", func() {\n\t\t\treplica := 3\n\t\t\t\/\/Generate the random string and assign as a UID\n\t\t\tUID = \"edgecore-depl-app-\" + utils.GetRandomString(5)\n\t\t\tpodlist := CreateDeploymentTest(replica, UID, nodeName, nodeSelector, ctx)\n\t\t\tfor _, pod := range podlist.Items {\n\t\t\t\t_, StatusCode := utils.DeletePods(ctx.Cfg.K8SMasterForKubeEdge + constants.AppHandler + \"\/\" + pod.Name)\n\t\t\t\tExpect(StatusCode).Should(Equal(http.StatusOK))\n\t\t\t}\n\t\t\tutils.CheckPodDeleteState(ctx.Cfg.K8SMasterForKubeEdge+constants.AppHandler, podlist)\n\t\t\tlabel := nodeName\n\t\t\tpodlist, err := utils.GetPods(ctx.Cfg.K8SMasterForKubeEdge+constants.AppHandler, label)\n\t\t\tExpect(err).To(BeNil())\n\t\t\tExpect(len(podlist.Items)).Should(Equal(replica))\n\t\t\tutils.WaitforPodsRunning(ctx.Cfg.KubeConfigPath, podlist, 240*time.Second)\n\t\t})\n\n\t})\n\tContext(\"Test application deployment using Pod spec\", func() {\n\t\tBeforeEach(func() {\n\t\t\t\/\/ Get current test description\n\t\t\ttestDescription = CurrentGinkgoTestDescription()\n\t\t\t\/\/ Start test timer\n\t\t\ttestTimer = DeploymentTestTimerGroup.NewTestTimer(testDescription.TestText)\n\t\t})\n\t\tAfterEach(func() {\n\t\t\t\/\/ End test timer\n\t\t\ttestTimer.End()\n\t\t\t\/\/ Print result\n\t\t\ttestTimer.PrintResult()\n\t\t\tvar podlist corev1.PodList\n\t\t\tlabel := nodeName\n\t\t\tpodlist, err := utils.GetPods(ctx.Cfg.K8SMasterForKubeEdge+constants.AppHandler, label)\n\t\t\tExpect(err).To(BeNil())\n\t\t\tfor _, pod := range podlist.Items {\n\t\t\t\t_, StatusCode := utils.DeletePods(ctx.Cfg.K8SMasterForKubeEdge + constants.AppHandler + \"\/\" + pod.Name)\n\t\t\t\tExpect(StatusCode).Should(Equal(http.StatusOK))\n\t\t\t}\n\t\t\tutils.CheckPodDeleteState(ctx.Cfg.K8SMasterForKubeEdge+constants.AppHandler, podlist)\n\t\t\tutils.PrintTestcaseNameandStatus()\n\t\t})\n\n\t\tIt(\"E2E_POD_DEPLOYMENT_1: Create a pod and check the pod is coming up correctly\", func() {\n\t\t\t\/\/Generate the random string and assign as podName\n\t\t\tpodName := \"pod-app-\" + utils.GetRandomString(5)\n\t\t\tpod := utils.NewPodObj(podName, ctx.Cfg.AppImageURL[0], nodeSelector)\n\n\t\t\tCreatePodTest(nodeName, podName, ctx, pod)\n\t\t})\n\n\t\tIt(\"E2E_POD_DEPLOYMENT_2: Create the pod and delete pod happening successfully\", func() {\n\t\t\t\/\/Generate the random string and assign as podName\n\t\t\tpodName := \"pod-app-\" + utils.GetRandomString(5)\n\t\t\tpod := utils.NewPodObj(podName, ctx.Cfg.AppImageURL[0], nodeSelector)\n\n\t\t\tpodlist := CreatePodTest(nodeName, podName, ctx, pod)\n\t\t\tfor _, pod := range podlist.Items {\n\t\t\t\t_, StatusCode := utils.DeletePods(ctx.Cfg.K8SMasterForKubeEdge + constants.AppHandler + \"\/\" + pod.Name)\n\t\t\t\tExpect(StatusCode).Should(Equal(http.StatusOK))\n\t\t\t}\n\t\t\tutils.CheckPodDeleteState(ctx.Cfg.K8SMasterForKubeEdge+constants.AppHandler, podlist)\n\t\t})\n\t\tIt(\"E2E_POD_DEPLOYMENT_3: Create pod and delete the pod successfully, and delete already deleted pod and check the behaviour\", func() {\n\t\t\t\/\/Generate the random string and assign as podName\n\t\t\tpodName := \"pod-app-\" + utils.GetRandomString(5)\n\t\t\tpod := utils.NewPodObj(podName, ctx.Cfg.AppImageURL[0], nodeSelector)\n\n\t\t\tpodlist := CreatePodTest(nodeName, podName, ctx, pod)\n\t\t\tfor _, pod := range podlist.Items {\n\t\t\t\t_, StatusCode := utils.DeletePods(ctx.Cfg.K8SMasterForKubeEdge + constants.AppHandler + \"\/\" + pod.Name)\n\t\t\t\tExpect(StatusCode).Should(Equal(http.StatusOK))\n\t\t\t}\n\t\t\tutils.CheckPodDeleteState(ctx.Cfg.K8SMasterForKubeEdge+constants.AppHandler, podlist)\n\t\t\t_, StatusCode := utils.DeletePods(ctx.Cfg.K8SMasterForKubeEdge + constants.AppHandler + \"\/\" + UID)\n\t\t\tExpect(StatusCode).Should(Equal(http.StatusNotFound))\n\t\t})\n\t\tIt(\"E2E_POD_DEPLOYMENT_4: Create and delete pod multiple times and check all the Pod created and deleted successfully\", func() {\n\t\t\t\/\/Generate the random string and assign as a UID\n\t\t\tfor i := 0; i < 10; i++ {\n\t\t\t\t\/\/Generate the random string and assign as podName\n\t\t\t\tpodName := \"pod-app-\" + utils.GetRandomString(5)\n\t\t\t\tpod := utils.NewPodObj(podName, ctx.Cfg.AppImageURL[0], nodeSelector)\n\n\t\t\t\tpodlist := CreatePodTest(nodeName, podName, ctx, pod)\n\t\t\t\tfor _, pod := range podlist.Items {\n\t\t\t\t\t_, StatusCode := utils.DeletePods(ctx.Cfg.K8SMasterForKubeEdge + constants.AppHandler + \"\/\" + pod.Name)\n\t\t\t\t\tExpect(StatusCode).Should(Equal(http.StatusOK))\n\t\t\t\t}\n\t\t\t\tutils.CheckPodDeleteState(ctx.Cfg.K8SMasterForKubeEdge+constants.AppHandler, podlist)\n\t\t\t}\n\t\t})\n\t\tIt(\"E2E_POD_DEPLOYMENT_5: Create pod with hostpath volume successfully\", func() {\n\t\t\t\/\/Generate the random string and assign as podName\n\t\t\tpodName := \"pod-app-\" + utils.GetRandomString(5)\n\t\t\tpod := utils.NewPodObj(podName, ctx.Cfg.AppImageURL[0], nodeSelector)\n\n\t\t\tpod.Spec.Containers[0].VolumeMounts = []corev1.VolumeMount{{\n\t\t\t\tName:      \"hp\",\n\t\t\t\tMountPath: \"\/hp\",\n\t\t\t}}\n\t\t\tpod.Spec.Volumes = []corev1.Volume{{\n\t\t\t\tName: \"hp\",\n\t\t\t\tVolumeSource: corev1.VolumeSource{\n\t\t\t\t\tHostPath: &corev1.HostPathVolumeSource{Path: \"\/tmp\"},\n\t\t\t\t},\n\t\t\t}}\n\n\t\t\tpodlist := CreatePodTest(nodeName, podName, ctx, pod)\n\t\t\tfor _, pod := range podlist.Items {\n\t\t\t\t_, StatusCode := utils.DeletePods(ctx.Cfg.K8SMasterForKubeEdge + constants.AppHandler + \"\/\" + pod.Name)\n\t\t\t\tExpect(StatusCode).Should(Equal(http.StatusOK))\n\t\t\t}\n\t\t\tutils.CheckPodDeleteState(ctx.Cfg.K8SMasterForKubeEdge+constants.AppHandler, podlist)\n\t\t})\n\t})\n})\n","lang_cluster":"Go","length":197,"code_uid":"e02de47394eb499fb560d3f00906326a"}
{"diff_hunk":"@@ -167,5 +167,5 @@ func setup(conf config) (_ *frontend, _ *processor, cleanup func(), err error) {\n \t\tbucket:     bucket,\n \t\tcoll:       coll,\n \t}\n-\treturn f, p, nil, nil\n+\treturn f, p, cleanup, nil\n }","old_code":"\/\/ Copyright 2019 The Go Cloud Development Kit Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\/ioutil\"\n\t\"log\"\n\t\"os\"\n\t\"path\/filepath\"\n\n\t\"gocloud.dev\/blob\"\n\t\"gocloud.dev\/docstore\"\n\t\"gocloud.dev\/pubsub\"\n)\n\nvar (\n\trequestTopicURL  = flag.String(\"request-topic\", \"mem:\/\/requests\", \"gocloud.dev\/pubsub URL for request topic\")\n\trequestSubURL    = flag.String(\"request-sub\", \"mem:\/\/requests\", \"gocloud.dev\/pubsub URL for request subscription\")\n\tresponseTopicURL = flag.String(\"response-topic\", \"mem:\/\/responses\", \"gocloud.dev\/pubsub URL for response topic\")\n\tresponseSubURL   = flag.String(\"response-sub\", \"mem:\/\/responses\", \"gocloud.dev\/pubsub URL for response subscription\")\n\tbucketURL        = flag.String(\"bucket\", \"\", \"gocloud.dev\/blob URL for image bucket\")\n\tcollectionURL    = flag.String(\"collection\", \"mem:\/\/orders\/ID\", \"gocloud.dev\/docstore URL for order collection\")\n\n\tport         = flag.Int(\"port\", 10538, \"HTTP port for frontend\")\n\trunFrontend  = flag.Bool(\"frontend\", true, \"run the frontend\")\n\trunProcessor = flag.Bool(\"processor\", true, \"run the image processor\")\n)\n\nfunc main() {\n\tflag.Parse()\n\tconf := config{\n\t\trequestTopicURL:  *requestTopicURL,\n\t\trequestSubURL:    *requestSubURL,\n\t\tresponseTopicURL: *responseTopicURL,\n\t\tresponseSubURL:   *responseSubURL,\n\t\tbucketURL:        *bucketURL,\n\t\tcollectionURL:    *collectionURL,\n\t}\n\tfrontend, processor, cleanup, err := setup(conf)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer cleanup()\n\n\t\/\/ Run the frontend, or the processor, or both.\n\t\/\/ When we want to run both, one of them has to run in a goroutine.\n\t\/\/ So it's easier to run both in goroutines, even if we only need\n\t\/\/ to run one.\n\terrc := make(chan error, 2)\n\tif *runFrontend {\n\t\tgo func() { errc <- frontend.run(context.Background(), *port) }()\n\t\tfmt.Printf(\"listening on port %d\\n\", *port)\n\t} else {\n\t\terrc <- nil\n\t}\n\tif *runProcessor {\n\t\tgo func() { errc <- processor.run(context.Background()) }()\n\t} else {\n\t\terrc <- nil\n\t}\n\t\/\/ Each of the goroutines will send once to errc, so receive two values.\n\tfor i := 0; i < 2; i++ {\n\t\tif err := <-errc; err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t}\n}\n\n\/\/ config describes the URLs for the resources used by the order application.\ntype config struct {\n\trequestTopicURL  string\n\trequestSubURL    string\n\tresponseTopicURL string\n\tresponseSubURL   string\n\tbucketURL        string\n\tcollectionURL    string\n}\n\n\/\/ setup opens all the necessary resources for the application.\nfunc setup(conf config) (_ *frontend, _ *processor, cleanup func(), err error) {\n\t\/\/ TODO(jba): simplify cleanup logic\n\tvar cleanups []func()\n\tdefer func() {\n\t\t\/\/ Clean up on error; return cleanup func on success.\n\t\tf := func() {\n\t\t\tfor _, c := range cleanups {\n\t\t\t\tc()\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\tf()\n\t\t\tcleanup = nil\n\t\t} else {\n\t\t\tcleanup = f\n\t\t}\n\t}()\n\n\tctx := context.Background()\n\t\/\/ TODO(jba): This application assumes at-least-once processing. Enforce that here if possible.\n\treqTopic, err := pubsub.OpenTopic(ctx, conf.requestTopicURL)\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\tcleanups = append(cleanups, func() { reqTopic.Shutdown(ctx) })\n\n\treqSub, err := pubsub.OpenSubscription(ctx, conf.requestSubURL)\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\tcleanups = append(cleanups, func() { reqSub.Shutdown(ctx) })\n\n\tresTopic, err := pubsub.OpenTopic(ctx, conf.responseTopicURL)\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\tcleanups = append(cleanups, func() { resTopic.Shutdown(ctx) })\n\n\tresSub, err := pubsub.OpenSubscription(ctx, conf.responseSubURL)\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\tcleanups = append(cleanups, func() { resSub.Shutdown(ctx) })\n\n\tburl := conf.bucketURL\n\tif burl == \"\" {\n\t\tdir, err := ioutil.TempDir(\"\", \"gocdk-order\")\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\tburl = \"file:\/\/\" + filepath.ToSlash(dir)\n\t\tcleanups = append(cleanups, func() { os.Remove(dir) })\n\t}\n\tbucket, err := blob.OpenBucket(ctx, burl)\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\tcleanups = append(cleanups, func() { bucket.Close() })\n\n\tcoll, err := docstore.OpenCollection(ctx, conf.collectionURL)\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\tcleanups = append(cleanups, func() { coll.Close() })\n\n\tf := &frontend{\n\t\trequestTopic: reqTopic,\n\t\tbucket:       bucket,\n\t\tcoll:         coll,\n\t}\n\tp := &processor{\n\t\trequestSub: reqSub,\n\t\tbucket:     bucket,\n\t\tcoll:       coll,\n\t}\n\treturn f, p, nil, nil\n}\n","lang_cluster":"Go","length":171,"code_uid":"561e7e4a324e4f3199eae8c32d21b678"}
{"diff_hunk":"@@ -66,6 +66,9 @@ func (j journalBlockServer) RemoveBlockReferences(\n \tcontexts map[BlockID][]BlockContext) (\n \tliveCounts map[BlockID]int, err error) {\n \tif tlfJournal, ok := j.jServer.getTLFJournal(tlfID); ok {\n+\t\tdefer func() {\n+\t\t\terr = translateToBlockServerError(err)\n+\t\t}()\n \t\t\/\/ TODO: Get server counts without making a\n \t\t\/\/ RemoveBlockReferences call and merge it.\n \t\treturn tlfJournal.removeBlockReferences(ctx, contexts)","old_code":"\/\/ Copyright 2016 Keybase Inc. All rights reserved.\n\/\/ Use of this source code is governed by a BSD\n\/\/ license that can be found in the LICENSE file.\n\npackage libkbfs\n\nimport \"golang.org\/x\/net\/context\"\n\ntype journalBlockServer struct {\n\tjServer *JournalServer\n\tBlockServer\n\tenableAddBlockReference bool\n}\n\nvar _ BlockServer = journalBlockServer{}\n\nfunc (j journalBlockServer) Get(\n\tctx context.Context, tlfID TlfID, id BlockID, context BlockContext) (\n\t[]byte, BlockCryptKeyServerHalf, error) {\n\tif tlfJournal, ok := j.jServer.getTLFJournal(tlfID); ok {\n\t\tdata, serverHalf, err := tlfJournal.getBlockDataWithContext(\n\t\t\tid, context)\n\t\tswitch err.(type) {\n\t\tcase nil:\n\t\t\treturn data, serverHalf, nil\n\t\tcase BServerErrorBlockNonExistent:\n\t\t\treturn j.BlockServer.Get(ctx, tlfID, id, context)\n\t\tdefault:\n\t\t\treturn nil, BlockCryptKeyServerHalf{}, err\n\t\t}\n\t}\n\n\treturn j.BlockServer.Get(ctx, tlfID, id, context)\n}\n\nfunc (j journalBlockServer) Put(\n\tctx context.Context, tlfID TlfID, id BlockID, context BlockContext,\n\tbuf []byte, serverHalf BlockCryptKeyServerHalf) error {\n\tif tlfJournal, ok := j.jServer.getTLFJournal(tlfID); ok {\n\t\treturn tlfJournal.putBlockData(ctx, id, context, buf, serverHalf)\n\t}\n\n\treturn j.BlockServer.Put(ctx, tlfID, id, context, buf, serverHalf)\n}\n\nfunc (j journalBlockServer) AddBlockReference(\n\tctx context.Context, tlfID TlfID, id BlockID,\n\tcontext BlockContext) error {\n\tif !j.enableAddBlockReference {\n\t\t\/\/ TODO: Temporarily return an error until KBFS-1149 is\n\t\t\/\/ fixed. This is needed despite\n\t\t\/\/ journalBlockCache.CheckForBlockPtr, since CheckForBlockPtr\n\t\t\/\/ may be called before journaling is turned on for a TLF.\n\t\treturn BServerErrorBlockNonExistent{}\n\t}\n\n\tif tlfJournal, ok := j.jServer.getTLFJournal(tlfID); ok {\n\t\treturn tlfJournal.addBlockReference(ctx, id, context)\n\t}\n\n\treturn j.BlockServer.AddBlockReference(ctx, tlfID, id, context)\n}\n\nfunc (j journalBlockServer) RemoveBlockReferences(\n\tctx context.Context, tlfID TlfID,\n\tcontexts map[BlockID][]BlockContext) (\n\tliveCounts map[BlockID]int, err error) {\n\tif tlfJournal, ok := j.jServer.getTLFJournal(tlfID); ok {\n\t\t\/\/ TODO: Get server counts without making a\n\t\t\/\/ RemoveBlockReferences call and merge it.\n\t\treturn tlfJournal.removeBlockReferences(ctx, contexts)\n\t}\n\n\treturn j.BlockServer.RemoveBlockReferences(ctx, tlfID, contexts)\n}\n\nfunc (j journalBlockServer) ArchiveBlockReferences(\n\tctx context.Context, tlfID TlfID,\n\tcontexts map[BlockID][]BlockContext) error {\n\tif tlfJournal, ok := j.jServer.getTLFJournal(tlfID); ok {\n\t\treturn tlfJournal.archiveBlockReferences(ctx, contexts)\n\t}\n\n\treturn j.BlockServer.ArchiveBlockReferences(ctx, tlfID, contexts)\n}\n\nfunc (j journalBlockServer) Shutdown() {\n\tj.jServer.shutdown()\n}\n","lang_cluster":"Go","length":89,"code_uid":"5e1759fb14c54c3aaba2f5740777e9c2"}
{"diff_hunk":"@@ -27,7 +27,11 @@ import (\n \t\"github.com\/iotexproject\/iotex-proto\/golang\/iotextypes\"\n )\n \n+var nativeStakingContractCreator = address.ZeroAddress\n+var nativeStakingContractNonce = uint64(0)\n+\n type stakingCommittee struct {\n+\tcandidatesByHeight   CandidatesByHeight\n \tgetEpochHeight       GetEpochHeight\n \tgetEpochNum          GetEpochNum\n \telectionCommittee    committee.Committee","old_code":"\/\/ Copyright (c) 2019 IoTeX Foundation\n\/\/ This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n\/\/ warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n\/\/ permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n\/\/ License 2.0 that can be found in the LICENSE file.\n\npackage poll\n\nimport (\n\t\"context\"\n\t\"encoding\/hex\"\n\t\"math\/big\"\n\t\"time\"\n\n\t\"github.com\/pkg\/errors\"\n\t\"go.uber.org\/zap\"\n\n\t\"github.com\/iotexproject\/iotex-core\/action\"\n\t\"github.com\/iotexproject\/iotex-core\/action\/protocol\"\n\t\"github.com\/iotexproject\/iotex-core\/action\/protocol\/rolldpos\"\n\t\"github.com\/iotexproject\/iotex-core\/config\"\n\t\"github.com\/iotexproject\/iotex-core\/pkg\/log\"\n\t\"github.com\/iotexproject\/iotex-core\/state\"\n\t\"github.com\/iotexproject\/iotex-election\/committee\"\n\t\"github.com\/iotexproject\/iotex-election\/types\"\n\t\"github.com\/iotexproject\/iotex-election\/util\"\n\t\"github.com\/iotexproject\/iotex-proto\/golang\/iotextypes\"\n)\n\ntype stakingCommittee struct {\n\tgetEpochHeight       GetEpochHeight\n\tgetEpochNum          GetEpochNum\n\telectionCommittee    committee.Committee\n\tgovernanceStaking    Protocol\n\tnativeStaking        *NativeStaking\n\trp                   *rolldpos.Protocol\n\tscoreThreshold       *big.Int\n\tcurrentNativeBuckets []*types.Bucket\n}\n\n\/\/ NewStakingCommittee creates a staking committee which fetch result from governance chain and native staking\nfunc NewStakingCommittee(\n\tec committee.Committee,\n\tgs Protocol,\n\treadContract ReadContract,\n\tgetEpochHeight GetEpochHeight,\n\tgetEpochNum GetEpochNum,\n\tnativeStakingContractAddress string,\n\tnativeStakingContractCode string,\n\trp *rolldpos.Protocol,\n\tscoreThreshold *big.Int,\n) (Protocol, error) {\n\tif getEpochHeight == nil {\n\t\treturn nil, errors.New(\"failed to create native staking: empty getEpochHeight\")\n\t}\n\tif getEpochNum == nil {\n\t\treturn nil, errors.New(\"failed to create native staking: empty getEpochNum\")\n\t}\n\tvar ns *NativeStaking\n\tif nativeStakingContractAddress != \"\" || nativeStakingContractCode != \"\" {\n\t\tvar err error\n\t\tif ns, err = NewNativeStaking(readContract); err != nil {\n\t\t\treturn nil, errors.New(\"failed to create native staking\")\n\t\t}\n\t\tif nativeStakingContractAddress != \"\" {\n\t\t\tns.SetContract(nativeStakingContractAddress)\n\t\t}\n\t}\n\treturn &stakingCommittee{\n\t\telectionCommittee: ec,\n\t\tgovernanceStaking: gs,\n\t\tnativeStaking:     ns,\n\t\tgetEpochHeight:    getEpochHeight,\n\t\tgetEpochNum:       getEpochNum,\n\t\trp:                rp,\n\t\tscoreThreshold:    scoreThreshold,\n\t}, nil\n}\n\nfunc (sc *stakingCommittee) Initialize(ctx context.Context, sm protocol.StateManager) error {\n\treturn sc.governanceStaking.Initialize(ctx, sm)\n}\n\nfunc (sc *stakingCommittee) Handle(ctx context.Context, act action.Action, sm protocol.StateManager) (*action.Receipt, error) {\n\treceipt, err := sc.governanceStaking.Handle(ctx, act, sm)\n\tif err := sc.persistNativeBuckets(ctx, receipt, err); err != nil {\n\t\treturn nil, err\n\t}\n\treturn receipt, err\n}\n\nfunc (sc *stakingCommittee) Validate(ctx context.Context, act action.Action) error {\n\treturn validate(ctx, sc, act)\n}\n\nfunc (sc *stakingCommittee) DelegatesByHeight(ctx context.Context, height uint64) (state.CandidateList, error) {\n\tcand, err := sc.governanceStaking.DelegatesByHeight(ctx, height)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvaCtx := protocol.MustGetValidateActionsCtx(ctx)\n\thu := config.NewHeightUpgrade(&vaCtx.Genesis)\n\t\/\/ convert to epoch start height\n\tif hu.IsPre(config.Cook, sc.getEpochHeight(sc.getEpochNum(height))) {\n\t\treturn sc.filterDelegates(cand), nil\n\t}\n\t\/\/ native staking starts from Cook\n\tif sc.nativeStaking == nil {\n\t\treturn nil, errors.New(\"native staking was not set after cook height\")\n\t}\n\n\tnativeVotes, err := sc.nativeStaking.Votes(vaCtx.Tip.Height, vaCtx.Tip.Timestamp)\n\tif err == ErrNoData {\n\t\t\/\/ no native staking data\n\t\treturn sc.filterDelegates(cand), nil\n\t}\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to get native chain candidates\")\n\t}\n\tsc.currentNativeBuckets = nativeVotes.Buckets\n\n\treturn sc.mergeDelegates(cand, nativeVotes, vaCtx.Tip.Timestamp), nil\n}\n\nfunc (sc *stakingCommittee) ReadState(ctx context.Context, sm protocol.StateManager, method []byte, args ...[]byte) ([]byte, error) {\n\treturn sc.governanceStaking.ReadState(ctx, sm, method, args...)\n}\n\n\/\/ SetNativeStakingContract sets the address of native staking contract\nfunc (sc *stakingCommittee) SetNativeStakingContract(contract string) {\n\tsc.nativeStaking.SetContract(contract)\n}\n\n\/\/ return candidates whose votes are above threshold\nfunc (sc *stakingCommittee) filterDelegates(candidates state.CandidateList) state.CandidateList {\n\tvar cand state.CandidateList\n\tfor _, c := range candidates {\n\t\tif c.Votes.Cmp(sc.scoreThreshold) >= 0 {\n\t\t\tcand = append(cand, c)\n\t\t}\n\t}\n\treturn cand\n}\n\nfunc (sc *stakingCommittee) mergeDelegates(list state.CandidateList, votes *VoteTally, ts time.Time) state.CandidateList {\n\t\/\/ as of now, native staking does not have register contract, only voting\/staking contract\n\t\/\/ it is assumed that all votes done on native staking target for delegates registered on Ethereum\n\t\/\/ votes cast to all outside address will not be counted and simply ignored\n\tcandidates := make(map[string]*state.Candidate)\n\tcandidateScores := make(map[string]*big.Int)\n\tfor _, cand := range list {\n\t\tclone := cand.Clone()\n\t\tname := to12Bytes(clone.CanName)\n\t\tif v, ok := votes.Candidates[name]; ok {\n\t\t\tclone.Votes.Add(clone.Votes, v.Votes)\n\t\t}\n\t\tif clone.Votes.Cmp(sc.scoreThreshold) >= 0 {\n\t\t\tcandidates[hex.EncodeToString(name[:])] = clone\n\t\t\tcandidateScores[hex.EncodeToString(name[:])] = clone.Votes\n\t\t}\n\t}\n\tsorted := util.Sort(candidateScores, uint64(ts.Unix()))\n\tvar merged state.CandidateList\n\tfor _, name := range sorted {\n\t\tmerged = append(merged, candidates[name])\n\t}\n\treturn merged\n}\n\nfunc (sc *stakingCommittee) persistNativeBuckets(ctx context.Context, receipt *action.Receipt, err error) error {\n\t\/\/ Start to write native buckets archive after cook and only when the action is executed successfully\n\traCtx := protocol.MustGetRunActionsCtx(ctx)\n\tepochHeight := sc.getEpochHeight(sc.getEpochNum(raCtx.BlockHeight))\n\thu := config.NewHeightUpgrade(&raCtx.Genesis)\n\tif hu.IsPre(config.Cook, epochHeight) {\n\t\treturn nil\n\t}\n\tif err != nil {\n\t\treturn nil\n\t}\n\tif receipt == nil || receipt.Status != uint64(iotextypes.ReceiptStatus_Success) {\n\t\treturn nil\n\t}\n\tlog.L().Info(\"Store native buckets to election db\", zap.Int(\"size\", len(sc.currentNativeBuckets)))\n\tif err := sc.electionCommittee.PutNativePollByEpoch(\n\t\tsc.rp.GetEpochNum(raCtx.BlockHeight)+1, \/\/ The native buckets recorded in this epoch will be used in next one\n\t\traCtx.Tip.Timestamp,                    \/\/ The timestamp of last block is used to represent the current buckets timestamp\n\t\tsc.currentNativeBuckets,\n\t); err != nil {\n\t\treturn err\n\t}\n\tsc.currentNativeBuckets = nil\n\treturn nil\n}\n","lang_cluster":"Go","length":194,"code_uid":"bc0b5f0b961644fc965065f9473fe842"}
{"diff_hunk":"@@ -53,7 +53,8 @@ var telemetryStatusCmd = &cobra.Command{\n \tShort: \"Print the node's telemetry status\",\n \tLong:  `Print the node's telemetry status`,\n \tRun: func(cmd *cobra.Command, args []string) {\n-\t\tcfg, err := logging.EnsureTelemetryConfig(nil, \"\")\n+\t\tmaybeUpdateDataDirFromEnv()\n+\t\tcfg, err := logging.EnsureTelemetryConfig(&dataDir, \"\")\n \n \t\t\/\/ If error loading config, can't disable \/ no need to disable\n \t\tif err != nil {","old_code":"\/\/ Copyright (C) 2019 Algorand, Inc.\n\/\/ This file is part of go-algorand\n\/\/\n\/\/ go-algorand is free software: you can redistribute it and\/or modify\n\/\/ it under the terms of the GNU Affero General Public License as\n\/\/ published by the Free Software Foundation, either version 3 of the\n\/\/ License, or (at your option) any later version.\n\/\/\n\/\/ go-algorand is distributed in the hope that it will be useful,\n\/\/ but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n\/\/ GNU Affero General Public License for more details.\n\/\/\n\/\/ You should have received a copy of the GNU Affero General Public License\n\/\/ along with go-algorand.  If not, see <https:\/\/www.gnu.org\/licenses\/>.\n\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com\/spf13\/cobra\"\n\n\t\"github.com\/algorand\/go-algorand\/logging\"\n)\n\nvar (\n\tnodeName string\n\turi      string\n)\n\nfunc init() {\n\ttelemetryCmd.AddCommand(telemetryStatusCmd)\n\ttelemetryCmd.AddCommand(telemetryEnableCmd)\n\ttelemetryCmd.AddCommand(telemetryDisableCmd)\n\ttelemetryCmd.AddCommand(telemetryNameCmd)\n\ttelemetryCmd.AddCommand(telemetryEndpointCmd)\n\n\t\/\/ Enable Logging : node name\n\ttelemetryNameCmd.Flags().StringVarP(&nodeName, \"name\", \"n\", \"\", \"Friendly-name to use for node\")\n\ttelemetryEndpointCmd.Flags().StringVarP(&uri, \"endpoint\", \"e\", \"\", \"Endpoint's URI\")\n}\n\nvar telemetryCmd = &cobra.Command{\n\tUse:   \"telemetry\",\n\tShort: \"Control and manage Algorand logging\",\n\tLong:  `Enable\/disable and configure Algorand remote logging`,\n\tRun:   telemetryStatusCmd.Run,\n}\n\nvar telemetryStatusCmd = &cobra.Command{\n\tUse:   \"status\",\n\tShort: \"Print the node's telemetry status\",\n\tLong:  `Print the node's telemetry status`,\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tcfg, err := logging.EnsureTelemetryConfig(nil, \"\")\n\n\t\t\/\/ If error loading config, can't disable \/ no need to disable\n\t\tif err != nil {\n\t\t\tfmt.Println(err)\n\t\t\tfmt.Println(loggingNotConfigured)\n\t\t} else if cfg.Enable == false {\n\t\t\tfmt.Println(loggingNotEnabled)\n\t\t} else {\n\t\t\tfmt.Printf(loggingEnabled, cfg.Name, cfg.GUID)\n\t\t}\n\t},\n}\n\nvar telemetryEnableCmd = &cobra.Command{\n\tUse:   \"enable\",\n\tShort: \"Enable Algorand remote logging\",\n\tLong:  `Enable Algorand remote logging`,\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tcfg, err := logging.EnsureTelemetryConfig(nil, \"\")\n\n\t\t\/\/ If error loading config, can't disable \/ no need to disable\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tcfg.Enable = true\n\t\tcfg.Save(cfg.FilePath)\n\t\tfmt.Printf(\"Telemetry logging enabled: Name = %s, Guid = %s\\n\", cfg.Name, cfg.GUID)\n\t},\n}\n\nvar telemetryDisableCmd = &cobra.Command{\n\tUse:   \"disable\",\n\tShort: \"Disable Algorand remote logging\",\n\tLong:  `Disable Algorand remote logging`,\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tcfg, err := logging.EnsureTelemetryConfig(nil, \"\")\n\n\t\t\/\/ If error loading config, can't disable \/ no need to disable\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tcfg.Enable = false\n\t\tcfg.Save(cfg.FilePath)\n\t\tfmt.Printf(\"Telemetry logging disabled: Name = %s, Guid = %s\\n\", cfg.Name, cfg.GUID)\n\t},\n}\n\nvar telemetryNameCmd = &cobra.Command{\n\tUse:   \"name -n nodeName\",\n\tShort: \"Enable Algorand remote logging\",\n\tLong:  `Enable Algorand remote logging with specified node name`,\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tcfg, err := logging.EnsureTelemetryConfig(nil, \"\")\n\t\tif err != nil {\n\t\t\tfmt.Println(err)\n\t\t\treturn\n\t\t}\n\t\tcfg.Enable = true\n\t\tif len(nodeName) > 0 {\n\t\t\tcfg.Name = nodeName\n\t\t}\n\t\tcfg.Save(cfg.FilePath)\n\t\tfmt.Printf(\"Telemetry logging: Name = %s, Guid = %s\\n\", cfg.Name, cfg.GUID)\n\t},\n}\n\nvar telemetryEndpointCmd = &cobra.Command{\n\tUse:   \"endpoint -e <url>\",\n\tShort: \"sets the \\\"URI\\\" property\",\n\tLong:  `Sets the \"URI\" property in the telemetry configuration`,\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tcfg, err := logging.EnsureTelemetryConfig(nil, \"\")\n\t\tif err != nil {\n\t\t\tfmt.Println(err)\n\t\t\treturn\n\t\t}\n\t\tcfg.URI = uri\n\t\tcfg.Save(cfg.FilePath)\n\t\tfmt.Printf(\"Telemetry logging: Name = %s, Guid = %s, URI = %s\\n\", cfg.Name, cfg.GUID, cfg.URI)\n\t},\n}\n","lang_cluster":"Go","length":139,"code_uid":"b032fd8dff7d4cc1a5893794fcbca377"}
{"diff_hunk":"@@ -84,7 +84,7 @@ func byteOrNil(data []byte) *[]byte {\n \treturn &data\n }\n \n-func computeAssetIndexInPayset(tx node.TxnWithStatus, txnCounter uint64, payset []transactions.SignedTxnWithAD) (aidx *uint64) {\n+func computeCreatableIndexInPayset(tx node.TxnWithStatus, txnCounter uint64, payset []transactions.SignedTxnWithAD) (cidx *uint64) {\n \t\/\/ Compute transaction index in block\n \toffset := -1\n \tfor idx, stxnib := range payset {","old_code":"\/\/ Copyright (C) 2019-2020 Algorand, Inc.\n\/\/ This file is part of go-algorand\n\/\/\n\/\/ go-algorand is free software: you can redistribute it and\/or modify\n\/\/ it under the terms of the GNU Affero General Public License as\n\/\/ published by the Free Software Foundation, either version 3 of the\n\/\/ License, or (at your option) any later version.\n\/\/\n\/\/ go-algorand is distributed in the hope that it will be useful,\n\/\/ but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n\/\/ GNU Affero General Public License for more details.\n\/\/\n\/\/ You should have received a copy of the GNU Affero General Public License\n\/\/ along with go-algorand.  If not, see <https:\/\/www.gnu.org\/licenses\/>.\n\npackage v2\n\nimport (\n\t\"fmt\"\n\t\"net\/http\"\n\t\"strings\"\n\n\t\"github.com\/algorand\/go-codec\/codec\"\n\t\"github.com\/labstack\/echo\/v4\"\n\n\t\"github.com\/algorand\/go-algorand\/daemon\/algod\/api\/server\/v2\/generated\"\n\t\"github.com\/algorand\/go-algorand\/data\"\n\t\"github.com\/algorand\/go-algorand\/data\/basics\"\n\t\"github.com\/algorand\/go-algorand\/data\/transactions\"\n\t\"github.com\/algorand\/go-algorand\/logging\"\n\t\"github.com\/algorand\/go-algorand\/node\"\n\t\"github.com\/algorand\/go-algorand\/protocol\"\n)\n\n\/\/ returnError logs an internal message while returning the encoded response.\nfunc returnError(ctx echo.Context, code int, internal error, external string, logger logging.Logger) error {\n\tlogger.Info(internal)\n\treturn ctx.JSON(code, generated.ErrorResponse{Message: external})\n}\n\nfunc badRequest(ctx echo.Context, internal error, external string, log logging.Logger) error {\n\treturn returnError(ctx, http.StatusBadRequest, internal, external, log)\n}\n\nfunc serviceUnavailable(ctx echo.Context, internal error, external string, log logging.Logger) error {\n\treturn returnError(ctx, http.StatusServiceUnavailable, internal, external, log)\n}\n\nfunc internalError(ctx echo.Context, internal error, external string, log logging.Logger) error {\n\treturn returnError(ctx, http.StatusInternalServerError, internal, external, log)\n}\n\nfunc notFound(ctx echo.Context, internal error, external string, log logging.Logger) error {\n\treturn returnError(ctx, http.StatusNotFound, internal, external, log)\n}\n\nfunc addrOrNil(addr basics.Address) *string {\n\tif addr.IsZero() {\n\t\treturn nil\n\t}\n\tret := addr.String()\n\treturn &ret\n}\n\nfunc strOrNil(str string) *string {\n\tif str == \"\" {\n\t\treturn nil\n\t}\n\treturn &str\n}\n\nfunc numOrNil(num uint64) *uint64 {\n\tif num == 0 {\n\t\treturn nil\n\t}\n\treturn &num\n}\n\nfunc byteOrNil(data []byte) *[]byte {\n\tif len(data) == 0 {\n\t\treturn nil\n\t}\n\treturn &data\n}\n\nfunc computeAssetIndexInPayset(tx node.TxnWithStatus, txnCounter uint64, payset []transactions.SignedTxnWithAD) (aidx *uint64) {\n\t\/\/ Compute transaction index in block\n\toffset := -1\n\tfor idx, stxnib := range payset {\n\t\tif tx.Txn.Txn.ID() == stxnib.Txn.ID() {\n\t\t\toffset = idx\n\t\t\tbreak\n\t\t}\n\t}\n\n\t\/\/ Sanity check that txn was in fetched block\n\tif offset < 0 {\n\t\treturn nil\n\t}\n\n\t\/\/ Count into block to get created asset index\n\tidx := txnCounter - uint64(len(payset)) + uint64(offset) + 1\n\treturn &idx\n}\n\n\/\/ computeAssetIndexFromTxn returns the created asset index given a confirmed\n\/\/ transaction whose confirmation block is available in the ledger. Note that\n\/\/ 0 is an invalid asset index (they start at 1).\nfunc computeAssetIndexFromTxn(tx node.TxnWithStatus, l *data.Ledger) (aidx *uint64) {\n\t\/\/ Must have ledger\n\tif l == nil {\n\t\treturn nil\n\t}\n\t\/\/ Transaction must be confirmed\n\tif tx.ConfirmedRound == 0 {\n\t\treturn nil\n\t}\n\t\/\/ Transaction must be AssetConfig transaction\n\tif tx.Txn.Txn.AssetConfigTxnFields == (transactions.AssetConfigTxnFields{}) {\n\t\treturn nil\n\t}\n\t\/\/ Transaction must be creating an asset\n\tif tx.Txn.Txn.AssetConfigTxnFields.ConfigAsset != 0 {\n\t\treturn nil\n\t}\n\n\t\/\/ Look up block where transaction was confirmed\n\tblk, err := l.Block(tx.ConfirmedRound)\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\tpayset, err := blk.DecodePaysetFlat()\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\treturn computeAssetIndexInPayset(tx, blk.BlockHeader.TxnCounter, payset)\n}\n\n\/\/ getCodecHandle converts a format string into the encoder + content type\nfunc getCodecHandle(formatPtr *string) (codec.Handle, string, error) {\n\tformat := \"json\"\n\tif formatPtr != nil {\n\t\tformat = strings.ToLower(*formatPtr)\n\t}\n\n\tswitch format {\n\tcase \"json\":\n\t\treturn protocol.JSONHandle, \"application\/json\", nil\n\tcase \"msgpack\":\n\t\tfallthrough\n\tcase \"msgp\":\n\t\treturn protocol.CodecHandle, \"application\/msgpack\", nil\n\tdefault:\n\t\treturn nil, \"\", fmt.Errorf(\"invalid format: %s\", format)\n\t}\n}\n\nfunc encode(handle codec.Handle, obj interface{}) ([]byte, error) {\n\tvar output []byte\n\tenc := codec.NewEncoderBytes(&output, handle)\n\n\terr := enc.Encode(obj)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to encode object: %v\", err)\n\t}\n\treturn output, nil\n}\n\nfunc decode(handle codec.Handle, data []byte, v interface{}) error {\n\tenc := codec.NewDecoderBytes(data, handle)\n\n\terr := enc.Decode(v)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to decode object: %v\", err)\n\t}\n\treturn nil\n}\n","lang_cluster":"Go","length":180,"code_uid":"68a86df048b9458caff1f69a0c8ba962"}
{"diff_hunk":"@@ -107,8 +107,8 @@ var orderCreateCmd = &cobra.Command{\n }\n \n var orderCancelCmd = &cobra.Command{\n-\tUse:   \"cancel <order_id>\",\n-\tShort: \"Cancel order on Marketplace\",\n+\tUse:   \"cancel <order_id>...\",\n+\tShort: \"Cancel given orders on Marketplace\",\n \tArgs:  cobra.MinimumNArgs(1),\n \tRunE: func(cmd *cobra.Command, args []string) error {\n \t\tctx, cancel := newTimeoutContext()","old_code":"package commands\n\nimport (\n\t\"fmt\"\n\n\t\"github.com\/sonm-io\/core\/cmd\/cli\/task_config\"\n\tpb \"github.com\/sonm-io\/core\/proto\"\n\t\"github.com\/spf13\/cobra\"\n)\n\nvar (\n\tordersSearchLimit uint64 = 0\n)\n\nfunc init() {\n\torderListCmd.PersistentFlags().Uint64Var(&ordersSearchLimit, \"limit\", 10, \"Orders count to show\")\n\n\torderRootCmd.AddCommand(\n\t\torderListCmd,\n\t\torderStatusCmd,\n\t\torderCreateCmd,\n\t\torderCancelCmd,\n\t\torderPurgeCmd,\n\t)\n}\n\nvar orderRootCmd = &cobra.Command{\n\tUse:               \"order\",\n\tShort:             \"Manage orders\",\n\tPersistentPreRunE: loadKeyStoreWrapper,\n}\n\nvar orderListCmd = &cobra.Command{\n\tUse:   \"list\",\n\tShort: \"Show your active orders\",\n\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\tctx, cancel := newTimeoutContext()\n\t\tdefer cancel()\n\n\t\tmarket, err := newMarketClient(ctx)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"\u0441annot create client connection: %v\", err)\n\t\t}\n\n\t\treq := &pb.Count{Count: ordersSearchLimit}\n\t\treply, err := market.GetOrders(ctx, req)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"cannot receive orders from marketplace: %v\", err)\n\t\t}\n\n\t\tprintOrdersList(cmd, reply.Orders)\n\t\treturn nil\n\t},\n}\n\nvar orderStatusCmd = &cobra.Command{\n\tUse:   \"status <order_id>\",\n\tShort: \"Show order stats\",\n\tArgs:  cobra.MinimumNArgs(1),\n\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\tctx, cancel := newTimeoutContext()\n\t\tdefer cancel()\n\n\t\tmarket, err := newMarketClient(ctx)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"cannot create client connection: %v\", err)\n\t\t}\n\n\t\torderID := args[0]\n\t\torder, err := market.GetOrderByID(ctx, &pb.ID{Id: orderID})\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"cannot get order by ID: %v\", err)\n\t\t}\n\n\t\tprintOrderDetails(cmd, order)\n\t\treturn nil\n\t},\n}\n\nvar orderCreateCmd = &cobra.Command{\n\tUse:   \"create <bid.yaml>\",\n\tShort: \"Place new Bid order on Marketplace\",\n\tArgs:  cobra.MinimumNArgs(1),\n\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\tctx, cancel := newTimeoutContext()\n\t\tdefer cancel()\n\n\t\tmarket, err := newMarketClient(ctx)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"cannot create client connection: %v\", err)\n\t\t}\n\n\t\tpath := args[0]\n\t\tbid := &pb.BidOrder{}\n\t\tif err := task_config.LoadFromFile(path, bid); err != nil {\n\t\t\treturn fmt.Errorf(\"cannot load order definition: %v\", err)\n\t\t}\n\n\t\tcreated, err := market.CreateOrder(ctx, bid)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"cannot create order on marketplace: %v\", err)\n\t\t}\n\n\t\tprintID(cmd, created.GetId().Unwrap().String())\n\t\treturn nil\n\t},\n}\n\nvar orderCancelCmd = &cobra.Command{\n\tUse:   \"cancel <order_id>\",\n\tShort: \"Cancel order on Marketplace\",\n\tArgs:  cobra.MinimumNArgs(1),\n\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\tctx, cancel := newTimeoutContext()\n\t\tdefer cancel()\n\n\t\tmarket, err := newMarketClient(ctx)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"cannot create client connection: %v\", err)\n\t\t}\n\n\t\torderID := args[0]\n\t\t_, err = market.CancelOrder(ctx, &pb.ID{Id: orderID})\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"cannot cancel order on Marketplace: %v\", err)\n\t\t}\n\n\t\tshowOk(cmd)\n\t\treturn nil\n\t},\n}\n\nvar orderPurgeCmd = &cobra.Command{\n\tUse:   \"purge\",\n\tShort: \"Remove all your orders from Marketplace\",\n\tRunE: func(cmd *cobra.Command, _ []string) error {\n\t\tctx, cancel := newTimeoutContext()\n\t\tdefer cancel()\n\n\t\tmarket, err := newMarketClient(ctx)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"cannot create client connection: %v\", err)\n\t\t}\n\n\t\tif _, err := market.Purge(ctx, &pb.Empty{}); err != nil {\n\t\t\treturn fmt.Errorf(\"cannot purge orders: %v\", err)\n\t\t}\n\n\t\tshowOk(cmd)\n\t\treturn nil\n\t},\n}\n","lang_cluster":"Go","length":152,"code_uid":"3f50f8a0a54c444fa7c79e1969aa4fb8"}
{"diff_hunk":"@@ -134,6 +134,10 @@ func (e *dErrImpl) Error() string {\n \treturn \"Multiple errors:\\n\" + strings.Join(lines, \"\\n\")\n }\n \n+func (e *dErrImpl) AnonymizedErrs() []string {\n+\treturn e.anonymizedErrs\n+}\n+\n func (e *dErrImpl) len() int {\n \treturn len(e.errs)\n }","old_code":"\/\/  Copyright 2017 Google Inc. All Rights Reserved.\n\/\/\n\/\/  Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/  you may not use this file except in compliance with the License.\n\/\/  You may obtain a copy of the License at\n\/\/\n\/\/      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/  Unless required by applicable law or agreed to in writing, software\n\/\/  distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/  See the License for the specific language governing permissions and\n\/\/  limitations under the License.\n\npackage daisy\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n)\n\nconst (\n\tuntypedError              = \"\"\n\tmultiError                = \"MultiError\"\n\tfileIOError               = \"FileIOError\"\n\tresourceDNEError          = \"ResourceDoesNotExist\"\n\timageObsoleteDeletedError = \"ImageObsoleteOrDeleted\"\n\n\tapiError    = \"APIError\"\n\tapiError404 = \"APIError404\"\n)\n\n\/\/ dErr is a Daisy internal error type.\n\/\/ It has:\n\/\/ - optional error typing\n\/\/ - multiple error aggregation\n\/\/\n\/\/ Default implementation:\n\/\/ The default dErr implementation is flat, dErr.add(anotherDErr) will merge the two dErrs\n\/\/ into a single, flat dErr instead of making anotherDErr a child to dErr.\ntype dErr interface {\n\terror\n\n\t\/\/ add shouldn't be called directly, instead call addErrs(dErr, error).\n\t\/\/ This assists with nil dErrs. addErrs(nil, e) will return a new dErr.\n\tadd(error)\n\tType() string\n}\n\n\/\/ addErrs adds an error to a dErr.\n\/\/ The dErr can be nil. If both the dErr and errors are nil, a nil dErr is returned.\n\/\/ If dErr is nil, but errors are not nil, a new dErr is instantiated, the errors are added,\n\/\/ and the new dErr is returned.\n\/\/ Any nil error in errs is disregarded. Therefore, `var e dErr; e = addErrs(e, nil)`\n\/\/ preserves e's nil-ness.\nfunc addErrs(e dErr, errs ...error) dErr {\n\tfor _, err := range errs {\n\t\tif err != nil {\n\t\t\tif e == nil {\n\t\t\t\te = &dErrImpl{}\n\t\t\t}\n\t\t\te.add(err)\n\t\t}\n\t}\n\treturn e\n}\n\nfunc errf(format string, a ...interface{}) dErr {\n\treturn newErr(fmt.Errorf(format, a...))\n}\n\n\/\/ newErr returns a dErr. newErr is used to wrap another error as a dErr.\n\/\/ If e is already a dErr, e is copied and returned.\n\/\/ If e is nil, nil is returned.\nfunc newErr(e error) dErr {\n\tif e == nil {\n\t\treturn nil\n\t}\n\tif dE, ok := e.(*dErrImpl); ok {\n\t\treturn dE\n\t}\n\treturn &dErrImpl{errs: []error{e}}\n}\n\nfunc typedErr(errType string, e error) dErr {\n\tif e == nil {\n\t\treturn nil\n\t}\n\tdE := newErr(e)\n\tdE.(*dErrImpl).errType = errType\n\treturn dE\n}\n\nfunc typedErrf(errType, format string, a ...interface{}) dErr {\n\treturn typedErr(errType, fmt.Errorf(format, a...))\n}\n\ntype dErrImpl struct {\n\terrs    []error\n\terrType string\n}\n\nfunc (e *dErrImpl) add(err error) {\n\tif e2, ok := err.(*dErrImpl); ok {\n\t\te.merge(e2)\n\t} else if !ok {\n\t\t\/\/ This is some other error type. Add it.\n\t\te.errs = append(e.errs, err)\n\t}\n\tif e.len() > 1 {\n\t\te.errType = multiError\n\t}\n}\n\nfunc (e *dErrImpl) Error() string {\n\tif e.len() == 0 {\n\t\treturn \"\"\n\t}\n\tif e.len() == 1 {\n\t\terrStr := e.errs[0].Error()\n\t\tif e.errType != \"\" {\n\t\t\treturn fmt.Sprintf(\"%s: %s\", e.errType, errStr)\n\t\t}\n\t\treturn errStr\n\t}\n\n\t\/\/ Multiple error handling.\n\tpre := \"* \"\n\tlines := make([]string, e.len())\n\tfor i, err := range e.errs {\n\t\tlines[i] = pre + err.Error()\n\t}\n\n\treturn \"Multiple errors:\\n\" + strings.Join(lines, \"\\n\")\n}\n\nfunc (e *dErrImpl) len() int {\n\treturn len(e.errs)\n}\n\nfunc (e *dErrImpl) merge(e2 *dErrImpl) {\n\tif e2.len() > 0 {\n\t\te.errs = append(e.errs, e2.errs...)\n\t\t\/\/ Take e2's type. This solves the situation of e having 0 errors, and e2 having 1.\n\t\t\/\/ Of course, there is a possibility of len(e) > 0 and len(e2) > 1, in which case,\n\t\t\/\/ the type should be a multiError.\n\t\te.errType = e2.errType\n\t\tif e.len() > 1 {\n\t\t\te.errType = multiError\n\t\t}\n\t}\n}\n\nfunc (e *dErrImpl) Type() string {\n\treturn e.errType\n}\n","lang_cluster":"Go","length":156,"code_uid":"42974fe647bc4bc1a49d7969e9ca2192"}
{"diff_hunk":"@@ -2,7 +2,10 @@ package h2quic\n \n import (\n \t\"bytes\"\n+\t\"errors\"\n+\t\"fmt\"\n \t\"net\/http\"\n+\t\"net\/url\"\n \t\"strconv\"\n \t\"strings\"\n \t\"sync\"","old_code":"package h2quic\n\nimport (\n\t\"bytes\"\n\t\"net\/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\n\tquic \"github.com\/lucas-clemente\/quic-go\"\n\t\"github.com\/lucas-clemente\/quic-go\/internal\/protocol\"\n\t\"github.com\/lucas-clemente\/quic-go\/internal\/utils\"\n\t\"golang.org\/x\/net\/http2\"\n\t\"golang.org\/x\/net\/http2\/hpack\"\n)\n\ntype responseWriter struct {\n\tdataStreamID protocol.StreamID\n\tdataStream   quic.Stream\n\n\theaderStream      quic.Stream\n\theaderStreamMutex *sync.Mutex\n\n\theader        http.Header\n\tstatus        int \/\/ status code passed to WriteHeader\n\theaderWritten bool\n\n\tsettings *sessionSettings\n}\n\nfunc newResponseWriter(headerStream quic.Stream, headerStreamMutex *sync.Mutex, dataStream quic.Stream, dataStreamID protocol.StreamID, settings *sessionSettings) *responseWriter {\n\treturn &responseWriter{\n\t\theader:            http.Header{},\n\t\theaderStream:      headerStream,\n\t\theaderStreamMutex: headerStreamMutex,\n\t\tdataStream:        dataStream,\n\t\tdataStreamID:      dataStreamID,\n\t\tsettings:          settings,\n\t}\n}\n\nfunc (w *responseWriter) Header() http.Header {\n\treturn w.header\n}\n\nfunc (w *responseWriter) WriteHeader(status int) {\n\tif w.headerWritten {\n\t\treturn\n\t}\n\tw.headerWritten = true\n\tw.status = status\n\n\tvar headers bytes.Buffer\n\tenc := hpack.NewEncoder(&headers)\n\tenc.WriteField(hpack.HeaderField{Name: \":status\", Value: strconv.Itoa(status)})\n\n\tfor k, v := range w.header {\n\t\tfor index := range v {\n\t\t\tenc.WriteField(hpack.HeaderField{Name: strings.ToLower(k), Value: v[index]})\n\t\t}\n\t}\n\n\tutils.Infof(\"Responding with %d\", status)\n\tw.headerStreamMutex.Lock()\n\tdefer w.headerStreamMutex.Unlock()\n\th2framer := http2.NewFramer(w.headerStream, nil)\n\terr := h2framer.WriteHeaders(http2.HeadersFrameParam{\n\t\tStreamID:      uint32(w.dataStreamID),\n\t\tEndHeaders:    true,\n\t\tBlockFragment: headers.Bytes(),\n\t})\n\tif err != nil {\n\t\tutils.Errorf(\"could not write h2 header: %s\", err.Error())\n\t}\n}\n\nfunc (w *responseWriter) Write(p []byte) (int, error) {\n\tif !w.headerWritten {\n\t\tw.WriteHeader(200)\n\t}\n\tif !bodyAllowedForStatus(w.status) {\n\t\treturn 0, http.ErrBodyNotAllowed\n\t}\n\treturn w.dataStream.Write(p)\n}\n\nfunc (w *responseWriter) Flush() {}\n\n\/\/ This is a NOP. Use http.Request.Context\nfunc (w *responseWriter) CloseNotify() <-chan bool { return make(<-chan bool) }\n\n\/\/ test that we implement http.Flusher\nvar _ http.Flusher = &responseWriter{}\n\n\/\/ test that we implement http.CloseNotifier\nvar _ http.CloseNotifier = &responseWriter{}\n\n\/\/ copied from http2\/http2.go\n\/\/ bodyAllowedForStatus reports whether a given response status code\n\/\/ permits a body. See RFC 2616, section 4.4.\nfunc bodyAllowedForStatus(status int) bool {\n\tswitch {\n\tcase status >= 100 && status <= 199:\n\t\treturn false\n\tcase status == 204:\n\t\treturn false\n\tcase status == 304:\n\t\treturn false\n\t}\n\treturn true\n}\n","lang_cluster":"Go","length":111,"code_uid":"717172904ca94b40965ff92f799bd61c"}
{"diff_hunk":"@@ -129,7 +129,7 @@ func (pl *RoundRobin) Remove(pid transport.PeerIdentifier) error {\n \tpl.lock.Lock()\n \tdefer pl.lock.Unlock()\n \n-\tif err := pl.pr.Remove(pid); err != nil {\n+\tif err := pl.removeByPeerIdentifier(pid); err != nil {\n \t\t\/\/ The peer has already been removed\n \t\treturn err\n \t}","old_code":"\/\/ Copyright (c) 2016 Uber Technologies, Inc.\n\/\/\n\/\/ Permission is hereby granted, free of charge, to any person obtaining a copy\n\/\/ of this software and associated documentation files (the \"Software\"), to deal\n\/\/ in the Software without restriction, including without limitation the rights\n\/\/ to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n\/\/ copies of the Software, and to permit persons to whom the Software is\n\/\/ furnished to do so, subject to the following conditions:\n\/\/\n\/\/ The above copyright notice and this permission notice shall be included in\n\/\/ all copies or substantial portions of the Software.\n\/\/\n\/\/ THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\/\/ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\/\/ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\/\/ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\/\/ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\/\/ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n\/\/ THE SOFTWARE.\n\npackage roundrobin\n\nimport (\n\t\"context\"\n\t\"sync\"\n\n\tyerrors \"go.uber.org\/yarpc\/internal\/errors\"\n\t\"go.uber.org\/yarpc\/transport\"\n\t\"go.uber.org\/yarpc\/transport\/internal\/errors\"\n\n\t\"go.uber.org\/atomic\"\n)\n\n\/\/ New creates a new round robin PeerList using\nfunc New(peerIDs []transport.PeerIdentifier, agent transport.Agent) (*RoundRobin, error) {\n\trr := &RoundRobin{\n\t\tpr:                 NewPeerRing(len(peerIDs)),\n\t\tagent:              agent,\n\t\tpeerAvailableEvent: make(chan struct{}, 1),\n\t}\n\n\terr := rr.addAll(peerIDs)\n\treturn rr, err\n}\n\n\/\/ RoundRobin is a PeerList which rotates which peers are to be selected in a circle\ntype RoundRobin struct {\n\tlock sync.Mutex\n\n\tpr                 *PeerRing\n\tpeerAvailableEvent chan struct{}\n\tagent              transport.Agent\n\tstarted            atomic.Bool\n}\n\nfunc (pl *RoundRobin) addAll(peerIDs []transport.PeerIdentifier) error {\n\tpl.lock.Lock()\n\tdefer pl.lock.Unlock()\n\n\tvar errs []error\n\n\tfor _, peerID := range peerIDs {\n\t\tif err := pl.addPeer(peerID); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t}\n\n\treturn yerrors.MultiError(errs)\n}\n\n\/\/ Add a peer identifier to the round robin\nfunc (pl *RoundRobin) Add(pid transport.PeerIdentifier) error {\n\tpl.lock.Lock()\n\terr := pl.addPeer(pid)\n\tpl.lock.Unlock()\n\treturn err\n}\n\n\/\/ Must be run inside a mutex.Lock()\nfunc (pl *RoundRobin) addPeer(pid transport.PeerIdentifier) error {\n\tp, err := pl.agent.RetainPeer(pid, pl)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err = pl.pr.Add(p); err != nil {\n\t\treturn err\n\t}\n\n\tpl.notifyPeerAvailable()\n\treturn nil\n}\n\n\/\/ Start notifies the RoundRobin that requests will start coming\nfunc (pl *RoundRobin) Start() error {\n\tif pl.started.Swap(true) {\n\t\treturn errors.ErrPeerListAlreadyStarted(\"RoundRobinList\")\n\t}\n\treturn nil\n}\n\n\/\/ Stop notifies the RoundRobin that requests will stop coming\nfunc (pl *RoundRobin) Stop() error {\n\tif !pl.started.Swap(false) {\n\t\treturn errors.ErrPeerListNotStarted(\"RoundRobinList\")\n\t}\n\treturn pl.clearPeers()\n}\n\nfunc (pl *RoundRobin) clearPeers() error {\n\tpl.lock.Lock()\n\tdefer pl.lock.Unlock()\n\n\tvar errs []error\n\n\tpeers := pl.pr.RemoveAll()\n\tfor _, p := range peers {\n\t\tif err := pl.agent.ReleasePeer(p, pl); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\n\treturn yerrors.MultiError(errs)\n}\n\n\/\/ Remove a peer identifier from the round robin\nfunc (pl *RoundRobin) Remove(pid transport.PeerIdentifier) error {\n\tpl.lock.Lock()\n\tdefer pl.lock.Unlock()\n\n\tif err := pl.pr.Remove(pid); err != nil {\n\t\t\/\/ The peer has already been removed\n\t\treturn err\n\t}\n\n\treturn pl.agent.ReleasePeer(pid, pl)\n}\n\n\/\/ ChoosePeer selects the next available peer in the round robin\nfunc (pl *RoundRobin) ChoosePeer(ctx context.Context, req *transport.Request) (transport.Peer, error) {\n\tif !pl.started.Load() {\n\t\treturn nil, errors.ErrPeerListNotStarted(\"RoundRobinList\")\n\t}\n\n\tfor {\n\t\tif nextPeer := pl.nextPeer(); nextPeer != nil {\n\t\t\tpl.notifyPeerAvailable()\n\t\t\treturn nextPeer, nil\n\t\t}\n\n\t\tif err := pl.waitForPeerAddedEvent(ctx); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n}\n\n\/\/ nextPeer grabs the next available peer from the PeerRing and returns it,\n\/\/ if there are no available peers it returns nil\nfunc (pl *RoundRobin) nextPeer() transport.Peer {\n\tpl.lock.Lock()\n\tpeer := pl.pr.Next()\n\tpl.lock.Unlock()\n\treturn peer\n}\n\n\/\/ notifyPeerAvailable writes to a channel indicating that a Peer is currently\n\/\/ available for requests\nfunc (pl *RoundRobin) notifyPeerAvailable() {\n\tselect {\n\tcase pl.peerAvailableEvent <- struct{}{}:\n\tdefault:\n\t}\n}\n\n\/\/ waitForPeerAddedEvent waits until a peer is added to the peer list or the\n\/\/ given context finishes.\n\/\/ Must NOT be run in a mutex.Lock()\nfunc (pl *RoundRobin) waitForPeerAddedEvent(ctx context.Context) error {\n\tif _, ok := ctx.Deadline(); !ok {\n\t\treturn errors.ErrChooseContextHasNoDeadline(\"RoundRobinList\")\n\t}\n\n\tselect {\n\tcase <-pl.peerAvailableEvent:\n\t\treturn nil\n\tcase <-ctx.Done():\n\t\treturn ctx.Err()\n\t}\n}\n\n\/\/ NotifyStatusChanged when the peer's status changes\nfunc (pl *RoundRobin) NotifyStatusChanged(transport.Peer) {}\n","lang_cluster":"Go","length":193,"code_uid":"16cc07fcf0824861b4e7d93f5599ddd0"}
{"diff_hunk":"@@ -17,9 +17,13 @@ limitations under the License.\n package cainjector\n \n import (\n+\t\"context\"\n+\t\"fmt\"\n \t\"io\/ioutil\"\n \n \tlogf \"github.com\/jetstack\/cert-manager\/pkg\/logs\"\n+\t\"github.com\/pkg\/errors\"\n+\t\"golang.org\/x\/sync\/errgroup\"\n \n \tadmissionreg \"k8s.io\/api\/admissionregistration\/v1beta1\"\n \tapiext \"k8s.io\/apiextensions-apiserver\/pkg\/apis\/apiextensions\/v1beta1\"","old_code":"\/*\nCopyright 2019 The Jetstack cert-manager contributors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage cainjector\n\nimport (\n\t\"io\/ioutil\"\n\n\tlogf \"github.com\/jetstack\/cert-manager\/pkg\/logs\"\n\n\tadmissionreg \"k8s.io\/api\/admissionregistration\/v1beta1\"\n\tapiext \"k8s.io\/apiextensions-apiserver\/pkg\/apis\/apiextensions\/v1beta1\"\n\t\"k8s.io\/apimachinery\/pkg\/api\/meta\"\n\t\"k8s.io\/apimachinery\/pkg\/runtime\"\n\tapireg \"k8s.io\/kube-aggregator\/pkg\/apis\/apiregistration\/v1beta1\"\n\tctrl \"sigs.k8s.io\/controller-runtime\"\n)\n\n\/\/ injectorSet describes a particular setup of the injector controller\ntype injectorSetup struct {\n\tresourceName string\n\tinjector     CertInjector\n\tlistType     runtime.Object\n}\n\nvar (\n\tMutatingWebhookSetup = injectorSetup{\n\t\tresourceName: \"mutatingwebhookconfiguration\",\n\t\tinjector:     mutatingWebhookInjector{},\n\t\tlistType:     &admissionreg.MutatingWebhookConfigurationList{},\n\t}\n\n\tValidatingWebhookSetup = injectorSetup{\n\t\tresourceName: \"validatingwebhookconfiguration\",\n\t\tinjector:     validatingWebhookInjector{},\n\t\tlistType:     &admissionreg.ValidatingWebhookConfigurationList{},\n\t}\n\n\tAPIServiceSetup = injectorSetup{\n\t\tresourceName: \"apiservice\",\n\t\tinjector:     apiServiceInjector{},\n\t\tlistType:     &apireg.APIServiceList{},\n\t}\n\n\tCRDSetup = injectorSetup{\n\t\tresourceName: \"customresourcedefinition\",\n\t\tinjector:     crdConversionInjector{},\n\t\tlistType:     &apiext.CustomResourceDefinitionList{},\n\t}\n\n\tinjectorSetups  = []injectorSetup{MutatingWebhookSetup, ValidatingWebhookSetup, APIServiceSetup, CRDSetup}\n\tControllerNames []string\n)\n\n\/\/ registerAllInjectors registers all injectors and based on the\n\/\/ graduation state of the injector decides how to log no kind\/resource match errors\nfunc registerAllInjectors(mgr ctrl.Manager, sources ...caDataSource) error {\n\tfor _, setup := range injectorSetups {\n\t\tif err := Register(mgr, setup, sources...); err != nil {\n\t\t\tif !meta.IsNoMatchError(err) || !setup.injector.IsAlpha() {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tctrl.Log.V(logf.WarnLevel).Info(\"unable to register injector which is still in an alpha phase.\"+\n\t\t\t\t\" Enable the feature on the API server in order to use this injector\",\n\t\t\t\t\"injector\", setup.resourceName)\n\t\t}\n\t}\n\treturn nil\n}\n\n\/\/ Register registers an injection controller with the given manager, and adds relevant indicies.\nfunc Register(mgr ctrl.Manager, setup injectorSetup, sources ...caDataSource) error {\n\ttyp := setup.injector.NewTarget().AsObject()\n\tbuilder := ctrl.NewControllerManagedBy(mgr).For(typ)\n\tfor _, s := range sources {\n\t\tif err := s.ApplyTo(mgr, setup, builder); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn builder.Complete(&genericInjectReconciler{\n\t\tClient:       mgr.GetClient(),\n\t\tsources:      sources,\n\t\tlog:          ctrl.Log.WithName(\"inject-controller\"),\n\t\tresourceName: setup.resourceName,\n\t\tinjector:     setup.injector,\n\t})\n}\n\n\/\/ dataFromSliceOrFile returns data from the slice (if non-empty), or from the file,\n\/\/ or an error if an error occurred reading the file\nfunc dataFromSliceOrFile(data []byte, file string) ([]byte, error) {\n\tif len(data) > 0 {\n\t\treturn data, nil\n\t}\n\tif len(file) > 0 {\n\t\tfileData, err := ioutil.ReadFile(file)\n\t\tif err != nil {\n\t\t\treturn []byte{}, err\n\t\t}\n\t\treturn fileData, nil\n\t}\n\treturn nil, nil\n}\n\n\/\/ RegisterCertificateBased registers all known injection controllers that\n\/\/ target Certificate resources with the  given manager, and adds relevant\n\/\/ indices.\n\/\/ The registered controllers require the cert-manager API to be available\n\/\/ in order to run.\nfunc RegisterCertificateBased(mgr ctrl.Manager) error {\n\tsources := []caDataSource{\n\t\t&certificateDataSource{client: mgr.GetClient()},\n\t}\n\treturn registerAllInjectors(mgr, sources...)\n}\n\n\/\/ RegisterSecretBased registers all known injection controllers that\n\/\/ target Secret resources with the  given manager, and adds relevant\n\/\/ indices.\n\/\/ The registered controllers only require the corev1 APi to be available in\n\/\/ order to run.\nfunc RegisterSecretBased(mgr ctrl.Manager) error {\n\tsources := []caDataSource{\n\t\t&secretDataSource{client: mgr.GetClient()},\n\t\t&kubeconfigDataSource{},\n\t}\n\treturn registerAllInjectors(mgr, sources...)\n}\n","lang_cluster":"Go","length":142,"code_uid":"8801311a9f2740b1a0493576ac9731a5"}
{"diff_hunk":"@@ -8,7 +8,8 @@ var (\n \tnewlineTabRE           = regexp.MustCompile(`\\n\\t`)\n \tcertificateTimeErrorRE = regexp.MustCompile(`: current time \\S+ is after \\S+`)\n \t\/\/ aws\n-\tawsRequestIDRE = regexp.MustCompile(`(, )*(?i)(request id: )(?:[-[:xdigit:]]+)`)\n+\tawsRequestIDRE   = regexp.MustCompile(`(, )*(?i)(request id: )(?:[-[:xdigit:]]+)`)\n+\tawsNotAuthorized = regexp.MustCompile(`(User: arn:aws:sts::)\\S+(:assumed-role\/[^\/]+\/)\\S+( is not authorized to perform: \\S+ on resource: arn:aws:iam::)[^:]+(:\\S+)`)\n \t\/\/ azure\n \tazureErrorDescriptionRE = regexp.MustCompile(`\\\"error_description\\\":\\\"(.*?)\\\\r\\\\n`)\n )","old_code":"package utils\n\nimport (\n\t\"regexp\"\n)\n\nvar (\n\tnewlineTabRE           = regexp.MustCompile(`\\n\\t`)\n\tcertificateTimeErrorRE = regexp.MustCompile(`: current time \\S+ is after \\S+`)\n\t\/\/ aws\n\tawsRequestIDRE = regexp.MustCompile(`(, )*(?i)(request id: )(?:[-[:xdigit:]]+)`)\n\t\/\/ azure\n\tazureErrorDescriptionRE = regexp.MustCompile(`\\\"error_description\\\":\\\"(.*?)\\\\r\\\\n`)\n)\n\n\/\/ ErrorScrub scrubs cloud error messages destined for CRD status to remove things that\n\/\/ change every attempt, such as request IDs, which subsequently cause an infinite update\/reconcile loop.\nfunc ErrorScrub(err error) string {\n\tif err == nil {\n\t\treturn \"\"\n\t}\n\ts := newlineTabRE.ReplaceAllString(err.Error(), \", \")\n\ts = awsRequestIDRE.ReplaceAllString(s, \"\")\n\ts = certificateTimeErrorRE.ReplaceAllString(s, \"\")\n\t\/\/ if Azure error, return just the error description\n\tmatch := azureErrorDescriptionRE.FindStringSubmatch(s)\n\tif len(match) > 0 {\n\t\treturn match[1]\n\t}\n\treturn s\n}\n","lang_cluster":"Go","length":31,"code_uid":"a185cccec6444dfcb589b699d6f5b9d6"}
{"diff_hunk":"@@ -24,11 +24,14 @@ import (\n \tintv1alpha1 \"github.com\/google\/knative-gcp\/pkg\/apis\/intevents\/v1alpha1\"\n \tbcreconciler \"github.com\/google\/knative-gcp\/pkg\/client\/injection\/reconciler\/intevents\/v1alpha1\/brokercell\"\n \t\"github.com\/google\/knative-gcp\/pkg\/reconciler\"\n+\t\"github.com\/google\/knative-gcp\/pkg\/reconciler\/brokercell\/testingdata\"\n \t. \"github.com\/google\/knative-gcp\/pkg\/reconciler\/testing\"\n+\tappsv1 \"k8s.io\/api\/apps\/v1\"\n \tcorev1 \"k8s.io\/api\/core\/v1\"\n \t\"k8s.io\/apimachinery\/pkg\/runtime\"\n \t\"k8s.io\/client-go\/kubernetes\/scheme\"\n \tclientgotesting \"k8s.io\/client-go\/testing\"\n+\tfakekubeclient \"knative.dev\/pkg\/client\/injection\/kube\/client\/fake\"\n \t\"knative.dev\/pkg\/configmap\"\n \t\"knative.dev\/pkg\/controller\"\n \tlogtesting \"knative.dev\/pkg\/logging\/testing\"","old_code":"\/*\nCopyright 2020 Google LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage brokercell\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\n\tintv1alpha1 \"github.com\/google\/knative-gcp\/pkg\/apis\/intevents\/v1alpha1\"\n\tbcreconciler \"github.com\/google\/knative-gcp\/pkg\/client\/injection\/reconciler\/intevents\/v1alpha1\/brokercell\"\n\t\"github.com\/google\/knative-gcp\/pkg\/reconciler\"\n\t. \"github.com\/google\/knative-gcp\/pkg\/reconciler\/testing\"\n\tcorev1 \"k8s.io\/api\/core\/v1\"\n\t\"k8s.io\/apimachinery\/pkg\/runtime\"\n\t\"k8s.io\/client-go\/kubernetes\/scheme\"\n\tclientgotesting \"k8s.io\/client-go\/testing\"\n\t\"knative.dev\/pkg\/configmap\"\n\t\"knative.dev\/pkg\/controller\"\n\tlogtesting \"knative.dev\/pkg\/logging\/testing\"\n\t. \"knative.dev\/pkg\/reconciler\/testing\"\n)\n\nconst (\n\ttestNS         = \"testnamespace\"\n\tbrokerCellName = \"test-brokercell\"\n\n\tbrokerCellFinalizerName = \"brokercells.internal.events.cloud.google.com\"\n)\n\nvar (\n\ttestKey = fmt.Sprintf(\"%s\/%s\", testNS, brokerCellName)\n\n\tbrokerCellFinalizerUpdatedEvent = Eventf(corev1.EventTypeNormal, \"FinalizerUpdate\", `Updated \"test-brokercell\" finalizers`)\n\tbrokerCellReconciledEvent       = Eventf(corev1.EventTypeNormal, \"BrokerCellReconciled\", `BrokerCell reconciled: \"testnamespace\/test-brokercell\"`)\n\tbrokerCellFinalizedEvent        = Eventf(corev1.EventTypeNormal, \"BrokerCellFinalized\", `BrokerCell finalized: \"testnamespace\/test-brokercell\"`)\n)\n\nfunc init() {\n\t\/\/ Add types to scheme\n\t_ = intv1alpha1.AddToScheme(scheme.Scheme)\n}\n\nfunc TestAllCases(t *testing.T) {\n\ttable := TableTest{{\n\t\tName: \"bad workqueue key\",\n\t\tKey:  \"too\/many\/parts\",\n\t}, {\n\t\tName: \"key not found\",\n\t\tKey:  testKey,\n\t}, {\n\t\tName: \"BrokerCell is being deleted\",\n\t\tKey:  testKey,\n\t\tObjects: []runtime.Object{\n\t\t\tNewBrokerCell(brokerCellName, testNS,\n\t\t\t\tWithInitBrokerCellConditions,\n\t\t\t\tWithBrokerCellDeletionTimestamp),\n\t\t},\n\t\tWantEvents: []string{\n\t\t\tbrokerCellFinalizedEvent,\n\t\t},\n\t}, {\n\t\tName: \"BrokerCell created\",\n\t\tKey:  testKey,\n\t\tObjects: []runtime.Object{\n\t\t\tNewBrokerCell(brokerCellName, testNS),\n\t\t},\n\t\tWantStatusUpdates: []clientgotesting.UpdateActionImpl{{\n\t\t\tObject: NewBrokerCell(brokerCellName, testNS,\n\t\t\t\tWithInitBrokerCellConditions,\n\t\t\t),\n\t\t}},\n\t\tWantEvents: []string{\n\t\t\tbrokerCellFinalizerUpdatedEvent,\n\t\t\tbrokerCellReconciledEvent,\n\t\t},\n\t\tWantPatches: []clientgotesting.PatchActionImpl{\n\t\t\tpatchFinalizers(testNS, brokerCellName, brokerCellFinalizerName),\n\t\t},\n\t}}\n\n\tdefer logtesting.ClearAll()\n\ttable.Test(t, MakeFactory(func(ctx context.Context, listers *Listers, cmw configmap.Watcher, testData map[string]interface{}) controller.Reconciler {\n\t\tr := &Reconciler{\n\t\t\tBase: reconciler.NewBase(ctx, controllerAgentName, cmw),\n\t\t}\n\t\treturn bcreconciler.NewReconciler(ctx, r.Logger, r.RunClientSet, listers.GetBrokerCellLister(), r.Recorder, r)\n\t}))\n}\n\nfunc patchFinalizers(namespace, name, finalizer string) clientgotesting.PatchActionImpl {\n\taction := clientgotesting.PatchActionImpl{}\n\taction.Name = name\n\taction.Namespace = namespace\n\tpatch := `{\"metadata\":{\"finalizers\":[\"` + finalizer + `\"],\"resourceVersion\":\"\"}}`\n\taction.Patch = []byte(patch)\n\treturn action\n}\n\nfunc patchRemoveFinalizers(namespace, name string) clientgotesting.PatchActionImpl {\n\taction := clientgotesting.PatchActionImpl{}\n\taction.Name = name\n\taction.Namespace = namespace\n\tpatch := `{\"metadata\":{\"finalizers\":[],\"resourceVersion\":\"\"}}`\n\taction.Patch = []byte(patch)\n\treturn action\n}\n","lang_cluster":"Go","length":121,"code_uid":"b4bf90ef56684d7689551f33eda25202"}
{"diff_hunk":"@@ -104,7 +104,7 @@ func EncodeSimulatedMessage(msg Message, timestamp, blockNumber *big.Int, execut\n \ttx := ovmTransaction{\n \t\ttimestamp,\n \t\tblockNumber,\n-\t\tuint8(msg.QueueOrigin().Uint64()),\n+\t\tuint8(msg.QueueOrigin()),\n \t\t*msg.L1MessageSender(),\n \t\t*to,\n \t\tbig.NewInt(int64(msg.Gas())),","old_code":"package core\n\nimport (\n\t\"fmt\"\n\t\"math\/big\"\n\n\t\"github.com\/ethereum\/go-ethereum\/common\"\n\t\"github.com\/ethereum\/go-ethereum\/core\/types\"\n\t\"github.com\/ethereum\/go-ethereum\/core\/vm\"\n\t\"github.com\/ethereum\/go-ethereum\/rollup\/dump\"\n)\n\nvar ZeroAddress = common.HexToAddress(\"0x0000000000000000000000000000000000000000\")\n\ntype ovmTransaction struct {\n\tTimestamp     *big.Int       `json:\"timestamp\"`\n\tBlockNumber   *big.Int       `json:\"blockNumber\"`\n\tL1QueueOrigin uint8          `json:\"l1QueueOrigin\"`\n\tL1TxOrigin    common.Address `json:\"l1TxOrigin\"`\n\tEntrypoint    common.Address `json:\"entrypoint\"`\n\tGasLimit      *big.Int       `json:\"gasLimit\"`\n\tData          []uint8        `json:\"data\"`\n}\n\nfunc toExecutionManagerRun(evm *vm.EVM, msg Message) (Message, error) {\n\ttx := ovmTransaction{\n\t\tevm.Context.Time,\n\t\tmsg.L1BlockNumber(),\n\t\tuint8(msg.QueueOrigin().Uint64()),\n\t\t*msg.L1MessageSender(),\n\t\t*msg.To(),\n\t\tbig.NewInt(int64(msg.Gas())),\n\t\tmsg.Data(),\n\t}\n\n\tvar abi = evm.Context.OvmExecutionManager.ABI\n\tvar args = []interface{}{\n\t\ttx,\n\t\tevm.Context.OvmStateManager.Address,\n\t}\n\n\tret, err := abi.Pack(\"run\", args...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\toutputmsg, err := modMessage(\n\t\tmsg,\n\t\tmsg.From(),\n\t\t&evm.Context.OvmExecutionManager.Address,\n\t\tret,\n\t\tevm.Context.GasLimit,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn outputmsg, nil\n}\n\nfunc AsOvmMessage(tx *types.Transaction, signer types.Signer, decompressor common.Address, gasLimit uint64) (Message, error) {\n\tmsg, err := tx.AsMessage(signer)\n\tif err != nil {\n\t\t\/\/ This should only be allowed to pass if the transaction is in the ctc\n\t\t\/\/ already. The presence of `Index` should specify this.\n\t\tindex := tx.GetMeta().Index\n\t\tif index == nil {\n\t\t\treturn msg, fmt.Errorf(\"Cannot convert tx to message in asOvmMessage: %w\", err)\n\t\t}\n\t}\n\n\t\/\/ Queue origin L1ToL2 transactions do not go through the\n\t\/\/ sequencer entrypoint. The calldata is expected to be in the\n\t\/\/ correct format when deserialized from the EVM events, see\n\t\/\/ rollup\/sync_service.go.\n\tqo := msg.QueueOrigin()\n\tif qo != nil && qo.Uint64() == uint64(types.QueueOriginL1ToL2) {\n\t\treturn msg, nil\n\t}\n\n\t\/\/ Sequencer transactions get sent to the \"sequencer entrypoint,\" a contract that decompresses\n\t\/\/ the incoming transaction data.\n\toutmsg, err := modMessage(\n\t\tmsg,\n\t\tmsg.From(),\n\t\t&decompressor,\n\t\ttx.GetMeta().RawTransaction,\n\t\tgasLimit,\n\t)\n\n\tif err != nil {\n\t\treturn msg, fmt.Errorf(\"Cannot mod message: %w\", err)\n\t}\n\n\treturn outmsg, nil\n}\n\nfunc EncodeSimulatedMessage(msg Message, timestamp, blockNumber *big.Int, executionManager, stateManager dump.OvmDumpAccount) (Message, error) {\n\tto := msg.To()\n\tif to == nil {\n\t\tto = &common.Address{0}\n\t}\n\n\ttx := ovmTransaction{\n\t\ttimestamp,\n\t\tblockNumber,\n\t\tuint8(msg.QueueOrigin().Uint64()),\n\t\t*msg.L1MessageSender(),\n\t\t*to,\n\t\tbig.NewInt(int64(msg.Gas())),\n\t\tmsg.Data(),\n\t}\n\n\tfrom := msg.From()\n\tvar args = []interface{}{\n\t\ttx,\n\t\tfrom,\n\t\tstateManager.Address,\n\t}\n\n\toutput, err := executionManager.ABI.Pack(\"simulateMessage\", args...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"Cannot pack simulateMessage: %w\", err)\n\t}\n\n\treturn modMessage(\n\t\tmsg,\n\t\tcommon.Address{},\n\t\t&executionManager.Address,\n\t\toutput,\n\t\tmsg.Gas(),\n\t)\n}\n\nfunc modMessage(\n\tmsg Message,\n\tfrom common.Address,\n\tto *common.Address,\n\tdata []byte,\n\tgasLimit uint64,\n) (Message, error) {\n\tqueueOrigin, err := getQueueOrigin(msg.QueueOrigin())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\toutmsg := types.NewMessage(\n\t\tfrom,\n\t\tto,\n\t\tmsg.Nonce(),\n\t\tcommon.Big0,\n\t\tgasLimit,\n\t\tmsg.GasPrice(),\n\t\tdata,\n\t\tfalse,\n\t\tmsg.L1MessageSender(),\n\t\tmsg.L1BlockNumber(),\n\t\tqueueOrigin,\n\t\tmsg.SignatureHashType(),\n\t)\n\n\treturn outmsg, nil\n}\n\nfunc getQueueOrigin(\n\tqueueOrigin *big.Int,\n) (types.QueueOrigin, error) {\n\tif queueOrigin.Cmp(big.NewInt(0)) == 0 {\n\t\treturn types.QueueOriginSequencer, nil\n\t} else if queueOrigin.Cmp(big.NewInt(1)) == 0 {\n\t\treturn types.QueueOriginL1ToL2, nil\n\t} else if queueOrigin.Cmp(big.NewInt(2)) == 0 {\n\t\treturn types.QueueOriginL1ToL2, nil\n\t} else {\n\t\treturn types.QueueOriginSequencer, fmt.Errorf(\"invalid queue origin: %d\", queueOrigin)\n\t}\n}\n","lang_cluster":"Go","length":177,"code_uid":"e1d9369c96a04552bc10ab85f66defd8"}
{"diff_hunk":"@@ -97,7 +97,7 @@ func (a *GCPActuator) getZones(region string) ([]string, error) {\n \tpageToken := \"\"\n \n \tfor {\n-\t\tzoneList, err := a.client.ListComputeZones(gcpclient.ListComputeZonesOptions{\n+\t\tzoneList, err := a.gcpClient.ListComputeZones(gcpclient.ListComputeZonesOptions{\n \t\t\tFilter:    zoneFilter,\n \t\t\tPageToken: pageToken,\n \t\t})","old_code":"package remotemachineset\n\nimport (\n\t\"fmt\"\n\n\t\"github.com\/pkg\/errors\"\n\tlog \"github.com\/sirupsen\/logrus\"\n\n\tmachineapi \"github.com\/openshift\/cluster-api\/pkg\/apis\/machine\/v1beta1\"\n\tcorev1 \"k8s.io\/api\/core\/v1\"\n\n\tinstallgcp \"github.com\/openshift\/installer\/pkg\/asset\/machines\/gcp\"\n\tinstallertypes \"github.com\/openshift\/installer\/pkg\/types\"\n\tinstallertypesgcp \"github.com\/openshift\/installer\/pkg\/types\/gcp\"\n\n\thivev1 \"github.com\/openshift\/hive\/pkg\/apis\/hive\/v1\"\n\t\"github.com\/openshift\/hive\/pkg\/gcpclient\"\n)\n\n\/\/ GCPActuator encapsulates the pieces necessary to be able to generate\n\/\/ a list of MachineSets to sync to the remote cluster.\ntype GCPActuator struct {\n\tclient gcpclient.Client\n\tlogger log.FieldLogger\n}\n\nvar _ Actuator = &GCPActuator{}\n\n\/\/ NewGCPActuator is the constructor for building a GCPActuator\nfunc NewGCPActuator(gcpCreds *corev1.Secret, logger log.FieldLogger) (*GCPActuator, error) {\n\tgcpClient, err := gcpclient.NewClientFromSecret(gcpCreds)\n\tif err != nil {\n\t\tlogger.WithError(err).Warn(\"failed to create GCP client with creds in clusterDeployment's secret\")\n\t\treturn nil, err\n\t}\n\tactuator := &GCPActuator{\n\t\tclient: gcpClient,\n\t\tlogger: logger,\n\t}\n\treturn actuator, nil\n}\n\n\/\/ GenerateMachineSets satisfies the Actuator interface and will take a clusterDeployment and return a list of MachineSets\n\/\/ to sync to the remote cluster.\nfunc (a *GCPActuator) GenerateMachineSets(cd *hivev1.ClusterDeployment, pool *hivev1.MachinePool, logger log.FieldLogger) ([]*machineapi.MachineSet, error) {\n\tif cd.Spec.ClusterMetadata == nil {\n\t\treturn nil, errors.New(\"ClusterDeployment does not have cluster metadata\")\n\t}\n\tif cd.Spec.Platform.GCP == nil {\n\t\treturn nil, errors.New(\"ClusterDeployment is not for GCP\")\n\t}\n\tif pool.Spec.Platform.GCP == nil {\n\t\treturn nil, errors.New(\"MachinePool is not for GCP\")\n\t}\n\n\tic := &installertypes.InstallConfig{\n\t\tPlatform: installertypes.Platform{\n\t\t\tGCP: &installertypesgcp.Platform{\n\t\t\t\tRegion: cd.Spec.Platform.GCP.Region,\n\t\t\t},\n\t\t},\n\t}\n\n\tcomputePool := baseMachinePool(pool)\n\tcomputePool.Platform.GCP = &installertypesgcp.MachinePool{\n\t\tZones:        pool.Spec.Platform.GCP.Zones,\n\t\tInstanceType: pool.Spec.Platform.GCP.InstanceType,\n\t}\n\n\t\/\/ get image ID for the generated machine sets\n\timageID, err := a.getImageID(cd, logger)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to find image ID for the machine sets\")\n\t}\n\n\tif len(computePool.Platform.GCP.Zones) == 0 {\n\t\tzones, err := a.getZones(cd.Spec.Platform.GCP.Region)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"compute pool not providing list of zones and failed to fetch list of zones\")\n\t\t}\n\t\tif len(zones) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"zero zones returned for region %s\", cd.Spec.Platform.GCP.Region)\n\t\t}\n\t\tcomputePool.Platform.GCP.Zones = zones\n\t}\n\n\tinstallerMachineSets, err := installgcp.MachineSets(cd.Spec.ClusterMetadata.InfraID, ic, computePool, imageID, pool.Spec.Name, \"worker-user-data\")\n\treturn installerMachineSets, errors.Wrap(err, \"failed to generate machinesets\")\n}\n\nfunc (a *GCPActuator) getZones(region string) ([]string, error) {\n\tzones := []string{}\n\n\t\/\/ Filter to regions matching '.*<region>.*' (where the zone is actually UP)\n\tzoneFilter := fmt.Sprintf(\"(region eq '.*%s.*') (status eq UP)\", region)\n\n\tpageToken := \"\"\n\n\tfor {\n\t\tzoneList, err := a.client.ListComputeZones(gcpclient.ListComputeZonesOptions{\n\t\t\tFilter:    zoneFilter,\n\t\t\tPageToken: pageToken,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn zones, err\n\t\t}\n\n\t\tfor _, zone := range zoneList.Items {\n\t\t\tzones = append(zones, zone.Name)\n\t\t}\n\n\t\tif zoneList.NextPageToken == \"\" {\n\t\t\tbreak\n\t\t}\n\t\tpageToken = zoneList.NextPageToken\n\t}\n\n\treturn zones, nil\n}\n\nfunc (a *GCPActuator) getImageID(cd *hivev1.ClusterDeployment, logger log.FieldLogger) (string, error) {\n\tinfra := cd.Spec.ClusterMetadata.InfraID\n\n\t\/\/ find names of the form '<infra>-.*'\n\tfilter := fmt.Sprintf(\"name eq \\\"%s-.*\\\"\", infra)\n\tresult, err := a.client.ListComputeImages(gcpclient.ListComputeImagesOptions{Filter: filter})\n\tif err != nil {\n\t\tlogger.WithError(err).Warnf(\"failed to find a GCP image starting with name: %s\", infra)\n\t\treturn \"\", err\n\t}\n\tswitch len(result.Items) {\n\tcase 0:\n\t\tmsg := fmt.Sprintf(\"found 0 results searching for GCP image starting with name: %s\", infra)\n\t\tlogger.Warnf(msg)\n\t\treturn \"\", errors.New(msg)\n\tcase 1:\n\t\tlogger.Debugf(\"using image with name %s for machine sets\", result.Items[0].Name)\n\t\treturn result.Items[0].Name, nil\n\tdefault:\n\t\tmsg := fmt.Sprintf(\"unexpected number of results when looking for GCP image with name starting with %s\", infra)\n\t\tlogger.Warnf(msg)\n\t\treturn \"\", errors.New(msg)\n\t}\n}\n","lang_cluster":"Go","length":144,"code_uid":"cdb6d53e47914dae86fdaf9d202fc4d4"}
{"diff_hunk":"@@ -94,5 +94,5 @@ func (f *Factory) FromRole(roleARN string, region string) (*session.Session, err\n \t\treturn nil, err\n \t}\n \tsess.Handlers.Build.PushBackNamed(userAgentHandler())\n-\treturn sess, err\n+\treturn sess, nil\n }","old_code":"\/\/ Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\/\/ SPDX-License-Identifier: Apache-2.0\n\n\/\/ Package session provides functions that return AWS sessions to use in the AWS SDK.\npackage session\n\nimport (\n\t\"fmt\"\n\t\"runtime\"\n\n\t\"github.com\/aws\/amazon-ecs-cli-v2\/internal\/pkg\/version\"\n\n\t\"github.com\/aws\/aws-sdk-go\/aws\"\n\t\"github.com\/aws\/aws-sdk-go\/aws\/credentials\/stscreds\"\n\t\"github.com\/aws\/aws-sdk-go\/aws\/request\"\n\t\"github.com\/aws\/aws-sdk-go\/aws\/session\"\n)\n\nconst userAgentHeader = \"User-Agent\"\n\n\/\/ userAgentHandler returns a http request handler that sets a custom user agent to all aws requests.\nfunc userAgentHandler() request.NamedHandler {\n\treturn request.NamedHandler{\n\t\tName: \"UserAgentHandler\",\n\t\tFn: func(r *request.Request) {\n\t\t\tuserAgent := r.HTTPRequest.Header.Get(userAgentHeader)\n\t\t\tr.HTTPRequest.Header.Set(userAgentHeader,\n\t\t\t\tfmt.Sprintf(\"aws-ecs-cli-v2\/%s (%s) %s\", version.Version, runtime.GOOS, userAgent))\n\t\t},\n\t}\n}\n\n\/\/ Factory holds methods to create sessions.\ntype Factory struct{}\n\n\/\/ Default returns a session configured against the \"default\" AWS profile.\nfunc (f *Factory) Default() (*session.Session, error) {\n\tsess, err := session.NewSessionWithOptions(session.Options{\n\t\tConfig: aws.Config{\n\t\t\tCredentialsChainVerboseErrors: aws.Bool(true),\n\t\t},\n\t\tSharedConfigState: session.SharedConfigEnable,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsess.Handlers.Build.PushBackNamed(userAgentHandler())\n\treturn sess, err\n}\n\n\/\/ DefaultWithRegion returns a session configured against the \"default\" AWS profile and the input region.\nfunc (f *Factory) DefaultWithRegion(region string) (*session.Session, error) {\n\tsess, err := session.NewSession(&aws.Config{\n\t\tRegion: aws.String(region),\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsess.Handlers.Build.PushBackNamed(userAgentHandler())\n\treturn sess, err\n}\n\n\/\/ FromProfile returns a session configured against the input profile name.\nfunc (f *Factory) FromProfile(name string) (*session.Session, error) {\n\tsess, err := session.NewSessionWithOptions(session.Options{\n\t\tConfig: aws.Config{\n\t\t\tCredentialsChainVerboseErrors: aws.Bool(true),\n\t\t},\n\t\tSharedConfigState: session.SharedConfigEnable,\n\t\tProfile:           name,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsess.Handlers.Build.PushBackNamed(userAgentHandler())\n\treturn sess, err\n}\n\n\/\/ FromRole returns a session configured against the input role and region.\nfunc (f *Factory) FromRole(roleARN string, region string) (*session.Session, error) {\n\tdefaultSession, err := f.Default()\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error creating default session: %w\", err)\n\t}\n\n\tcreds := stscreds.NewCredentials(defaultSession, roleARN)\n\tsess, err := session.NewSession(&aws.Config{\n\t\tCredentialsChainVerboseErrors: aws.Bool(true),\n\t\tCredentials:                   creds,\n\t\tRegion:                        &region,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsess.Handlers.Build.PushBackNamed(userAgentHandler())\n\treturn sess, err\n}\n","lang_cluster":"Go","length":98,"code_uid":"6d37ef778ce14a98abc19ef787108765"}
{"diff_hunk":"@@ -45,6 +45,7 @@ import (\n \ttriggerinformer \"github.com\/google\/knative-gcp\/pkg\/client\/injection\/informers\/broker\/v1beta1\/trigger\"\n \ttriggerreconciler \"github.com\/google\/knative-gcp\/pkg\/client\/injection\/reconciler\/broker\/v1beta1\/trigger\"\n \t\"github.com\/google\/knative-gcp\/pkg\/reconciler\"\n+\treconcilerutils \"github.com\/google\/knative-gcp\/pkg\/reconciler\/utils\"\n \t\"github.com\/google\/knative-gcp\/pkg\/utils\"\n )\n ","old_code":"\/*\nCopyright 2020 Google LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage trigger\n\nimport (\n\t\"context\"\n\n\t\"github.com\/google\/knative-gcp\/pkg\/reconciler\/celltenant\"\n\n\t\"cloud.google.com\/go\/pubsub\"\n\t\"go.uber.org\/zap\"\n\t\"k8s.io\/apimachinery\/pkg\/labels\"\n\t\"k8s.io\/client-go\/tools\/cache\"\n\n\t\"github.com\/google\/knative-gcp\/pkg\/logging\"\n\t\"knative.dev\/eventing\/pkg\/apis\/eventing\"\n\teventingv1beta1 \"knative.dev\/eventing\/pkg\/apis\/eventing\/v1beta1\"\n\t\"knative.dev\/eventing\/pkg\/duck\"\n\t\"knative.dev\/pkg\/client\/injection\/ducks\/duck\/v1\/addressable\"\n\t\"knative.dev\/pkg\/client\/injection\/ducks\/duck\/v1\/source\"\n\t\"knative.dev\/pkg\/configmap\"\n\t\"knative.dev\/pkg\/controller\"\n\tpkgcontroller \"knative.dev\/pkg\/controller\"\n\t\"knative.dev\/pkg\/injection\"\n\tpkgreconciler \"knative.dev\/pkg\/reconciler\"\n\t\"knative.dev\/pkg\/resolver\"\n\n\tbrokerv1beta1 \"github.com\/google\/knative-gcp\/pkg\/apis\/broker\/v1beta1\"\n\t\"github.com\/google\/knative-gcp\/pkg\/apis\/configs\/dataresidency\"\n\tbrokerinformer \"github.com\/google\/knative-gcp\/pkg\/client\/injection\/informers\/broker\/v1beta1\/broker\"\n\ttriggerinformer \"github.com\/google\/knative-gcp\/pkg\/client\/injection\/informers\/broker\/v1beta1\/trigger\"\n\ttriggerreconciler \"github.com\/google\/knative-gcp\/pkg\/client\/injection\/reconciler\/broker\/v1beta1\/trigger\"\n\t\"github.com\/google\/knative-gcp\/pkg\/reconciler\"\n\t\"github.com\/google\/knative-gcp\/pkg\/utils\"\n)\n\nconst (\n\t\/\/ controllerAgentName is the string used by this controller to identify\n\t\/\/ itself when creating events.\n\tcontrollerAgentName = \"trigger-controller\"\n\t\/\/ finalizerName is the name of the finalizer that this controller adds to the Triggers that it reconciles.\n\tfinalizerName = \"googlecloud\"\n)\n\n\/\/ filterBroker is the function to filter brokers with proper brokerclass.\nvar filterBroker = pkgreconciler.AnnotationFilterFunc(eventingv1beta1.BrokerClassAnnotationKey, brokerv1beta1.BrokerClass, false \/*allowUnset*\/)\n\ntype Constructor injection.ControllerConstructor\n\n\/\/ NewConstructor creates a constructor to make a Trigger controller.\nfunc NewConstructor(dataresidencyss *dataresidency.StoreSingleton) Constructor {\n\treturn func(ctx context.Context, cmw configmap.Watcher) *controller.Impl {\n\t\treturn newController(ctx, cmw, dataresidencyss.Store(ctx, cmw))\n\t}\n}\n\nfunc newController(ctx context.Context, cmw configmap.Watcher, drs *dataresidency.Store) *controller.Impl {\n\ttriggerInformer := triggerinformer.Get(ctx)\n\n\tvar client *pubsub.Client\n\t\/\/ If there is an error, the projectID will be empty. The reconciler will retry\n\t\/\/ to get the projectID during reconciliation.\n\tprojectID, err := utils.ProjectIDOrDefault(\"\")\n\tif err != nil {\n\t\tlogging.FromContext(ctx).Error(\"Failed to get project ID\", zap.Error(err))\n\t} else {\n\t\t\/\/ Attempt to create a pubsub client for all worker threads to use. If this\n\t\t\/\/ fails, pass a nil value to the Reconciler. They will attempt to\n\t\t\/\/ create a client on reconcile.\n\t\tif client, err = pubsub.NewClient(ctx, projectID); err != nil {\n\t\t\tclient = nil\n\t\t\tlogging.FromContext(ctx).Error(\"Failed to create controller-wide Pub\/Sub client\", zap.Error(err))\n\t\t}\n\t}\n\n\tif client != nil {\n\t\tgo func() {\n\t\t\t<-ctx.Done()\n\t\t\tclient.Close()\n\t\t}()\n\t}\n\tr := &Reconciler{\n\t\tBase:         reconciler.NewBase(ctx, controllerAgentName, cmw),\n\t\tbrokerLister: brokerinformer.Get(ctx).Lister(),\n\t\ttargetReconciler: &celltenant.TargetReconciler{\n\t\t\tProjectID:          projectID,\n\t\t\tPubsubClient:       client,\n\t\t\tDataresidencyStore: drs,\n\t\t},\n\t}\n\n\timpl := triggerreconciler.NewImpl(ctx, r, withAgentAndFinalizer)\n\tr.sourceTracker = duck.NewListableTracker(ctx, source.Get, impl.EnqueueKey, controller.GetTrackerLease(ctx))\n\tr.addressableTracker = duck.NewListableTracker(ctx, addressable.Get, impl.EnqueueKey, controller.GetTrackerLease(ctx))\n\tr.uriResolver = resolver.NewURIResolver(ctx, impl.EnqueueKey)\n\n\tr.Logger.Info(\"Setting up event handlers\")\n\n\ttriggerInformer.Informer().AddEventHandlerWithResyncPeriod(controller.HandleAll(impl.Enqueue), reconciler.DefaultResyncPeriod)\n\n\t\/\/ Watch brokers.\n\tbrokerinformer.Get(ctx).Informer().AddEventHandler(\n\t\tcache.FilteringResourceEventHandler{\n\t\t\t\/\/ Only care about brokers with the proper class annotation\n\t\t\tFilterFunc: filterBroker,\n\t\t\tHandler: controller.HandleAll(func(obj interface{}) {\n\t\t\t\tif b, ok := obj.(*brokerv1beta1.Broker); ok {\n\t\t\t\t\ttriggers, err := triggerinformer.Get(ctx).Lister().Triggers(b.Namespace).List(labels.SelectorFromSet(map[string]string{eventing.BrokerLabelKey: b.Name}))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tr.Logger.Warn(\"Failed to list triggers\", zap.String(\"Namespace\", b.Namespace), zap.String(\"Broker\", b.Name))\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tfor _, trigger := range triggers {\n\t\t\t\t\t\timpl.Enqueue(trigger)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}),\n\t\t},\n\t)\n\n\treturn impl\n}\n\nfunc withAgentAndFinalizer(_ *pkgcontroller.Impl) pkgcontroller.Options {\n\treturn pkgcontroller.Options{\n\t\tFinalizerName: finalizerName,\n\t\tAgentName:     controllerAgentName,\n\t}\n}\n","lang_cluster":"Go","length":143,"code_uid":"f0e7dac3582742c9ad6b19813600cade"}
{"diff_hunk":"@@ -6,7 +6,8 @@ import (\n \t\"fmt\"\n \t\"html\"\n \t\"net\/http\"\n-\t\"strings\"\n+\t\"net\/url\"\n+\t\"regexp\"\n \n \t\"github.com\/mholt\/caddy\/middleware\"\n )","old_code":"\/\/ Package redirect is middleware for redirecting certain requests\n\/\/ to other locations.\npackage redirect\n\nimport (\n\t\"fmt\"\n\t\"html\"\n\t\"net\/http\"\n\t\"strings\"\n\n\t\"github.com\/mholt\/caddy\/middleware\"\n)\n\n\/\/ Redirect is middleware to respond with HTTP redirects\ntype Redirect struct {\n\tNext  middleware.Handler\n\tRules []Rule\n}\n\n\/\/ ServeHTTP implements the middleware.Handler interface.\nfunc (rd Redirect) ServeHTTP(w http.ResponseWriter, r *http.Request) (int, error) {\n\tfor _, rule := range rd.Rules {\n\t\tif rule.From == \"\/\" {\n\t\t\t\/\/ Catchall redirect preserves path (TODO: Standardize\/formalize this behavior)\n\t\t\tnewPath := strings.TrimSuffix(rule.To, \"\/\") + r.URL.Path\n\t\t\tif rule.Meta {\n\t\t\t\tfmt.Fprintf(w, metaRedir, html.EscapeString(newPath))\n\t\t\t} else {\n\t\t\t\thttp.Redirect(w, r, newPath, rule.Code)\n\t\t\t}\n\t\t\treturn 0, nil\n\t\t}\n\t\tif r.URL.Path == rule.From {\n\t\t\tif rule.Meta {\n\t\t\t\tfmt.Fprintf(w, metaRedir, html.EscapeString(rule.To))\n\t\t\t} else {\n\t\t\t\thttp.Redirect(w, r, rule.To, rule.Code)\n\t\t\t}\n\t\t\treturn 0, nil\n\t\t}\n\t}\n\treturn rd.Next.ServeHTTP(w, r)\n}\n\n\/\/ Rule describes an HTTP redirect rule.\ntype Rule struct {\n\tFrom, To string\n\tCode     int\n\tMeta     bool\n}\n\nvar metaRedir = `<html>\n<head>\n  <meta http-equiv=\"refresh\" content=\"0;URL='%s'\">\n<\/head>\n<body>redirecting...<\/body>\n<\/html>`\n","lang_cluster":"Go","length":57,"code_uid":"0417de18a16a41a2b92032a73634f417"}
{"diff_hunk":"@@ -173,8 +173,3 @@ func (o *ocDistAggregator) Count() (uint64, error) {\n func (o *ocDistAggregator) Histogram() (aggregation.Buckets, error) {\n \treturn o.buckets, nil\n }\n-\n-\/\/ end returns the time the histogram was measured.\n-func (o *ocDistAggregator) end() time.Time {\n-\treturn o.endTime\n-}","old_code":"\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage opencensus \/\/ import \"go.opentelemetry.io\/otel\/bridge\/opencensus\"\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"go.opencensus.io\/metric\/metricdata\"\n\n\t\"go.opentelemetry.io\/otel\/metric\/number\"\n\t\"go.opentelemetry.io\/otel\/sdk\/export\/metric\/aggregation\"\n)\n\nvar (\n\terrIncompatibleType = errors.New(\"incompatible type for aggregation\")\n\terrEmpty            = errors.New(\"points may not be empty\")\n\terrBadPoint         = errors.New(\"point cannot be converted\")\n)\n\n\/\/ aggregationWithEndTime is an aggregation that can also provide the timestamp\n\/\/ of the last recorded point.\ntype aggregationWithEndTime interface {\n\taggregation.Aggregation\n\tend() time.Time\n}\n\n\/\/ newAggregationFromPoints creates an OpenTelemetry aggregation from\n\/\/ OpenCensus points.  Points may not be empty and must be either\n\/\/ all (int|float)64 or all *metricdata.Distribution.\nfunc newAggregationFromPoints(points []metricdata.Point) (aggregationWithEndTime, error) {\n\tif len(points) == 0 {\n\t\treturn nil, errEmpty\n\t}\n\tswitch t := points[0].Value.(type) {\n\tcase int64:\n\t\treturn newExactAggregator(points)\n\tcase float64:\n\t\treturn newExactAggregator(points)\n\tcase *metricdata.Distribution:\n\t\treturn newDistributionAggregator(points)\n\tdefault:\n\t\t\/\/ TODO add *metricdata.Summary support\n\t\treturn nil, fmt.Errorf(\"%w: %v\", errIncompatibleType, t)\n\t}\n}\n\nvar _ aggregation.Aggregation = &ocExactAggregator{}\nvar _ aggregation.LastValue = &ocExactAggregator{}\nvar _ aggregation.Points = &ocExactAggregator{}\n\n\/\/ newExactAggregator creates an OpenTelemetry aggreation from OpenCensus points.\n\/\/ Points may not be empty, and must only contain integers or floats.\nfunc newExactAggregator(pts []metricdata.Point) (aggregationWithEndTime, error) {\n\tpoints := make([]aggregation.Point, len(pts))\n\tfor i, pt := range pts {\n\t\tswitch t := pt.Value.(type) {\n\t\tcase int64:\n\t\t\tpoints[i] = aggregation.Point{\n\t\t\t\tNumber: number.NewInt64Number(pt.Value.(int64)),\n\t\t\t\tTime:   pt.Time,\n\t\t\t}\n\t\tcase float64:\n\t\t\tpoints[i] = aggregation.Point{\n\t\t\t\tNumber: number.NewFloat64Number(pt.Value.(float64)),\n\t\t\t\tTime:   pt.Time,\n\t\t\t}\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"%w: %v\", errIncompatibleType, t)\n\t\t}\n\t}\n\treturn &ocExactAggregator{\n\t\tpoints: points,\n\t}, nil\n}\n\ntype ocExactAggregator struct {\n\tpoints []aggregation.Point\n}\n\n\/\/ Kind returns the kind of aggregation this is.\nfunc (o *ocExactAggregator) Kind() aggregation.Kind {\n\treturn aggregation.ExactKind\n}\n\n\/\/ Points returns access to the raw data set.\nfunc (o *ocExactAggregator) Points() ([]aggregation.Point, error) {\n\treturn o.points, nil\n}\n\n\/\/ LastValue returns the last point.\nfunc (o *ocExactAggregator) LastValue() (number.Number, time.Time, error) {\n\tlast := o.points[len(o.points)-1]\n\treturn last.Number, last.Time, nil\n}\n\n\/\/ end returns the timestamp of the last point\nfunc (o *ocExactAggregator) end() time.Time {\n\t_, t, _ := o.LastValue()\n\treturn t\n}\n\nvar _ aggregation.Aggregation = &ocDistAggregator{}\nvar _ aggregation.Histogram = &ocDistAggregator{}\n\n\/\/ newDistributionAggregator creates an OpenTelemetry aggreation from\n\/\/ OpenCensus points. Points may not be empty, and must only contain\n\/\/ Distributions.  The most recent disribution will be used in the aggregation.\nfunc newDistributionAggregator(pts []metricdata.Point) (aggregationWithEndTime, error) {\n\t\/\/ only use the most recent datapoint for now.\n\tpt := pts[len(pts)-1]\n\tval, ok := pt.Value.(*metricdata.Distribution)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"%w: %v\", errBadPoint, pt.Value)\n\t}\n\tbucketCounts := make([]uint64, len(val.Buckets))\n\tfor i, bucket := range val.Buckets {\n\t\tif bucket.Count < 0 {\n\t\t\treturn nil, fmt.Errorf(\"%w: bucket count may not be negative\", errBadPoint)\n\t\t}\n\t\tbucketCounts[i] = uint64(bucket.Count)\n\t}\n\tif val.Count < 0 {\n\t\treturn nil, fmt.Errorf(\"%w: count may not be negative\", errBadPoint)\n\t}\n\treturn &ocDistAggregator{\n\t\tsum:   number.NewFloat64Number(val.Sum),\n\t\tcount: uint64(val.Count),\n\t\tbuckets: aggregation.Buckets{\n\t\t\tBoundaries: val.BucketOptions.Bounds,\n\t\t\tCounts:     bucketCounts,\n\t\t},\n\t\tendTime: pts[len(pts)-1].Time,\n\t}, nil\n}\n\ntype ocDistAggregator struct {\n\tsum     number.Number\n\tcount   uint64\n\tbuckets aggregation.Buckets\n\tendTime time.Time\n}\n\n\/\/ Kind returns the kind of aggregation this is.\nfunc (o *ocDistAggregator) Kind() aggregation.Kind {\n\treturn aggregation.HistogramKind\n}\n\n\/\/ Sum returns the sum of values.\nfunc (o *ocDistAggregator) Sum() (number.Number, error) {\n\treturn o.sum, nil\n}\n\n\/\/ Count returns the number of values.\nfunc (o *ocDistAggregator) Count() (uint64, error) {\n\treturn o.count, nil\n}\n\n\/\/ Histogram returns the count of events in pre-determined buckets.\nfunc (o *ocDistAggregator) Histogram() (aggregation.Buckets, error) {\n\treturn o.buckets, nil\n}\n\n\/\/ end returns the time the histogram was measured.\nfunc (o *ocDistAggregator) end() time.Time {\n\treturn o.endTime\n}\n","lang_cluster":"Go","length":180,"code_uid":"f908716ed104431496253ae3f6239041"}
{"diff_hunk":"@@ -8,10 +8,12 @@ package staking\n \n import (\n \t\"context\"\n+\t\"math\/big\"\n \n \t\"github.com\/gogo\/protobuf\/proto\"\n \t\"github.com\/pkg\/errors\"\n \n+\t\"github.com\/iotexproject\/iotex-address\/address\"\n \t\"github.com\/iotexproject\/iotex-proto\/golang\/iotextypes\"\n \n \t\"github.com\/iotexproject\/iotex-core\/db\"","old_code":"\/\/ Copyright (c) 2020 IoTeX Foundation\n\/\/ This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n\/\/ warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n\/\/ permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n\/\/ License 2.0 that can be found in the LICENSE file.\n\npackage staking\n\nimport (\n\t\"context\"\n\n\t\"github.com\/gogo\/protobuf\/proto\"\n\t\"github.com\/pkg\/errors\"\n\n\t\"github.com\/iotexproject\/iotex-proto\/golang\/iotextypes\"\n\n\t\"github.com\/iotexproject\/iotex-core\/db\"\n\t\"github.com\/iotexproject\/iotex-core\/pkg\/util\/byteutil\"\n)\n\nconst (\n\t\/\/ StakingCandidatesNamespace is a namespace to store candidates with epoch start height\n\tStakingCandidatesNamespace = \"stakingCandidates\"\n\t\/\/ StakingBucketsNamespace is a namespace to store vote buckets with epoch start height\n\tStakingBucketsNamespace = \"stakingBuckets\"\n)\n\nconst indexerHeightKey = \"latestHeight\"\n\n\/\/ CandidatesBucketsIndexer is an indexer to store candidates by given height\ntype CandidatesBucketsIndexer struct {\n\tlatestCandidatesHeight uint64\n\tlatestBucketsHeight    uint64\n\tkvStore                db.KVStore\n}\n\n\/\/ NewStakingCandidatesBucketsIndexer creates a new StakingCandidatesIndexer\nfunc NewStakingCandidatesBucketsIndexer(kv db.KVStore) (*CandidatesBucketsIndexer, error) {\n\tif kv == nil {\n\t\treturn nil, ErrMissingField\n\t}\n\treturn &CandidatesBucketsIndexer{\n\t\tkvStore: kv,\n\t}, nil\n}\n\n\/\/ Start starts the indexer\nfunc (cbi *CandidatesBucketsIndexer) Start(ctx context.Context) error {\n\tif err := cbi.kvStore.Start(ctx); err != nil {\n\t\treturn err\n\t}\n\tret, err := cbi.kvStore.Get(StakingCandidatesNamespace, []byte(indexerHeightKey))\n\tswitch errors.Cause(err) {\n\tcase nil:\n\t\tcbi.latestCandidatesHeight = byteutil.BytesToUint64BigEndian(ret)\n\tcase db.ErrNotExist:\n\t\tcbi.latestCandidatesHeight = 0\n\tdefault:\n\t\treturn err\n\t}\n\n\tret, err = cbi.kvStore.Get(StakingBucketsNamespace, []byte(indexerHeightKey))\n\tswitch errors.Cause(err) {\n\tcase nil:\n\t\tcbi.latestBucketsHeight = byteutil.BytesToUint64BigEndian(ret)\n\tcase db.ErrNotExist:\n\t\tcbi.latestBucketsHeight = 0\n\tdefault:\n\t\treturn err\n\t}\n\treturn nil\n}\n\n\/\/ Stop stops the indexer\nfunc (cbi *CandidatesBucketsIndexer) Stop(ctx context.Context) error {\n\treturn cbi.kvStore.Stop(ctx)\n}\n\n\/\/ PutCandidates puts candidates into indexer\nfunc (cbi *CandidatesBucketsIndexer) PutCandidates(height uint64, candidates *iotextypes.CandidateListV2) error {\n\tcandidatesBytes, err := proto.Marshal(candidates)\n\tif err != nil {\n\t\treturn err\n\t}\n\theightBytes := byteutil.Uint64ToBytesBigEndian(height)\n\tif err := cbi.kvStore.Put(StakingCandidatesNamespace, heightBytes, candidatesBytes); err != nil {\n\t\treturn err\n\t}\n\tif err := cbi.kvStore.Put(StakingCandidatesNamespace, []byte(indexerHeightKey), heightBytes); err != nil {\n\t\treturn err\n\t}\n\tcbi.latestCandidatesHeight = height\n\treturn nil\n}\n\n\/\/ GetCandidates gets candidates from indexer given epoch start height\nfunc (cbi *CandidatesBucketsIndexer) GetCandidates(height uint64, offset, limit uint32) ([]byte, uint64, error) {\n\tif height > cbi.latestCandidatesHeight {\n\t\theight = cbi.latestCandidatesHeight\n\t}\n\tcandidateList := &iotextypes.CandidateListV2{}\n\tret, err := cbi.kvStore.Get(StakingCandidatesNamespace, byteutil.Uint64ToBytesBigEndian(height))\n\tif errors.Cause(err) == db.ErrNotExist {\n\t\td, err := proto.Marshal(candidateList)\n\t\treturn d, height, err\n\t}\n\tif err != nil {\n\t\treturn nil, height, err\n\t}\n\tif err := proto.Unmarshal(ret, candidateList); err != nil {\n\t\treturn nil, height, err\n\t}\n\tlength := uint32(len(candidateList.Candidates))\n\tif offset >= length {\n\t\td, err := proto.Marshal(&iotextypes.CandidateListV2{})\n\t\treturn d, height, err\n\t}\n\tend := offset + limit\n\tif end > uint32(len(candidateList.Candidates)) {\n\t\tend = uint32(len(candidateList.Candidates))\n\t}\n\tcandidateList.Candidates = candidateList.Candidates[offset:end]\n\td, err := proto.Marshal(candidateList)\n\treturn d, height, err\n}\n\n\/\/ PutBuckets puts vote buckets into indexer\nfunc (cbi *CandidatesBucketsIndexer) PutBuckets(height uint64, buckets *iotextypes.VoteBucketList) error {\n\tbucketsBytes, err := proto.Marshal(buckets)\n\tif err != nil {\n\t\treturn err\n\t}\n\theightBytes := byteutil.Uint64ToBytesBigEndian(height)\n\tif err := cbi.kvStore.Put(StakingBucketsNamespace, heightBytes, bucketsBytes); err != nil {\n\t\treturn err\n\t}\n\tif err := cbi.kvStore.Put(StakingBucketsNamespace, []byte(indexerHeightKey), heightBytes); err != nil {\n\t\treturn err\n\t}\n\tcbi.latestBucketsHeight = height\n\treturn nil\n}\n\n\/\/ GetBuckets gets vote buckets from indexer given epoch start height\nfunc (cbi *CandidatesBucketsIndexer) GetBuckets(height uint64, offset, limit uint32) ([]byte, uint64, error) {\n\tif height > cbi.latestBucketsHeight {\n\t\theight = cbi.latestBucketsHeight\n\t}\n\tbuckets := &iotextypes.VoteBucketList{}\n\tret, err := cbi.kvStore.Get(StakingBucketsNamespace, byteutil.Uint64ToBytesBigEndian(height))\n\tif errors.Cause(err) == db.ErrNotExist {\n\t\td, err := proto.Marshal(buckets)\n\t\treturn d, height, err\n\t}\n\tif err != nil {\n\t\treturn nil, height, err\n\t}\n\tif err := proto.Unmarshal(ret, buckets); err != nil {\n\t\treturn nil, height, err\n\t}\n\tlength := uint32(len(buckets.Buckets))\n\tif offset >= length {\n\t\td, err := proto.Marshal(&iotextypes.VoteBucketList{})\n\t\treturn d, height, err\n\t}\n\tend := offset + limit\n\tif end > uint32(len(buckets.Buckets)) {\n\t\tend = uint32(len(buckets.Buckets))\n\t}\n\tbuckets.Buckets = buckets.Buckets[offset:end]\n\td, err := proto.Marshal(buckets)\n\treturn d, height, err\n}\n","lang_cluster":"Go","length":173,"code_uid":"e07e35ffd4754bf2834d3e97a38c6d97"}
{"diff_hunk":"@@ -1,8 +1,9 @@\n package structs\n \n import (\n-\t\"fmt\"\n+\t\"strings\"\n \n+\t\"github.com\/pborman\/uuid\"\n \t\"github.com\/pkg\/errors\"\n \t\"github.com\/sonm-io\/core\/proto\"\n )","old_code":"package structs\n\nimport (\n\t\"fmt\"\n\n\t\"github.com\/pkg\/errors\"\n\t\"github.com\/sonm-io\/core\/proto\"\n)\n\ntype Network interface {\n\t\/\/ ID returns a unique identifier that will be used as a new network name.\n\tID() string\n\t\/\/ NetworkType returns a network driver name used to establish networking.\n\tNetworkType() string\n\t\/\/ NetworkOptions return configuration map, passed directly to network driver, this map should not be mutated.\n\tNetworkOptions() map[string]string\n\t\/\/ Returns network subnet in CIDR notation if applicable.\n\tNetworkCIDR() string\n\t\/\/ Returns specified addr to join the network.\n\tNetworkAddr() string\n}\n\ntype NetworkSpec struct {\n\t*sonm.NetworkSpec\n\tId string\n}\n\nfunc (n *NetworkSpec) ID() string {\n\treturn n.Id\n}\n\nfunc (n *NetworkSpec) NetworkType() string {\n\treturn n.GetType()\n}\n\nfunc (n *NetworkSpec) NetworkOptions() map[string]string {\n\treturn n.GetOptions()\n}\n\nfunc (n *NetworkSpec) NetworkCIDR() string {\n\treturn n.GetSubnet()\n}\n\nfunc (n *NetworkSpec) NetworkAddr() string {\n\treturn n.GetAddr()\n}\n\nfunc validateNetworkSpec(id string, spec *sonm.NetworkSpec) error {\n\tif spec.Type == \"tinc\" {\n\t\tif len(spec.Addr) == 0 || len(spec.Subnet) == 0 {\n\t\t\treturn errors.New(\"address and subnet are required for tinc driver\")\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc NewNetworkSpec(id string, spec *sonm.NetworkSpec) (*NetworkSpec, error) {\n\terr := validateNetworkSpec(id, spec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &NetworkSpec{spec, id}, nil\n}\n\nfunc NewNetworkSpecs(idPrefix string, specs []*sonm.NetworkSpec) ([]Network, error) {\n\tresult := make([]Network, 0, len(specs))\n\tfor i, s := range specs {\n\t\tspec, err := NewNetworkSpec(idPrefix+\"__\"+fmt.Sprint(i), s)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tresult = append(result, spec)\n\t}\n\treturn result, nil\n}\n","lang_cluster":"Go","length":75,"code_uid":"9191f07f3735472ea967318fdf054a0d"}
{"diff_hunk":"@@ -29,6 +29,8 @@ import org.springframework.web.reactive.result.method.HandlerMethodArgumentResol\n import org.springframework.web.server.ServerWebExchange;\n import reactor.core.publisher.Mono;\n \n+import java.util.Optional;\n+\n \/**\n  * An implementation of a {@link HandlerMethodArgumentResolver} that is capable\n  * of resolving a method parameter to an argument value of type {@link OAuth2AuthorizedClient}.","old_code":"\/*\n * Copyright 2002-2018 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage org.springframework.security.oauth2.client.web.reactive.result.method.annotation;\n\nimport org.springframework.core.MethodParameter;\nimport org.springframework.core.annotation.AnnotatedElementUtils;\nimport org.springframework.security.oauth2.client.OAuth2AuthorizedClient;\nimport org.springframework.security.oauth2.client.annotation.RegisteredOAuth2AuthorizedClient;\nimport org.springframework.security.oauth2.client.registration.ReactiveClientRegistrationRepository;\nimport org.springframework.security.oauth2.client.web.server.ServerOAuth2AuthorizedClientRepository;\nimport org.springframework.util.Assert;\nimport org.springframework.util.StringUtils;\nimport org.springframework.web.reactive.BindingContext;\nimport org.springframework.web.reactive.result.method.HandlerMethodArgumentResolver;\nimport org.springframework.web.server.ServerWebExchange;\nimport reactor.core.publisher.Mono;\n\n\/**\n * An implementation of a {@link HandlerMethodArgumentResolver} that is capable\n * of resolving a method parameter to an argument value of type {@link OAuth2AuthorizedClient}.\n *\n * <p>\n * For example:\n * <pre>\n * &#64;Controller\n * public class MyController {\n *     &#64;GetMapping(\"\/authorized-client\")\n *     public Mono&lt;String&gt; authorizedClient(@RegisteredOAuth2AuthorizedClient(\"login-client\") OAuth2AuthorizedClient authorizedClient) {\n *         \/\/ do something with authorizedClient\n *     }\n * }\n * <\/pre>\n *\n * @author Rob Winch\n * @since 5.1\n * @see RegisteredOAuth2AuthorizedClient\n *\/\npublic final class OAuth2AuthorizedClientArgumentResolver implements HandlerMethodArgumentResolver {\n\n\tprivate final OAuth2AuthorizedClientResolver authorizedClientResolver;\n\n\t\/**\n\t * Constructs an {@code OAuth2AuthorizedClientArgumentResolver} using the provided parameters.\n\t *\n\t * @param authorizedClientRepository the authorized client repository\n\t *\/\n\tpublic OAuth2AuthorizedClientArgumentResolver(ReactiveClientRegistrationRepository clientRegistrationRepository, ServerOAuth2AuthorizedClientRepository authorizedClientRepository) {\n\t\tAssert.notNull(authorizedClientRepository, \"authorizedClientRepository cannot be null\");\n\t\tthis.authorizedClientResolver = new OAuth2AuthorizedClientResolver(clientRegistrationRepository, authorizedClientRepository);\n\t\tthis.authorizedClientResolver.setDefaultOAuth2AuthorizedClient(true);\n\t}\n\n\t@Override\n\tpublic boolean supportsParameter(MethodParameter parameter) {\n\t\treturn AnnotatedElementUtils.findMergedAnnotation(parameter.getParameter(), RegisteredOAuth2AuthorizedClient.class) != null;\n\t}\n\n\t@Override\n\tpublic Mono<Object> resolveArgument(\n\t\t\tMethodParameter parameter, BindingContext bindingContext, ServerWebExchange exchange) {\n\t\treturn Mono.defer(() -> {\n\t\t\tRegisteredOAuth2AuthorizedClient authorizedClientAnnotation = AnnotatedElementUtils\n\t\t\t\t\t.findMergedAnnotation(parameter.getParameter(), RegisteredOAuth2AuthorizedClient.class);\n\n\t\t\tString clientRegistrationId = StringUtils.hasLength(authorizedClientAnnotation.registrationId()) ?\n\t\t\t\t\tauthorizedClientAnnotation.registrationId() : null;\n\n\t\t\treturn this.authorizedClientResolver.createDefaultedRequest(clientRegistrationId, null, exchange)\n\t\t\t\t\t.flatMap(this.authorizedClientResolver::loadAuthorizedClient);\n\t\t});\n\t}\n}\n","lang_cluster":"Java","length":86,"code_uid":"463d5060ae0141c5b7f0b7860c4741d6"}
{"diff_hunk":"@@ -75,7 +75,7 @@ abstract class BaseDeltaTaskWriter extends BaseTaskWriter<RowData> {\n \n       case DELETE:\n       case UPDATE_BEFORE:\n-        writer.delete(row);\n+        writer.deleteKey(projectDeleteData(row));\n         break;\n \n       default:","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.flink.sink;\n\nimport java.io.IOException;\nimport java.util.List;\nimport org.apache.flink.table.data.RowData;\nimport org.apache.flink.table.types.logical.RowType;\nimport org.apache.iceberg.FileFormat;\nimport org.apache.iceberg.PartitionKey;\nimport org.apache.iceberg.PartitionSpec;\nimport org.apache.iceberg.Schema;\nimport org.apache.iceberg.StructLike;\nimport org.apache.iceberg.flink.RowDataWrapper;\nimport org.apache.iceberg.io.BaseTaskWriter;\nimport org.apache.iceberg.io.FileAppenderFactory;\nimport org.apache.iceberg.io.FileIO;\nimport org.apache.iceberg.io.OutputFileFactory;\nimport org.apache.iceberg.relocated.com.google.common.collect.Sets;\nimport org.apache.iceberg.types.TypeUtil;\n\nabstract class BaseDeltaTaskWriter extends BaseTaskWriter<RowData> {\n\n  private final Schema schema;\n  private final Schema deleteSchema;\n  private final RowDataWrapper wrapper;\n\n  BaseDeltaTaskWriter(PartitionSpec spec,\n                      FileFormat format,\n                      FileAppenderFactory<RowData> appenderFactory,\n                      OutputFileFactory fileFactory,\n                      FileIO io,\n                      long targetFileSize,\n                      Schema schema,\n                      RowType flinkSchema,\n                      List<Integer> equalityFieldIds) {\n    super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n    this.schema = schema;\n    this.deleteSchema = TypeUtil.select(schema, Sets.newHashSet(equalityFieldIds));\n    this.wrapper = new RowDataWrapper(flinkSchema, schema.asStruct());\n  }\n\n  abstract RowDataDeltaWriter route(RowData row);\n\n  RowDataWrapper wrapper() {\n    return wrapper;\n  }\n\n  @Override\n  public void write(RowData row) throws IOException {\n    RowDataDeltaWriter writer = route(row);\n\n    switch (row.getRowKind()) {\n      case INSERT:\n      case UPDATE_AFTER:\n        writer.write(row);\n        break;\n\n      case DELETE:\n      case UPDATE_BEFORE:\n        writer.delete(row);\n        break;\n\n      default:\n        throw new UnsupportedOperationException(\"Unknown row kind: \" + row.getRowKind());\n    }\n  }\n\n  protected class RowDataDeltaWriter extends BaseEqualityDeltaWriter {\n    RowDataDeltaWriter(PartitionKey partition) {\n      super(partition, schema, deleteSchema);\n    }\n\n    @Override\n    protected StructLike asStructLike(RowData data) {\n      return wrapper.wrap(data);\n    }\n  }\n}\n","lang_cluster":"Java","length":96,"code_uid":"8f522b26e4eb4de29f64ec24fa775d71"}
{"diff_hunk":"@@ -38,15 +38,9 @@ public class TransactionPoolFactory {\n       final Wei minTransactionGasPrice,\n       final TransactionPoolConfiguration transactionPoolConfiguration) {\n \n-    final PendingTransactions pendingTransactions =\n-        new PendingTransactions(\n-            transactionPoolConfiguration.getPendingTxRetentionPeriod(),\n-            transactionPoolConfiguration.getTxPoolMaxSize(),\n-            transactionPoolConfiguration.getPooledTransactionHashesSize(),\n-            clock,\n-            metricsSystem,\n-            protocolContext.getBlockchain()::getChainHeadHeader,\n-            transactionPoolConfiguration.getPriceBump());\n+    final AbstractPendingTransactionsSorter pendingTransactions =\n+        createPendingTransactionsSorter(\n+            protocolSchedule, protocolContext, clock, metricsSystem, transactionPoolConfiguration);\n \n     final PeerTransactionTracker transactionTracker = new PeerTransactionTracker();\n     final TransactionsMessageSender transactionsMessageSender =","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.ethereum.eth.transactions;\n\nimport org.hyperledger.besu.ethereum.ProtocolContext;\nimport org.hyperledger.besu.ethereum.core.Wei;\nimport org.hyperledger.besu.ethereum.eth.manager.EthContext;\nimport org.hyperledger.besu.ethereum.eth.messages.EthPV62;\nimport org.hyperledger.besu.ethereum.eth.messages.EthPV65;\nimport org.hyperledger.besu.ethereum.eth.sync.state.SyncState;\nimport org.hyperledger.besu.ethereum.mainnet.ProtocolSchedule;\nimport org.hyperledger.besu.metrics.BesuMetricCategory;\nimport org.hyperledger.besu.plugin.services.MetricsSystem;\n\nimport java.time.Clock;\n\npublic class TransactionPoolFactory {\n\n  public static TransactionPool createTransactionPool(\n      final ProtocolSchedule protocolSchedule,\n      final ProtocolContext protocolContext,\n      final EthContext ethContext,\n      final Clock clock,\n      final MetricsSystem metricsSystem,\n      final SyncState syncState,\n      final Wei minTransactionGasPrice,\n      final TransactionPoolConfiguration transactionPoolConfiguration) {\n\n    final PendingTransactions pendingTransactions =\n        new PendingTransactions(\n            transactionPoolConfiguration.getPendingTxRetentionPeriod(),\n            transactionPoolConfiguration.getTxPoolMaxSize(),\n            transactionPoolConfiguration.getPooledTransactionHashesSize(),\n            clock,\n            metricsSystem,\n            protocolContext.getBlockchain()::getChainHeadHeader,\n            transactionPoolConfiguration.getPriceBump());\n\n    final PeerTransactionTracker transactionTracker = new PeerTransactionTracker();\n    final TransactionsMessageSender transactionsMessageSender =\n        new TransactionsMessageSender(transactionTracker);\n\n    final PeerPendingTransactionTracker pendingTransactionTracker =\n        new PeerPendingTransactionTracker(pendingTransactions);\n    final PendingTransactionsMessageSender pendingTransactionsMessageSender =\n        new PendingTransactionsMessageSender(pendingTransactionTracker);\n\n    return createTransactionPool(\n        protocolSchedule,\n        protocolContext,\n        ethContext,\n        metricsSystem,\n        syncState,\n        minTransactionGasPrice,\n        transactionPoolConfiguration,\n        pendingTransactions,\n        transactionTracker,\n        transactionsMessageSender,\n        pendingTransactionTracker,\n        pendingTransactionsMessageSender);\n  }\n\n  static TransactionPool createTransactionPool(\n      final ProtocolSchedule protocolSchedule,\n      final ProtocolContext protocolContext,\n      final EthContext ethContext,\n      final MetricsSystem metricsSystem,\n      final SyncState syncState,\n      final Wei minTransactionGasPrice,\n      final TransactionPoolConfiguration transactionPoolConfiguration,\n      final PendingTransactions pendingTransactions,\n      final PeerTransactionTracker transactionTracker,\n      final TransactionsMessageSender transactionsMessageSender,\n      final PeerPendingTransactionTracker pendingTransactionTracker,\n      final PendingTransactionsMessageSender pendingTransactionsMessageSender) {\n    final TransactionPool transactionPool =\n        new TransactionPool(\n            pendingTransactions,\n            protocolSchedule,\n            protocolContext,\n            new TransactionSender(transactionTracker, transactionsMessageSender, ethContext),\n            new PendingTransactionSender(\n                pendingTransactionTracker, pendingTransactionsMessageSender, ethContext),\n            syncState,\n            ethContext,\n            transactionTracker,\n            pendingTransactionTracker,\n            minTransactionGasPrice,\n            metricsSystem,\n            transactionPoolConfiguration);\n    final TransactionsMessageHandler transactionsMessageHandler =\n        new TransactionsMessageHandler(\n            ethContext.getScheduler(),\n            new TransactionsMessageProcessor(\n                transactionTracker,\n                transactionPool,\n                metricsSystem.createCounter(\n                    BesuMetricCategory.TRANSACTION_POOL,\n                    \"transactions_messages_skipped_total\",\n                    \"Total number of transactions messages skipped by the processor.\")),\n            transactionPoolConfiguration.getTxMessageKeepAliveSeconds());\n    ethContext.getEthMessages().subscribe(EthPV62.TRANSACTIONS, transactionsMessageHandler);\n    final PendingTransactionsMessageHandler pooledTransactionsMessageHandler =\n        new PendingTransactionsMessageHandler(\n            ethContext.getScheduler(),\n            new PendingTransactionsMessageProcessor(\n                pendingTransactionTracker,\n                transactionPool,\n                transactionPoolConfiguration,\n                metricsSystem.createCounter(\n                    BesuMetricCategory.TRANSACTION_POOL,\n                    \"pending_transactions_messages_skipped_total\",\n                    \"Total number of pending transactions messages skipped by the processor.\"),\n                ethContext,\n                metricsSystem,\n                syncState),\n            transactionPoolConfiguration.getTxMessageKeepAliveSeconds());\n    ethContext\n        .getEthMessages()\n        .subscribe(EthPV65.NEW_POOLED_TRANSACTION_HASHES, pooledTransactionsMessageHandler);\n    ethContext.getEthPeers().subscribeDisconnect(pendingTransactionTracker);\n\n    protocolContext.getBlockchain().observeBlockAdded(transactionPool);\n    ethContext.getEthPeers().subscribeDisconnect(transactionTracker);\n    return transactionPool;\n  }\n}\n","lang_cluster":"Java","length":139,"code_uid":"028ab71444fb4e47ae9f4e2c9734c60f"}
{"diff_hunk":"@@ -38,7 +38,6 @@ public class CommonMetrics {\n   public static final String SUBMIT_FLOW_FAIL_METER_NAME = \"submit-flow-fail-meter\";\n   public static final String SUBMIT_FLOW_SKIP_METER_NAME = \"submit-flow-skip-meter\";\n   public static final String OOM_WAITING_JOB_COUNT_NAME = \"OOM-waiting-job-count\";\n-  public static final String QUEUE_WAIT_HISTOGRAM_NAME = \"queue-wait-histogram\";\n   public static final String UPLOAD_FAT_PROJECT_METER_NAME = \"upload-fat-project-meter\";\n   public static final String UPLOAD_THIN_PROJECT_METER_NAME = \"upload-thin-project-meter\";\n ","old_code":"\/*\n * Copyright 2017 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n *\/\n\npackage azkaban.metrics;\n\nimport com.codahale.metrics.Counter;\nimport com.codahale.metrics.Histogram;\nimport com.codahale.metrics.Meter;\nimport javax.inject.Inject;\nimport javax.inject.Singleton;\n\n\/**\n * This singleton class CommonMetrics is in charge of collecting varieties of metrics which are\n * accessed in both web and exec modules. That said, these metrics will be exposed in both Web\n * server and executor.\n *\/\n@Singleton\npublic class CommonMetrics {\n  public static final String FLOW_FAIL_METER_NAME = \"flow-fail-meter\";\n  public static final String DISPATCH_FAIL_METER_NAME = \"dispatch-fail-meter\";\n  public static final String DISPATCH_SUCCESS_METER_NAME = \"dispatch-success-meter\";\n  public static final String SEND_EMAIL_FAIL_METER_NAME = \"send-email-fail-meter\";\n  public static final String SEND_EMAIL_SUCCESS_METER_NAME = \"send-email-success-meter\";\n  public static final String SUBMIT_FLOW_SUCCESS_METER_NAME = \"submit-flow-success-meter\";\n  public static final String SUBMIT_FLOW_FAIL_METER_NAME = \"submit-flow-fail-meter\";\n  public static final String SUBMIT_FLOW_SKIP_METER_NAME = \"submit-flow-skip-meter\";\n  public static final String OOM_WAITING_JOB_COUNT_NAME = \"OOM-waiting-job-count\";\n  public static final String QUEUE_WAIT_HISTOGRAM_NAME = \"queue-wait-histogram\";\n  public static final String UPLOAD_FAT_PROJECT_METER_NAME = \"upload-fat-project-meter\";\n  public static final String UPLOAD_THIN_PROJECT_METER_NAME = \"upload-thin-project-meter\";\n\n  private Counter OOMWaitingJobCount;\n  private final MetricsManager metricsManager;\n  private Meter flowFailMeter;\n  private Meter dispatchFailMeter;\n  private Meter dispatchSuccessMeter;\n  private Meter sendEmailFailMeter;\n  private Meter sendEmailSuccessMeter;\n  private Meter submitFlowSuccessMeter;\n  private Meter submitFlowFailMeter;\n  private Meter submitFlowSkipMeter;\n  private Meter uploadFatProjectMeter;\n  private Meter uploadThinProjectMeter;\n  private Histogram queueWaitMeter;\n\n  @Inject\n  public CommonMetrics(final MetricsManager metricsManager) {\n    this.metricsManager = metricsManager;\n    setupAllMetrics();\n  }\n\n  private void setupAllMetrics() {\n    this.flowFailMeter = this.metricsManager.addMeter(FLOW_FAIL_METER_NAME);\n    this.dispatchFailMeter = this.metricsManager.addMeter(DISPATCH_FAIL_METER_NAME);\n    this.dispatchSuccessMeter = this.metricsManager.addMeter(DISPATCH_SUCCESS_METER_NAME);\n    this.sendEmailFailMeter = this.metricsManager.addMeter(SEND_EMAIL_FAIL_METER_NAME);\n    this.sendEmailSuccessMeter = this.metricsManager.addMeter(SEND_EMAIL_SUCCESS_METER_NAME);\n    this.submitFlowSuccessMeter = this.metricsManager.addMeter(SUBMIT_FLOW_SUCCESS_METER_NAME);\n    this.submitFlowFailMeter = this.metricsManager.addMeter(SUBMIT_FLOW_FAIL_METER_NAME);\n    this.submitFlowSkipMeter = this.metricsManager.addMeter(SUBMIT_FLOW_SKIP_METER_NAME);\n    this.OOMWaitingJobCount = this.metricsManager.addCounter(OOM_WAITING_JOB_COUNT_NAME);\n    this.queueWaitMeter = this.metricsManager.addHistogram(QUEUE_WAIT_HISTOGRAM_NAME);\n    this.uploadFatProjectMeter = this.metricsManager.addMeter(UPLOAD_FAT_PROJECT_METER_NAME);\n    this.uploadThinProjectMeter = this.metricsManager.addMeter(UPLOAD_THIN_PROJECT_METER_NAME);\n  }\n\n  \/**\n   * Mark flowFailMeter when a flow is considered as FAILED. This method could be called by Web\n   * Server or Executor, as they both detect flow failure.\n   *\/\n  public void markFlowFail() {\n    this.flowFailMeter.mark();\n  }\n\n  \/**\n   * Mark dispatchFailMeter when web server fails to dispatch a flow to executor.\n   *\/\n  public void markDispatchFail() {\n    this.dispatchFailMeter.mark();\n  }\n\n  \/**\n   * Mark dispatchSuccessMeter when web server successfully dispatches a flow to executor.\n   *\/\n  public void markDispatchSuccess() {\n    this.dispatchSuccessMeter.mark();\n  }\n\n  \/**\n   * Mark sendEmailFailMeter when an email fails to be sent out.\n   *\/\n  public void markSendEmailFail() {\n    this.sendEmailFailMeter.mark();\n  }\n\n  \/**\n   * Mark sendEmailSuccessMeter when an email is sent out successfully.\n   *\/\n  public void markSendEmailSuccess() {\n    this.sendEmailSuccessMeter.mark();\n  }\n\n  \/**\n   * Mark submitFlowSuccessMeter when a flow is submitted for execution successfully.\n   *\/\n  public void markSubmitFlowSuccess() {\n    this.submitFlowSuccessMeter.mark();\n  }\n\n  \/**\n   * Mark submitFlowFailMeter when a flow submitted for execution is skipped.\n   *\/\n  public void markSubmitFlowSkip() {\n    this.submitFlowSkipMeter.mark();\n  }\n\n  \/**\n   * Mark submitFlowFailMeter when a flow fails to be submitted for execution.\n   *\/\n  public void markSubmitFlowFail() {\n    this.submitFlowFailMeter.mark();\n  }\n\n  \/**\n   * Mark uploadFatProjectMeter when a fat project zip is uploaded to the web server.\n   *\/\n  public void markUploadFatProject() { this.uploadFatProjectMeter.mark(); }\n\n  \/**\n   * Mark uploadThinProjectMeter when a thin project zip is uploaded to the web server.\n   *\/\n  public void markUploadThinProject() { this.uploadThinProjectMeter.mark(); }\n\n  \/**\n   * Mark the occurrence of an job waiting event due to OOM\n   *\/\n  public void incrementOOMJobWaitCount() {\n    this.OOMWaitingJobCount.inc();\n  }\n\n  \/**\n   * Unmark the occurrence of an job waiting event due to OOM\n   *\/\n  public void decrementOOMJobWaitCount() {\n    this.OOMWaitingJobCount.dec();\n  }\n\n  \/**\n   * Add the queue wait time for a flow to the metrics.\n   *\n   * @param time queue wait time for a flow.\n   *\/\n  public void addQueueWait(final long time) {\n    this.queueWaitMeter.update(time);\n  }\n}\n","lang_cluster":"Java","length":169,"code_uid":"5afa04fe93924e2ab39208847621fdc0"}
{"diff_hunk":"@@ -26,7 +26,9 @@ import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapreduce.InputSplit;\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.mr.InputFormatConfig;\n import org.apache.iceberg.mr.SerializationUtil;\n ","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.mr.mapreduce;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.mapreduce.InputSplit;\nimport org.apache.iceberg.CombinedScanTask;\nimport org.apache.iceberg.FileScanTask;\nimport org.apache.iceberg.hadoop.Util;\nimport org.apache.iceberg.mr.InputFormatConfig;\nimport org.apache.iceberg.mr.SerializationUtil;\n\n\/\/ Since this class extends `mapreduce.InputSplit and implements `mapred.InputSplit`, it can be returned by both MR v1\n\/\/ and v2 file formats.\npublic class IcebergSplit extends InputSplit implements org.apache.hadoop.mapred.InputSplit, IcebergSplitContainer {\n\n  public static final String[] ANYWHERE = new String[]{\"*\"};\n\n  private CombinedScanTask task;\n\n  private transient String[] locations;\n  private transient Configuration conf;\n\n  \/\/ public no-argument constructor for deserialization\n  public IcebergSplit() {\n  }\n\n  IcebergSplit(Configuration conf, CombinedScanTask task) {\n    this.task = task;\n    this.conf = conf;\n  }\n\n  public CombinedScanTask task() {\n    return task;\n  }\n\n  @Override\n  public IcebergSplit icebergSplit() {\n    return this;\n  }\n\n  @Override\n  public long getLength() {\n    return task.files().stream().mapToLong(FileScanTask::length).sum();\n  }\n\n  @Override\n  public String[] getLocations() {\n    if (locations == null) {\n      boolean localityPreferred = conf.getBoolean(InputFormatConfig.LOCALITY, false);\n      locations = localityPreferred ? Util.blockLocations(task, conf) : ANYWHERE;\n    }\n\n    return locations;\n  }\n\n  @Override\n  public void write(DataOutput out) throws IOException {\n    byte[] data = SerializationUtil.serializeToBytes(this.task);\n    out.writeInt(data.length);\n    out.write(data);\n  }\n\n  @Override\n  public void readFields(DataInput in) throws IOException {\n    byte[] data = new byte[in.readInt()];\n    in.readFully(data);\n    this.task = SerializationUtil.deserializeFromBytes(data);\n  }\n}\n","lang_cluster":"Java","length":90,"code_uid":"c15bc73b005945099ae92b8aac249ea4"}
{"diff_hunk":"@@ -17,6 +17,7 @@\n \n package org.apache.servicecomb.swagger.generator.core.processor.parameter;\n \n+import org.apache.commons.lang3.StringUtils;\n import org.apache.servicecomb.swagger.generator.core.OperationGenerator;\n import org.apache.servicecomb.swagger.generator.core.ParameterAnnotationProcessor;\n import org.apache.servicecomb.swagger.generator.core.utils.ParamUtils;","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage org.apache.servicecomb.swagger.generator.core.processor.parameter;\n\nimport org.apache.servicecomb.swagger.generator.core.OperationGenerator;\nimport org.apache.servicecomb.swagger.generator.core.ParameterAnnotationProcessor;\nimport org.apache.servicecomb.swagger.generator.core.utils.ParamUtils;\n\nimport io.swagger.models.parameters.AbstractSerializableParameter;\n\npublic abstract class AbstractParameterProcessor<T extends AbstractSerializableParameter<?>>\n    implements ParameterAnnotationProcessor {\n  @Override\n  public void process(Object annotation, OperationGenerator operationGenerator, int paramIdx) {\n    T parameter = createParameter();\n\n    fillParameter(annotation, operationGenerator, paramIdx, parameter);\n\n    operationGenerator.addProviderParameter(parameter);\n  }\n\n  protected void fillParameter(Object annotation, OperationGenerator operationGenerator, int paramIdx,\n      T parameter) {\n    setParameterName(annotation, operationGenerator, paramIdx, parameter);\n    setParameterType(operationGenerator, paramIdx, parameter);\n  }\n\n  protected void setParameterType(OperationGenerator operationGenerator, int paramIdx,\n      T parameter) {\n    ParamUtils.setParameterType(operationGenerator.getSwagger(),\n        operationGenerator.getProviderMethod(),\n        paramIdx,\n        parameter);\n  }\n\n  protected void setParameterName(Object annotation, OperationGenerator operationGenerator, int paramIdx,\n      T parameter) {\n    String paramName = getAnnotationParameterName(annotation);\n    paramName = ParamUtils.getParameterName(paramName, operationGenerator.getProviderMethod(), paramIdx);\n    parameter.setName(paramName);\n  }\n\n  protected abstract T createParameter();\n\n  protected abstract String getAnnotationParameterName(Object annotation);\n}\n","lang_cluster":"Java","length":61,"code_uid":"cf9a33d539cf40b1b70fc8a2750c5158"}
{"diff_hunk":"@@ -181,6 +181,9 @@ public abstract class AbstractBlockProcessor implements BlockProcessor {\n \n     worldState.persist(blockHeader.getHash());\n     return AbstractBlockProcessor.Result.successful(receipts);\n+        } finally {\n+        globalProcessBlock.end();\n+        }\n   }\n \n   protected MiningBeneficiaryCalculator getMiningBeneficiaryCalculator() {","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.ethereum.mainnet;\n\nimport org.hyperledger.besu.ethereum.chain.Blockchain;\nimport org.hyperledger.besu.ethereum.core.Address;\nimport org.hyperledger.besu.ethereum.core.BlockHeader;\nimport org.hyperledger.besu.ethereum.core.MutableWorldState;\nimport org.hyperledger.besu.ethereum.core.Transaction;\nimport org.hyperledger.besu.ethereum.core.TransactionReceipt;\nimport org.hyperledger.besu.ethereum.core.Wei;\nimport org.hyperledger.besu.ethereum.core.WorldState;\nimport org.hyperledger.besu.ethereum.core.WorldUpdater;\nimport org.hyperledger.besu.ethereum.core.fees.TransactionGasBudgetCalculator;\nimport org.hyperledger.besu.ethereum.privacy.storage.PrivateMetadataUpdater;\nimport org.hyperledger.besu.ethereum.processing.TransactionProcessingResult;\nimport org.hyperledger.besu.ethereum.vm.BlockHashLookup;\nimport org.hyperledger.besu.ethereum.vm.OperationTracer;\nimport org.hyperledger.besu.plugin.data.TransactionType;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport com.google.common.collect.ImmutableList;\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\n\npublic abstract class AbstractBlockProcessor implements BlockProcessor {\n  @FunctionalInterface\n  public interface TransactionReceiptFactory {\n\n    TransactionReceipt create(\n        TransactionType transactionType,\n        TransactionProcessingResult result,\n        WorldState worldState,\n        long gasUsed);\n  }\n\n  private static final Logger LOG = LogManager.getLogger();\n\n  static final int MAX_GENERATION = 6;\n\n  public static class Result implements BlockProcessor.Result {\n\n    private static final AbstractBlockProcessor.Result FAILED =\n        new AbstractBlockProcessor.Result(false, null);\n\n    private final boolean successful;\n\n    private final List<TransactionReceipt> receipts;\n\n    public static AbstractBlockProcessor.Result successful(\n        final List<TransactionReceipt> receipts) {\n      return new AbstractBlockProcessor.Result(true, ImmutableList.copyOf(receipts));\n    }\n\n    public static AbstractBlockProcessor.Result failed() {\n      return FAILED;\n    }\n\n    Result(final boolean successful, final List<TransactionReceipt> receipts) {\n      this.successful = successful;\n      this.receipts = receipts;\n    }\n\n    @Override\n    public List<TransactionReceipt> getReceipts() {\n      return receipts;\n    }\n\n    @Override\n    public boolean isSuccessful() {\n      return successful;\n    }\n  }\n\n  private final MainnetTransactionProcessor transactionProcessor;\n\n  private final AbstractBlockProcessor.TransactionReceiptFactory transactionReceiptFactory;\n\n  final Wei blockReward;\n\n  private final boolean skipZeroBlockRewards;\n\n  private final MiningBeneficiaryCalculator miningBeneficiaryCalculator;\n\n  private final TransactionGasBudgetCalculator gasBudgetCalculator;\n\n  protected AbstractBlockProcessor(\n      final MainnetTransactionProcessor transactionProcessor,\n      final TransactionReceiptFactory transactionReceiptFactory,\n      final Wei blockReward,\n      final MiningBeneficiaryCalculator miningBeneficiaryCalculator,\n      final boolean skipZeroBlockRewards,\n      final TransactionGasBudgetCalculator gasBudgetCalculator) {\n    this.transactionProcessor = transactionProcessor;\n    this.transactionReceiptFactory = transactionReceiptFactory;\n    this.blockReward = blockReward;\n    this.miningBeneficiaryCalculator = miningBeneficiaryCalculator;\n    this.skipZeroBlockRewards = skipZeroBlockRewards;\n    this.gasBudgetCalculator = gasBudgetCalculator;\n  }\n\n  @Override\n  public AbstractBlockProcessor.Result processBlock(\n      final Blockchain blockchain,\n      final MutableWorldState worldState,\n      final BlockHeader blockHeader,\n      final List<Transaction> transactions,\n      final List<BlockHeader> ommers,\n      final PrivateMetadataUpdater privateMetadataUpdater) {\n\n    final List<TransactionReceipt> receipts = new ArrayList<>();\n    long currentGasUsed = 0;\n    for (final Transaction transaction : transactions) {\n      final long remainingGasBudget = blockHeader.getGasLimit() - currentGasUsed;\n      if (!gasBudgetCalculator.hasBudget(\n          transaction, blockHeader.getNumber(), blockHeader.getGasLimit(), currentGasUsed)) {\n        LOG.info(\n            \"Block processing error: transaction gas limit {} exceeds available block budget\"\n                + \" remaining {}. Block {} Transaction {}\",\n            transaction.getGasLimit(),\n            remainingGasBudget,\n            blockHeader.getHash().toHexString(),\n            transaction.getHash().toHexString());\n        return AbstractBlockProcessor.Result.failed();\n      }\n\n      final WorldUpdater worldStateUpdater = worldState.updater();\n      final BlockHashLookup blockHashLookup = new BlockHashLookup(blockHeader, blockchain);\n      final Address miningBeneficiary =\n          miningBeneficiaryCalculator.calculateBeneficiary(blockHeader);\n\n      final TransactionProcessingResult result =\n          transactionProcessor.processTransaction(\n              blockchain,\n              worldStateUpdater,\n              blockHeader,\n              transaction,\n              miningBeneficiary,\n              OperationTracer.NO_TRACING,\n              blockHashLookup,\n              true,\n              TransactionValidationParams.processingBlock(),\n              privateMetadataUpdater);\n      if (result.isInvalid()) {\n        LOG.info(\n            \"Block processing error: transaction invalid '{}'. Block {} Transaction {}\",\n            result.getValidationResult().getInvalidReason(),\n            blockHeader.getHash().toHexString(),\n            transaction.getHash().toHexString());\n        return AbstractBlockProcessor.Result.failed();\n      }\n\n      worldStateUpdater.commit();\n\n      currentGasUsed += transaction.getGasLimit() - result.getGasRemaining();\n\n      final TransactionReceipt transactionReceipt =\n          transactionReceiptFactory.create(\n              transaction.getType(), result, worldState, currentGasUsed);\n      receipts.add(transactionReceipt);\n    }\n\n    if (!rewardCoinbase(worldState, blockHeader, ommers, skipZeroBlockRewards)) {\n      \/\/ no need to log, rewardCoinbase logs the error.\n      return AbstractBlockProcessor.Result.failed();\n    }\n\n    worldState.persist(blockHeader.getHash());\n    return AbstractBlockProcessor.Result.successful(receipts);\n  }\n\n  protected MiningBeneficiaryCalculator getMiningBeneficiaryCalculator() {\n    return miningBeneficiaryCalculator;\n  }\n\n  abstract boolean rewardCoinbase(\n      final MutableWorldState worldState,\n      final BlockHeader header,\n      final List<BlockHeader> ommers,\n      final boolean skipZeroBlockRewards);\n}\n","lang_cluster":"Java","length":195,"code_uid":"d3ef9a2d8cbe40079cd57412372e4b5e"}
{"diff_hunk":"@@ -19,6 +19,7 @@ import com.google.api.codegen.config.PageStreamingConfig;\n import com.google.api.codegen.util.Name;\n import com.google.api.codegen.viewmodel.PageStreamingDescriptorClassView;\n import com.google.api.codegen.viewmodel.PageStreamingDescriptorView;\n+import com.google.api.codegen.viewmodel.PageStreamingFactoryClassView;\n import com.google.api.tools.framework.model.Field;\n import com.google.api.tools.framework.model.Method;\n import com.google.api.tools.framework.model.TypeRef;","old_code":"\/* Copyright 2016 Google Inc\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage com.google.api.codegen.transformer;\n\nimport com.google.api.codegen.config.MethodConfig;\nimport com.google.api.codegen.config.PageStreamingConfig;\nimport com.google.api.codegen.util.Name;\nimport com.google.api.codegen.viewmodel.PageStreamingDescriptorClassView;\nimport com.google.api.codegen.viewmodel.PageStreamingDescriptorView;\nimport com.google.api.tools.framework.model.Field;\nimport com.google.api.tools.framework.model.Method;\nimport com.google.api.tools.framework.model.TypeRef;\nimport java.util.ArrayList;\nimport java.util.List;\n\n\/**\n * PageStreamingTransformer generates view objects for page streaming from a service model.\n *\/\npublic class PageStreamingTransformer {\n\n  public List<PageStreamingDescriptorView> generateDescriptors(SurfaceTransformerContext context) {\n    List<PageStreamingDescriptorView> descriptors = new ArrayList<>();\n\n    for (Method method : context.getPageStreamingMethods()) {\n      MethodConfig methodConfig = context.getMethodConfig(method);\n      context.getNamer().addPageStreamingDescriptorImports(context.getTypeTable());\n      PageStreamingConfig pageStreaming = methodConfig.getPageStreaming();\n\n      PageStreamingDescriptorView.Builder descriptor = PageStreamingDescriptorView.newBuilder();\n      descriptor.varName(context.getNamer().getPageStreamingDescriptorName(method));\n      descriptor.requestTokenFieldName(pageStreaming.getRequestTokenField().getSimpleName());\n      if (pageStreaming.hasPageSizeField()) {\n        descriptor.requestPageSizeFieldName(pageStreaming.getPageSizeField().getSimpleName());\n      }\n      descriptor.responseTokenFieldName(pageStreaming.getResponseTokenField().getSimpleName());\n      descriptor.resourcesFieldName(pageStreaming.getResourcesField().getSimpleName());\n      descriptor.methodName(Name.upperCamel(method.getSimpleName()).toLowerCamel());\n\n      descriptors.add(descriptor.build());\n    }\n\n    return descriptors;\n  }\n\n  public List<PageStreamingDescriptorClassView> generateDescriptorClasses(\n      SurfaceTransformerContext context) {\n    List<PageStreamingDescriptorClassView> descriptors = new ArrayList<>();\n\n    context.getNamer().addPageStreamingDescriptorImports(context.getTypeTable());\n    for (Method method : context.getPageStreamingMethods()) {\n      descriptors.add(generateDescriptorClass(context.asMethodContext(method)));\n    }\n\n    return descriptors;\n  }\n\n  private PageStreamingDescriptorClassView generateDescriptorClass(\n      MethodTransformerContext context) {\n    SurfaceNamer namer = context.getNamer();\n    ModelTypeTable typeTable = context.getTypeTable();\n    Method method = context.getMethod();\n    PageStreamingConfig pageStreaming = context.getMethodConfig().getPageStreaming();\n    FeatureConfig featureConfig = context.getFeatureConfig();\n\n    PageStreamingDescriptorClassView.Builder desc = PageStreamingDescriptorClassView.newBuilder();\n\n    Field resourceField = pageStreaming.getResourcesField();\n    TypeRef resourceType = resourceField.getType();\n\n    desc.name(namer.getPageStreamingDescriptorConstName(method));\n    desc.typeName(\n        namer.getAndSavePagedResponseTypeName(\n            featureConfig,\n            typeTable,\n            method.getInputType(),\n            method.getOutputType(),\n            resourceField));\n    desc.requestTypeName(typeTable.getAndSaveNicknameFor(method.getInputType()));\n    desc.responseTypeName(typeTable.getAndSaveNicknameFor(method.getOutputType()));\n    desc.resourceTypeName(\n        namer.getAndSaveElementFieldTypeName(featureConfig, typeTable, resourceField));\n\n    TypeRef tokenType = pageStreaming.getResponseTokenField().getType();\n    desc.tokenTypeName(typeTable.getAndSaveNicknameFor(tokenType));\n    desc.defaultTokenValue(context.getTypeTable().getZeroValueAndSaveNicknameFor(tokenType));\n\n    \/\/ The resource fields are \"repeated\" in the proto.\n    \/\/ We `makeOptional` so that we get the zero value of the resource,\n    \/\/ not the zero value of the array\/list of resources.\n    desc.resourceZeroValue(\n        context.getTypeTable().getZeroValueAndSaveNicknameFor(resourceType.makeOptional()));\n\n    desc.requestTokenSetFunction(\n        namer.getFieldSetFunctionName(featureConfig, pageStreaming.getRequestTokenField()));\n    if (pageStreaming.hasPageSizeField()) {\n      desc.requestPageSizeSetFunction(\n          namer.getFieldSetFunctionName(featureConfig, pageStreaming.getPageSizeField()));\n      desc.requestPageSizeGetFunction(\n          namer.getFieldGetFunctionName(featureConfig, pageStreaming.getPageSizeField()));\n    }\n    desc.responseTokenGetFunction(\n        namer.getFieldGetFunctionName(featureConfig, pageStreaming.getResponseTokenField()));\n    desc.resourcesFieldGetFunction(\n        namer.getFieldGetFunctionName(featureConfig, pageStreaming.getResourcesField()));\n\n    return desc.build();\n  }\n}\n","lang_cluster":"Java","length":120,"code_uid":"155a7507d0ae4f2da3aa31734200df06"}
{"diff_hunk":"@@ -1,5 +1,6 @@\n package de.danoeh.antennapod.dialog;\n \n+import android.app.Activity;\n import android.app.Dialog;\n import android.content.Context;\n import android.content.DialogInterface;","old_code":"package de.danoeh.antennapod.dialog;\n\nimport android.app.Dialog;\nimport android.content.Context;\nimport android.content.DialogInterface;\nimport android.os.Bundle;\nimport android.view.View;\nimport android.view.inputmethod.InputMethodManager;\nimport android.widget.ArrayAdapter;\nimport android.widget.Button;\nimport android.widget.CheckBox;\nimport android.widget.EditText;\nimport android.widget.LinearLayout;\nimport android.widget.Spinner;\nimport android.widget.TextView;\nimport androidx.annotation.NonNull;\nimport androidx.appcompat.app.AlertDialog;\nimport androidx.fragment.app.DialogFragment;\nimport com.google.android.material.snackbar.Snackbar;\nimport de.danoeh.antennapod.R;\nimport de.danoeh.antennapod.core.preferences.SleepTimerPreferences;\nimport de.danoeh.antennapod.core.service.playback.PlaybackService;\nimport de.danoeh.antennapod.core.util.Converter;\nimport de.danoeh.antennapod.core.util.playback.PlaybackController;\nimport io.reactivex.Observable;\nimport io.reactivex.android.schedulers.AndroidSchedulers;\nimport io.reactivex.disposables.Disposable;\n\nimport java.util.concurrent.TimeUnit;\n\npublic class SleepTimerDialog extends DialogFragment {\n    private PlaybackController controller;\n    private Disposable timeUpdater;\n\n    private EditText etxtTime;\n    private Spinner spTimeUnit;\n    private LinearLayout timeSetup;\n    private LinearLayout timeDisplay;\n    private TextView time;\n\n    public SleepTimerDialog() {\n\n    }\n\n    @Override\n    public void onStart() {\n        super.onStart();\n        controller = new PlaybackController(getActivity()) {\n            @Override\n            public void setupGUI() {\n                updateTime();\n            }\n\n            @Override\n            public void onSleepTimerUpdate() {\n                updateTime();\n            }\n        };\n        controller.init();\n        timeUpdater = Observable.interval(1, TimeUnit.SECONDS)\n                .observeOn(AndroidSchedulers.mainThread())\n                .subscribe(tick -> updateTime());\n    }\n\n    @Override\n    public void onStop() {\n        super.onStop();\n        if (controller != null) {\n            controller.release();\n        }\n        if (timeUpdater != null) {\n            timeUpdater.dispose();\n        }\n    }\n\n    @NonNull\n    @Override\n    public Dialog onCreateDialog(Bundle savedInstanceState) {\n        View content = View.inflate(getContext(), R.layout.time_dialog, null);\n        AlertDialog.Builder builder = new AlertDialog.Builder(getContext());\n        builder.setTitle(R.string.sleep_timer_label);\n        builder.setView(content);\n        builder.setPositiveButton(R.string.close_label, null);\n\n        etxtTime = content.findViewById(R.id.etxtTime);\n        spTimeUnit = content.findViewById(R.id.spTimeUnit);\n        timeSetup = content.findViewById(R.id.timeSetup);\n        timeDisplay = content.findViewById(R.id.timeDisplay);\n        time = content.findViewById(R.id.time);\n\n        etxtTime.setText(SleepTimerPreferences.lastTimerValue());\n        etxtTime.postDelayed(() -> {\n            InputMethodManager imm = (InputMethodManager) getContext().getSystemService(Context.INPUT_METHOD_SERVICE);\n            imm.showSoftInput(etxtTime, InputMethodManager.SHOW_IMPLICIT);\n        }, 100);\n\n        String[] spinnerContent = new String[] {\n                getString(R.string.time_seconds),\n                getString(R.string.time_minutes),\n                getString(R.string.time_hours) };\n        ArrayAdapter<String> spinnerAdapter = new ArrayAdapter<>(getContext(),\n                android.R.layout.simple_spinner_item, spinnerContent);\n        spinnerAdapter.setDropDownViewResource(android.R.layout.simple_spinner_dropdown_item);\n        spTimeUnit.setAdapter(spinnerAdapter);\n        spTimeUnit.setSelection(SleepTimerPreferences.lastTimerTimeUnit());\n\n        CheckBox cbShakeToReset = content.findViewById(R.id.cbShakeToReset);\n        CheckBox cbVibrate = content.findViewById(R.id.cbVibrate);\n        CheckBox chAutoEnable = content.findViewById(R.id.chAutoEnable);\n\n        cbShakeToReset.setChecked(SleepTimerPreferences.shakeToReset());\n        cbVibrate.setChecked(SleepTimerPreferences.vibrate());\n        chAutoEnable.setChecked(SleepTimerPreferences.autoEnable());\n\n        cbShakeToReset.setOnCheckedChangeListener((buttonView, isChecked)\n                -> SleepTimerPreferences.setShakeToReset(isChecked));\n        cbVibrate.setOnCheckedChangeListener((buttonView, isChecked)\n                -> SleepTimerPreferences.setVibrate(isChecked));\n        chAutoEnable.setOnCheckedChangeListener((compoundButton, isChecked)\n                -> SleepTimerPreferences.setAutoEnable(isChecked));\n\n        Button disableButton = content.findViewById(R.id.disableSleeptimerButton);\n        disableButton.setOnClickListener(v -> {\n            if (controller != null) {\n                controller.disableSleepTimer();\n            }\n        });\n        Button setButton = content.findViewById(R.id.setSleeptimerButton);\n        setButton.setOnClickListener(v -> {\n            if (!PlaybackService.isRunning) {\n                Snackbar.make(content, R.string.no_media_playing_label, Snackbar.LENGTH_LONG).show();\n                return;\n            }\n            try {\n                SleepTimerPreferences.setLastTimer(etxtTime.getText().toString(), spTimeUnit.getSelectedItemPosition());\n                long time = SleepTimerPreferences.timerMillis();\n                if (controller != null) {\n                    controller.setSleepTimer(time);\n                }\n            } catch (NumberFormatException e) {\n                e.printStackTrace();\n                Snackbar.make(content, R.string.time_dialog_invalid_input, Snackbar.LENGTH_LONG).show();\n            }\n        });\n        return builder.create();\n    }\n\n    private void updateTime() {\n        if (controller == null) {\n            return;\n        }\n        timeSetup.setVisibility(controller.sleepTimerActive() ? View.GONE : View.VISIBLE);\n        timeDisplay.setVisibility(controller.sleepTimerActive() ? View.VISIBLE : View.GONE);\n        time.setText(Converter.getDurationStringLong((int) controller.getSleepTimerTimeLeft()));\n    }\n}\n","lang_cluster":"Java","length":156,"code_uid":"0fe752e4f1084e03a84a2910d8196c95"}
{"diff_hunk":"@@ -19,10 +19,14 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.util.Map;\n+import java.util.Set;\n import org.apache.arrow.vector.NullCheckingForGet;\n import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.io.CloseableIterable;","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.spark.source;\n\nimport org.apache.arrow.vector.NullCheckingForGet;\nimport org.apache.iceberg.CombinedScanTask;\nimport org.apache.iceberg.FileFormat;\nimport org.apache.iceberg.FileScanTask;\nimport org.apache.iceberg.Schema;\nimport org.apache.iceberg.encryption.EncryptionManager;\nimport org.apache.iceberg.io.CloseableIterable;\nimport org.apache.iceberg.io.CloseableIterator;\nimport org.apache.iceberg.io.FileIO;\nimport org.apache.iceberg.io.InputFile;\nimport org.apache.iceberg.mapping.NameMappingParser;\nimport org.apache.iceberg.parquet.Parquet;\nimport org.apache.iceberg.relocated.com.google.common.base.Preconditions;\nimport org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\nimport org.apache.spark.sql.vectorized.ColumnarBatch;\n\nclass BatchDataReader extends BaseDataReader<ColumnarBatch> {\n  private final Schema expectedSchema;\n  private final String nameMapping;\n  private final boolean caseSensitive;\n  private final int batchSize;\n\n  BatchDataReader(\n      CombinedScanTask task, Schema expectedSchema, String nameMapping, FileIO fileIo,\n      EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n    super(task, fileIo, encryptionManager);\n    this.expectedSchema = expectedSchema;\n    this.nameMapping = nameMapping;\n    this.caseSensitive = caseSensitive;\n    this.batchSize = size;\n  }\n\n  @Override\n  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema, \/* setArrowValidityVector *\/ NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \/\/ Spark eagerly consumes the batches. So the underlying memory allocated could be reused\n          \/\/ without worrying about subsequent reads clobbering over each other. This improves\n          \/\/ read performance as every batch read doesn't have to pay the cost of allocating memory.\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n}\n","lang_cluster":"Java","length":84,"code_uid":"a084af381f394f2fb9f2520383d9a082"}
{"diff_hunk":"@@ -32,6 +32,7 @@ import java.util.UUID;\n public class OpenSamlAuthenticationRequestFactory implements Saml2AuthenticationRequestFactory {\n \tprivate Clock clock = Clock.systemUTC();\n \tprivate final OpenSamlImplementation saml = OpenSamlImplementation.getInstance();\n+\tprivate String protocolBinding = SAMLConstants.SAML2_POST_BINDING_URI;\n \n \t\/**\n \t * {@inheritDoc}","old_code":"\/*\n * Copyright 2002-2019 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage org.springframework.security.saml2.provider.service.authentication;\n\nimport org.springframework.util.Assert;\n\nimport org.joda.time.DateTime;\nimport org.opensaml.saml.saml2.core.AuthnRequest;\nimport org.opensaml.saml.saml2.core.Issuer;\n\nimport java.time.Clock;\nimport java.time.Instant;\nimport java.util.UUID;\n\n\/**\n * @since 5.2\n *\/\npublic class OpenSamlAuthenticationRequestFactory implements Saml2AuthenticationRequestFactory {\n\tprivate Clock clock = Clock.systemUTC();\n\tprivate final OpenSamlImplementation saml = OpenSamlImplementation.getInstance();\n\n\t\/**\n\t * {@inheritDoc}\n\t *\/\n\t@Override\n\tpublic String createAuthenticationRequest(Saml2AuthenticationRequest request) {\n\t\tAuthnRequest auth = this.saml.buildSAMLObject(AuthnRequest.class);\n\t\tauth.setID(\"ARQ\" + UUID.randomUUID().toString().substring(1));\n\t\tauth.setIssueInstant(new DateTime(this.clock.millis()));\n\t\tauth.setForceAuthn(Boolean.FALSE);\n\t\tauth.setIsPassive(Boolean.FALSE);\n\t\tauth.setProtocolBinding(\"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect\");\n\t\tIssuer issuer = this.saml.buildSAMLObject(Issuer.class);\n\t\tissuer.setValue(request.getIssuer());\n\t\tauth.setIssuer(issuer);\n\t\tauth.setDestination(request.getDestination());\n\t\tauth.setAssertionConsumerServiceURL(request.getAssertionConsumerServiceUrl());\n\t\treturn this.saml.toXml(\n\t\t\t\tauth,\n\t\t\t\trequest.getCredentials(),\n\t\t\t\trequest.getIssuer()\n\t\t);\n\t}\n\n\t\/**\n\t * '\n\t * Use this {@link Clock} with {@link Instant#now()} for generating\n\t * timestamps\n\t *\n\t * @param clock\n\t *\/\n\tpublic void setClock(Clock clock) {\n\t\tAssert.notNull(clock, \"clock cannot be null\");\n\t\tthis.clock = clock;\n\t}\n}\n","lang_cluster":"Java","length":70,"code_uid":"4b9eb47ecb16441896090ab24bd72d77"}
{"diff_hunk":"@@ -15,6 +15,7 @@\n package org.hyperledger.besu.config;\n \n import java.util.Optional;\n+import java.util.OptionalLong;\n \n public interface QbftConfigOptions extends BftConfigOptions {\n ","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.config;\n\nimport java.util.Optional;\n\npublic interface QbftConfigOptions extends BftConfigOptions {\n\n  Optional<String> getValidatorContractAddress();\n\n  default boolean isValidatorContractMode() {\n    return getValidatorContractAddress().isPresent();\n  }\n}\n","lang_cluster":"Java","length":26,"code_uid":"f33504197d0e4880b9f7b0c6adc79d11"}
{"diff_hunk":"@@ -18,6 +18,7 @@ import com.google.api.codegen.util.NamePath;\n import com.google.api.codegen.util.TypeAlias;\n import com.google.api.codegen.util.TypeName;\n import com.google.api.codegen.util.TypeTable;\n+import com.google.common.collect.ImmutableSet;\n \n import java.util.ArrayList;\n import java.util.List;","old_code":"\/* Copyright 2016 Google Inc\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage com.google.api.codegen.util.go;\n\nimport com.google.api.codegen.util.NamePath;\nimport com.google.api.codegen.util.TypeAlias;\nimport com.google.api.codegen.util.TypeName;\nimport com.google.api.codegen.util.TypeTable;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.TreeMap;\n\npublic class GoTypeTable implements TypeTable {\n\n  private final TreeMap<String, String> imports = new TreeMap<>();\n\n  @Override\n  public TypeTable cloneEmpty() {\n    return new GoTypeTable();\n  }\n\n  @Override\n  public TypeName getTypeName(String fullName) {\n    String[] parts = fullName.split(\";\", -1);\n    if (parts.length != 4) {\n      return new TypeName(fullName);\n    }\n    return new TypeName(fullName, parts[3] + parts[1] + \".\" + parts[2]);\n  }\n\n  @Override\n  public NamePath getNamePath(String fullName) {\n    return NamePath.dotted(fullName);\n  }\n\n  @Override\n  public TypeName getContainerTypeName(String containerFullName, String... elementFullNames) {\n    return getTypeName(containerFullName);\n  }\n\n  @Override\n  public String getAndSaveNicknameFor(String fullName) {\n    return getAndSaveNicknameFor(getTypeName(fullName));\n  }\n\n  @Override\n  public String getAndSaveNicknameFor(TypeName typeName) {\n    return typeName.getAndSaveNicknameIn(this);\n  }\n\n  @Override\n  public String getAndSaveNicknameFor(TypeAlias alias) {\n    String[] parts = alias.getFullName().split(\";\", -1);\n    if (parts.length == 4) {\n      imports.put(parts[0], parts[1]);\n    }\n    return alias.getNickname();\n  }\n\n  @Override\n  public Map<String, String> getImports() {\n    return imports;\n  }\n\n  public static List<String> formatImports(Map<String, String> imports) {\n    List<String> standard = new ArrayList<>(imports.size());\n    List<String> thirdParty = new ArrayList<>(imports.size());\n\n    for (Map.Entry<String, String> imp : imports.entrySet()) {\n      String importPath = imp.getKey();\n      String packageRename = imp.getValue();\n      List<String> target = isStandardImport(importPath) ? standard : thirdParty;\n      if (packageRename.equals(\"\")) {\n        target.add(String.format(\"\\\"%s\\\"\", importPath));\n      } else {\n        target.add(String.format(\"%s \\\"%s\\\"\", packageRename, importPath));\n      }\n    }\n\n    List<String> merge = new ArrayList<>(standard);\n    if (!standard.isEmpty() && !thirdParty.isEmpty()) {\n      merge.add(\"\");\n    }\n    merge.addAll(thirdParty);\n    return merge;\n  }\n\n  private static boolean isStandardImport(String importPath) {\n    \/\/ TODO(pongad): Some packages in standard library have slashes,\n    \/\/ we might have to special case them.\n    return !importPath.contains(\"\/\");\n  }\n}\n","lang_cluster":"Java","length":107,"code_uid":"ad9a53cda563417a86707dd4f7842943"}
{"diff_hunk":"@@ -57,15 +57,17 @@ public final class ASTMethodDeclaration extends AbstractMethodOrConstructorDecla\n \n     \/**\n      * Returns the simple name of the method.\n+     *\n+     * @deprecated Use {@link #getName()}\n      *\/\n+    @Deprecated\n     public String getMethodName() {\n-        return getFirstChildOfType(ASTMethodDeclarator.class).getImage();\n+        return getName();\n     }\n \n-\n     @Override\n     public String getName() {\n-        return getMethodName();\n+        return getImage();\n     }\n \n ","old_code":"\/**\n * BSD-style license; for more info see http:\/\/pmd.sourceforge.net\/license.html\n *\/\n\npackage net.sourceforge.pmd.lang.java.ast;\n\nimport net.sourceforge.pmd.annotation.InternalApi;\nimport net.sourceforge.pmd.lang.ast.Node;\nimport net.sourceforge.pmd.lang.dfa.DFAGraphMethod;\n\n\n\/**\n * A method declaration, in a class or interface declaration. This cannot\n * be found in {@linkplain ASTAnnotationTypeDeclaration annotation types},\n * which instead have {@linkplain ASTAnnotationMethodDeclaration annotation methods}.\n *\n * <pre class=\"grammar\">\n *\n * MethodDeclaration ::= MethodModifier*\n *                       {@link ASTTypeParameters TypeParameters}?\n *                       {@link ASTResultType ResultType}\n *                       {@link ASTMethodDeclarator MethodDeclarator}\n *                       (\"throws\" {@link ASTNameList NameList})?\n *                       ({@link ASTBlock Block} | \";\" )\n *\n *\n * MethodModifier ::= \"public\" | \"private\"  | \"protected\" | \"static\"\n *                  | \"final\"  | \"abstract\" | \"native\"\n *                  | {@linkplain ASTAnnotation Annotation}\n *\n * <\/pre>\n *\/\npublic final class ASTMethodDeclaration extends AbstractMethodOrConstructorDeclaration implements DFAGraphMethod {\n\n\n    @InternalApi\n    @Deprecated\n    public ASTMethodDeclaration(int id) {\n        super(id);\n    }\n\n    ASTMethodDeclaration(JavaParser p, int id) {\n        super(p, id);\n    }\n\n    @Override\n    public Object jjtAccept(JavaParserVisitor visitor, Object data) {\n        return visitor.visit(this, data);\n    }\n\n\n    @Override\n    public <T> void jjtAccept(SideEffectingVisitor<T> visitor, T data) {\n        visitor.visit(this, data);\n    }\n\n\n    \/**\n     * Returns the simple name of the method.\n     *\/\n    public String getMethodName() {\n        return getFirstChildOfType(ASTMethodDeclarator.class).getImage();\n    }\n\n\n    @Override\n    public String getName() {\n        return getMethodName();\n    }\n\n\n    \/**\n     * Returns true if this method is explicitly modified by\n     * the {@code public} modifier.\n     *\/\n    public boolean isSyntacticallyPublic() {\n        return super.isPublic();\n    }\n\n\n    \/**\n     * Returns true if this method is explicitly modified by\n     * the {@code abstract} modifier.\n     *\/\n    public boolean isSyntacticallyAbstract() {\n        return super.isAbstract();\n    }\n\n\n    \/**\n     * Returns true if this method has public visibility.\n     * Non-private interface members are implicitly public,\n     * whether they declare the {@code public} modifier or\n     * not.\n     *\/\n    @Override\n    public boolean isPublic() {\n        \/\/ interface methods are public by default, but could be private since java9\n        return isInterfaceMember() && !isPrivate() || super.isPublic();\n    }\n\n\n    \/**\n     * Returns true if this method is abstract, so doesn't\n     * declare a body. Interface members are\n     * implicitly abstract, whether they declare the\n     * {@code abstract} modifier or not. Default interface\n     * methods are not abstract though, consistently with the\n     * standard reflection API.\n     *\/\n    @Override\n    public boolean isAbstract() {\n        return isInterfaceMember() && !isDefault() || super.isAbstract();\n    }\n\n\n    \/**\n     * Returns true if this method declaration is a member of an interface type.\n     *\/\n    public boolean isInterfaceMember() {\n        \/\/ for a real class\/interface the 3rd parent is a ClassOrInterfaceDeclaration,\n        \/\/ for anonymous classes, the parent is e.g. a AllocationExpression\n        Node potentialTypeDeclaration = getNthParent(3);\n\n        return potentialTypeDeclaration instanceof ASTClassOrInterfaceDeclaration\n            && ((ASTClassOrInterfaceDeclaration) potentialTypeDeclaration).isInterface();\n    }\n\n\n    \/**\n     * Returns true if the result type of this method is {@code void}.\n     *\/\n    public boolean isVoid() {\n        return getResultType().isVoid();\n    }\n\n\n    \/**\n     * Returns the result type node of the method.\n     *\/\n    public ASTResultType getResultType() {\n        return getFirstChildOfType(ASTResultType.class);\n    }\n\n\n    \/**\n     * Returns the block defined by this method, or\n     * null if the method is abstract.\n     *\/\n    public ASTBlock getBlock() {\n        return getFirstChildOfType(ASTBlock.class);\n    }\n\n\n    \/**\n     * Returns the exception names listed in the {@code throws} clause\n     * of this method declaration, or null if there are none.\n     *\/\n    public ASTNameList getThrows() {\n        return getFirstChildOfType(ASTNameList.class);\n    }\n\n\n    @Override\n    public MethodLikeKind getKind() {\n        return MethodLikeKind.METHOD;\n    }\n\n    public ASTTypeParameters getTypeParameters() {\n        return getFirstChildOfType(ASTTypeParameters.class);\n    }\n\n    \/\/@Override \/\/ enable this with PMD 7.0.0 - see interface ASTMethodOrConstructorDeclaration\n    public ASTFormalParameters getFormalParameters() {\n        return getFirstChildOfType(ASTMethodDeclarator.class).getFirstChildOfType(ASTFormalParameters.class);\n    }\n\n\n    \/**\n     * Returns the method declarator. Never null.\n     *\/\n    public ASTMethodDeclarator getMethodDeclarator() {\n        return getFirstChildOfType(ASTMethodDeclarator.class);\n    }\n}\n","lang_cluster":"Java","length":185,"code_uid":"40ab4774a325488b83451239fd960d91"}
{"diff_hunk":"@@ -194,7 +194,7 @@ public class ExecutorApiGateway {\n         JSONUtils.toJSON(executionIdsList));\n \n     return callWithExecutionId(executor.getHost(), executor.getPort(),\n-        ConnectorParams.UPDATE_ACTION, null, null, executionIds, updateTimes);\n+        ConnectorParams.UPDATE_ACTION, null, null, null, executionIds, updateTimes);\n   }\n \n }","old_code":"\/*\n * Copyright 2017 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n *\/\n\npackage azkaban.executor;\n\nimport azkaban.Constants;\nimport azkaban.Constants.ConfigurationKeys;\nimport azkaban.DispatchMethod;\nimport azkaban.utils.JSONUtils;\nimport azkaban.utils.Pair;\nimport azkaban.utils.Props;\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.inject.Inject;\nimport java.io.IOException;\nimport java.net.URI;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.function.BiFunction;\nimport javax.inject.Singleton;\nimport org.codehaus.jackson.map.ObjectMapper;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n@Singleton\npublic class ExecutorApiGateway {\n  private final static Logger logger = LoggerFactory.getLogger(ExecutorApiGateway.class);\n  public final static String DEFAULT_EXECUTION_RESOURCE = \"executor\";\n  public final static String CONTAINERIZED_EXECUTION_RESOURCE = \"container\";\n\n  \/\/ Default procedure for modifying a resource path that a reverse proxy, such as an\n  \/\/ ingress-controller, can use to route the request to correct endpoint.\n  \/\/   - This is a first-class function to make it easier to switch to a different mechanism of\n  \/\/     creating the path, depending on how the reverse-proxy is configured.\n  \/\/   - In future this implementation could be guice-injected (possibly based on a config property)\n  \/\/   - This implementation simply prefixes resource name with the execution-id and assumes that\n  \/\/     that a reverse proxy can route the request correctly based on this prefix.\n  private final static BiFunction<Integer, String, String> executionResourceNameModifier =\n      ((e,r) -> String.join(\"\/\",  e.toString(), r));\n\n  private final static Executor defaultEmptyExecutor = new Executor(-1, \"\", 1, false);\n  private final ExecutorApiClient apiClient;\n  private final String executionResourceName;\n  private final boolean isReverseProxyEnabled;\n\n  @Inject\n  public ExecutorApiGateway(final ExecutorApiClient apiClient, Props azkProps) {\n    this.apiClient = apiClient;\n    isReverseProxyEnabled =\n        azkProps.getBoolean(ConfigurationKeys.AZKABAN_EXECUTOR_REVERSE_PROXY_ENABLED,\n            false);\n    String executionResourceName = DEFAULT_EXECUTION_RESOURCE;\n    if (DispatchMethod.getDispatchMethod(azkProps\n        .getString(Constants.ConfigurationKeys.AZKABAN_EXECUTION_DISPATCH_METHOD,\n            DispatchMethod.PUSH.name())) == DispatchMethod.CONTAINERIZED) {\n      executionResourceName = CONTAINERIZED_EXECUTION_RESOURCE;\n    }\n    this.executionResourceName = executionResourceName;\n  }\n\n  Map<String, Object> callWithExecutable(final ExecutableFlow exflow,\n      final Executor executor, final String action) throws ExecutorManagerException {\n    return callWithExecutionId(executor.getHost(), executor.getPort(), action,\n        exflow.getExecutionId(), null, (Pair<String, String>[]) null);\n  }\n\n  Map<String, Object> callWithReference(final ExecutionReference ref, final String action,\n      final Pair<String, String>... params) throws ExecutorManagerException {\n    final Executor executor = (isReverseProxyEnabled ? defaultEmptyExecutor : ref.getExecutor().get());\n    return callWithExecutionId(executor.getHost(), executor.getPort(), action, ref.getExecId(),\n        null, params);\n  }\n\n  public Map<String, Object> callWithReferenceByUser(final ExecutionReference ref,\n      final String action, final String user, final Pair<String, String>... params)\n      throws ExecutorManagerException {\n    final Executor executor = (isReverseProxyEnabled ? defaultEmptyExecutor : ref.getExecutor().get());\n    return callWithExecutionId(executor.getHost(), executor.getPort(), action,\n        ref.getExecId(), user, params);\n  }\n\n  @VisibleForTesting\n  String createExecutionPath(final Optional<Integer> executionId) throws ExecutorManagerException {\n    if (!isReverseProxyEnabled) {\n      return \"\/\" + executionResourceName;\n    }\n\n    if(!executionId.isPresent()) {\n      final String errorMessage = \"Execution Id must be provided when reverse-proxy is enabled\";\n      logger.error(errorMessage);\n      throw new ExecutorManagerException(errorMessage);\n    }\n    return \"\/\" + executionResourceNameModifier.apply(executionId.get(), executionResourceName);\n  }\n\n  Map<String, Object> callWithExecutionId(final String host, final int port,\n      final String action, final Integer executionId, final String user,\n      final Pair<String, String>... params) throws ExecutorManagerException {\n    try {\n      final List<Pair<String, String>> paramList = new ArrayList<>();\n\n      if (params != null) {\n        paramList.addAll(Arrays.asList(params));\n      }\n\n      paramList\n          .add(new Pair<>(ConnectorParams.ACTION_PARAM, action));\n      paramList.add(new Pair<>(ConnectorParams.EXECID_PARAM, String\n          .valueOf(executionId)));\n      paramList.add(new Pair<>(ConnectorParams.USER_PARAM, user));\n\n      \/\/ Ideally we should throw an exception if executionId is null but some existing code\n      \/\/ (updateExecutions()) expects to call this method with a null executionId.\n      String executionPath = createExecutionPath(Optional.ofNullable(executionId));\n      return callForJsonObjectMap(host, port, executionPath, paramList);\n    } catch (final IOException e) {\n      throw new ExecutorManagerException(e.getMessage(), e);\n    }\n  }\n\n  \/**\n   * Call executor and parse the JSON response as an instance of the class given as an argument.\n   *\/\n  <T> T callForJsonType(final String host, final int port, final String path,\n      final List<Pair<String, String>> paramList, final Class<T> valueType) throws IOException {\n    final String responseString = callForJsonString(host, port, path, paramList);\n    if (null == responseString || responseString.length() == 0) {\n      return null;\n    }\n    return new ObjectMapper().readValue(responseString, valueType);\n  }\n\n  \/*\n   * Call executor and return json object map.\n   *\/\n  Map<String, Object> callForJsonObjectMap(final String host, final int port,\n      final String path, final List<Pair<String, String>> paramList) throws IOException {\n    final String responseString =\n        callForJsonString(host, port, path, paramList);\n\n    @SuppressWarnings(\"unchecked\") final Map<String, Object> jsonResponse =\n        (Map<String, Object>) JSONUtils.parseJSONFromString(responseString);\n    final String error = (String) jsonResponse.get(ConnectorParams.RESPONSE_ERROR);\n    if (error != null) {\n      throw new IOException(error);\n    }\n    return jsonResponse;\n  }\n\n  \/*\n   * Call executor and return raw json string.\n   *\/\n  private String callForJsonString(final String host, final int port, final String path,\n      List<Pair<String, String>> paramList) throws IOException {\n    if (paramList == null) {\n      paramList = new ArrayList<>();\n    }\n\n    @SuppressWarnings(\"unchecked\") final URI uri =\n        apiClient.buildExecutorUri(host, port, path, true);\n\n    return this.apiClient.httpPost(uri, paramList);\n  }\n\n  public Map<String, Object> updateExecutions(final Executor executor,\n      final List<ExecutableFlow> executions) throws ExecutorManagerException {\n    final List<Long> updateTimesList = new ArrayList<>();\n    final List<Integer> executionIdsList = new ArrayList<>();\n    \/\/ We pack the parameters of the same host together before query\n    for (final ExecutableFlow flow : executions) {\n      executionIdsList.add(flow.getExecutionId());\n      updateTimesList.add(flow.getUpdateTime());\n    }\n    final Pair<String, String> updateTimes = new Pair<>(\n        ConnectorParams.UPDATE_TIME_LIST_PARAM,\n        JSONUtils.toJSON(updateTimesList));\n    final Pair<String, String> executionIds = new Pair<>(\n        ConnectorParams.EXEC_ID_LIST_PARAM,\n        JSONUtils.toJSON(executionIdsList));\n\n    return callWithExecutionId(executor.getHost(), executor.getPort(),\n        ConnectorParams.UPDATE_ACTION, null, null, executionIds, updateTimes);\n  }\n\n}\n","lang_cluster":"Java","length":200,"code_uid":"d6bc6c9c5dd74921a3a2860e1f66bca3"}
{"diff_hunk":"@@ -27,6 +27,7 @@ import com.github.javaparser.ast.NodeList;\n import com.github.javaparser.ast.expr.AnnotationExpr;\n import com.github.javaparser.ast.expr.SimpleName;\n import com.github.javaparser.ast.nodeTypes.modifiers.NodeWithAbstractModifier;\n+import com.github.javaparser.ast.type.Type;\n import com.github.javaparser.ast.visitor.CloneVisitor;\n import com.github.javaparser.ast.visitor.GenericVisitor;\n import com.github.javaparser.ast.visitor.VoidVisitor;","old_code":"\/*\n * Copyright (C) 2007-2010 J\u00falio Vilmar Gesser.\n * Copyright (C) 2011, 2013-2016 The JavaParser Team.\n *\n * This file is part of JavaParser.\n *\n * JavaParser can be used either under the terms of\n * a) the GNU Lesser General Public License as published by\n *     the Free Software Foundation, either version 3 of the License, or\n *     (at your option) any later version.\n * b) the terms of the Apache License\n *\n * You should have received a copy of both licenses in LICENCE.LGPL and\n * LICENCE.APACHE. Please refer to those files for details.\n *\n * JavaParser is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU Lesser General Public License for more details.\n *\/\npackage com.github.javaparser.ast.body;\n\nimport com.github.javaparser.ast.AllFieldsConstructor;\nimport com.github.javaparser.ast.Modifier;\nimport com.github.javaparser.ast.Node;\nimport com.github.javaparser.ast.NodeList;\nimport com.github.javaparser.ast.expr.AnnotationExpr;\nimport com.github.javaparser.ast.expr.SimpleName;\nimport com.github.javaparser.ast.nodeTypes.modifiers.NodeWithAbstractModifier;\nimport com.github.javaparser.ast.visitor.CloneVisitor;\nimport com.github.javaparser.ast.visitor.GenericVisitor;\nimport com.github.javaparser.ast.visitor.VoidVisitor;\nimport com.github.javaparser.metamodel.AnnotationDeclarationMetaModel;\nimport com.github.javaparser.metamodel.JavaParserMetaModel;\nimport com.github.javaparser.TokenRange;\nimport com.github.javaparser.resolution.Resolvable;\nimport com.github.javaparser.resolution.declarations.ResolvedAnnotationDeclaration;\nimport java.util.function.Consumer;\nimport java.util.Optional;\nimport com.github.javaparser.ast.Generated;\n\n\/**\n * An annotation type declaration.<br\/><code>@interface X { ... }<\/code>\n *\n * @author Julio Vilmar Gesser\n *\/\npublic class AnnotationDeclaration extends TypeDeclaration<AnnotationDeclaration> implements NodeWithAbstractModifier<AnnotationDeclaration>, Resolvable<ResolvedAnnotationDeclaration> {\n\n    public AnnotationDeclaration() {\n        this(null, new NodeList<>(), new NodeList<>(), new SimpleName(), new NodeList<>());\n    }\n\n    public AnnotationDeclaration(NodeList<Modifier> modifiers, String name) {\n        this(null, modifiers, new NodeList<>(), new SimpleName(name), new NodeList<>());\n    }\n\n    @AllFieldsConstructor\n    public AnnotationDeclaration(NodeList<Modifier> modifiers, NodeList<AnnotationExpr> annotations, SimpleName name, NodeList<BodyDeclaration<?>> members) {\n        this(null, modifiers, annotations, name, members);\n    }\n\n    \/**\n     * This constructor is used by the parser and is considered private.\n     *\/\n    @Generated(\"com.github.javaparser.generator.core.node.MainConstructorGenerator\")\n    public AnnotationDeclaration(TokenRange tokenRange, NodeList<Modifier> modifiers, NodeList<AnnotationExpr> annotations, SimpleName name, NodeList<BodyDeclaration<?>> members) {\n        super(tokenRange, modifiers, annotations, name, members);\n        customInitialization();\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.AcceptGenerator\")\n    public <R, A> R accept(final GenericVisitor<R, A> v, final A arg) {\n        return v.visit(this, arg);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.AcceptGenerator\")\n    public <A> void accept(final VoidVisitor<A> v, final A arg) {\n        v.visit(this, arg);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.RemoveMethodGenerator\")\n    public boolean remove(Node node) {\n        if (node == null)\n            return false;\n        return super.remove(node);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.CloneGenerator\")\n    public AnnotationDeclaration clone() {\n        return (AnnotationDeclaration) accept(new CloneVisitor(), null);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.GetMetaModelGenerator\")\n    public AnnotationDeclarationMetaModel getMetaModel() {\n        return JavaParserMetaModel.annotationDeclarationMetaModel;\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.ReplaceMethodGenerator\")\n    public boolean replace(Node node, Node replacementNode) {\n        if (node == null)\n            return false;\n        return super.replace(node, replacementNode);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.TypeCastingGenerator\")\n    public boolean isAnnotationDeclaration() {\n        return true;\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.TypeCastingGenerator\")\n    public AnnotationDeclaration asAnnotationDeclaration() {\n        return this;\n    }\n\n    @Generated(\"com.github.javaparser.generator.core.node.TypeCastingGenerator\")\n    public void ifAnnotationDeclaration(Consumer<AnnotationDeclaration> action) {\n        action.accept(this);\n    }\n\n    @Override\n    public ResolvedAnnotationDeclaration resolve() {\n        return getSymbolResolver().resolveDeclaration(this, ResolvedAnnotationDeclaration.class);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.TypeCastingGenerator\")\n    public Optional<AnnotationDeclaration> toAnnotationDeclaration() {\n        return Optional.of(this);\n    }\n}\n","lang_cluster":"Java","length":138,"code_uid":"ed525ca7b91844eabedf32e86081dc49"}
{"diff_hunk":"@@ -111,8 +111,13 @@ public class Stats {\n     }\n     blockCountByPrefixLen[frame.prefix]++;\n     startBlockCount++;\n-    totalBlockSuffixBytes += frame.suffixesReader.length();\n+    totalBlockSuffixBytes += frame.totalSuffixBytes;\n+    totalUncompressedBlockSuffixBytes += frame.suffixesReader.length();\n+    if (frame.suffixesReader != frame.suffixLengthsReader) {\n+      totalUncompressedBlockSuffixBytes += frame.suffixLengthsReader.length();\n+    }\n     totalBlockStatsBytes += frame.statsReader.length();\n+    compressionAlgorithms[frame.compressionAlg.code]++;\n   }\n \n   void endBlock(SegmentTermsEnumFrame frame) {","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage org.apache.lucene.codecs.blocktree;\n\n\nimport java.io.ByteArrayOutputStream;\nimport java.io.PrintStream;\nimport java.io.UnsupportedEncodingException;\nimport java.util.Locale;\n\nimport org.apache.lucene.codecs.PostingsReaderBase;\nimport org.apache.lucene.util.ArrayUtil;\nimport org.apache.lucene.util.BytesRef;\nimport org.apache.lucene.util.IOUtils;\n\n\/**\n * BlockTree statistics for a single field \n * returned by {@link FieldReader#getStats()}.\n * @lucene.internal\n *\/\npublic class Stats {\n  \/** Byte size of the index. *\/\n  public long indexNumBytes;\n\n  \/** Total number of terms in the field. *\/\n  public long totalTermCount;\n\n  \/** Total number of bytes (sum of term lengths) across all terms in the field. *\/\n  public long totalTermBytes;\n\n  \/** The number of normal (non-floor) blocks in the terms file. *\/\n  public int nonFloorBlockCount;\n\n  \/** The number of floor blocks (meta-blocks larger than the\n   *  allowed {@code maxItemsPerBlock}) in the terms file. *\/\n  public int floorBlockCount;\n    \n  \/** The number of sub-blocks within the floor blocks. *\/\n  public int floorSubBlockCount;\n\n  \/** The number of \"internal\" blocks (that have both\n   *  terms and sub-blocks). *\/\n  public int mixedBlockCount;\n\n  \/** The number of \"leaf\" blocks (blocks that have only\n   *  terms). *\/\n  public int termsOnlyBlockCount;\n\n  \/** The number of \"internal\" blocks that do not contain\n   *  terms (have only sub-blocks). *\/\n  public int subBlocksOnlyBlockCount;\n\n  \/** Total number of blocks. *\/\n  public int totalBlockCount;\n\n  \/** Number of blocks at each prefix depth. *\/\n  public int[] blockCountByPrefixLen = new int[10];\n  private int startBlockCount;\n  private int endBlockCount;\n\n  \/** Total number of bytes used to store term suffixes. *\/\n  public long totalBlockSuffixBytes;\n\n  \/** Total number of bytes used to store term stats (not\n   *  including what the {@link PostingsReaderBase}\n   *  stores. *\/\n  public long totalBlockStatsBytes;\n\n  \/** Total bytes stored by the {@link PostingsReaderBase},\n   *  plus the other few vInts stored in the frame. *\/\n  public long totalBlockOtherBytes;\n\n  \/** Segment name. *\/\n  public final String segment;\n\n  \/** Field name. *\/\n  public final String field;\n\n  Stats(String segment, String field) {\n    this.segment = segment;\n    this.field = field;\n  }\n\n  void startBlock(SegmentTermsEnumFrame frame, boolean isFloor) {\n    totalBlockCount++;\n    if (isFloor) {\n      if (frame.fp == frame.fpOrig) {\n        floorBlockCount++;\n      }\n      floorSubBlockCount++;\n    } else {\n      nonFloorBlockCount++;\n    }\n\n    if (blockCountByPrefixLen.length <= frame.prefix) {\n      blockCountByPrefixLen = ArrayUtil.grow(blockCountByPrefixLen, 1+frame.prefix);\n    }\n    blockCountByPrefixLen[frame.prefix]++;\n    startBlockCount++;\n    totalBlockSuffixBytes += frame.suffixesReader.length();\n    totalBlockStatsBytes += frame.statsReader.length();\n  }\n\n  void endBlock(SegmentTermsEnumFrame frame) {\n    final int termCount = frame.isLeafBlock ? frame.entCount : frame.state.termBlockOrd;\n    final int subBlockCount = frame.entCount - termCount;\n    totalTermCount += termCount;\n    if (termCount != 0 && subBlockCount != 0) {\n      mixedBlockCount++;\n    } else if (termCount != 0) {\n      termsOnlyBlockCount++;\n    } else if (subBlockCount != 0) {\n      subBlocksOnlyBlockCount++;\n    } else {\n      throw new IllegalStateException();\n    }\n    endBlockCount++;\n    final long otherBytes = frame.fpEnd - frame.fp - frame.suffixesReader.length() - frame.statsReader.length();\n    assert otherBytes > 0 : \"otherBytes=\" + otherBytes + \" frame.fp=\" + frame.fp + \" frame.fpEnd=\" + frame.fpEnd;\n    totalBlockOtherBytes += otherBytes;\n  }\n\n  void term(BytesRef term) {\n    totalTermBytes += term.length;\n  }\n\n  void finish() {\n    assert startBlockCount == endBlockCount: \"startBlockCount=\" + startBlockCount + \" endBlockCount=\" + endBlockCount;\n    assert totalBlockCount == floorSubBlockCount + nonFloorBlockCount: \"floorSubBlockCount=\" + floorSubBlockCount + \" nonFloorBlockCount=\" + nonFloorBlockCount + \" totalBlockCount=\" + totalBlockCount;\n    assert totalBlockCount == mixedBlockCount + termsOnlyBlockCount + subBlocksOnlyBlockCount: \"totalBlockCount=\" + totalBlockCount + \" mixedBlockCount=\" + mixedBlockCount + \" subBlocksOnlyBlockCount=\" + subBlocksOnlyBlockCount + \" termsOnlyBlockCount=\" + termsOnlyBlockCount;\n  }\n\n  @Override\n  public String toString() {\n    final ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);\n    PrintStream out;\n    try {\n      out = new PrintStream(bos, false, IOUtils.UTF_8);\n    } catch (UnsupportedEncodingException bogus) {\n      throw new RuntimeException(bogus);\n    }\n      \n    out.println(\"  index FST:\");\n    out.println(\"    \" + indexNumBytes + \" bytes\");\n    out.println(\"  terms:\");\n    out.println(\"    \" + totalTermCount + \" terms\");\n    out.println(\"    \" + totalTermBytes + \" bytes\" + (totalTermCount != 0 ? \" (\" + String.format(Locale.ROOT, \"%.1f\", ((double) totalTermBytes)\/totalTermCount) + \" bytes\/term)\" : \"\"));\n    out.println(\"  blocks:\");\n    out.println(\"    \" + totalBlockCount + \" blocks\");\n    out.println(\"    \" + termsOnlyBlockCount + \" terms-only blocks\");\n    out.println(\"    \" + subBlocksOnlyBlockCount + \" sub-block-only blocks\");\n    out.println(\"    \" + mixedBlockCount + \" mixed blocks\");\n    out.println(\"    \" + floorBlockCount + \" floor blocks\");\n    out.println(\"    \" + (totalBlockCount-floorSubBlockCount) + \" non-floor blocks\");\n    out.println(\"    \" + floorSubBlockCount + \" floor sub-blocks\");\n    out.println(\"    \" + totalBlockSuffixBytes + \" term suffix bytes\" + (totalBlockCount != 0 ? \" (\" + String.format(Locale.ROOT, \"%.1f\", ((double) totalBlockSuffixBytes)\/totalBlockCount) + \" suffix-bytes\/block)\" : \"\"));\n    out.println(\"    \" + totalBlockStatsBytes + \" term stats bytes\" + (totalBlockCount != 0 ? \" (\" + String.format(Locale.ROOT, \"%.1f\", ((double) totalBlockStatsBytes)\/totalBlockCount) + \" stats-bytes\/block)\" : \"\"));\n    out.println(\"    \" + totalBlockOtherBytes + \" other bytes\" + (totalBlockCount != 0 ? \" (\" + String.format(Locale.ROOT, \"%.1f\", ((double) totalBlockOtherBytes)\/totalBlockCount) + \" other-bytes\/block)\" : \"\"));\n    if (totalBlockCount != 0) {\n      out.println(\"    by prefix length:\");\n      int total = 0;\n      for(int prefix=0;prefix<blockCountByPrefixLen.length;prefix++) {\n        final int blockCount = blockCountByPrefixLen[prefix];\n        total += blockCount;\n        if (blockCount != 0) {\n          out.println(\"      \" + String.format(Locale.ROOT, \"%2d\", prefix) + \": \" + blockCount);\n        }\n      }\n      assert totalBlockCount == total;\n    }\n\n    try {\n      return bos.toString(IOUtils.UTF_8);\n    } catch (UnsupportedEncodingException bogus) {\n      throw new RuntimeException(bogus);\n    }\n  }\n}\n","lang_cluster":"Java","length":192,"code_uid":"2dde18177ad34c98bdb9b9800d78c036"}
{"diff_hunk":"@@ -9,6 +9,8 @@ import static net.sourceforge.pmd.lang.java.ast.AccessNode.Visibility.V_PRIVATE;\n import net.sourceforge.pmd.lang.java.ast.ASTAnyTypeDeclaration;\n import net.sourceforge.pmd.lang.java.ast.ASTClassOrInterfaceDeclaration;\n import net.sourceforge.pmd.lang.java.ast.ASTConstructorDeclaration;\n+import net.sourceforge.pmd.lang.java.ast.ASTMethodDeclaration;\n+import net.sourceforge.pmd.lang.java.ast.JModifier;\n import net.sourceforge.pmd.lang.java.rule.AbstractJavaRulechainRule;\n import net.sourceforge.pmd.lang.java.types.TypeTestUtil;\n ","old_code":"\/*\n * BSD-style license; for more info see http:\/\/pmd.sourceforge.net\/license.html\n *\/\n\npackage net.sourceforge.pmd.lang.java.rule.design;\n\nimport static net.sourceforge.pmd.lang.java.ast.AccessNode.Visibility.V_PRIVATE;\n\nimport net.sourceforge.pmd.lang.java.ast.ASTAnyTypeDeclaration;\nimport net.sourceforge.pmd.lang.java.ast.ASTClassOrInterfaceDeclaration;\nimport net.sourceforge.pmd.lang.java.ast.ASTConstructorDeclaration;\nimport net.sourceforge.pmd.lang.java.rule.AbstractJavaRulechainRule;\nimport net.sourceforge.pmd.lang.java.types.TypeTestUtil;\n\npublic class ClassWithOnlyPrivateConstructorsShouldBeFinalRule extends AbstractJavaRulechainRule {\n\n    public ClassWithOnlyPrivateConstructorsShouldBeFinalRule() {\n        super(ASTClassOrInterfaceDeclaration.class);\n    }\n\n    @Override\n    public Object visit(ASTClassOrInterfaceDeclaration node, Object data) {\n        if (node.isRegularClass()\n            && !node.isFinal()\n            && hasOnlyPrivateCtors(node)\n            && hasNoSubclasses(node)) {\n            addViolation(data, node);\n        }\n        return null;\n    }\n\n    private boolean hasNoSubclasses(ASTClassOrInterfaceDeclaration klass) {\n        return klass.getRoot()\n                    .descendants(ASTAnyTypeDeclaration.class)\n                    .crossFindBoundaries()\n                    .none(it -> doesExtend(it, klass));\n    }\n\n    private boolean doesExtend(ASTAnyTypeDeclaration sub, ASTClassOrInterfaceDeclaration superClass) {\n        return sub != superClass && TypeTestUtil.isA(superClass.getTypeMirror(), sub);\n    }\n\n    private boolean hasOnlyPrivateCtors(ASTClassOrInterfaceDeclaration node) {\n        return node.getDeclarations(ASTConstructorDeclaration.class).all(it -> it.getVisibility() == V_PRIVATE)\n            && (node.getVisibility() == V_PRIVATE \/\/ then the default ctor is private\n            || node.getDeclarations(ASTConstructorDeclaration.class).nonEmpty());\n    }\n\n}\n","lang_cluster":"Java","length":49,"code_uid":"29876ae36366453898a0c14ebf5798a3"}
{"diff_hunk":"@@ -42,22 +42,32 @@ class BoundZmqEventBus implements EventBus {\n   private final ZMQ.Socket xsub;\n   private final ExecutorService executor;\n \n+\n   BoundZmqEventBus(ZContext context, String publishConnection, String subscribeConnection) {\n     String address = new NetworkUtils().getHostAddress();\n     Addresses xpubAddr = deriveAddresses(address, publishConnection);\n     Addresses xsubAddr = deriveAddresses(address, subscribeConnection);\n \n+    Curve curve = new Curve();\n+    String[] serverKeys = curve.keypairZ85();\n+    String[] clientKeys = curve.keypairZ85();\n+\n     LOG.info(String.format(\"XPUB binding to %s, XSUB binding to %s\", xpubAddr, xsubAddr));\n \n     xpub = context.createSocket(SocketType.XPUB);\n     xpub.setIPv6(xpubAddr.isIPv6);\n     xpub.setImmediate(true);\n     xpub.bind(xpubAddr.bindTo);\n+    xpub.setCurvePublicKey(serverKeys[0].getBytes());\n+    xpub.setCurveSecretKey(serverKeys[1].getBytes());\n \n     xsub = context.createSocket(SocketType.XSUB);\n     xsub.setIPv6(xsubAddr.isIPv6);\n     xsub.setImmediate(true);\n     xsub.bind(xsubAddr.bindTo);\n+    xsub.setCurvePublicKey(clientKeys[0].getBytes());\n+    xsub.setCurveSecretKey(clientKeys[1].getBytes());\n+    xsub.setCurveServerKey(serverKeys[0].getBytes());\n \n     executor = Executors.newCachedThreadPool(r -> {\n       Thread thread = new Thread(r, \"Message Bus Proxy\");","old_code":"\/\/ Licensed to the Software Freedom Conservancy (SFC) under one\n\/\/ or more contributor license agreements.  See the NOTICE file\n\/\/ distributed with this work for additional information\n\/\/ regarding copyright ownership.  The SFC licenses this file\n\/\/ to you under the Apache License, Version 2.0 (the\n\/\/ \"License\"); you may not use this file except in compliance\n\/\/ with the License.  You may obtain a copy of the License at\n\/\/\n\/\/   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing,\n\/\/ software distributed under the License is distributed on an\n\/\/ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\/\/ KIND, either express or implied.  See the License for the\n\/\/ specific language governing permissions and limitations\n\/\/ under the License.\n\npackage org.openqa.selenium.events.zeromq;\n\nimport org.openqa.selenium.events.Event;\nimport org.openqa.selenium.events.EventBus;\nimport org.openqa.selenium.events.Type;\nimport org.openqa.selenium.net.NetworkUtils;\nimport org.zeromq.SocketType;\nimport org.zeromq.ZContext;\nimport org.zeromq.ZMQ;\n\nimport java.net.Inet6Address;\nimport java.net.InetAddress;\nimport java.net.UnknownHostException;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.function.Consumer;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\n\nclass BoundZmqEventBus implements EventBus {\n\n  private static final Logger LOG = Logger.getLogger(EventBus.class.getName());\n  private final UnboundZmqEventBus delegate;\n  private final ZMQ.Socket xpub;\n  private final ZMQ.Socket xsub;\n  private final ExecutorService executor;\n\n  BoundZmqEventBus(ZContext context, String publishConnection, String subscribeConnection) {\n    String address = new NetworkUtils().getHostAddress();\n    Addresses xpubAddr = deriveAddresses(address, publishConnection);\n    Addresses xsubAddr = deriveAddresses(address, subscribeConnection);\n\n    LOG.info(String.format(\"XPUB binding to %s, XSUB binding to %s\", xpubAddr, xsubAddr));\n\n    xpub = context.createSocket(SocketType.XPUB);\n    xpub.setIPv6(xpubAddr.isIPv6);\n    xpub.setImmediate(true);\n    xpub.bind(xpubAddr.bindTo);\n\n    xsub = context.createSocket(SocketType.XSUB);\n    xsub.setIPv6(xsubAddr.isIPv6);\n    xsub.setImmediate(true);\n    xsub.bind(xsubAddr.bindTo);\n\n    executor = Executors.newCachedThreadPool(r -> {\n      Thread thread = new Thread(r, \"Message Bus Proxy\");\n      thread.setDaemon(true);\n      return thread;\n    });\n    executor.submit(() -> ZMQ.proxy(xsub, xpub, null));\n\n    delegate = new UnboundZmqEventBus(context, xpubAddr.advertise, xsubAddr.advertise);\n\n    LOG.info(\"Event bus ready\");\n  }\n\n  @Override\n  public boolean isReady() {\n    return !executor.isShutdown();\n  }\n\n  @Override\n  public void addListener(Type type, Consumer<Event> onType) {\n    delegate.addListener(type, onType);\n  }\n\n  @Override\n  public void fire(Event event) {\n    delegate.fire(event);\n  }\n\n  @Override\n  public void close() {\n    delegate.close();\n    executor.shutdown();\n    xsub.close();\n    xpub.close();\n  }\n\n  private Addresses deriveAddresses(String host, String connection) {\n    if (connection.startsWith(\"inproc:\")) {\n      return new Addresses(connection, connection, false);\n    }\n\n    if (!connection.startsWith(\"tcp:\/\/\")) {\n      throw new IllegalArgumentException(\"Connection string must begin with inproc:\/\/ or tcp:\/\/\");\n    }\n\n    int length = \"tcp:\/\/\".length();\n    int colon = connection.indexOf(\":\", length);\n    if (colon == -1) {\n      throw new IllegalArgumentException(\"Unable to determine hostname from \" + connection);\n    }\n    String hostName = connection.substring(length, colon);\n\n    int port = Integer.parseInt(connection.substring(colon + 1));\n\n    if (!\"*\".equals(hostName)) {\n      host = hostName;\n    }\n\n    boolean isAddressIPv6 = false;\n    try {\n      if (InetAddress.getByName(host) instanceof Inet6Address ) {\n        isAddressIPv6 = true;\n        if (!host.startsWith(\"[\")) {\n          host = String.format(\"[%s]\", host);\n        }\n      }\n    } catch (UnknownHostException e) {\n      LOG.log(Level.WARNING, \"Could not determine if host address is IPv6 or IPv4\", e);\n    }\n\n    return new Addresses(\n        connection,\n        String.format(\"tcp:\/\/%s:%d\", host, port),\n        isAddressIPv6\n    );\n  }\n\n  private static class Addresses {\n    Addresses(String bindTo, String advertise, boolean isIPv6) {\n      this.bindTo = bindTo;\n      this.advertise = advertise;\n      this.isIPv6 = isIPv6;\n    }\n\n    String bindTo;\n    String advertise;\n    boolean isIPv6;\n\n    @Override\n    public String toString() {\n      return String.format(\"[binding to %s, advertising as %s]\", bindTo, advertise);\n    }\n  }\n}\n","lang_cluster":"Java","length":154,"code_uid":"3bf068c796c14db3ae5a02f0f65db7a4"}
{"diff_hunk":"@@ -102,4 +102,19 @@ public class Invoker implements InvocationHandler {\n \n     throw ExceptionFactory.convertConsumerException(response.getResult());\n   }\n+\n+  protected CompletableFuture<Object> completableFutureInvoke(Invocation invocation,\n+      SwaggerConsumerOperation consumerOperation) {\n+    CompletableFuture<Object> future = new CompletableFuture<>();\n+    InvokerUtils.reactiveInvoke(invocation, response -> {\n+      if (response.isSuccessed()) {\n+        Object result = consumerOperation.getResponseMapper().mapResponse(response);\n+        future.complete(result);\n+        return;\n+      }\n+\n+      future.completeExceptionally(response.getResult());\n+    });\n+    return future;\n+  }\n }","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage io.servicecomb.provider.pojo;\n\nimport java.lang.reflect.InvocationHandler;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Proxy;\n\nimport org.springframework.util.StringUtils;\n\nimport io.servicecomb.core.CseContext;\nimport io.servicecomb.core.Invocation;\nimport io.servicecomb.core.definition.MicroserviceMeta;\nimport io.servicecomb.core.definition.SchemaMeta;\nimport io.servicecomb.core.invocation.InvocationFactory;\nimport io.servicecomb.core.provider.consumer.InvokerUtils;\nimport io.servicecomb.core.provider.consumer.ReferenceConfig;\nimport io.servicecomb.core.provider.consumer.ReferenceConfigUtils;\nimport io.servicecomb.swagger.engine.SwaggerConsumer;\nimport io.servicecomb.swagger.engine.SwaggerConsumerOperation;\nimport io.servicecomb.swagger.invocation.Response;\nimport io.servicecomb.swagger.invocation.exception.ExceptionFactory;\n\npublic class Invoker implements InvocationHandler {\n  \/\/ \u539f\u59cb\u6570\u636e\n  private String microserviceName;\n\n  private String schemaId;\n\n  private Class<?> consumerIntf;\n\n  \/\/ \u751f\u6210\u7684\u6570\u636e\n  private SchemaMeta schemaMeta;\n\n  private ReferenceConfig referenceConfig;\n\n  private SwaggerConsumer swaggerConsumer;\n\n  @SuppressWarnings(\"unchecked\")\n  public static <T> T createProxy(String microserviceName, String schemaId, Class<?> consumerIntf) {\n    Invoker invoker = new Invoker(microserviceName, schemaId, consumerIntf);\n    return (T) Proxy.newProxyInstance(consumerIntf.getClassLoader(), new Class<?>[] {consumerIntf}, invoker);\n  }\n\n  public Invoker(String microserviceName, String schemaId, Class<?> consumerIntf) {\n    this.microserviceName = microserviceName;\n    this.schemaId = schemaId;\n    this.consumerIntf = consumerIntf;\n  }\n\n  protected void prepare() {\n    referenceConfig = ReferenceConfigUtils.getForInvoke(microserviceName);\n    MicroserviceMeta microserviceMeta = referenceConfig.getMicroserviceMeta();\n\n    if (StringUtils.isEmpty(schemaId)) {\n      \/\/ \u672a\u6307\u5b9aschemaId\uff0c\u770b\u770bconsumer\u63a5\u53e3\u662f\u5426\u7b49\u4e8e\u5951\u7ea6\u63a5\u53e3\n      schemaMeta = microserviceMeta.findSchemaMeta(consumerIntf);\n      if (schemaMeta == null) {\n        \/\/ \u5c1d\u8bd5\u7528consumer\u63a5\u53e3\u540d\u4f5c\u4e3aschemaId\n        schemaId = consumerIntf.getName();\n        schemaMeta = microserviceMeta.ensureFindSchemaMeta(schemaId);\n      }\n    } else {\n      schemaMeta = microserviceMeta.ensureFindSchemaMeta(schemaId);\n    }\n\n    this.swaggerConsumer = CseContext.getInstance().getSwaggerEnvironment().createConsumer(consumerIntf,\n        schemaMeta.getSwaggerIntf());\n  }\n\n  @Override\n  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n    if (swaggerConsumer == null) {\n      prepare();\n    }\n\n    Invocation invocation =\n        InvocationFactory.forConsumer(referenceConfig, schemaMeta, method.getName(), null);\n\n    SwaggerConsumerOperation consumerOperation = swaggerConsumer.findOperation(method.getName());\n    consumerOperation.getArgumentsMapper().toInvocation(args, invocation);\n\n    Response response = InvokerUtils.innerSyncInvoke(invocation);\n    if (response.isSuccessed()) {\n      return consumerOperation.getResponseMapper().mapResponse(response);\n    }\n\n    throw ExceptionFactory.convertConsumerException(response.getResult());\n  }\n}\n","lang_cluster":"Java","length":105,"code_uid":"f1dceef8094145f1b41ba4b7ac6182d3"}
{"diff_hunk":"@@ -7,9 +7,12 @@ package net.sourceforge.pmd.lang.ecmascript.rule;\n import net.sourceforge.pmd.Rule;\n import net.sourceforge.pmd.RuleContext;\n import net.sourceforge.pmd.RuleViolation;\n+import net.sourceforge.pmd.autofix.AutoFixableRuleViolation;\n+import net.sourceforge.pmd.autofix.RuleViolationFix;\n import net.sourceforge.pmd.lang.ast.Node;\n import net.sourceforge.pmd.lang.ecmascript.ast.EcmascriptNode;\n import net.sourceforge.pmd.lang.rule.AbstractRuleViolationFactory;\n+import net.sourceforge.pmd.lang.rule.AutoFixableParametricRuleViolation;\n import net.sourceforge.pmd.lang.rule.ParametricRuleViolation;\n \n public final class EcmascriptRuleViolationFactory extends AbstractRuleViolationFactory {","old_code":"\/**\n * BSD-style license; for more info see http:\/\/pmd.sourceforge.net\/license.html\n *\/\n\npackage net.sourceforge.pmd.lang.ecmascript.rule;\n\nimport net.sourceforge.pmd.Rule;\nimport net.sourceforge.pmd.RuleContext;\nimport net.sourceforge.pmd.RuleViolation;\nimport net.sourceforge.pmd.lang.ast.Node;\nimport net.sourceforge.pmd.lang.ecmascript.ast.EcmascriptNode;\nimport net.sourceforge.pmd.lang.rule.AbstractRuleViolationFactory;\nimport net.sourceforge.pmd.lang.rule.ParametricRuleViolation;\n\npublic final class EcmascriptRuleViolationFactory extends AbstractRuleViolationFactory {\n\n    public static final EcmascriptRuleViolationFactory INSTANCE = new EcmascriptRuleViolationFactory();\n\n    private EcmascriptRuleViolationFactory() {\n    }\n\n    @SuppressWarnings(\"rawtypes\")\n    @Override\n    protected RuleViolation createRuleViolation(Rule rule, RuleContext ruleContext, Node node, String message) {\n        return new ParametricRuleViolation<>(rule, ruleContext, (EcmascriptNode) node, message);\n    }\n\n    protected RuleViolation createRuleViolation(Rule rule, RuleContext ruleContext, Node node, String message,\n            int beginLine, int endLine) {\n        return null; \/\/ FIXME\n    }\n}\n","lang_cluster":"Java","length":32,"code_uid":"fa75d8e1902b49809f76e92fd1caf769"}
{"diff_hunk":"@@ -1,6 +1,7 @@\n package com.fsck.k9.mail.ssl;\n \n import com.fsck.k9.mail.MessagingException;\n+import com.fsck.k9.mail.ProxySettings;\n \n import java.io.IOException;\n import java.net.Socket;","old_code":"package com.fsck.k9.mail.ssl;\n\nimport com.fsck.k9.mail.MessagingException;\n\nimport java.io.IOException;\nimport java.net.Socket;\nimport java.security.KeyManagementException;\nimport java.security.NoSuchAlgorithmException;\n\npublic interface TrustedSocketFactory {\n    Socket createSocket(Socket socket, String host, int port, String clientCertificateAlias)\n            throws NoSuchAlgorithmException, KeyManagementException, MessagingException, IOException;\n}\n","lang_cluster":"Java","length":13,"code_uid":"247f23381b5e42e0b8c955066f50aed3"}
{"diff_hunk":"@@ -102,25 +102,20 @@ public final class BaselineVersions implements Plugin<Project> {\n             if (project.file(\"versions.props\").exists()) {\n                 extension.propertiesFile(ImmutableMap.of(\"file\", project.file(\"versions.props\")));\n             }\n-        } else {\n-            TaskProvider<CheckBomConflictTask> checkBomConflict = project.getTasks().register(\n-                    \"checkBomConflict\", CheckBomConflictTask.class, task -> task.setPropsFile(rootVersionsPropsFile));\n-            TaskProvider<CheckNoUnusedPinTask> checkNoUnusedPin = project.getTasks().register(\n-                    \"checkNoUnusedPin\", CheckNoUnusedPinTask.class, task -> task.setPropsFile(rootVersionsPropsFile));\n-\n-            project.getTasks().register(\"checkVersionsProps\", CheckVersionsPropsTask.class, task -> {\n-                task.dependsOn(checkBomConflict, checkNoUnusedPin);\n-                \/\/ If we just run checkVersionsProps --fix, we want to propagate its option to its dependent tasks\n-                checkBomConflict.get().setShouldFix(task.getShouldFix());\n-                checkNoUnusedPin.get().setShouldFix(task.getShouldFix());\n-            });\n-            \/\/ If we run with --parallel --fix, both checkNoUnusedPin and checkBomConflict will try to overwrite the\n-            \/\/ versions file at the same time. Therefore, make sure checkBomConflict runs first.\n-            checkNoUnusedPin.configure(task -> task.mustRunAfter(checkBomConflict));\n-\n-            project.getPluginManager().apply(BasePlugin.class);\n-            project.getTasks().named(\"check\").configure(task -> task.dependsOn(\"checkVersionsProps\"));\n         }\n+\n+        TaskProvider<CheckBomConflictTask> checkBomConflict = project.getTasks().register(\n+                \"checkBomConflict\", CheckBomConflictTask.class, task -> {\n+                    task.setPropsFile(rootVersionsPropsFile);\n+                    \/\/ If we run with --parallel --fix, both checkNoUnusedPin and checkBomConflict will try to overwrite\n+                    \/\/ the versions file at the same time. Therefore, make sure checkNoUnusedPin runs first.\n+                    task.mustRunAfter(checkNoUnusedPin);\n+                });\n+\n+        checkVersionsProps.configure(task -> {\n+            task.dependsOn(checkBomConflict);\n+            checkBomConflict.get().setShouldFix(task.getShouldFix());\n+        });\n     }\n \n     private static File rootVersionsPropsFile(Project project) {","old_code":"\/*\n * (c) Copyright 2018 Palantir Technologies Inc. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage com.palantir.baseline.plugins.versions;\n\nimport com.google.common.base.CharMatcher;\nimport com.google.common.collect.ImmutableMap;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.util.Comparator;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport netflix.nebula.dependency.recommender.DependencyRecommendationsPlugin;\nimport netflix.nebula.dependency.recommender.RecommendationStrategies;\nimport netflix.nebula.dependency.recommender.provider.FuzzyVersionResolver;\nimport netflix.nebula.dependency.recommender.provider.RecommendationProviderContainer;\nimport org.gradle.api.Plugin;\nimport org.gradle.api.Project;\nimport org.gradle.api.artifacts.Configuration;\nimport org.gradle.api.artifacts.component.ModuleComponentIdentifier;\nimport org.gradle.api.artifacts.result.ResolutionResult;\nimport org.gradle.api.logging.Logger;\nimport org.gradle.api.logging.Logging;\nimport org.gradle.api.plugins.BasePlugin;\nimport org.gradle.api.tasks.TaskProvider;\n\n\/**\n * Transitively applies nebula.dependency recommender to replace the following common gradle snippet.\n *\n * <pre>\n * buildscript {\n *     dependencies {\n *         classpath 'com.netflix.nebula:nebula-dependency-recommender:5.2.0'\n *     }\n * }\n *\n * allprojects {\n *     apply plugin: 'nebula.dependency-recommender'\n *\n *     dependencyRecommendations {\n *         strategy OverrideTransitives\n *         propertiesFile file: project.rootProject.file('versions.props')\n *         if (file('versions.props').exists()) {\n *             propertiesFile file: project.file('versions.props')\n *         }\n *     }\n * }\n * <\/pre>\n *\/\npublic final class BaselineVersions implements Plugin<Project> {\n\n    private static final Logger log = Logging.getLogger(BaselineVersions.class);\n    static final String GROUP = \"com.palantir.baseline-versions\";\n    \/**\n     * System property which, when true, instructs {@code nebula.dependency-recommender} to only support sourcing\n     * constraints from a BOM. In that case, nebula doesn't support sourcing recommendations from\n     * {@code versions.props} anymore.\n     *\/\n    public static final boolean IS_CORE_BOM_ENABLED = Boolean.getBoolean(\"nebula.features.coreBomSupport\");\n    public static final String DISABLE_PROPERTY = \"com.palantir.baseline-versions.disable\";\n\n    @Override\n    public void apply(Project project) {\n        if (project.hasProperty(DISABLE_PROPERTY)) {\n            log.info(\"Not configuring com.palantir.baseline-versions because \" + DISABLE_PROPERTY + \" was set\");\n            return;\n        }\n        \/\/ apply plugin: \"nebula.dependency-recommender\"\n        project.getPluginManager().apply(DependencyRecommendationsPlugin.class);\n\n        if (IS_CORE_BOM_ENABLED) {\n            log.info(\"Not configuring nebula.dependency-recommender because coreBomSupport is enabled\");\n            return;\n        }\n\n        \/\/ get dependencyRecommendations extension\n        RecommendationProviderContainer extension = project\n                .getExtensions()\n                .getByType(RecommendationProviderContainer.class);\n\n        extension.setStrategy(RecommendationStrategies.OverrideTransitives); \/\/ default is 'ConflictResolved'\n\n        File rootVersionsPropsFile = rootVersionsPropsFile(project);\n        extension.propertiesFile(ImmutableMap.of(\"file\", rootVersionsPropsFile));\n\n        if (project != project.getRootProject()) {\n            \/\/ allow nested projects to specify their own nested versions.props file\n            if (project.file(\"versions.props\").exists()) {\n                extension.propertiesFile(ImmutableMap.of(\"file\", project.file(\"versions.props\")));\n            }\n        } else {\n            TaskProvider<CheckBomConflictTask> checkBomConflict = project.getTasks().register(\n                    \"checkBomConflict\", CheckBomConflictTask.class, task -> task.setPropsFile(rootVersionsPropsFile));\n            TaskProvider<CheckNoUnusedPinTask> checkNoUnusedPin = project.getTasks().register(\n                    \"checkNoUnusedPin\", CheckNoUnusedPinTask.class, task -> task.setPropsFile(rootVersionsPropsFile));\n\n            project.getTasks().register(\"checkVersionsProps\", CheckVersionsPropsTask.class, task -> {\n                task.dependsOn(checkBomConflict, checkNoUnusedPin);\n                \/\/ If we just run checkVersionsProps --fix, we want to propagate its option to its dependent tasks\n                checkBomConflict.get().setShouldFix(task.getShouldFix());\n                checkNoUnusedPin.get().setShouldFix(task.getShouldFix());\n            });\n            \/\/ If we run with --parallel --fix, both checkNoUnusedPin and checkBomConflict will try to overwrite the\n            \/\/ versions file at the same time. Therefore, make sure checkBomConflict runs first.\n            checkNoUnusedPin.configure(task -> task.mustRunAfter(checkBomConflict));\n\n            project.getPluginManager().apply(BasePlugin.class);\n            project.getTasks().named(\"check\").configure(task -> task.dependsOn(\"checkVersionsProps\"));\n        }\n    }\n\n    private static File rootVersionsPropsFile(Project project) {\n        File file = project.getRootProject().file(\"versions.props\");\n        if (!file.canRead()) {\n            try {\n                log.info(\"Could not find 'versions.props' file, creating...\");\n                Files.createFile(file.toPath());\n            } catch (IOException e) {\n                log.warn(\"Unable to create empty versions.props file, please create this manually\", e);\n            }\n        }\n        return file;\n    }\n\n    static Set<String> getAllProjectsResolvedModuleIdentifiers(Project project) {\n        return project.getRootProject().getAllprojects()\n                .stream()\n                .flatMap(project2 -> getResolvedModuleIdentifiers(project2).stream())\n                .collect(Collectors.toSet());\n    }\n\n    static Set<String> getResolvedModuleIdentifiers(Project project) {\n        return project.getConfigurations().stream()\n                .filter(Configuration::isCanBeResolved)\n                .flatMap(configuration -> {\n                    try {\n                        ResolutionResult resolutionResult = configuration.getIncoming().getResolutionResult();\n                        return resolutionResult\n                                .getAllComponents()\n                                .stream()\n                                .map(result -> result.getId())\n                                .filter(cid -> !cid.equals(resolutionResult.getRoot().getId())) \/\/ remove the project\n                                .filter(cid -> cid instanceof ModuleComponentIdentifier)\n                                .map(mcid -> ((ModuleComponentIdentifier) mcid).getModuleIdentifier())\n                                .map(mid -> mid.getGroup() + \":\" + mid.getName());\n                    } catch (Exception e) {\n                        throw new RuntimeException(String.format(\"Error during resolution of the dependency graph of \"\n                                + \"configuration %s\", configuration), e);\n                    }\n                })\n                .collect(Collectors.toSet());\n    }\n\n    \/**\n     * Compares {@code versions.props} matchers by weight. Higher weight means the matcher is more specific.\n     * For example,\n     * <pre>\n     *     com.google.guava:guava\n     * <\/pre>\n     * is more specific than\n     * <pre>\n     *     com.google.guava:*\n     * <\/pre>\n     *\/\n    static final Comparator<String> VERSIONS_PROPS_ENTRY_SPECIFIC_COMPARATOR =\n            Comparator.comparing(BaselineVersions::versionsPropsMatcherWeight);\n\n    \/**\n     * The weight of a matcher in {@code versions.props} according to the disambiguation logic defined in\n     * {@code nebula.dependency-recommender}.\n     *\n     * This matches the logic in {@link FuzzyVersionResolver}.\n     *\/\n    private static int versionsPropsMatcherWeight(String matcher) {\n        return CharMatcher.isNot('*').countIn(matcher);\n    }\n}\n","lang_cluster":"Java","length":191,"code_uid":"8fbcb21fb34a4850b2b6fde16436722d"}
{"diff_hunk":"@@ -14,6 +14,8 @@ import org.junit.jupiter.api.Test;\n import java.io.IOException;\n \n import static com.github.javaparser.ast.Modifier.Keyword.STATIC;\n+import static com.github.javaparser.utils.TestUtils.assertEqualsNoEol;\n+import static com.github.javaparser.utils.Utils.EOL;\n \n \/**\n  * These tests are more \"high level\" than the ones in LexicalPreservingPrinterTest.","old_code":"package com.github.javaparser.printer.lexicalpreservation;\n\nimport com.github.javaparser.ast.body.FieldDeclaration;\nimport com.github.javaparser.ast.body.MethodDeclaration;\nimport com.github.javaparser.ast.body.VariableDeclarator;\nimport com.github.javaparser.ast.expr.Expression;\nimport com.github.javaparser.ast.expr.NameExpr;\nimport com.github.javaparser.ast.expr.NullLiteralExpr;\nimport com.github.javaparser.ast.stmt.ReturnStmt;\nimport com.github.javaparser.ast.type.ArrayType;\nimport com.github.javaparser.ast.type.PrimitiveType;\nimport org.junit.jupiter.api.Test;\n\nimport java.io.IOException;\n\nimport static com.github.javaparser.ast.Modifier.Keyword.STATIC;\n\n\/**\n * These tests are more \"high level\" than the ones in LexicalPreservingPrinterTest.\n * The idea is to perform some transformations on the code, print it back and see if the generated code\n * is the expected one. We do not care about the internal state of LexicalPreservingPrinter, just the visible result.\n *\/\nclass TransformationsTest extends  AbstractLexicalPreservingTest {\n\n    @Test\n    void unchangedSimpleClasses() throws IOException {\n        assertUnchanged(\"Example1\");\n        assertUnchanged(\"Example2\");\n    }\n\n    @Test\n    void unchangedComplexFile() throws IOException {\n        assertUnchanged(\"Example4\");\n    }\n\n    @Test\n    void example1() throws IOException {\n        considerExample(\"Example1_original\");\n        cu.getClassByName(\"A\").get().getFieldByName(\"a\").get().setModifiers(STATIC);\n        assertTransformed(\"Example1\", cu);\n    }\n\n    @Test\n    void example2() throws IOException {\n        considerExample(\"Example2_original\");\n        cu.getClassByName(\"A\").get().getFieldByName(\"a\").get().getVariable(0).setInitializer(\"10\");\n        assertTransformed(\"Example2\", cu);\n    }\n\n    @Test\n    void example3() throws IOException {\n        considerExample(\"Example3_original\");\n        cu.getClassByName(\"A\").get().getFieldByName(\"a\").get().getVariable(0).setInitializer((Expression) null);\n        assertTransformed(\"Example3\", cu);\n    }\n\n    @Test\n    void example5() throws IOException {\n        considerExample(\"Example5_original\");\n        cu.getClassByName(\"A\").get().getFieldByName(\"a\").get().getVariable(0).setInitializer(new NullLiteralExpr());\n        assertTransformed(\"Example5\", cu);\n    }\n\n    @Test\n    void example6() throws IOException {\n        considerExample(\"Example6_original\");\n        cu.getClassByName(\"A\").get().getFieldByName(\"a\").get().getVariable(0).setName(\"someOtherName\");\n        assertTransformed(\"Example6\", cu);\n    }\n\n    @Test\n    void example7() throws IOException {\n        considerExample(\"Example7_original\");\n        cu.getClassByName(\"A\").get().getFieldByName(\"a\").get().getVariable(0).setType(new ArrayType(PrimitiveType.intType()));\n        assertTransformed(\"Example7\", cu);\n    }\n\n    @Test\n    void example8() throws IOException {\n        considerExample(\"Example8_original\");\n        FieldDeclaration fd = cu.getClassByName(\"A\").get().getMember(0).asFieldDeclaration();\n        fd.addVariable(new VariableDeclarator(PrimitiveType.intType(), \"b\"));\n        assertTransformed(\"Example8\", cu);\n    }\n\n    @Test\n    void example9() throws IOException {\n        considerExample(\"Example9_original\");\n        FieldDeclaration fd = cu.getClassByName(\"A\").get().getMember(0).asFieldDeclaration();\n        fd.addVariable(new VariableDeclarator(new ArrayType(PrimitiveType.intType()), \"b\"));\n        assertTransformed(\"Example9\", cu);\n    }\n\n    @Test\n    void example10() throws IOException {\n        considerExample(\"Example10_original\");\n        cu.getClassByName(\"A\").get().getMembers().remove(0);\n        assertTransformed(\"Example10\", cu);\n    }\n\n    @Test\n    void exampleParam1() throws IOException {\n        considerExample(\"Example_param1_original\");\n        MethodDeclaration md = cu.getClassByName(\"A\").get().getMember(0).asMethodDeclaration();\n        md.addParameter(\"int\", \"p1\");\n        assertTransformed(\"Example_param1\", cu);\n    }\n\n    @Test\n    void exampleParam2() throws IOException {\n        considerExample(\"Example_param1_original\");\n        MethodDeclaration md = cu.getClassByName(\"A\").get().getMember(0).asMethodDeclaration();\n        md.addParameter(new ArrayType(PrimitiveType.intType()), \"p1\");\n        md.addParameter(\"char\", \"p2\");\n        assertTransformed(\"Example_param2\", cu);\n    }\n\n    @Test\n    void exampleParam3() throws IOException {\n        considerExample(\"Example_param3_original\");\n        MethodDeclaration md = cu.getClassByName(\"A\").get().getMember(0).asMethodDeclaration();\n        md.getParameters().remove(0);\n        assertTransformed(\"Example_param3\", cu);\n    }\n\n    @Test\n    void exampleParam4() throws IOException {\n        considerExample(\"Example_param3_original\");\n        MethodDeclaration md = cu.getClassByName(\"A\").get().getMember(0).asMethodDeclaration();\n        md.getParameters().remove(1);\n        assertTransformed(\"Example_param4\", cu);\n    }\n\n    @Test\n    void exampleParam5() throws IOException {\n        considerExample(\"Example_param3_original\");\n        MethodDeclaration md = cu.getClassByName(\"A\").get().getMember(0).asMethodDeclaration();\n        md.setType(PrimitiveType.intType());\n        assertTransformed(\"Example_param5b\", cu);\n        md.getBody().get().getStatements().add(new ReturnStmt(new NameExpr(\"p1\")));\n        assertTransformed(\"Example_param5\", cu);\n    }\n}\n","lang_cluster":"Java","length":143,"code_uid":"3ef6f09ba7c74b3ebafdcc207a95190d"}
{"diff_hunk":"@@ -31,17 +31,25 @@ import org.springframework.context.annotation.Bean;\n import org.springframework.context.annotation.Configuration;\n import org.springframework.core.io.support.PathMatchingResourcePatternResolver;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.net.MalformedURLException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n @Configuration\n public class RepositoryConfiguration {\n-    @Value(\"${application.repository.configuration}\")\n-    private String repositoryConfiguration;\n-\n     @Value(\"${application.repository.forceIncompatibleOperatingSystems:false}\")\n     private boolean enforceUncompatibleOperatingSystems;\n \n     @Value(\"${application.user.cache}\")\n     private String cacheDirectoryPath;\n \n+    @Value(\"${application.repository.list}\")\n+    private String repositoryListPath;\n+\n     @Autowired\n     private MultithreadingConfiguration multithreadingConfiguration;\n ","old_code":"\/*\n * Copyright (C) 2015-2017 P\u00c2RIS Quentin\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n *\/\n\npackage org.phoenicis.repository;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.phoenicis.repository.repositoryTypes.BackgroundRepository;\nimport org.phoenicis.repository.repositoryTypes.ClasspathRepository;\nimport org.phoenicis.repository.repositoryTypes.LocalRepository;\nimport org.phoenicis.multithreading.MultithreadingConfiguration;\nimport org.phoenicis.tools.ToolsConfiguration;\nimport org.phoenicis.tools.files.FileUtilities;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.core.io.support.PathMatchingResourcePatternResolver;\n\n@Configuration\npublic class RepositoryConfiguration {\n    @Value(\"${application.repository.configuration}\")\n    private String repositoryConfiguration;\n\n    @Value(\"${application.repository.forceIncompatibleOperatingSystems:false}\")\n    private boolean enforceUncompatibleOperatingSystems;\n\n    @Value(\"${application.user.cache}\")\n    private String cacheDirectoryPath;\n\n    @Autowired\n    private MultithreadingConfiguration multithreadingConfiguration;\n\n    @Autowired\n    private ToolsConfiguration toolsConfiguration;\n\n    @Autowired\n    private FileUtilities fileUtilities;\n\n    @Bean\n    public RepositoryManager repositoryManager() {\n        RepositoryManager repositoryManager = new DefaultRepositoryManager(\n                multithreadingConfiguration.appsExecutorService(), enforceUncompatibleOperatingSystems,\n                toolsConfiguration, cacheDirectoryPath, fileUtilities, localRepositoryFactory(),\n                classPathRepositoryFactory(), backgroundRepositoryFactory());\n\n        \/\/ set initial repositories\n        repositoryManager.addRepositories(this.repositoryConfiguration.split(\";\"));\n\n        return repositoryManager;\n    }\n\n    @Bean\n    ClasspathRepository.Factory classPathRepositoryFactory() {\n        return new ClasspathRepository.Factory(objectMapper(), new PathMatchingResourcePatternResolver());\n    }\n\n    @Bean\n    LocalRepository.Factory localRepositoryFactory() {\n        return new LocalRepository.Factory(objectMapper());\n    }\n\n    @Bean\n    BackgroundRepository.Factory backgroundRepositoryFactory() {\n        return new BackgroundRepository.Factory();\n    }\n\n    @Bean\n    ObjectMapper objectMapper() {\n        return new ObjectMapper();\n    }\n\n}\n","lang_cluster":"Java","length":87,"code_uid":"286cf92e443840949af7c038a4cbb3cf"}
{"diff_hunk":"@@ -24,6 +24,7 @@ import java.io.File;\n import java.io.IOException;\n import java.time.LocalDateTime;\n import java.time.OffsetDateTime;\n+import java.util.Collections;\n import java.util.List;\n import java.util.TimeZone;\n import org.apache.iceberg.Files;","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.data.orc;\n\nimport com.google.common.collect.Lists;\nimport java.io.File;\nimport java.io.IOException;\nimport java.time.LocalDateTime;\nimport java.time.OffsetDateTime;\nimport java.util.List;\nimport java.util.TimeZone;\nimport org.apache.iceberg.Files;\nimport org.apache.iceberg.Schema;\nimport org.apache.iceberg.data.DataTest;\nimport org.apache.iceberg.data.DataTestHelpers;\nimport org.apache.iceberg.data.GenericRecord;\nimport org.apache.iceberg.data.RandomGenericData;\nimport org.apache.iceberg.data.Record;\nimport org.apache.iceberg.io.CloseableIterable;\nimport org.apache.iceberg.io.FileAppender;\nimport org.apache.iceberg.orc.ORC;\nimport org.apache.iceberg.types.Types;\nimport org.junit.Assert;\nimport org.junit.Test;\n\nimport static org.apache.iceberg.types.Types.NestedField.required;\n\npublic class TestGenericData extends DataTest {\n\n  @Override\n  protected void writeAndValidate(Schema schema) throws IOException {\n    List<Record> expected = RandomGenericData.generate(schema, 100, 0L);\n\n    File testFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", testFile.delete());\n\n    try (FileAppender<Record> writer = ORC.write(Files.localOutput(testFile))\n        .schema(schema)\n        .createWriterFunc(GenericOrcWriter::buildWriter)\n        .build()) {\n      for (Record rec : expected) {\n        writer.add(rec);\n      }\n    }\n\n    List<Record> rows;\n    try (CloseableIterable<Record> reader = ORC.read(Files.localInput(testFile))\n        .project(schema)\n        .createReaderFunc(fileSchema -> GenericOrcReader.buildReader(schema, fileSchema))\n        .build()) {\n      rows = Lists.newArrayList(reader);\n    }\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      DataTestHelpers.assertEquals(schema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n\n  @Test\n  public void writeAndValidateTimestamps() throws IOException {\n    Schema timestampSchema = new Schema(\n        required(1, \"tsTzCol\", Types.TimestampType.withZone()),\n        required(2, \"tsCol\", Types.TimestampType.withoutZone())\n    );\n\n    \/\/ Write using America\/New_York timezone\n    TimeZone.setDefault(TimeZone.getTimeZone(\"America\/New_York\"));\n    GenericRecord record1 = GenericRecord.create(timestampSchema);\n    record1.setField(\"tsTzCol\", OffsetDateTime.parse(\"2017-01-16T17:10:34-08:00\"));\n    record1.setField(\"tsCol\", LocalDateTime.parse(\"1970-01-01T00:01:00\"));\n    GenericRecord record2 = GenericRecord.create(timestampSchema);\n    record2.setField(\"tsTzCol\", OffsetDateTime.parse(\"2017-05-16T17:10:34-08:00\"));\n    record2.setField(\"tsCol\", LocalDateTime.parse(\"1970-05-01T00:01:00\"));\n    GenericRecord record3 = GenericRecord.create(timestampSchema);\n    record3.setField(\"tsTzCol\", OffsetDateTime.parse(\"1935-01-16T17:10:34-08:00\"));\n    record3.setField(\"tsCol\", LocalDateTime.parse(\"1935-01-01T00:01:00\"));\n    GenericRecord record4 = GenericRecord.create(timestampSchema);\n    record4.setField(\"tsTzCol\", OffsetDateTime.parse(\"1935-05-16T17:10:34-08:00\"));\n    record4.setField(\"tsCol\", LocalDateTime.parse(\"1935-05-01T00:01:00\"));\n\n    File testFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", testFile.delete());\n\n    try (FileAppender<Record> writer = ORC.write(Files.localOutput(testFile))\n        .schema(timestampSchema)\n        .createWriterFunc(GenericOrcWriter::buildWriter)\n        .build()) {\n      writer.add(record1);\n      writer.add(record2);\n      writer.add(record3);\n      writer.add(record4);\n    }\n\n    \/\/ Read using Asia\/Kolkata timezone\n    TimeZone.setDefault(TimeZone.getTimeZone(\"Asia\/Kolkata\"));\n    List<Record> rows;\n    try (CloseableIterable<Record> reader = ORC.read(Files.localInput(testFile))\n        .project(timestampSchema)\n        .createReaderFunc(fileSchema -> GenericOrcReader.buildReader(timestampSchema, fileSchema))\n        .build()) {\n      rows = Lists.newArrayList(reader);\n    }\n\n    Assert.assertEquals(OffsetDateTime.parse(\"2017-01-17T01:10:34Z\"), rows.get(0).getField(\"tsTzCol\"));\n    Assert.assertEquals(LocalDateTime.parse(\"1970-01-01T00:01:00\"), rows.get(0).getField(\"tsCol\"));\n    Assert.assertEquals(OffsetDateTime.parse(\"2017-05-17T01:10:34Z\"), rows.get(1).getField(\"tsTzCol\"));\n    Assert.assertEquals(LocalDateTime.parse(\"1970-05-01T00:01:00\"), rows.get(1).getField(\"tsCol\"));\n    Assert.assertEquals(OffsetDateTime.parse(\"1935-01-17T01:10:34Z\"), rows.get(2).getField(\"tsTzCol\"));\n    Assert.assertEquals(LocalDateTime.parse(\"1935-01-01T00:01:00\"), rows.get(2).getField(\"tsCol\"));\n    Assert.assertEquals(OffsetDateTime.parse(\"1935-05-17T01:10:34Z\"), rows.get(3).getField(\"tsTzCol\"));\n    Assert.assertEquals(LocalDateTime.parse(\"1935-05-01T00:01:00\"), rows.get(3).getField(\"tsCol\"));\n  }\n}\n","lang_cluster":"Java","length":130,"code_uid":"8c76f96b73d44388aaa67308d73c8738"}
{"diff_hunk":"@@ -57,6 +57,9 @@ public class AzkabanCommonModule extends AbstractModule {\n     bind(ProjectLoader.class).to(JdbcProjectLoader.class).in(Scopes.SINGLETON);\n     bind(Props.class).toInstance(props);\n     bind(Storage.class).to(resolveStorageClassType()).in(Scopes.SINGLETON);\n+    bind(AzDBOperator.class).to(AzDBOperatorImpl.class).in(Scopes.SINGLETON);\n+    \/\/todo kunkun-tang : Consider both H2 DataSource and MysqlDatasource case.\n+    bind(AzkabanDataSource.class).toInstance(dataSource);\n   }\n \n   public Class<? extends Storage> resolveStorageClassType() {","old_code":"\/*\n * Copyright 2017 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n *\n *\/\npackage azkaban;\n\nimport azkaban.project.JdbcProjectLoader;\nimport azkaban.project.ProjectLoader;\nimport azkaban.spi.Storage;\nimport azkaban.spi.StorageException;\nimport azkaban.storage.LocalStorage;\nimport azkaban.storage.StorageConfig;\nimport azkaban.storage.StorageImplementationType;\nimport azkaban.utils.Props;\nimport com.google.inject.AbstractModule;\nimport com.google.inject.Inject;\nimport com.google.inject.Provides;\nimport com.google.inject.Scopes;\nimport com.google.inject.Singleton;\nimport java.io.File;\n\nimport static azkaban.storage.StorageImplementationType.*;\n\n\npublic class AzkabanCommonModule extends AbstractModule {\n  private final Props props;\n  \/**\n   * Storage Implementation\n   * This can be any of the {@link StorageImplementationType} values in which case {@link StorageFactory} will create\n   * the appropriate storage instance. Or one can feed in a custom implementation class using the full qualified\n   * path required by a classloader.\n   *\n   * examples: LOCAL, DATABASE, azkaban.storage.MyFavStorage\n   *\n   *\/\n  private final String storageImplementation;\n\n  public AzkabanCommonModule(Props props) {\n    this.props = props;\n    this.storageImplementation = props.getString(Constants.ConfigurationKeys.AZKABAN_STORAGE_TYPE, LOCAL.name());\n  }\n\n  @Override\n  protected void configure() {\n    bind(ProjectLoader.class).to(JdbcProjectLoader.class).in(Scopes.SINGLETON);\n    bind(Props.class).toInstance(props);\n    bind(Storage.class).to(resolveStorageClassType()).in(Scopes.SINGLETON);\n  }\n\n  public Class<? extends Storage> resolveStorageClassType() {\n    final StorageImplementationType type = StorageImplementationType.from(storageImplementation);\n    if (type != null) {\n      return type.getImplementationClass();\n    } else {\n      return loadCustomStorageClass(storageImplementation);\n    }\n  }\n\n  private Class<? extends Storage> loadCustomStorageClass(String storageImplementation) {\n    try {\n      return (Class<? extends Storage>) Class.forName(storageImplementation);\n    } catch (ClassNotFoundException e) {\n      throw new StorageException(e);\n    }\n  }\n\n  @Inject\n  public @Provides\n  LocalStorage createLocalStorage(StorageConfig config) {\n    return new LocalStorage(new File(config.getBaseDirectoryPath()));\n  }\n}\n","lang_cluster":"Java","length":84,"code_uid":"875bdcae7a1b40a2b56e4e108663ffc7"}
{"diff_hunk":"@@ -89,11 +89,11 @@ public class PkiKeyStoreConfiguration {\n \n     private String keyStoreType = DEFAULT_KEYSTORE_TYPE;\n     private Path keyStorePath;\n-    private Supplier<String> keyStorePasswordSupplier;\n+    private Path keyStorePasswordPath;\n     private String certificateAlias = DEFAULT_CERTIFICATE_ALIAS;\n     private String trustStoreType = DEFAULT_KEYSTORE_TYPE;\n     private Path trustStorePath;\n-    private Supplier<String> trustStorePasswordSupplier;\n+    private Path trustStorePasswordPath;\n     private Path crlFilePath;\n \n     public Builder() {}","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.pki.config;\n\nimport static java.util.Objects.requireNonNull;\n\nimport java.nio.file.Path;\nimport java.util.Optional;\nimport java.util.function.Supplier;\n\npublic class PkiKeyStoreConfiguration {\n\n  public static String DEFAULT_KEYSTORE_TYPE = \"PKCS12\";\n  public static String DEFAULT_CERTIFICATE_ALIAS = \"validator\";\n\n  private final String keyStoreType;\n  private final Path keyStorePath;\n  private final Supplier<String> keyStorePasswordSupplier;\n  private final String certificateAlias;\n  private final String trustStoreType;\n  private final Path trustStorePath;\n  private final Supplier<String> trustStorePasswordSupplier;\n  private final Optional<Path> crlFilePath;\n\n  public PkiKeyStoreConfiguration(\n      final String keyStoreType,\n      final Path keyStorePath,\n      final Supplier<String> keyStorePasswordSupplier,\n      final String certificateAlias,\n      final String trustStoreType,\n      final Path trustStorePath,\n      final Supplier<String> trustStorePasswordSupplier,\n      final Optional<Path> crlFilePath) {\n    this.keyStoreType = keyStoreType;\n    this.keyStorePath = keyStorePath;\n    this.keyStorePasswordSupplier = keyStorePasswordSupplier;\n    this.certificateAlias = certificateAlias;\n    this.trustStoreType = trustStoreType;\n    this.trustStorePath = trustStorePath;\n    this.trustStorePasswordSupplier = trustStorePasswordSupplier;\n    this.crlFilePath = crlFilePath;\n  }\n\n  public String getKeyStoreType() {\n    return keyStoreType;\n  }\n\n  public Path getKeyStorePath() {\n    return keyStorePath;\n  }\n\n  public String getKeyStorePassword() {\n    return null == keyStorePasswordSupplier ? null : keyStorePasswordSupplier.get();\n  }\n\n  public String getCertificateAlias() {\n    return certificateAlias;\n  }\n\n  public String getTrustStoreType() {\n    return trustStoreType;\n  }\n\n  public Path getTrustStorePath() {\n    return trustStorePath;\n  }\n\n  public String getTrustStorePassword() {\n    return trustStorePasswordSupplier.get();\n  }\n\n  public Optional<Path> getCrlFilePath() {\n    return crlFilePath;\n  }\n\n  public static final class Builder {\n\n    private String keyStoreType = DEFAULT_KEYSTORE_TYPE;\n    private Path keyStorePath;\n    private Supplier<String> keyStorePasswordSupplier;\n    private String certificateAlias = DEFAULT_CERTIFICATE_ALIAS;\n    private String trustStoreType = DEFAULT_KEYSTORE_TYPE;\n    private Path trustStorePath;\n    private Supplier<String> trustStorePasswordSupplier;\n    private Path crlFilePath;\n\n    public Builder() {}\n\n    public Builder withKeyStoreType(final String keyStoreType) {\n      this.keyStoreType = keyStoreType;\n      return this;\n    }\n\n    public Builder withKeyStorePath(final Path keyStorePath) {\n      this.keyStorePath = keyStorePath;\n      return this;\n    }\n\n    public Builder withKeyStorePasswordSupplier(final Supplier<String> keyStorePasswordSupplier) {\n      this.keyStorePasswordSupplier = keyStorePasswordSupplier;\n      return this;\n    }\n\n    public Builder withCertificateAlias(final String certificateAlias) {\n      this.certificateAlias = certificateAlias;\n      return this;\n    }\n\n    public Builder withTrustStoreType(final String trustStoreType) {\n      this.trustStoreType = trustStoreType;\n      return this;\n    }\n\n    public Builder withTrustStorePath(final Path trustStorePath) {\n      this.trustStorePath = trustStorePath;\n      return this;\n    }\n\n    public Builder withTrustStorePasswordSupplier(\n        final Supplier<String> trustStorePasswordSupplier) {\n      this.trustStorePasswordSupplier = trustStorePasswordSupplier;\n      return this;\n    }\n\n    public Builder withCrlFilePath(final Path filePath) {\n      this.crlFilePath = filePath;\n      return this;\n    }\n\n    public PkiKeyStoreConfiguration build() {\n      requireNonNull(keyStoreType, \"Key Store Type must not be null\");\n      requireNonNull(keyStorePasswordSupplier, \"Key Store password supplier must not be null\");\n      return new PkiKeyStoreConfiguration(\n          keyStoreType,\n          keyStorePath,\n          keyStorePasswordSupplier,\n          certificateAlias,\n          trustStoreType,\n          trustStorePath,\n          trustStorePasswordSupplier,\n          Optional.ofNullable(crlFilePath));\n    }\n  }\n}\n","lang_cluster":"Java","length":156,"code_uid":"627838b5109f44c18a0d981596391132"}
{"diff_hunk":"@@ -136,7 +136,7 @@ public class TestLoadBalancer {\n     List<Server> servers = new ArrayList<Server>();\n     Server server = Mockito.mock(Server.class);\n     servers.add(server);\n-    Mockito.when(serverList.getInitialListOfServers()).thenReturn(servers);\n+    loadBalancer.setServerList(servers);\n \n     TransactionControlFilter filter = Mockito.mock(TransactionControlFilter.class);\n     Mockito.when(filter.getFilteredListOfServers(servers)).thenReturn(servers);","old_code":"\/*\n * Copyright 2017 Huawei Technologies Co., Ltd\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage io.servicecomb.loadbalance;\n\nimport static org.junit.Assert.assertNotNull;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.junit.Assert;\nimport org.junit.Test;\nimport org.mockito.Mockito;\n\nimport com.netflix.loadbalancer.AbstractLoadBalancer.ServerGroup;\nimport com.netflix.loadbalancer.IRule;\nimport com.netflix.loadbalancer.Server;\n\nimport io.servicecomb.loadbalance.filter.SimpleTransactionControlFilter;\nimport io.servicecomb.loadbalance.filter.TransactionControlFilter;\n\npublic class TestLoadBalancer {\n\n  private CseServerList serverList = Mockito.mock(CseServerList.class);\n\n  private IRule rule = Mockito.mock(IRule.class);\n\n  private LoadBalancer loadBalancer = new LoadBalancer(serverList, rule);\n\n  @Test\n  public void testLoadBalancerFullOperationWithoutException() {\n\n    List<Server> newServers = new ArrayList<Server>();\n    Server server = Mockito.mock(Server.class);\n    newServers.add(server);\n\n    loadBalancer.chooseServer();\n\n    Object key = Mockito.mock(Object.class);\n\n    loadBalancer.chooseServer(key);\n    loadBalancer.getAllServers();\n    loadBalancer.getLoadBalancerStats();\n    loadBalancer.getReachableServers();\n\n    assertNotNull(loadBalancer.getAllServers());\n  }\n\n  @Test\n  public void testAddServerException() {\n    boolean status = true;\n    List<Server> newServers = new ArrayList<Server>();\n    Server server = Mockito.mock(Server.class);\n\n    newServers.add(server);\n\n    try {\n\n      loadBalancer.addServers(newServers);\n    } catch (Exception e) {\n\n      status = false;\n\n      Assert.assertEquals(\"Not implemented.\", e.getMessage());\n    }\n\n    Assert.assertFalse(status);\n  }\n\n  @Test\n  public void testServerListException() {\n    boolean status = true;\n    List<Server> newServers = new ArrayList<Server>();\n    Server server = Mockito.mock(Server.class);\n\n    newServers.add(server);\n\n    try {\n\n      loadBalancer.getServerList(ServerGroup.ALL);\n    } catch (Exception e) {\n\n      status = false;\n\n      Assert.assertEquals(\"Not implemented.\", e.getMessage());\n    }\n\n    Assert.assertFalse(status);\n  }\n\n  @Test\n  public void testMarkServerDownException() {\n    boolean status = true;\n    List<Server> newServers = new ArrayList<Server>();\n    Server server = Mockito.mock(Server.class);\n\n    newServers.add(server);\n\n    try {\n\n      loadBalancer.markServerDown(server);\n    } catch (Exception e) {\n\n      status = false;\n\n      Assert.assertEquals(\"Not implemented.\", e.getMessage());\n    }\n\n    Assert.assertFalse(status);\n  }\n\n  @Test\n  public void testFilter() {\n    Assert.assertEquals(0, loadBalancer.getFilterSize());\n\n    TransactionControlFilter filter = new SimpleTransactionControlFilter();\n    loadBalancer.putFilter(\"test\", filter);\n    Assert.assertEquals(1, loadBalancer.getFilterSize());\n  }\n\n  @Test\n  public void testGetAllServers() {\n    List<Server> servers = new ArrayList<Server>();\n    Server server = Mockito.mock(Server.class);\n    servers.add(server);\n    Mockito.when(serverList.getInitialListOfServers()).thenReturn(servers);\n\n    TransactionControlFilter filter = Mockito.mock(TransactionControlFilter.class);\n    Mockito.when(filter.getFilteredListOfServers(servers)).thenReturn(servers);\n    Assert.assertEquals(servers, loadBalancer.getAllServers());\n  }\n}\n","lang_cluster":"Java","length":145,"code_uid":"a20afcc2a3d34dedb1758467d2826ad5"}
{"diff_hunk":"@@ -1,35 +1,10 @@\n #include \"flatbuffers\/grpc.h\"\n #include \"monster_test_generated.h\"\n+#include \"test_assert.h\"\n+#include \"test_builder.h\"\n \n-static int builder_test_error = 0;\n-\n-#define test_assert(condition) do { \\\n-  if(!(condition)) { \\\n-    fprintf(stderr, \"%s:%d: %s failed.\\n\", __FILE__, __LINE__, #condition);\\\n-    builder_test_error = 1;\\\n-  } \\\n-} while(0)\n-\n-using namespace MyGame::Example;\n-\n-const std::string m1_name = \"Cyberdemon\";\n-const Color m1_color = Color_Red;\n-const std::string m2_name = \"Imp\";\n-const Color m2_color = Color_Green;\n-\n-flatbuffers::Offset<Monster> populate1(flatbuffers::FlatBufferBuilder &builder) {\n-  auto name_offset = builder.CreateString(m1_name);\n-  return CreateMonster(builder, nullptr, 0, 0, name_offset, 0, m1_color);\n-}\n-\n-flatbuffers::Offset<Monster> populate2(flatbuffers::FlatBufferBuilder &builder) {\n-  auto name_offset = builder.CreateString(m2_name);\n-  return CreateMonster(builder, nullptr, 0, 0, name_offset, 0, m2_color);\n-}\n-\n-bool release_n_verify(flatbuffers::FlatBufferBuilder &fbb, const std::string &expected_name, Color color) {\n-  flatbuffers::DetachedBuffer buf = fbb.Release();\n-  const Monster *monster = flatbuffers::GetRoot<Monster>(buf.data());\n+bool verify(flatbuffers::grpc::Message<Monster> &msg, const std::string &expected_name, Color color) {\n+  const Monster *monster = msg.GetRoot();\n   return (monster->name()->str() == expected_name) && (monster->color() == color);\n }\n ","old_code":"#include \"flatbuffers\/grpc.h\"\n#include \"monster_test_generated.h\"\n\nstatic int builder_test_error = 0;\n\n#define test_assert(condition) do { \\\n  if(!(condition)) { \\\n    fprintf(stderr, \"%s:%d: %s failed.\\n\", __FILE__, __LINE__, #condition);\\\n    builder_test_error = 1;\\\n  } \\\n} while(0)\n\nusing namespace MyGame::Example;\n\nconst std::string m1_name = \"Cyberdemon\";\nconst Color m1_color = Color_Red;\nconst std::string m2_name = \"Imp\";\nconst Color m2_color = Color_Green;\n\nflatbuffers::Offset<Monster> populate1(flatbuffers::FlatBufferBuilder &builder) {\n  auto name_offset = builder.CreateString(m1_name);\n  return CreateMonster(builder, nullptr, 0, 0, name_offset, 0, m1_color);\n}\n\nflatbuffers::Offset<Monster> populate2(flatbuffers::FlatBufferBuilder &builder) {\n  auto name_offset = builder.CreateString(m2_name);\n  return CreateMonster(builder, nullptr, 0, 0, name_offset, 0, m2_color);\n}\n\nbool release_n_verify(flatbuffers::FlatBufferBuilder &fbb, const std::string &expected_name, Color color) {\n  flatbuffers::DetachedBuffer buf = fbb.Release();\n  const Monster *monster = flatbuffers::GetRoot<Monster>(buf.data());\n  return (monster->name()->str() == expected_name) && (monster->color() == color);\n}\n\nbool release_n_verify(flatbuffers::grpc::MessageBuilder &mbb, const std::string &expected_name, Color color) {\n  flatbuffers::grpc::Message<Monster> msg = mbb.ReleaseMessage<Monster>();\n  const Monster *monster = msg.GetRoot();\n  return (monster->name()->str() == expected_name) && (monster->color() == color);\n}\n\nstruct OwnedAllocator : public flatbuffers::DefaultAllocator {};\n\nstruct TestHeapMessageBuilder : public flatbuffers::FlatBufferBuilder {\n  TestHeapMessageBuilder()\n    : flatbuffers::FlatBufferBuilder(2048, new OwnedAllocator(), true) {}\n};\n\ntemplate <class Builder>\nstruct BuilderTests {\n  static void empty_builder_movector_test() {\n    Builder b1;\n    size_t b1_size = b1.GetSize();\n    Builder b2(std::move(b1));\n    size_t b2_size = b2.GetSize();\n    test_assert(b1_size == 0);\n    test_assert(b1_size == b2_size);\n  }\n\n  static void nonempty_builder_movector_test() {\n    Builder b1;\n    populate1(b1);\n    size_t b1_size = b1.GetSize();\n    Builder b2(std::move(b1));\n    test_assert(b1_size == b2.GetSize());\n    test_assert(0 == b1.GetSize());\n  }\n\n  static void builder_movector_before_finish_test() {\n    Builder b1;\n    auto root_offset1 = populate1(b1);\n    Builder b2(std::move(b1));\n    b2.Finish(root_offset1);\n    test_assert(release_n_verify(b2, m1_name, m1_color));\n    test_assert(0 == b1.GetSize());\n  }\n\n  static void builder_movector_after_finish_test() {\n    Builder b1;\n    auto root_offset1 = populate1(b1);\n    b1.Finish(root_offset1);\n    Builder b2(std::move(b1));\n    test_assert(release_n_verify(b2, m1_name, m1_color));\n    test_assert(0 == b1.GetSize());\n  }\n\n  static void builder_move_assign_before_finish_test() {\n    Builder b1;\n    auto root_offset1 = populate1(b1);\n    Builder b2;\n    populate2(b2);\n    b2 = std::move(b1);\n    b2.Finish(root_offset1);\n    test_assert(release_n_verify(b2, m1_name, m1_color));\n    test_assert(0 == b1.GetSize());\n  }\n\n  static void builder_move_assign_after_finish_test() {\n    Builder b1;\n    auto root_offset1 = populate1(b1);\n    b1.Finish(root_offset1);\n    Builder b2;\n    auto root_offset2 = populate2(b2);\n    b2.Finish(root_offset2);\n    b2 = std::move(b1);\n    test_assert(release_n_verify(b2, m1_name, m1_color));\n    test_assert(0 == b1.GetSize());\n  }\n\n  static void builder_swap_before_finish_test() {\n    Builder b1;\n    auto root_offset1 = populate1(b1);\n    auto size1 = b1.GetSize();\n    Builder b2;\n    auto root_offset2 = populate2(b2);\n    auto size2 = b2.GetSize();\n    b1.Swap(b2);\n    b1.Finish(root_offset2);\n    b2.Finish(root_offset1);\n    test_assert(b1.GetSize() > size2);\n    test_assert(b2.GetSize() > size1);\n    test_assert(release_n_verify(b1, m2_name, m2_color));\n    test_assert(release_n_verify(b2, m1_name, m1_color));\n  }\n\n  static void builder_swap_after_finish_test() {\n    Builder b1;\n    auto root_offset1 = populate1(b1);\n    b1.Finish(root_offset1);\n    auto size1 = b1.GetSize();\n    Builder b2;\n    auto root_offset2 = populate2(b2);\n    b2.Finish(root_offset2);\n    auto size2 = b2.GetSize();\n    b1.Swap(b2);\n    test_assert(b1.GetSize() == size2);\n    test_assert(b2.GetSize() == size1);\n    test_assert(release_n_verify(b1, m2_name, m2_color));\n    test_assert(release_n_verify(b2, m1_name, m1_color));\n  }\n\n  static void all_tests() {\n    empty_builder_movector_test();\n    nonempty_builder_movector_test();\n    builder_movector_before_finish_test();\n    builder_movector_after_finish_test();\n    builder_move_assign_before_finish_test();\n    builder_move_assign_after_finish_test();\n    builder_swap_before_finish_test();\n    builder_swap_after_finish_test();\n  }\n};\n\nint builder_tests() {\n  BuilderTests<flatbuffers::grpc::MessageBuilder>::all_tests();\n  BuilderTests<flatbuffers::FlatBufferBuilder>::all_tests();\n  BuilderTests<TestHeapMessageBuilder>::all_tests();\n  return builder_test_error;\n}\n","lang_cluster":"Java","length":159,"code_uid":"761228eadde24af0b239f328b915f4a2"}
{"diff_hunk":"@@ -14,6 +14,8 @@\n  *\/\n package org.hyperledger.besu.ethereum.core.encoding;\n \n+import static org.hyperledger.besu.ethereum.core.Transaction.GO_QUORUM_PRIVATE_TRANSACTION_V_VALUE_MAX;\n+import static org.hyperledger.besu.ethereum.core.Transaction.GO_QUORUM_PRIVATE_TRANSACTION_V_VALUE_MIN;\n import static org.hyperledger.besu.ethereum.core.Transaction.REPLAY_PROTECTED_V_BASE;\n import static org.hyperledger.besu.ethereum.core.Transaction.REPLAY_PROTECTED_V_MIN;\n import static org.hyperledger.besu.ethereum.core.Transaction.REPLAY_UNPROTECTED_V_BASE;","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.ethereum.core.encoding;\n\nimport static org.hyperledger.besu.ethereum.core.Transaction.REPLAY_PROTECTED_V_BASE;\nimport static org.hyperledger.besu.ethereum.core.Transaction.REPLAY_PROTECTED_V_MIN;\nimport static org.hyperledger.besu.ethereum.core.Transaction.REPLAY_UNPROTECTED_V_BASE;\nimport static org.hyperledger.besu.ethereum.core.Transaction.REPLAY_UNPROTECTED_V_BASE_PLUS_1;\nimport static org.hyperledger.besu.ethereum.core.Transaction.TWO;\n\nimport org.hyperledger.besu.config.experimental.ExperimentalEIPs;\nimport org.hyperledger.besu.crypto.SECP256K1;\nimport org.hyperledger.besu.ethereum.core.Address;\nimport org.hyperledger.besu.ethereum.core.Transaction;\nimport org.hyperledger.besu.ethereum.core.Wei;\nimport org.hyperledger.besu.ethereum.rlp.RLPInput;\n\nimport java.math.BigInteger;\nimport java.util.Optional;\n\nimport org.apache.tuweni.bytes.Bytes;\n\n@FunctionalInterface\npublic interface TransactionRLPDecoder {\n\n  TransactionRLPDecoder FRONTIER = frontierDecoder();\n  TransactionRLPDecoder EIP1559 = eip1559Decoder();\n\n  static Transaction decodeTransaction(final RLPInput input) {\n    return (ExperimentalEIPs.eip1559Enabled ? EIP1559 : FRONTIER).decode(input);\n  }\n\n  Transaction decode(RLPInput input);\n\n  static TransactionRLPDecoder frontierDecoder() {\n    return input -> {\n      input.enterList();\n      final Transaction.Builder builder =\n          Transaction.builder()\n              .nonce(input.readLongScalar())\n              .gasPrice(Wei.of(input.readUInt256Scalar()))\n              .gasLimit(input.readLongScalar())\n              .to(input.readBytes(v -> v.size() == 0 ? null : Address.wrap(v)))\n              .value(Wei.of(input.readUInt256Scalar()))\n              .payload(input.readBytes());\n\n      final BigInteger v = input.readBigIntegerScalar();\n      final byte recId;\n      Optional<BigInteger> chainId = Optional.empty();\n      if (v.equals(REPLAY_UNPROTECTED_V_BASE) || v.equals(REPLAY_UNPROTECTED_V_BASE_PLUS_1)) {\n        recId = v.subtract(REPLAY_UNPROTECTED_V_BASE).byteValueExact();\n      } else if (v.compareTo(REPLAY_PROTECTED_V_MIN) > 0) {\n        chainId = Optional.of(v.subtract(REPLAY_PROTECTED_V_BASE).divide(TWO));\n        recId =\n            v.subtract(TWO.multiply(chainId.get()).add(REPLAY_PROTECTED_V_BASE)).byteValueExact();\n      } else {\n        throw new RuntimeException(\n            String.format(\"An unsupported encoded `v` value of %s was found\", v));\n      }\n      final BigInteger r = input.readUInt256Scalar().toBytes().toUnsignedBigInteger();\n      final BigInteger s = input.readUInt256Scalar().toBytes().toUnsignedBigInteger();\n      final SECP256K1.Signature signature = SECP256K1.Signature.create(r, s, recId);\n\n      input.leaveList();\n\n      chainId.ifPresent(builder::chainId);\n      return builder.signature(signature).build();\n    };\n  }\n\n  static TransactionRLPDecoder eip1559Decoder() {\n    return input -> {\n      input.enterList();\n\n      final Transaction.Builder builder =\n          Transaction.builder()\n              .nonce(input.readLongScalar())\n              .gasPrice(Wei.of(input.readUInt256Scalar()))\n              .gasLimit(input.readLongScalar())\n              .to(input.readBytes(v -> v.size() == 0 ? null : Address.wrap(v)))\n              .value(Wei.of(input.readUInt256Scalar()))\n              .payload(input.readBytes());\n\n      final Bytes maybeGasPremiumOrV = input.readBytes();\n      final Bytes maybeFeeCapOrR = input.readBytes();\n      final Bytes maybeVOrS = input.readBytes();\n      final BigInteger v, r, s;\n      \/\/ if this is the end of the list we are processing a legacy transaction\n      if (input.isEndOfCurrentList()) {\n        v = maybeGasPremiumOrV.toUnsignedBigInteger();\n        r = maybeFeeCapOrR.toUnsignedBigInteger();\n        s = maybeVOrS.toUnsignedBigInteger();\n      } else {\n        \/\/ otherwise this is an EIP-1559 transaction\n        builder\n            .gasPremium(Wei.of(maybeGasPremiumOrV.toBigInteger()))\n            .feeCap(Wei.of(maybeFeeCapOrR.toBigInteger()));\n        v = maybeVOrS.toBigInteger();\n        r = input.readUInt256Scalar().toBytes().toUnsignedBigInteger();\n        s = input.readUInt256Scalar().toBytes().toUnsignedBigInteger();\n      }\n      final byte recId;\n      Optional<BigInteger> chainId = Optional.empty();\n      if (v.equals(REPLAY_UNPROTECTED_V_BASE) || v.equals(REPLAY_UNPROTECTED_V_BASE_PLUS_1)) {\n        recId = v.subtract(REPLAY_UNPROTECTED_V_BASE).byteValueExact();\n      } else if (v.compareTo(REPLAY_PROTECTED_V_MIN) > 0) {\n        chainId = Optional.of(v.subtract(REPLAY_PROTECTED_V_BASE).divide(TWO));\n        recId =\n            v.subtract(TWO.multiply(chainId.get()).add(REPLAY_PROTECTED_V_BASE)).byteValueExact();\n      } else {\n        throw new RuntimeException(\n            String.format(\"An unsupported encoded `v` value of %s was found\", v));\n      }\n      final SECP256K1.Signature signature = SECP256K1.Signature.create(r, s, recId);\n      input.leaveList();\n      chainId.ifPresent(builder::chainId);\n      return builder.signature(signature).build();\n    };\n  }\n}\n","lang_cluster":"Java","length":132,"code_uid":"705566e8e04a41018ba759c87d8a99de"}
{"diff_hunk":"@@ -133,8 +133,20 @@ public class StorageCallbacksImpl implements StorageCallbacks {\n                     + \" FROM \" + PodDBAdapter.TABLE_NAME_FEEDS\n                     + \" WHERE \" + PodDBAdapter.TABLE_NAME_FEEDS + \".\" + PodDBAdapter.KEY_ID\n                     + \" = \" + PodDBAdapter.TABLE_NAME_FEED_ITEMS + \".\" + PodDBAdapter.KEY_FEED + \")\");\n+\n             db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS\n                     + \" ADD COLUMN \" + PodDBAdapter.KEY_HIDE + \" TEXT\");\n+\n+            \n+            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS\n+                    + \" ADD COLUMN \" + PodDBAdapter.KEY_LAST_UPDATE_FAILED + \" INTEGER DEFAULT 0\");\n+\n+            \/\/ create indexes\n+            db.execSQL(PodDBAdapter.CREATE_INDEX_FEEDITEMS_FEED);\n+            db.execSQL(PodDBAdapter.CREATE_INDEX_FEEDITEMS_IMAGE);\n+            db.execSQL(PodDBAdapter.CREATE_INDEX_FEEDMEDIA_FEEDITEM);\n+            db.execSQL(PodDBAdapter.CREATE_INDEX_QUEUE_FEEDITEM);\n+            db.execSQL(PodDBAdapter.CREATE_INDEX_SIMPLECHAPTERS_FEEDITEM);\n         }\n     }\n }","old_code":"package de.danoeh.antennapod.config;\n\n\nimport android.content.ContentValues;\nimport android.database.Cursor;\nimport android.database.sqlite.SQLiteDatabase;\nimport android.util.Log;\n\nimport de.danoeh.antennapod.core.StorageCallbacks;\nimport de.danoeh.antennapod.core.storage.PodDBAdapter;\n\npublic class StorageCallbacksImpl implements StorageCallbacks {\n\n    @Override\n    public int getDatabaseVersion() {\n        return 15;\n    }\n\n    @Override\n    public void onUpgrade(SQLiteDatabase db, int oldVersion, int newVersion) {\n        Log.w(\"DBAdapter\", \"Upgrading from version \" + oldVersion + \" to \"\n                + newVersion + \".\");\n        if (oldVersion <= 1) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS + \" ADD COLUMN \"\n                    + PodDBAdapter.KEY_TYPE + \" TEXT\");\n        }\n        if (oldVersion <= 2) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_SIMPLECHAPTERS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_LINK + \" TEXT\");\n        }\n        if (oldVersion <= 3) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEED_ITEMS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_ITEM_IDENTIFIER + \" TEXT\");\n        }\n        if (oldVersion <= 4) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS + \" ADD COLUMN \"\n                    + PodDBAdapter.KEY_FEED_IDENTIFIER + \" TEXT\");\n        }\n        if (oldVersion <= 5) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_DOWNLOAD_LOG\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_REASON_DETAILED + \" TEXT\");\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_DOWNLOAD_LOG\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_DOWNLOADSTATUS_TITLE + \" TEXT\");\n        }\n        if (oldVersion <= 6) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_SIMPLECHAPTERS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_CHAPTER_TYPE + \" INTEGER\");\n        }\n        if (oldVersion <= 7) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEED_MEDIA\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_PLAYBACK_COMPLETION_DATE\n                    + \" INTEGER\");\n        }\n        if (oldVersion <= 8) {\n            final int KEY_ID_POSITION = 0;\n            final int KEY_MEDIA_POSITION = 1;\n\n            \/\/ Add feeditem column to feedmedia table\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEED_MEDIA\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_FEEDITEM\n                    + \" INTEGER\");\n            Cursor feeditemCursor = db.query(PodDBAdapter.TABLE_NAME_FEED_ITEMS,\n                    new String[]{PodDBAdapter.KEY_ID, PodDBAdapter.KEY_MEDIA}, \"? > 0\",\n                    new String[]{PodDBAdapter.KEY_MEDIA}, null, null, null);\n            if (feeditemCursor.moveToFirst()) {\n                db.beginTransaction();\n                ContentValues contentValues = new ContentValues();\n                do {\n                    long mediaId = feeditemCursor.getLong(KEY_MEDIA_POSITION);\n                    contentValues.put(PodDBAdapter.KEY_FEEDITEM, feeditemCursor.getLong(KEY_ID_POSITION));\n                    db.update(PodDBAdapter.TABLE_NAME_FEED_MEDIA, contentValues, PodDBAdapter.KEY_ID + \"=?\", new String[]{String.valueOf(mediaId)});\n                    contentValues.clear();\n                } while (feeditemCursor.moveToNext());\n                db.setTransactionSuccessful();\n                db.endTransaction();\n            }\n            feeditemCursor.close();\n        }\n        if (oldVersion <= 9) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_AUTO_DOWNLOAD\n                    + \" INTEGER DEFAULT 1\");\n        }\n        if (oldVersion <= 10) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_FLATTR_STATUS\n                    + \" INTEGER\");\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEED_ITEMS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_FLATTR_STATUS\n                    + \" INTEGER\");\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEED_MEDIA\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_PLAYED_DURATION\n                    + \" INTEGER\");\n        }\n        if (oldVersion <= 11) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_USERNAME\n                    + \" TEXT\");\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_PASSWORD\n                    + \" TEXT\");\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEED_ITEMS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_IMAGE\n                    + \" INTEGER\");\n        }\n        if (oldVersion <= 12) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_IS_PAGED + \" INTEGER DEFAULT 0\");\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_NEXT_PAGE_LINK + \" TEXT\");\n        }\n        if (oldVersion <= 13) {\n            \/\/ remove duplicate rows in \"Chapters\" table that were created because of a bug.\n            db.execSQL(String.format(\"DELETE FROM %s WHERE %s NOT IN \" +\n                            \"(SELECT MIN(%s) as %s FROM %s GROUP BY %s,%s,%s,%s,%s)\",\n                    PodDBAdapter.TABLE_NAME_SIMPLECHAPTERS,\n                    PodDBAdapter.KEY_ID,\n                    PodDBAdapter.KEY_ID,\n                    PodDBAdapter.KEY_ID,\n                    PodDBAdapter.TABLE_NAME_SIMPLECHAPTERS,\n                    PodDBAdapter.KEY_TITLE,\n                    PodDBAdapter.KEY_START,\n                    PodDBAdapter.KEY_FEEDITEM,\n                    PodDBAdapter.KEY_LINK,\n                    PodDBAdapter.KEY_CHAPTER_TYPE));\n        }\n        if(oldVersion <= 14) {\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEED_ITEMS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_AUTO_DOWNLOAD + \" INTEGER\");\n            db.execSQL(\"UPDATE \" + PodDBAdapter.TABLE_NAME_FEED_ITEMS\n                    + \" SET \" + PodDBAdapter.KEY_AUTO_DOWNLOAD + \" = \"\n                    + \"(SELECT \" + PodDBAdapter.KEY_AUTO_DOWNLOAD\n                    + \" FROM \" + PodDBAdapter.TABLE_NAME_FEEDS\n                    + \" WHERE \" + PodDBAdapter.TABLE_NAME_FEEDS + \".\" + PodDBAdapter.KEY_ID\n                    + \" = \" + PodDBAdapter.TABLE_NAME_FEED_ITEMS + \".\" + PodDBAdapter.KEY_FEED + \")\");\n            db.execSQL(\"ALTER TABLE \" + PodDBAdapter.TABLE_NAME_FEEDS\n                    + \" ADD COLUMN \" + PodDBAdapter.KEY_HIDE + \" TEXT\");\n        }\n    }\n}\n","lang_cluster":"Java","length":140,"code_uid":"5a35a05715194b4c8e299c5d3e1f7ef8"}
{"diff_hunk":"@@ -65,7 +65,7 @@ public class FileAnalyser {\n \n         try {\n             byte[] data = Files.readAllBytes(path);\n-            return Magic.getMagicMatch(data);\n+            return Magic.getMagicMatch(data, true);\n         } catch (MagicException | MagicParseException | IOException e) {\n             throw new IllegalStateException(\"Unable to detect mimetype of the file\", e);\n         }","old_code":"\/*\n * Copyright (C) 2015-2017 P\u00c2RIS Quentin\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n *\/\n\npackage org.phoenicis.tools.files;\n\nimport net.sf.jmimemagic.*;\nimport org.apache.commons.io.FileUtils;\nimport org.phoenicis.configuration.security.Safe;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport javax.activation.MimetypesFileTypeMap;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\n@Safe\npublic class FileAnalyser {\n    private static final Logger LOGGER = LoggerFactory.getLogger(FileAnalyser.class);\n\n    \/**\n     * Identify which line delimiter is used in a file\n     * \n     * @param fileContent\n     *            string to analyse\n     * @return the line separator as a string. Null if the file has no line\n     *         separator\n     *\/\n    public static String identifyLineDelimiter(String fileContent) {\n        if (fileContent.matches(\"(?s).*(\\\\r\\\\n).*\")) { \/\/ Windows \/\/$NON-NLS-1$\n            return \"\\r\\n\"; \/\/$NON-NLS-1$\n        } else if (fileContent.matches(\"(?s).*(\\\\n).*\")) { \/\/ Unix\/Linux \/\/$NON-NLS-1$\n            return \"\\n\"; \/\/$NON-NLS-1$\n        } else if (fileContent.matches(\"(?s).*(\\\\r).*\")) { \/\/ Legacy mac os 9. Newer OS X use \\n \/\/$NON-NLS-1$\n            return \"\\r\"; \/\/$NON-NLS-1$\n        } else {\n            return \"\\n\"; \/\/ fallback onto '\\n' if nothing matches. \/\/$NON-NLS-1$\n        }\n    }\n\n    public static String identifyLineDelimiter(File fileToAnalyse) throws IOException {\n        final String fileContent = FileUtils.readFileToString(fileToAnalyse);\n        return identifyLineDelimiter(fileContent);\n    }\n\n    private MagicMatch getMatch(File inputFile) throws MagicMatchNotFoundException {\n        final Path path = Paths.get(inputFile.getAbsolutePath());\n\n        try {\n            byte[] data = Files.readAllBytes(path);\n            return Magic.getMagicMatch(data);\n        } catch (MagicException | MagicParseException | IOException e) {\n            throw new IllegalStateException(\"Unable to detect mimetype of the file\", e);\n        }\n    }\n\n    public String getDescription(File inputFile) {\n        try {\n            return getMatch(inputFile).getDescription();\n        } catch (MagicMatchNotFoundException e) {\n            throw new IllegalStateException(\"Unable to detect mimetype of the file\", e);\n        }\n    }\n\n    public String getMimetype(File inputFile) {\n        try {\n            return getMatch(inputFile).getMimeType();\n        } catch (MagicMatchNotFoundException e) {\n            LOGGER.debug(\"Failed to get Mime Type\", e);\n            final MimetypesFileTypeMap mimeTypesMap = new MimetypesFileTypeMap();\n            return mimeTypesMap.getContentType(inputFile);\n        }\n    }\n}\n","lang_cluster":"Java","length":91,"code_uid":"41a96f1c91db41d182880bfb66a8309a"}
{"diff_hunk":"@@ -119,6 +119,8 @@ public class V2Request extends SolrRequest<V2Response> implements MapWriter {\n     private boolean useBinary = false;\n \n     private boolean forceV2EndPoint = false;\n+    private ResponseParser parser;\n+    private String mimeType;\n \n     \/**\n      * Create a Builder object based on the provided resource.","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage org.apache.solr.client.solrj.request;\n\nimport java.io.IOException;\nimport java.io.OutputStream;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport org.apache.solr.client.solrj.SolrClient;\nimport org.apache.solr.client.solrj.SolrRequest;\nimport org.apache.solr.client.solrj.response.V2Response;\nimport org.apache.solr.common.MapWriter;\nimport org.apache.solr.common.params.SolrParams;\nimport org.apache.solr.common.util.JavaBinCodec;\nimport org.apache.solr.common.util.Utils;\n\nimport static org.apache.solr.common.params.CommonParams.JAVABIN_MIME;\nimport static org.apache.solr.common.params.CommonParams.JSON_MIME;\n\npublic class V2Request extends SolrRequest<V2Response> implements MapWriter {\n  \/\/only for debugging purposes\n  public static final ThreadLocal<AtomicLong> v2Calls = new ThreadLocal<>();\n  static final Pattern COLL_REQ_PATTERN = Pattern.compile(\"\/(c|collections)\/([^\/])+\/(?!shards)\");\n  private Object payload;\n  private SolrParams solrParams;\n  public final boolean useBinary;\n  private String collection;\n  private boolean forceV2 = false;\n  private boolean isPerCollectionRequest = false;\n\n  private V2Request(METHOD m, String resource, boolean useBinary) {\n    super(m, resource);\n    Matcher matcher = COLL_REQ_PATTERN.matcher(getPath());\n    if (matcher.find()) {\n      this.collection = matcher.group(2);\n      isPerCollectionRequest = true;\n    }\n    this.useBinary = useBinary;\n\n  }\n\n  public boolean isForceV2(){\n    return forceV2;\n  }\n\n  @Override\n  public SolrParams getParams() {\n    return solrParams;\n  }\n\n  @Override\n  public RequestWriter.ContentWriter getContentWriter(String s) {\n    if (v2Calls.get() != null) v2Calls.get().incrementAndGet();\n    if (payload == null) return null;\n    if (payload instanceof String) {\n      return new RequestWriter.StringPayloadContentWriter((String) payload, JSON_MIME);\n    }\n    return new RequestWriter.ContentWriter() {\n      @Override\n      public void write(OutputStream os) throws IOException {\n        if (useBinary) {\n          new JavaBinCodec().marshal(payload, os);\n        } else {\n          Utils.writeJson(payload, os, false);\n        }\n      }\n\n      @Override\n      public String getContentType() {\n        return useBinary ? JAVABIN_MIME : JSON_MIME;\n      }\n    };\n  }\n\n  public boolean isPerCollectionRequest() {\n    return isPerCollectionRequest;\n  }\n\n  @Override\n  public String getCollection() {\n    return collection;\n  }\n\n  @Override\n  protected V2Response createResponse(SolrClient client) {\n    return new V2Response();\n  }\n\n  @Override\n  public void writeMap(EntryWriter ew) throws IOException {\n    ew.put(\"method\", getMethod().toString());\n    ew.put(\"path\", getPath());\n    ew.putIfNotNull(\"params\", solrParams);\n    ew.putIfNotNull(\"command\", payload);\n  }\n\n  public static class Builder {\n    private String resource;\n    private METHOD method = METHOD.GET;\n    private Object payload;\n    private SolrParams params;\n    private boolean useBinary = false;\n\n    private boolean forceV2EndPoint = false;\n\n    \/**\n     * Create a Builder object based on the provided resource.\n     * The default method is GET.\n     *\n     * @param resource resource of the request for example \"\/collections\" or \"\/cores\/core-name\"\n     *\/\n    public Builder(String resource) {\n      if (!resource.startsWith(\"\/\")) resource = \"\/\" + resource;\n      this.resource = resource;\n    }\n\n    public Builder withMethod(METHOD m) {\n      this.method = m;\n      return this;\n    }\n\n    \/**\n     * Only for testing. It's always true otherwise\n     *\/\n    public Builder forceV2(boolean flag) {\n      forceV2EndPoint = flag;\n      return this;\n    }\n\n    \/**\n     * Set payload for request.\n     *\n     * @param payload as UTF-8 String\n     * @return builder object\n     *\/\n    public Builder withPayload(String payload) {\n      if (payload != null) {\n        this.payload = payload;\n      }\n      return this;\n    }\n\n    public Builder withPayload(Object payload) {\n      this.payload = payload;\n      return this;\n    }\n\n\n    public Builder withParams(SolrParams params) {\n      this.params = params;\n      return this;\n    }\n\n    public Builder useBinary(boolean flag) {\n      this.useBinary = flag;\n      return this;\n    }\n\n    public V2Request build() {\n      V2Request v2Request = new V2Request(method, resource, useBinary);\n      v2Request.solrParams = params;\n      v2Request.payload = payload;\n      v2Request.forceV2 = forceV2EndPoint;\n      return v2Request;\n    }\n  }\n}\n","lang_cluster":"Java","length":184,"code_uid":"6ff5e50e0e654cf980588b958b13b097"}
{"diff_hunk":"@@ -27,7 +27,10 @@\n package com.salesforce.androidsdk.rest;\n \n import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n \n+import org.apache.http.Header;\n import org.apache.http.HttpEntity;\n import org.apache.http.HttpResponse;\n import org.apache.http.HttpStatus;","old_code":"\/*\n * Copyright (c) 2012, salesforce.com, inc.\n * All rights reserved.\n * Redistribution and use of this software in source and binary forms, with or\n * without modification, are permitted provided that the following conditions\n * are met:\n * - Redistributions of source code must retain the above copyright notice, this\n * list of conditions and the following disclaimer.\n * - Redistributions in binary form must reproduce the above copyright notice,\n * this list of conditions and the following disclaimer in the documentation\n * and\/or other materials provided with the distribution.\n * - Neither the name of salesforce.com, inc. nor the names of its contributors\n * may be used to endorse or promote products derived from this software without\n * specific prior written permission of salesforce.com, inc.\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n *\/\npackage com.salesforce.androidsdk.rest;\n\nimport java.io.IOException;\n\nimport org.apache.http.HttpEntity;\nimport org.apache.http.HttpResponse;\nimport org.apache.http.HttpStatus;\nimport org.apache.http.ParseException;\nimport org.apache.http.protocol.HTTP;\nimport org.apache.http.util.EntityUtils;\nimport org.json.JSONArray;\nimport org.json.JSONException;\nimport org.json.JSONObject;\n\nimport com.android.volley.NetworkResponse;\n\n\n\/**\n * RestResponse: Class to represent any REST response.\n * \n *\/\npublic class RestResponse {\n\t\n\tprivate final int statusCode;\n\tprivate final HttpResponse response;\n\n\t\/\/ Populated when \"consume\" is called\n\tprivate byte[] responseAsBytes;\n\tprivate String responseCharSet;\n\t\n\t\/\/ Lazily computed\n\tprivate String responseAsString;\n\tprivate JSONObject responseAsJSONObject;\n\tprivate JSONArray responseAsJSONArray;\n\n\t\/**\n\t * Constructor\n\t * @param response\n\t *\/\n\tpublic RestResponse(HttpResponse response) {\n\t\tthis.response = response;\n\t\tthis.statusCode = response.getStatusLine().getStatusCode();\n\t}\n\t\n\t\/**\n\t * Constructor\n\t * @param response\n\t *\/\n\tpublic RestResponse(NetworkResponse response) {\n\t\tthis.response = null;\n\t\tthis.statusCode = response.statusCode;\n\t\tthis.responseAsBytes = response.data;\n\t\t\t\t\n\t}\n\t\n\t\/**\n\t * @return HTTP status code of the response\n\t *\/\n\tpublic int getStatusCode() {\n\t\treturn statusCode; \n\t}\n\t\n\t\/**\n\t * @return the base HTTP response of the response. The can be useful for response that are not JSON, such as binary payloads.\n\t *\/\n\t\/**\n\t * @return true for response with 2xx status codes\n\t *\/\n\tpublic boolean isSuccess() {\n\t\t\/\/ 2xx success\n\t\treturn (statusCode >= HttpStatus.SC_OK && statusCode < HttpStatus.SC_MULTIPLE_CHOICES);\t\n\t}\n\t\n\t\/**\n\t * Fully consume response entity content and closes content stream\n\t * Must be called before returning control to the UI thread\n\t * @throws IOException \n\t *\/\n\tpublic void consume() throws IOException {\n\t\tif (responseAsBytes != null) {\n\t\t\t\/\/ already consumed\n\t\t\treturn;\t\t\t\n\t\t}\n\t\t\n\t\tHttpEntity entity = response.getEntity();\n\t\tif (entity != null) {\n\t\t\tresponseCharSet = EntityUtils.getContentCharSet(entity);\t\t\n\t\t\tresponseAsBytes = EntityUtils.toByteArray(entity);\n\t\t}\n\t\telse {\n\t\t\tresponseAsBytes = new byte[0];\n\t\t}\n\t}\n\n\t\/**\n\t * @return byte[] for entire response\n\t * @throws IOException\n\t *\/\n\tpublic byte[] asBytes() throws IOException {\n\t\tif (responseAsBytes == null) {\n\t\t\tconsume();\n\t\t}\n\t\treturn responseAsBytes;\n\t}\t\n\t\n\t\/**\n\t * String is built the first time the method is called.\n\t * \n\t * @return string for entire response\n\t * @throws ParseException\n\t * @throws IOException\n\t *\/\n\tpublic String asString() throws ParseException, IOException {\n\t\tif (responseAsString == null) {\n\t\t\tresponseAsString = new String(asBytes(), (responseCharSet == null ? HTTP.UTF_8 : responseCharSet));\n\t\t}\n\t\treturn responseAsString;\n\t}\n\t\n\t\/**\n\t * JSONObject is built the first time the method is called.\n\t * \n\t * @return JSONObject for response\n\t * @throws ParseException\n\t * @throws JSONException\n\t * @throws IOException\n\t *\/\n\tpublic JSONObject asJSONObject() throws ParseException, JSONException, IOException {\n\t\tif (responseAsJSONObject == null) {\n\t\t\tresponseAsJSONObject = new JSONObject(asString());\n\t\t}\n\t\treturn responseAsJSONObject;\n\t}\n\n\t\/**\n\t * JSONArray is built the first time the method is called.\n\t * \n\t * @return JSONObject for response\n\t * @throws ParseException\n\t * @throws JSONException\n\t * @throws IOException\n\t *\/\n\tpublic JSONArray asJSONArray() throws ParseException, JSONException, IOException {\n\t\tif (responseAsJSONArray == null) {\n\t\t\tresponseAsJSONArray = new JSONArray(asString());\n\t\t}\n\t\treturn responseAsJSONArray;\n\t}\n\t\n\t\n\t@Override\n\tpublic String toString() {\n\t\ttry {\n\t\t\treturn asString();\n\t\t} catch (Exception e) {\n\t\t\treturn response.toString();\n\t\t}\n\t}\n}\n","lang_cluster":"Java","length":185,"code_uid":"285f08dad81747ada41fd417e10b5be9"}
{"diff_hunk":"@@ -83,7 +83,11 @@ public class MetricsRestServiceImpl extends AbstractRestProcessEngineAware imple\n       metrics = query.interval();\n     }\n \n-    return convertToDtos(metrics);\n+    final List<MetricsIntervalResultDto> dtoList = convertToDtos(metrics);\n+    if (name != null) {\n+      dtoList.forEach(dto -> dto.setName(name));\n+    }\n+    return dtoList;\n   }\n \n   @Override","old_code":"\/*\n * Copyright Camunda Services GmbH and\/or licensed to Camunda Services GmbH\n * under one or more contributor license agreements. See the NOTICE file\n * distributed with this work for additional information regarding copyright\n * ownership. Camunda licenses this file to you under the Apache License,\n * Version 2.0; you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage org.camunda.bpm.engine.rest.impl;\n\nimport org.camunda.bpm.engine.rest.MetricsRestService;\nimport org.camunda.bpm.engine.rest.sub.metrics.MetricsResource;\nimport org.camunda.bpm.engine.rest.sub.metrics.MetricsResourceImpl;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport java.util.ArrayList;\nimport java.util.Date;\nimport java.util.List;\nimport javax.ws.rs.core.MultivaluedMap;\nimport javax.ws.rs.core.Response;\nimport javax.ws.rs.core.UriInfo;\nimport org.camunda.bpm.engine.management.MetricsQuery;\nimport org.camunda.bpm.engine.rest.dto.metrics.MetricsIntervalResultDto;\nimport org.camunda.bpm.engine.management.MetricIntervalValue;\nimport org.camunda.bpm.engine.rest.dto.converter.DateConverter;\nimport org.camunda.bpm.engine.rest.dto.converter.IntegerConverter;\nimport org.camunda.bpm.engine.rest.dto.converter.LongConverter;\n\n\/**\n * @author Daniel Meyer\n *\n *\/\npublic class MetricsRestServiceImpl extends AbstractRestProcessEngineAware implements MetricsRestService {\n\n  public static final String QUERY_PARAM_NAME = \"name\";\n  public static final String QUERY_PARAM_REPORTER = \"reporter\";\n  public static final String QUERY_PARAM_START_DATE = \"startDate\";\n  public static final String QUERY_PARAM_END_DATE = \"endDate\";\n  public static final String QUERY_PARAM_FIRST_RESULT = \"firstResult\";\n  public static final String QUERY_PARAM_MAX_RESULTS = \"maxResults\";\n  public static final String QUERY_PARAM_INTERVAL = \"interval\";\n  public static final String QUERY_PARAM_DATE = \"date\";\n  public static final String QUERY_PARAM_AGG_BY_REPORTER = \"aggregateByReporter\";\n\n  protected final DateConverter dateConverter;\n\n  public MetricsRestServiceImpl(String engineName, ObjectMapper objectMapper) {\n    super(engineName, objectMapper);\n    dateConverter = new DateConverter();\n    dateConverter.setObjectMapper(objectMapper);\n  }\n\n  @Override\n  public MetricsResource getMetrics(String name) {\n    return new MetricsResourceImpl(name, processEngine, objectMapper);\n  }\n\n  @Override\n  public List<MetricsIntervalResultDto> interval(UriInfo uriInfo) {\n    MultivaluedMap<String, String> queryParameters = uriInfo.getQueryParameters();\n    MetricsQuery query = processEngine.getManagementService()\n      .createMetricsQuery()\n      .name(queryParameters.getFirst(QUERY_PARAM_NAME))\n      .reporter(queryParameters.getFirst(QUERY_PARAM_REPORTER));\n\n    applyQueryParams(query, queryParameters);\n\n    List<MetricIntervalValue> metrics;\n    LongConverter longConverter = new LongConverter();\n    longConverter.setObjectMapper(objectMapper);\n    if (queryParameters.getFirst(QUERY_PARAM_INTERVAL) != null) {\n      long interval = longConverter.convertQueryParameterToType(queryParameters.getFirst(QUERY_PARAM_INTERVAL));\n      metrics = query.interval(interval);\n    } else {\n      metrics = query.interval();\n    }\n\n    return convertToDtos(metrics);\n  }\n\n  @Override\n  public Response deleteTaskMetrics(String dateString) {\n    Date date = dateConverter.convertQueryParameterToType(dateString);\n    processEngine.getManagementService().deleteTaskMetrics(date);\n\n    \/\/ return no content (204) since resource is deleted\n    return Response.noContent().build();\n  }\n\n  protected void applyQueryParams(MetricsQuery query, MultivaluedMap<String, String> queryParameters) {\n    if(queryParameters.getFirst(QUERY_PARAM_START_DATE) != null) {\n      Date startDate = dateConverter.convertQueryParameterToType(queryParameters.getFirst(QUERY_PARAM_START_DATE));\n      query.startDate(startDate);\n    }\n\n    if(queryParameters.getFirst(QUERY_PARAM_END_DATE) != null) {\n      Date endDate = dateConverter.convertQueryParameterToType(queryParameters.getFirst(QUERY_PARAM_END_DATE));\n      query.endDate(endDate);\n    }\n\n    IntegerConverter intConverter = new IntegerConverter();\n    intConverter.setObjectMapper(objectMapper);\n\n    if (queryParameters.getFirst(QUERY_PARAM_FIRST_RESULT) != null) {\n      int firstResult = intConverter.convertQueryParameterToType(queryParameters.getFirst(QUERY_PARAM_FIRST_RESULT));\n      query.offset(firstResult);\n    }\n\n    if (queryParameters.getFirst(QUERY_PARAM_MAX_RESULTS) != null) {\n      int maxResults = intConverter.convertQueryParameterToType(queryParameters.getFirst(QUERY_PARAM_MAX_RESULTS));\n      query.limit(maxResults);\n    }\n\n    if(queryParameters.getFirst(QUERY_PARAM_AGG_BY_REPORTER) != null) {\n      query.aggregateByReporter();\n    }\n  }\n\n  protected List<MetricsIntervalResultDto> convertToDtos(List<MetricIntervalValue> metrics) {\n    List<MetricsIntervalResultDto> intervalMetrics = new ArrayList<>();\n    for (MetricIntervalValue m : metrics) {\n      intervalMetrics.add(new MetricsIntervalResultDto(m));\n    }\n    return intervalMetrics;\n  }\n}\n","lang_cluster":"Java","length":134,"code_uid":"c58162ea8efe4920beb60661df6ddacd"}
{"diff_hunk":"@@ -52,6 +52,9 @@ public class JobTypePluginSet {\n     this.pluginJobPropsProcessor = new HashMap<>();\n     this.jobToClassName = new HashMap<>();\n     this.jobToClassLoaderURLs = new HashMap<>();\n+    this.jobToDefaultProxyUser = new HashMap<>();\n+    this.defaultProxyUsersJobTypeClasses = new HashSet<>();\n+    this.defaultProxyUsersFilter = new HashSet<>();\n   }\n \n   \/**","old_code":"\/*\n * Copyright 2014 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n *\/\npackage azkaban.jobtype;\n\nimport azkaban.utils.Props;\n\nimport java.net.URL;\nimport java.util.HashMap;\nimport java.util.Map;\n\n\/**\n * Container for job type plugins\n *\n * This contains the jobClass objects, the properties for loading plugins, and the properties given\n * by default to the plugin.\n *\n * This class is not thread safe, so adding to this class should only be populated and controlled by\n * the JobTypeManager\n *\/\npublic class JobTypePluginSet {\n  private static final URL[] EMPTY_URLS = new URL[0];\n  private final Map<String, Props> pluginJobPropsMap;\n  private final Map<String, Props> pluginLoadPropsMap;\n  private final Map<String, Props> pluginPrivatePropsMap;\n  private final Map<String, JobPropsProcessor> pluginJobPropsProcessor;\n  private final Map<String, String> jobToClassName;\n  private final Map<String, URL[]> jobToClassLoaderURLs;\n\n  private Props commonJobProps;\n  private Props commonLoadProps;\n\n  \/**\n   * Base constructor\n   *\/\n  public JobTypePluginSet() {\n    this.pluginJobPropsMap = new HashMap<>();\n    this.pluginLoadPropsMap = new HashMap<>();\n    this.pluginPrivatePropsMap = new HashMap<>();\n    this.pluginJobPropsProcessor = new HashMap<>();\n    this.jobToClassName = new HashMap<>();\n    this.jobToClassLoaderURLs = new HashMap<>();\n  }\n\n  \/**\n   * Copy constructor\n   *\/\n  public JobTypePluginSet(final JobTypePluginSet clone) {\n    this.pluginJobPropsMap = new HashMap<>(clone.pluginJobPropsMap);\n    this.pluginLoadPropsMap = new HashMap<>(clone.pluginLoadPropsMap);\n    this.pluginPrivatePropsMap = new HashMap<>(clone.pluginPrivatePropsMap);\n    this.commonJobProps = clone.commonJobProps;\n    this.commonLoadProps = clone.commonLoadProps;\n    this.pluginJobPropsProcessor = clone.pluginJobPropsProcessor;\n    this.jobToClassName = clone.jobToClassName;\n    this.jobToClassLoaderURLs = clone.jobToClassLoaderURLs;\n  }\n\n  \/**\n   * Gets common properties for every jobtype\n   *\/\n  public Props getCommonPluginJobProps() {\n    return this.commonJobProps;\n  }\n\n  \/**\n   * Sets the common properties shared in every jobtype\n   *\/\n  public void setCommonPluginJobProps(final Props commonJobProps) {\n    this.commonJobProps = commonJobProps;\n  }\n\n  \/**\n   * Gets the common properties used to load a plugin\n   *\/\n  public Props getCommonPluginLoadProps() {\n    return this.commonLoadProps;\n  }\n\n  \/**\n   * Sets the common properties used to load every plugin\n   *\/\n  public void setCommonPluginLoadProps(final Props commonLoadProps) {\n    this.commonLoadProps = commonLoadProps;\n  }\n\n  \/**\n   * Get the properties for a jobtype used to setup and load a plugin\n   *\/\n  public Props getPluginLoaderProps(final String jobTypeName) {\n    return this.pluginLoadPropsMap.get(jobTypeName);\n  }\n\n  \/**\n   * Get the plugin private properties for the jobtype\n   *\/\n  public Props getPluginPrivateProps(final String jobTypeName) {\n    return this.pluginPrivatePropsMap.get(jobTypeName);\n  }\n  \/**\n   * Get the properties that will be given to the plugin as default job properties.\n   *\/\n  public Props getPluginJobProps(final String jobTypeName) {\n    return this.pluginJobPropsMap.get(jobTypeName);\n  }\n\n  public void addPluginClassName(final String jobTypeName, final String jobTypeClassName) {\n    this.jobToClassName.put(jobTypeName, jobTypeClassName);\n  }\n\n  \/**\n   * Gets the plugin job class name\n   *\/\n  public String getPluginClassName(final String jobTypeName) {\n    return this.jobToClassName.get(jobTypeName);\n  }\n\n  \/**\n   * Get the resource URLs that should be added to its associated job ClassLoader.\n   *\/\n  public URL[] getPluginClassLoaderURLs(final String jobTypeName) {\n    return this.jobToClassLoaderURLs.getOrDefault(jobTypeName, EMPTY_URLS);\n  }\n\n  \/**\n   * Adds plugin job properties used as default runtime properties\n   *\/\n  public void addPluginJobProps(final String jobTypeName, final Props props) {\n    this.pluginJobPropsMap.put(jobTypeName, props);\n  }\n\n  \/**\n   * Add resource URLs that should be made available to ClassLoader of all jobs of the given jobtype.\n   *\/\n  public void addPluginClassLoaderURLs(final String jobTypeName, final URL[] urls) {\n    this.jobToClassLoaderURLs.put(jobTypeName, urls);\n  }\n\n  \/**\n   * Adds plugin load properties used to load the plugin\n   *\/\n  public void addPluginLoadProps(final String jobTypeName, final Props props) {\n    this.pluginLoadPropsMap.put(jobTypeName, props);\n  }\n\n  \/**\n   * Adds plugins private properties used by the plugin\n   *\/\n  public void addPluginPrivateProps(final String jobTypeName, final Props props) {\n    this.pluginPrivatePropsMap.put(jobTypeName, props);\n  }\n\n  public JobPropsProcessor getPluginJobPropsProcessor(\n      final String jobTypeName) {\n    return this.pluginJobPropsProcessor.get(jobTypeName);\n  }\n\n  public void addPluginJobPropsProcessor(final String jobTypeName,\n      JobPropsProcessor jobPropsProcessor) {\n    this.pluginJobPropsProcessor.put(jobTypeName, jobPropsProcessor);\n  }\n}\n","lang_cluster":"Java","length":174,"code_uid":"9c830622bbdc4f06b9828506fa5c3fc3"}
{"diff_hunk":"@@ -56,28 +56,6 @@ public class DefaultString {\n     return comment;\n   }\n \n-  private static final ImmutableMap<SampleKey, String> SAMPLE_STRINGS =\n-      ImmutableMap.<SampleKey, String>builder()\n-          .put(\n-              SampleKey.create(\"compute\", \"zone\", \"[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?\"),\n-              \"us-central1-f\")\n-          .put(\n-              SampleKey.create(\"autoscaler\", \"zone\", \"[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?\"),\n-              \"us-central1-f\")\n-          .put(\n-              SampleKey.create(\"clouduseraccounts\", \"zone\", \"[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?\"),\n-              \"us-central1-f\")\n-          .build();\n-\n-  public static String getSample(String apiName, String fieldName, String pattern) {\n-    String sample = null;\n-    if (pattern != null) {\n-      \/\/ If the pattern has a specially-recognized sample, use the sample.\n-      sample = SAMPLE_STRINGS.get(SampleKey.create(apiName, fieldName, pattern));\n-    }\n-    return sample == null ? \"\" : sample;\n-  }\n-\n   \/**\n    * Returns a non-trivial placeholder for pattern with a no-brace and lower-case format style. An\n    * empty string is returned for unrecognized patterns.","old_code":"\/* Copyright 2016 Google Inc\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage com.google.api.codegen.discovery;\n\nimport com.google.api.codegen.Inflector;\nimport com.google.api.codegen.LanguageUtil;\nimport com.google.auto.value.AutoValue;\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.ImmutableMap;\nimport java.util.ArrayList;\nimport java.util.List;\nimport javax.annotation.Nullable;\n\n\/** Creates default string from path patterns. *\/\npublic class DefaultString {\n\n  @AutoValue\n  abstract static class SampleKey {\n    abstract String getApiName();\n\n    abstract String getFieldName();\n\n    abstract String getRegexp();\n\n    static SampleKey create(String apiName, String fieldName, String regexp) {\n      return new AutoValue_DefaultString_SampleKey(apiName, fieldName, regexp);\n    }\n  }\n\n  private final String define;\n  private final String comment;\n\n  public DefaultString(String define, String comment) {\n    this.define = define;\n    this.comment = comment;\n  }\n\n  public String getDefine() {\n    return define;\n  }\n\n  public String getComment() {\n    return comment;\n  }\n\n  private static final ImmutableMap<SampleKey, String> SAMPLE_STRINGS =\n      ImmutableMap.<SampleKey, String>builder()\n          .put(\n              SampleKey.create(\"compute\", \"zone\", \"[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?\"),\n              \"us-central1-f\")\n          .put(\n              SampleKey.create(\"autoscaler\", \"zone\", \"[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?\"),\n              \"us-central1-f\")\n          .put(\n              SampleKey.create(\"clouduseraccounts\", \"zone\", \"[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?\"),\n              \"us-central1-f\")\n          .build();\n\n  public static String getSample(String apiName, String fieldName, String pattern) {\n    String sample = null;\n    if (pattern != null) {\n      \/\/ If the pattern has a specially-recognized sample, use the sample.\n      sample = SAMPLE_STRINGS.get(SampleKey.create(apiName, fieldName, pattern));\n    }\n    return sample == null ? \"\" : sample;\n  }\n\n  \/**\n   * Returns a non-trivial placeholder for pattern with a no-brace and lower-case format style. An\n   * empty string is returned for unrecognized patterns.\n   *\n   * <p>Variables are formatted according to format (ex: \"my-%s\").\n   *\n   * <p>For example: \"projects\/my-project\/logs\/my-log\" or \"my-project\"\n   *\/\n  public static String getNonTrivialPlaceholder(String pattern, String format) {\n    if (pattern != null) {\n      String def = forPattern(pattern, format);\n      if (def != null) {\n        return def;\n      }\n    }\n    return \"\";\n  }\n\n  private static final String WILDCARD_PATTERN = \"[^\/]*\";\n\n  \/** Returns a default string from `pattern`, or null if the pattern is not supported. *\/\n  @VisibleForTesting\n  static String forPattern(String pattern, String placeholderFormat) {\n    \/\/ We only care about patterns that have alternating literal and wildcard like\n    \/\/  ^foo\/[^\/]*\/bar\/[^\/]*$\n    \/\/ Ignore if what we get looks nothing like this.\n    if (pattern == null || !pattern.startsWith(\"^\") || !pattern.endsWith(\"$\")) {\n      return null;\n    }\n    pattern = pattern.substring(1, pattern.length() - 1);\n    ImmutableList<Elem> elems = parse(pattern);\n    if (!validElems(elems)) {\n      return null;\n    }\n\n    StringBuilder ret = new StringBuilder();\n    for (int i = 0; i < elems.size(); i += 2) {\n      String literal = elems.get(i).getLiteral();\n      String placeholder = Inflector.singularize(literal);\n      placeholder = LanguageUtil.lowerCamelToLowerUnderscore(placeholder).replace('_', '-');\n      ret.append('\/')\n          .append(literal)\n          .append(\"\/\")\n          .append(String.format(placeholderFormat, placeholder));\n    }\n    return ret.substring(1);\n  }\n\n  \/**\n   * Parses pattern, with the leading '^' and trailing '$' removed, into a list representing the\n   * pattern.\n   *\/\n  private static ImmutableList<Elem> parse(String pattern) {\n    List<Elem> elems = new ArrayList<>();\n    while (pattern.length() > 0) {\n      int slash;\n      if (pattern.startsWith(WILDCARD_PATTERN)) {\n        elems.add(Elem.WILDCARD);\n        pattern = pattern.substring(WILDCARD_PATTERN.length());\n      } else if ((slash = pattern.indexOf(\"\/\")) >= 0) {\n        elems.add(Elem.createLiteral(pattern.substring(0, slash)));\n        pattern = pattern.substring(slash);\n      } else {\n        elems.add(Elem.createLiteral(pattern));\n        pattern = \"\";\n      }\n\n      if (pattern.startsWith(\"\/\")) {\n        pattern = pattern.substring(1);\n      }\n    }\n    return ImmutableList.<Elem>copyOf(elems);\n  }\n\n  \/**\n   * Returns whether the pattern represented by the list is in a form we expect.\n   *\n   * <p>A valid pattern must have the same number of literals and wildcards, alternating, and starts\n   * with a literal. Literals must consists of only letters.\n   *\/\n  private static boolean validElems(ImmutableList<Elem> elems) {\n    if (elems.size() % 2 != 0) {\n      return false;\n    }\n    ImmutableList<ElemType> expect =\n        ImmutableList.<ElemType>of(ElemType.LITERAL, ElemType.WILDCARD);\n    for (int i = 0; i < elems.size(); i++) {\n      if (elems.get(i).getType() != expect.get(i % expect.size())) {\n        return false;\n      }\n    }\n    for (int i = 0; i < elems.size(); i += 2) {\n      for (char c : elems.get(i).getLiteral().toCharArray()) {\n        if (!Character.isLetter(c)) {\n          return false;\n        }\n      }\n    }\n    return true;\n  }\n\n  enum ElemType {\n    LITERAL,\n    WILDCARD\n  }\n\n  @AutoValue\n  abstract static class Elem {\n    abstract ElemType getType();\n\n    @Nullable\n    abstract String getLiteral();\n\n    private static final Elem WILDCARD = new AutoValue_DefaultString_Elem(ElemType.WILDCARD, null);\n\n    private static Elem createLiteral(String lit) {\n      return new AutoValue_DefaultString_Elem(ElemType.LITERAL, lit);\n    }\n  }\n}\n","lang_cluster":"Java","length":200,"code_uid":"751dc5a4e25949c39e4d7b6c11a5869e"}
{"diff_hunk":"@@ -109,11 +109,7 @@ public class BesuEventsImpl implements BesuEvents {\n             .collect(toUnmodifiableList());\n     final List<List<LogTopic>> besuTopics =\n         topics.stream()\n-            .map(\n-                subList ->\n-                    subList.stream()\n-                        .map(bytes -> LogTopic.wrap(bytes))\n-                        .collect(toUnmodifiableList()))\n+            .map(subList -> subList.stream().map(LogTopic::wrap).collect(toUnmodifiableList()))\n             .collect(toUnmodifiableList());\n \n     final LogsQuery logsQuery = new LogsQuery(besuAddresses, besuTopics);","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.services;\n\nimport static java.util.stream.Collectors.toUnmodifiableList;\n\nimport org.hyperledger.besu.ethereum.api.query.LogsQuery;\nimport org.hyperledger.besu.ethereum.chain.Blockchain;\nimport org.hyperledger.besu.ethereum.core.BlockBody;\nimport org.hyperledger.besu.ethereum.core.Difficulty;\nimport org.hyperledger.besu.ethereum.core.LogTopic;\nimport org.hyperledger.besu.ethereum.core.LogWithMetadata;\nimport org.hyperledger.besu.ethereum.eth.sync.BlockBroadcaster;\nimport org.hyperledger.besu.ethereum.eth.sync.state.SyncState;\nimport org.hyperledger.besu.ethereum.eth.transactions.TransactionPool;\nimport org.hyperledger.besu.plugin.data.Address;\nimport org.hyperledger.besu.plugin.data.BlockHeader;\nimport org.hyperledger.besu.plugin.data.PropagatedBlockContext;\nimport org.hyperledger.besu.plugin.services.BesuEvents;\n\nimport java.util.List;\nimport java.util.function.Supplier;\n\nimport org.apache.tuweni.bytes.Bytes32;\nimport org.apache.tuweni.units.bigints.UInt256;\n\npublic class BesuEventsImpl implements BesuEvents {\n  private final Blockchain blockchain;\n  private final BlockBroadcaster blockBroadcaster;\n  private final TransactionPool transactionPool;\n  private final SyncState syncState;\n\n  public BesuEventsImpl(\n      final Blockchain blockchain,\n      final BlockBroadcaster blockBroadcaster,\n      final TransactionPool transactionPool,\n      final SyncState syncState) {\n    this.blockchain = blockchain;\n    this.blockBroadcaster = blockBroadcaster;\n    this.transactionPool = transactionPool;\n    this.syncState = syncState;\n  }\n\n  @Override\n  public long addBlockPropagatedListener(final BlockPropagatedListener listener) {\n    return blockBroadcaster.subscribePropagateNewBlocks(\n        (block, totalDifficulty) ->\n            listener.onBlockPropagated(\n                blockPropagatedContext(block::getHeader, block::getBody, () -> totalDifficulty)));\n  }\n\n  @Override\n  public void removeBlockPropagatedListener(final long listenerIdentifier) {\n    blockBroadcaster.unsubscribePropagateNewBlocks(listenerIdentifier);\n  }\n\n  @Override\n  public long addTransactionAddedListener(final TransactionAddedListener listener) {\n    return transactionPool.subscribePendingTransactions(listener::onTransactionAdded);\n  }\n\n  @Override\n  public void removeTransactionAddedListener(final long listenerIdentifier) {\n    transactionPool.unsubscribePendingTransactions(listenerIdentifier);\n  }\n\n  @Override\n  public long addTransactionDroppedListener(\n      final TransactionDroppedListener transactionDroppedListener) {\n    return transactionPool.subscribeDroppedTransactions(\n        transactionDroppedListener::onTransactionDropped);\n  }\n\n  @Override\n  public void removeTransactionDroppedListener(final long listenerIdentifier) {\n    transactionPool.unsubscribeDroppedTransactions(listenerIdentifier);\n  }\n\n  @Override\n  public long addSyncStatusListener(final SyncStatusListener syncStatusListener) {\n    return syncState.subscribeSyncStatus(syncStatusListener);\n  }\n\n  @Override\n  public void removeSyncStatusListener(final long listenerIdentifier) {\n    syncState.unsubscribeSyncStatus(listenerIdentifier);\n  }\n\n  @Override\n  public long addLogListener(\n      final List<Address> addresses,\n      final List<List<Bytes32>> topics,\n      final LogListener logListener) {\n    final List<org.hyperledger.besu.ethereum.core.Address> besuAddresses =\n        addresses.stream()\n            .map(org.hyperledger.besu.ethereum.core.Address::fromPlugin)\n            .collect(toUnmodifiableList());\n    final List<List<LogTopic>> besuTopics =\n        topics.stream()\n            .map(\n                subList ->\n                    subList.stream()\n                        .map(bytes -> LogTopic.wrap(bytes))\n                        .collect(toUnmodifiableList()))\n            .collect(toUnmodifiableList());\n\n    final LogsQuery logsQuery = new LogsQuery(besuAddresses, besuTopics);\n\n    return blockchain.observeLogs(\n        logWithMetadata -> {\n          if (logsQuery.matches(LogWithMetadata.fromPlugin(logWithMetadata))) {\n            logListener.onLogEmitted(logWithMetadata);\n          }\n        });\n  }\n\n  @Override\n  public void removeLogListener(final long listenerIdentifier) {\n    blockchain.removeObserver(listenerIdentifier);\n  }\n\n  private static PropagatedBlockContext blockPropagatedContext(\n      final Supplier<BlockHeader> blockHeaderSupplier,\n      final Supplier<BlockBody> blockBodySupplier,\n      final Supplier<Difficulty> totalDifficultySupplier) {\n    return new PropagatedBlockContext() {\n      @Override\n      public BlockHeader getBlockHeader() {\n        return blockHeaderSupplier.get();\n      }\n\n      @Override\n      public BlockBody getBlockBody() {\n        return blockBodySupplier.get();\n      }\n\n      @Override\n      public UInt256 getTotalDifficulty() {\n        return totalDifficultySupplier.get().toUInt256();\n      }\n    };\n  }\n}\n","lang_cluster":"Java","length":155,"code_uid":"b90907daf726498b9b94af038ec52a6f"}
{"diff_hunk":"@@ -41,11 +41,13 @@ class ParquetReadSupport<T> extends ReadSupport<T> {\n   private final Schema expectedSchema;\n   private final ReadSupport<T> wrapped;\n   private final boolean callInit;\n+  private final NameMapping nameMapping;\n \n-  ParquetReadSupport(Schema expectedSchema, ReadSupport<T> readSupport, boolean callInit) {\n+  ParquetReadSupport(Schema expectedSchema, ReadSupport<T> readSupport, boolean callInit, NameMapping nameMapping) {\n     this.expectedSchema = expectedSchema;\n     this.wrapped = readSupport;\n     this.callInit = callInit;\n+    this.nameMapping = nameMapping;\n   }\n \n   @Override","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.parquet;\n\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.collect.Sets;\nimport java.util.Map;\nimport java.util.Set;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.iceberg.Schema;\nimport org.apache.iceberg.avro.AvroSchemaUtil;\nimport org.apache.parquet.avro.AvroReadSupport;\nimport org.apache.parquet.hadoop.api.InitContext;\nimport org.apache.parquet.hadoop.api.ReadSupport;\nimport org.apache.parquet.io.api.RecordMaterializer;\nimport org.apache.parquet.schema.MessageType;\n\n\/**\n * Parquet {@link ReadSupport} that handles column projection based on {@link Schema} column IDs.\n *\n * @param <T> Java type produced by this read support instance\n *\/\nclass ParquetReadSupport<T> extends ReadSupport<T> {\n  private final Schema expectedSchema;\n  private final ReadSupport<T> wrapped;\n  private final boolean callInit;\n\n  ParquetReadSupport(Schema expectedSchema, ReadSupport<T> readSupport, boolean callInit) {\n    this.expectedSchema = expectedSchema;\n    this.wrapped = readSupport;\n    this.callInit = callInit;\n  }\n\n  @Override\n  @SuppressWarnings(\"deprecation\")\n  public ReadContext init(Configuration configuration, Map<String, String> keyValueMetaData, MessageType fileSchema) {\n    \/\/ Columns are selected from the Parquet file by taking the read context's message type and\n    \/\/ matching to the file's columns by full path, so this must select columns by using the path\n    \/\/ in the file's schema.\n\n    MessageType projection = ParquetSchemaUtil.hasIds(fileSchema) ?\n        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n\n    \/\/ override some known backward-compatibility options\n    configuration.set(\"parquet.strict.typing\", \"false\");\n    configuration.set(\"parquet.avro.add-list-element-records\", \"false\");\n    configuration.set(\"parquet.avro.write-old-list-structure\", \"false\");\n\n    \/\/ set Avro schemas in case the reader is Avro\n    AvroReadSupport.setRequestedProjection(configuration,\n        AvroSchemaUtil.convert(expectedSchema, projection.getName()));\n    org.apache.avro.Schema avroReadSchema = AvroSchemaUtil.buildAvroProjection(\n        AvroSchemaUtil.convert(ParquetSchemaUtil.convert(projection), projection.getName()),\n        expectedSchema, ImmutableMap.of());\n    AvroReadSupport.setAvroReadSchema(configuration, ParquetAvro.parquetAvroSchema(avroReadSchema));\n\n    \/\/ let the context set up read support metadata, but always use the correct projection\n    ReadContext context = null;\n    if (callInit) {\n      try {\n        context = wrapped.init(configuration, keyValueMetaData, projection);\n      } catch (UnsupportedOperationException e) {\n        \/\/ try the InitContext version\n        context = wrapped.init(new InitContext(\n            configuration, makeMultimap(keyValueMetaData), projection));\n      }\n    }\n\n    return new ReadContext(projection,\n        context != null ? context.getReadSupportMetadata() : ImmutableMap.of());\n  }\n\n  @Override\n  public RecordMaterializer<T> prepareForRead(Configuration configuration,\n                                              Map<String, String> fileMetadata,\n                                              MessageType fileMessageType,\n                                              ReadContext readContext) {\n    \/\/ This is the type created in init that was based on the file's schema. The schema that this\n    \/\/ will pass to the wrapped ReadSupport needs to match the expected schema's names. Rather than\n    \/\/ renaming the file's schema, convert the expected schema to Parquet. This relies on writing\n    \/\/ files with the correct schema.\n    \/\/ TODO: this breaks when columns are reordered.\n    MessageType readSchema = ParquetSchemaUtil.convert(expectedSchema, fileMessageType.getName());\n    return wrapped.prepareForRead(configuration, fileMetadata, readSchema, readContext);\n  }\n\n  private Map<String, Set<String>> makeMultimap(Map<String, String> map) {\n    ImmutableMap.Builder<String, Set<String>> builder = ImmutableMap.builder();\n    for (Map.Entry<String, String> entry : map.entrySet()) {\n      builder.put(entry.getKey(), Sets.newHashSet(entry.getValue()));\n    }\n    return builder.build();\n  }\n}\n","lang_cluster":"Java","length":112,"code_uid":"b25f025375e545ec8796449b691e2261"}
{"diff_hunk":"@@ -19,6 +19,7 @@ import static org.hyperledger.besu.util.FutureUtils.exceptionallyCompose;\n import org.hyperledger.besu.ethereum.ProtocolContext;\n import org.hyperledger.besu.ethereum.core.BlockHeader;\n import org.hyperledger.besu.ethereum.eth.manager.EthContext;\n+import org.hyperledger.besu.ethereum.eth.manager.EthPeer;\n import org.hyperledger.besu.ethereum.eth.manager.task.WaitForPeersTask;\n import org.hyperledger.besu.ethereum.eth.sync.ChainDownloader;\n import org.hyperledger.besu.ethereum.eth.sync.SynchronizerConfiguration;","old_code":"\/*\n * Copyright 2019 ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\/\npackage org.hyperledger.besu.ethereum.eth.sync.fastsync;\n\nimport static java.util.concurrent.CompletableFuture.completedFuture;\nimport static org.hyperledger.besu.util.FutureUtils.completedExceptionally;\nimport static org.hyperledger.besu.util.FutureUtils.exceptionallyCompose;\n\nimport org.hyperledger.besu.ethereum.ProtocolContext;\nimport org.hyperledger.besu.ethereum.core.BlockHeader;\nimport org.hyperledger.besu.ethereum.eth.manager.EthContext;\nimport org.hyperledger.besu.ethereum.eth.manager.task.WaitForPeersTask;\nimport org.hyperledger.besu.ethereum.eth.sync.ChainDownloader;\nimport org.hyperledger.besu.ethereum.eth.sync.SynchronizerConfiguration;\nimport org.hyperledger.besu.ethereum.eth.sync.state.SyncState;\nimport org.hyperledger.besu.ethereum.mainnet.ProtocolSchedule;\nimport org.hyperledger.besu.metrics.BesuMetricCategory;\nimport org.hyperledger.besu.plugin.services.MetricsSystem;\nimport org.hyperledger.besu.plugin.services.metrics.Counter;\nimport org.hyperledger.besu.util.ExceptionUtils;\n\nimport java.time.Duration;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicLong;\n\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\n\npublic class FastSyncActions<C> {\n\n  private static final Logger LOG = LogManager.getLogger();\n  private final SynchronizerConfiguration syncConfig;\n  private final ProtocolSchedule<C> protocolSchedule;\n  private final ProtocolContext<C> protocolContext;\n  private final EthContext ethContext;\n  private final SyncState syncState;\n  private final MetricsSystem metricsSystem;\n  private final Counter pivotBlockSelectionCounter;\n  private final AtomicLong pivotBlockGauge = new AtomicLong(0);\n\n  public FastSyncActions(\n      final SynchronizerConfiguration syncConfig,\n      final ProtocolSchedule<C> protocolSchedule,\n      final ProtocolContext<C> protocolContext,\n      final EthContext ethContext,\n      final SyncState syncState,\n      final MetricsSystem metricsSystem) {\n    this.syncConfig = syncConfig;\n    this.protocolSchedule = protocolSchedule;\n    this.protocolContext = protocolContext;\n    this.ethContext = ethContext;\n    this.syncState = syncState;\n    this.metricsSystem = metricsSystem;\n\n    pivotBlockSelectionCounter =\n        metricsSystem.createCounter(\n            BesuMetricCategory.SYNCHRONIZER,\n            \"fast_sync_pivot_block_selected_count\",\n            \"Number of times a fast sync pivot block has been selected\");\n    metricsSystem.createLongGauge(\n        BesuMetricCategory.SYNCHRONIZER,\n        \"fast_sync_pivot_block_current\",\n        \"The current fast sync pivot block\",\n        pivotBlockGauge::get);\n  }\n\n  public CompletableFuture<FastSyncState> waitForSuitablePeers(final FastSyncState fastSyncState) {\n    if (fastSyncState.hasPivotBlockHeader()) {\n      return waitForAnyPeer().thenApply(ignore -> fastSyncState);\n    }\n\n    LOG.debug(\"Waiting for at least {} peers.\", syncConfig.getFastSyncMinimumPeerCount());\n    return waitForPeers(syncConfig.getFastSyncMinimumPeerCount())\n        .thenApply(successfulWaitResult -> fastSyncState);\n  }\n\n  private CompletableFuture<Void> waitForAnyPeer() {\n    final CompletableFuture<Void> waitForPeerResult =\n        ethContext.getScheduler().timeout(WaitForPeersTask.create(ethContext, 1, metricsSystem));\n    return exceptionallyCompose(\n        waitForPeerResult,\n        throwable -> {\n          if (ExceptionUtils.rootCause(throwable) instanceof TimeoutException) {\n            return waitForAnyPeer();\n          }\n          return completedExceptionally(throwable);\n        });\n  }\n\n  private CompletableFuture<Void> waitForPeers(final int count) {\n    final WaitForPeersTask waitForPeersTask =\n        WaitForPeersTask.create(ethContext, count, metricsSystem);\n    return waitForPeersTask.run();\n  }\n\n  public CompletableFuture<FastSyncState> selectPivotBlock(final FastSyncState fastSyncState) {\n    return fastSyncState.hasPivotBlockHeader()\n        ? completedFuture(fastSyncState)\n        : selectPivotBlockFromPeers();\n  }\n\n  private CompletableFuture<FastSyncState> selectPivotBlockFromPeers() {\n    return ethContext\n        .getEthPeers()\n        .bestPeerWithHeightEstimate()\n        \/\/ Only select a pivot block number when we have a minimum number of height estimates\n        .filter(\n            peer -> {\n              final long peerCount = countPeersWithEstimatedHeight();\n              final int minPeerCount = syncConfig.getFastSyncMinimumPeerCount();\n              if (peerCount < minPeerCount) {\n                LOG.info(\n                    \"Waiting for peers with chain height information.  {} \/ {} required peers currently available.\",\n                    peerCount,\n                    minPeerCount);\n                return false;\n              }\n              return true;\n            })\n        .map(\n            peer -> {\n              final long pivotBlockNumber =\n                  peer.chainState().getEstimatedHeight() - syncConfig.getFastSyncPivotDistance();\n              if (pivotBlockNumber <= BlockHeader.GENESIS_BLOCK_NUMBER) {\n                \/\/ Peer's chain isn't long enough, return an empty value so we can try again.\n                LOG.info(\"Waiting for peer with sufficient chain height\");\n                return null;\n              }\n              LOG.info(\"Selecting block number {} as fast sync pivot block.\", pivotBlockNumber);\n              pivotBlockSelectionCounter.inc();\n              pivotBlockGauge.set(pivotBlockNumber);\n              return completedFuture(new FastSyncState(pivotBlockNumber));\n            })\n        .orElseGet(this::retrySelectPivotBlockAfterDelay);\n  }\n\n  private long countPeersWithEstimatedHeight() {\n    return ethContext\n        .getEthPeers()\n        .streamAvailablePeers()\n        .filter(peer -> peer.chainState().hasEstimatedHeight())\n        .count();\n  }\n\n  private CompletableFuture<FastSyncState> retrySelectPivotBlockAfterDelay() {\n    return ethContext\n        .getScheduler()\n        .scheduleFutureTask(\n            () ->\n                waitForPeers(syncConfig.getFastSyncMinimumPeerCount())\n                    .thenCompose(ignore -> selectPivotBlockFromPeers()),\n            Duration.ofSeconds(1));\n  }\n\n  public CompletableFuture<FastSyncState> downloadPivotBlockHeader(\n      final FastSyncState currentState) {\n    if (currentState.getPivotBlockHeader().isPresent()) {\n      return completedFuture(currentState);\n    }\n    return new PivotBlockRetriever<>(\n            protocolSchedule,\n            ethContext,\n            metricsSystem,\n            currentState.getPivotBlockNumber().getAsLong())\n        .downloadPivotBlockHeader();\n  }\n\n  public ChainDownloader createChainDownloader(final FastSyncState currentState) {\n    return FastSyncChainDownloader.create(\n        syncConfig,\n        protocolSchedule,\n        protocolContext,\n        ethContext,\n        syncState,\n        metricsSystem,\n        currentState.getPivotBlockHeader().get());\n  }\n}\n","lang_cluster":"Java","length":189,"code_uid":"c70bc846b83848baa7d15d2bc9510411"}
{"diff_hunk":"@@ -43,10 +43,14 @@ import org.apache.spark.sql.connector.write.WriteBuilder;\n import org.apache.spark.sql.sources.Filter;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public class SparkTable implements org.apache.spark.sql.connector.catalog.Table,\n     SupportsRead, SupportsWrite, SupportsDelete {\n \n+  private static final Logger LOG = LoggerFactory.getLogger(SparkTable.class);\n+\n   private static final Set<String> RESERVED_PROPERTIES = Sets.newHashSet(\"provider\", \"format\", \"current-snapshot-id\");\n   private static final Set<TableCapability> CAPABILITIES = ImmutableSet.of(\n       TableCapability.BATCH_READ,","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.spark.source;\n\nimport java.util.Map;\nimport java.util.Set;\nimport org.apache.iceberg.Table;\nimport org.apache.iceberg.TableProperties;\nimport org.apache.iceberg.exceptions.ValidationException;\nimport org.apache.iceberg.expressions.Expression;\nimport org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\nimport org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\nimport org.apache.iceberg.relocated.com.google.common.collect.Sets;\nimport org.apache.iceberg.spark.Spark3Util;\nimport org.apache.iceberg.spark.SparkFilters;\nimport org.apache.iceberg.spark.SparkSchemaUtil;\nimport org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.connector.catalog.SupportsDelete;\nimport org.apache.spark.sql.connector.catalog.SupportsRead;\nimport org.apache.spark.sql.connector.catalog.SupportsWrite;\nimport org.apache.spark.sql.connector.catalog.TableCapability;\nimport org.apache.spark.sql.connector.expressions.Transform;\nimport org.apache.spark.sql.connector.read.ScanBuilder;\nimport org.apache.spark.sql.connector.write.LogicalWriteInfo;\nimport org.apache.spark.sql.connector.write.WriteBuilder;\nimport org.apache.spark.sql.sources.Filter;\nimport org.apache.spark.sql.types.StructType;\nimport org.apache.spark.sql.util.CaseInsensitiveStringMap;\n\npublic class SparkTable implements org.apache.spark.sql.connector.catalog.Table,\n    SupportsRead, SupportsWrite, SupportsDelete {\n\n  private static final Set<String> RESERVED_PROPERTIES = Sets.newHashSet(\"provider\", \"format\", \"current-snapshot-id\");\n  private static final Set<TableCapability> CAPABILITIES = ImmutableSet.of(\n      TableCapability.BATCH_READ,\n      TableCapability.BATCH_WRITE,\n      TableCapability.STREAMING_WRITE,\n      TableCapability.OVERWRITE_BY_FILTER,\n      TableCapability.OVERWRITE_DYNAMIC);\n\n  private final Table icebergTable;\n  private final StructType requestedSchema;\n  private final boolean refreshEagerly;\n  private StructType lazyTableSchema = null;\n  private SparkSession lazySpark = null;\n\n  public SparkTable(Table icebergTable, boolean refreshEagerly) {\n    this(icebergTable, null, refreshEagerly);\n  }\n\n  public SparkTable(Table icebergTable, StructType requestedSchema, boolean refreshEagerly) {\n    this.icebergTable = icebergTable;\n    this.requestedSchema = requestedSchema;\n    this.refreshEagerly = refreshEagerly;\n\n    if (requestedSchema != null) {\n      \/\/ convert the requested schema to throw an exception if any requested fields are unknown\n      SparkSchemaUtil.convert(icebergTable.schema(), requestedSchema);\n    }\n  }\n\n  private SparkSession sparkSession() {\n    if (lazySpark == null) {\n      this.lazySpark = SparkSession.active();\n    }\n\n    return lazySpark;\n  }\n\n  public Table table() {\n    return icebergTable;\n  }\n\n  @Override\n  public String name() {\n    return icebergTable.toString();\n  }\n\n  @Override\n  public StructType schema() {\n    if (lazyTableSchema == null) {\n      if (requestedSchema != null) {\n        this.lazyTableSchema = SparkSchemaUtil.convert(SparkSchemaUtil.prune(icebergTable.schema(), requestedSchema));\n      } else {\n        this.lazyTableSchema = SparkSchemaUtil.convert(icebergTable.schema());\n      }\n    }\n\n    return lazyTableSchema;\n  }\n\n  @Override\n  public Transform[] partitioning() {\n    return Spark3Util.toTransforms(icebergTable.spec());\n  }\n\n  @Override\n  public Map<String, String> properties() {\n    ImmutableMap.Builder<String, String> propsBuilder = ImmutableMap.builder();\n\n    String fileFormat = icebergTable.properties()\n        .getOrDefault(TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n    propsBuilder.put(\"format\", \"iceberg\/\" + fileFormat);\n    propsBuilder.put(\"provider\", \"iceberg\");\n    String currentSnapshotId = icebergTable.currentSnapshot() != null ?\n        String.valueOf(icebergTable.currentSnapshot().snapshotId()) : \"none\";\n    propsBuilder.put(\"current-snapshot-id\", currentSnapshotId);\n\n    icebergTable.properties().entrySet().stream()\n        .filter(entry -> !RESERVED_PROPERTIES.contains(entry.getKey()))\n        .forEach(propsBuilder::put);\n\n    return propsBuilder.build();\n  }\n\n  @Override\n  public Set<TableCapability> capabilities() {\n    return CAPABILITIES;\n  }\n\n  @Override\n  public ScanBuilder newScanBuilder(CaseInsensitiveStringMap options) {\n    if (refreshEagerly) {\n      icebergTable.refresh();\n    }\n\n    SparkScanBuilder scanBuilder = new SparkScanBuilder(sparkSession(), icebergTable, options);\n\n    if (requestedSchema != null) {\n      scanBuilder.pruneColumns(requestedSchema);\n    }\n\n    return scanBuilder;\n  }\n\n  @Override\n  public WriteBuilder newWriteBuilder(LogicalWriteInfo info) {\n    return new SparkWriteBuilder(sparkSession(), icebergTable, info);\n  }\n\n  @Override\n  public void deleteWhere(Filter[] filters) {\n    Expression deleteExpr = SparkFilters.convert(filters);\n\n    try {\n      icebergTable.newDelete()\n          .set(\"spark.app.id\", sparkSession().sparkContext().applicationId())\n          .deleteFromRowFilter(deleteExpr)\n          .commit();\n    } catch (ValidationException e) {\n      throw new IllegalArgumentException(\"Failed to cleanly delete data files matching: \" + deleteExpr, e);\n    }\n  }\n\n  @Override\n  public String toString() {\n    return icebergTable.toString();\n  }\n\n  @Override\n  public boolean equals(Object other) {\n    if (this == other) {\n      return true;\n    } else if (other == null || getClass() != other.getClass()) {\n      return false;\n    }\n\n    \/\/ use only name in order to correctly invalidate Spark cache\n    SparkTable that = (SparkTable) other;\n    return icebergTable.name().equals(that.icebergTable.name());\n  }\n\n  @Override\n  public int hashCode() {\n    \/\/ use only name in order to correctly invalidate Spark cache\n    return icebergTable.name().hashCode();\n  }\n}\n","lang_cluster":"Java","length":195,"code_uid":"d389f05d2bc447b9854155ba3bc26b41"}
{"diff_hunk":"@@ -28,6 +28,7 @@ import org.springframework.security.saml2.Saml2Exception;\n  *\n  * @author Josh Cummings\n  * @author Ryan Cassar\n+ * @author Marcus da Coregio\n  * @since 5.4\n  *\/\n public final class RelyingPartyRegistrations {","old_code":"\/*\n * Copyright 2002-2020 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage org.springframework.security.saml2.provider.service.registration;\n\nimport java.io.IOException;\nimport java.io.InputStream;\n\nimport org.springframework.core.io.DefaultResourceLoader;\nimport org.springframework.core.io.ResourceLoader;\nimport org.springframework.security.saml2.Saml2Exception;\n\n\/**\n * A utility class for constructing instances of {@link RelyingPartyRegistration}\n *\n * @author Josh Cummings\n * @author Ryan Cassar\n * @since 5.4\n *\/\npublic final class RelyingPartyRegistrations {\n\n\tprivate static final OpenSamlAssertingPartyMetadataConverter assertingPartyMetadataConverter = new OpenSamlAssertingPartyMetadataConverter();\n\n\tprivate static final ResourceLoader resourceLoader = new DefaultResourceLoader();\n\n\tprivate RelyingPartyRegistrations() {\n\t}\n\n\t\/**\n\t * Return a {@link RelyingPartyRegistration.Builder} based off of the given SAML 2.0\n\t * Asserting Party (IDP) metadata location.\n\t *\n\t * Valid locations can be classpath- or file-based or they can be HTTP endpoints. Some\n\t * valid endpoints might include:\n\t *\n\t * <pre>\n\t *   metadataLocation = \"classpath:asserting-party-metadata.xml\";\n\t *   metadataLocation = \"file:asserting-party-metadata.xml\";\n\t *   metadataLocation = \"https:\/\/ap.example.org\/metadata\";\n\t * <\/pre>\n\t *\n\t * Note that by default the registrationId is set to be the given metadata location,\n\t * but this will most often not be sufficient. To complete the configuration, most\n\t * applications will also need to provide a registrationId, like so:\n\t *\n\t * <pre>\n\t *\tRelyingPartyRegistration registration = RelyingPartyRegistrations\n\t * \t\t.fromMetadataLocation(metadataLocation)\n\t * \t\t.registrationId(\"registration-id\")\n\t * \t\t.build();\n\t * <\/pre>\n\t *\n\t * Also note that an {@code IDPSSODescriptor} typically only contains information\n\t * about the asserting party. Thus, you will need to remember to still populate\n\t * anything about the relying party, like any private keys the relying party will use\n\t * for signing AuthnRequests.\n\t * @param metadataLocation The classpath- or file-based locations or HTTP endpoints of\n\t * the asserting party metadata file\n\t * @return the {@link RelyingPartyRegistration.Builder} for further configuration\n\t *\/\n\tpublic static RelyingPartyRegistration.Builder fromMetadataLocation(String metadataLocation) {\n\t\ttry (InputStream source = resourceLoader.getResource(metadataLocation).getInputStream()) {\n\t\t\treturn assertingPartyMetadataConverter.convert(source);\n\t\t}\n\t\tcatch (IOException ex) {\n\t\t\tif (ex.getCause() instanceof Saml2Exception) {\n\t\t\t\tthrow (Saml2Exception) ex.getCause();\n\t\t\t}\n\t\t\tthrow new Saml2Exception(ex);\n\t\t}\n\t}\n\n}\n","lang_cluster":"Java","length":86,"code_uid":"9defc009c53d45b9bf937164216a5e39"}
{"diff_hunk":"@@ -40,6 +40,7 @@ import org.apache.iceberg.io.TaskWriter;\n import org.apache.iceberg.io.UnpartitionedWriter;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n import org.apache.spark.TaskContext;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.broadcast.Broadcast;","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.spark.source;\n\nimport java.io.Serializable;\nimport java.util.Collection;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.stream.Collectors;\nimport org.apache.iceberg.CombinedScanTask;\nimport org.apache.iceberg.DataFile;\nimport org.apache.iceberg.FileFormat;\nimport org.apache.iceberg.PartitionSpec;\nimport org.apache.iceberg.Schema;\nimport org.apache.iceberg.Table;\nimport org.apache.iceberg.TableProperties;\nimport org.apache.iceberg.encryption.EncryptionManager;\nimport org.apache.iceberg.io.FileIO;\nimport org.apache.iceberg.io.LocationProvider;\nimport org.apache.iceberg.io.OutputFileFactory;\nimport org.apache.iceberg.io.TaskWriter;\nimport org.apache.iceberg.io.UnpartitionedWriter;\nimport org.apache.iceberg.relocated.com.google.common.collect.Lists;\nimport org.apache.iceberg.spark.SparkSchemaUtil;\nimport org.apache.spark.TaskContext;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.broadcast.Broadcast;\nimport org.apache.spark.sql.catalyst.InternalRow;\nimport org.apache.spark.sql.types.StructType;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n\npublic class RowDataRewriter implements Serializable {\n\n  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n\n  private final Schema schema;\n  private final PartitionSpec spec;\n  private final Map<String, String> properties;\n  private final FileFormat format;\n  private final Broadcast<FileIO> io;\n  private final Broadcast<EncryptionManager> encryptionManager;\n  private final LocationProvider locations;\n  private final String nameMapping;\n  private final boolean caseSensitive;\n\n  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n                         Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager) {\n    this.schema = table.schema();\n    this.spec = spec;\n    this.locations = table.locationProvider();\n    this.properties = table.properties();\n    this.io = io;\n    this.encryptionManager = encryptionManager;\n\n    this.caseSensitive = caseSensitive;\n    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n\n    String formatString = table.properties().getOrDefault(\n        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n  }\n\n  public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {\n    JavaRDD<List<DataFile>> dataFilesRDD = taskRDD.map(this::rewriteDataForTask);\n\n    return dataFilesRDD.collect().stream()\n        .flatMap(Collection::stream)\n        .collect(Collectors.toList());\n  }\n\n  private List<DataFile> rewriteDataForTask(CombinedScanTask task) throws Exception {\n    TaskContext context = TaskContext.get();\n    int partitionId = context.partitionId();\n    long taskId = context.taskAttemptId();\n\n    RowDataReader dataReader = new RowDataReader(\n        task, schema, schema, nameMapping, io.value(), encryptionManager.value(), caseSensitive);\n\n    StructType structType = SparkSchemaUtil.convert(schema);\n    SparkAppenderFactory appenderFactory = new SparkAppenderFactory(properties, schema, structType);\n    OutputFileFactory fileFactory = new OutputFileFactory(\n        spec, format, locations, io.value(), encryptionManager.value(), partitionId, taskId);\n\n    TaskWriter<InternalRow> writer;\n    if (spec.fields().isEmpty()) {\n      writer = new UnpartitionedWriter<>(spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE);\n    } else {\n      writer = new SparkPartitionedWriter(spec, format, appenderFactory, fileFactory, io.value(), Long.MAX_VALUE,\n          schema, structType);\n    }\n\n    try {\n      while (dataReader.next()) {\n        InternalRow row = dataReader.get();\n        writer.write(row);\n      }\n\n      dataReader.close();\n      dataReader = null;\n\n      writer.close();\n      return Lists.newArrayList(writer.complete());\n\n    } catch (Throwable originalThrowable) {\n      try {\n        LOG.error(\"Aborting task\", originalThrowable);\n        context.markTaskFailed(originalThrowable);\n\n        LOG.error(\"Aborting commit for partition {} (task {}, attempt {}, stage {}.{})\",\n            partitionId, taskId, context.attemptNumber(), context.stageId(), context.stageAttemptNumber());\n        if (dataReader != null) {\n          dataReader.close();\n        }\n        writer.abort();\n        LOG.error(\"Aborted commit for partition {} (task {}, attempt {}, stage {}.{})\",\n            partitionId, taskId, context.taskAttemptId(), context.stageId(), context.stageAttemptNumber());\n\n      } catch (Throwable inner) {\n        if (originalThrowable != inner) {\n          originalThrowable.addSuppressed(inner);\n          LOG.warn(\"Suppressing exception in catch: {}\", inner.getMessage(), inner);\n        }\n      }\n\n      if (originalThrowable instanceof Exception) {\n        throw originalThrowable;\n      } else {\n        throw new RuntimeException(originalThrowable);\n      }\n    }\n  }\n}\n","lang_cluster":"Java","length":153,"code_uid":"3a07c5655f3846ddaf14a5e8c9f522e9"}
{"diff_hunk":"@@ -30,6 +30,7 @@ import org.json.JSONObject;\n \n import android.text.TextUtils;\n \n+import com.salesforce.androidsdk.smartsync.manager.SyncManager;\n import com.salesforce.androidsdk.smartsync.model.SalesforceObject;\n import com.salesforce.androidsdk.smartsync.util.Constants;\n ","old_code":"\/*\n * Copyright (c) 2014, salesforce.com, inc.\n * All rights reserved.\n * Redistribution and use of this software in source and binary forms, with or\n * without modification, are permitted provided that the following conditions\n * are met:\n * - Redistributions of source code must retain the above copyright notice, this\n * list of conditions and the following disclaimer.\n * - Redistributions in binary form must reproduce the above copyright notice,\n * this list of conditions and the following disclaimer in the documentation\n * and\/or other materials provided with the distribution.\n * - Neither the name of salesforce.com, inc. nor the names of its contributors\n * may be used to endorse or promote products derived from this software without\n * specific prior written permission of salesforce.com, inc.\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n *\/\npackage com.salesforce.samples.smartsyncexplorer.objects;\n\nimport org.json.JSONObject;\n\nimport android.text.TextUtils;\n\nimport com.salesforce.androidsdk.smartsync.model.SalesforceObject;\nimport com.salesforce.androidsdk.smartsync.util.Constants;\n\n\/**\n * A simple representation of a Contact object.\n *\n * @author bhariharan\n *\/\npublic class ContactObject extends SalesforceObject {\n\n\tpublic static final String LAST_NAME = \"LastName\";\n\tpublic static final String[] CONTACT_FIELDS = {\n\t\t\"Id\",\n\t\t\"Name\",\n\t\t\"FirstName\",\n\t\tLAST_NAME,\n\t\t\"Title\",\n\t\t\"Phone\",\n\t\t\"Email\",\n\t\t\"Department\",\n\t\t\"HomePhone\"\n\t};\n\n\t\n\t\/**\n\t * Parameterized constructor.\n\t *\n\t * @param data Raw data.\n\t *\/\n\tpublic ContactObject(JSONObject data) {\n\t\tsuper(data);\n\t\tobjectType = Constants.CONTACT;\n\t\tobjectId = data.optString(CONTACT_FIELDS[0]);\n\t\tname = data.optString(CONTACT_FIELDS[1]);\n\t}\n\n\t\/**\n\t * Returns first name of the contact.\n\t *\n\t * @return First name of the contact.\n\t *\/\n\tpublic String getFirstName() {\n\t\treturn sanitizeText(rawData.optString(CONTACT_FIELDS[2]));\n\t}\n\n\t\/**\n\t * Returns last name of the contact.\n\t *\n\t * @return Last name of the contact.\n\t *\/\n\tpublic String getLastName() {\n\t\treturn sanitizeText(rawData.optString(CONTACT_FIELDS[3]));\n\t}\n\n\t\/**\n\t * Returns title of the contact.\n\t *\n\t * @return Title of the contact.\n\t *\/\n\tpublic String getTitle() {\n\t\treturn sanitizeText(rawData.optString(CONTACT_FIELDS[4]));\n\t}\n\n\t\/**\n\t * Returns phone number of the contact.\n\t *\n\t * @return Phone number of the contact.\n\t *\/\n\tpublic String getPhone() {\n\t\treturn sanitizeText(rawData.optString(CONTACT_FIELDS[5]));\n\t}\n\n\t\/**\n\t * Returns e-mail address of the contact.\n\t *\n\t * @return E-mail address of the contact.\n\t *\/\n\tpublic String getEmail() {\n\t\treturn sanitizeText(rawData.optString(CONTACT_FIELDS[6]));\n\t}\n\n\t\/**\n\t * Returns department of the contact.\n\t *\n\t * @return Department of the contact.\n\t *\/\n\tpublic String getDepartment() {\n\t\treturn sanitizeText(rawData.optString(CONTACT_FIELDS[7]));\n\t}\n\n\t\/**\n\t * Returns home phone number of the contact.\n\t *\n\t * @return Home phone number of the contact.\n\t *\/\n\tpublic String getHomePhone() {\n\t\treturn sanitizeText(rawData.optString(CONTACT_FIELDS[8]));\n\t}\n\n\tprivate String sanitizeText(String text) {\n\t\tif (TextUtils.isEmpty(text) || text.equals(Constants.NULL_STRING)) {\n\t\t\treturn Constants.EMPTY_STRING;\n\t\t}\n\t\treturn text;\n\t}\n}\n","lang_cluster":"Java","length":138,"code_uid":"fed0cf6b23bb4c16a1f505e5876c85e8"}
{"diff_hunk":"@@ -102,24 +102,13 @@ public final class OperationSignature extends Signature {\n             }\n         }\n \n+\n         private static boolean isGetterOrSetter(ASTMethodDeclaration node) {\n             String name = node.getName();\n-            if (NAME_PATTERN.matcher(name).matches()) {\n+            if (GETTER_OR_SETTER_NAME_PATTERN.matcher(name).matches()) {\n                 return true;\n             }\n \n-            if (node.isAbstract()) {\n-                return false;\n-            }\n-\n-            int length = node.getEndLine() - node.getBeginLine();\n-\n-            if (length > 6) {\n-                return false;\n-            } else if (length > 4 && node.getFirstDescendantOfType(ASTIfStatement.class) == null) {\n-                return false;\n-            }\n-\n             ClassScope scope = node.getScope().getEnclosingScope(ClassScope.class);\n \n             \/\/ fields names mapped to their types","old_code":"\/**\n * BSD-style license; for more info see http:\/\/pmd.sourceforge.net\/license.html\n *\/\n\npackage net.sourceforge.pmd.lang.java.oom.signature;\n\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.regex.Pattern;\n\nimport net.sourceforge.pmd.lang.java.ast.ASTConstructorDeclaration;\nimport net.sourceforge.pmd.lang.java.ast.ASTFieldDeclaration;\nimport net.sourceforge.pmd.lang.java.ast.ASTFormalParameters;\nimport net.sourceforge.pmd.lang.java.ast.ASTIfStatement;\nimport net.sourceforge.pmd.lang.java.ast.ASTMethodDeclaration;\nimport net.sourceforge.pmd.lang.java.ast.ASTMethodOrConstructorDeclaration;\nimport net.sourceforge.pmd.lang.java.ast.ASTName;\nimport net.sourceforge.pmd.lang.java.ast.ASTPrimaryExpression;\nimport net.sourceforge.pmd.lang.java.ast.ASTPrimaryPrefix;\nimport net.sourceforge.pmd.lang.java.ast.ASTPrimarySuffix;\nimport net.sourceforge.pmd.lang.java.ast.ASTReturnStatement;\nimport net.sourceforge.pmd.lang.java.ast.ASTStatementExpression;\nimport net.sourceforge.pmd.lang.java.ast.ASTType;\nimport net.sourceforge.pmd.lang.java.symboltable.ClassScope;\nimport net.sourceforge.pmd.lang.java.symboltable.VariableNameDeclaration;\nimport net.sourceforge.pmd.lang.symboltable.NameOccurrence;\n\n\/**\n * Signature for an operation.\n *\n * @author Cl\u00e9ment Fournier\n *\/\npublic final class OperationSignature extends Signature {\n\n    private static final Map<Integer, OperationSignature> POOL = new HashMap<>();\n\n    public final Role role;\n    public final boolean isAbstract;\n\n\n    private OperationSignature(Visibility visibility, Role role, boolean isAbstract) {\n        super(visibility);\n        this.role = role;\n        this.isAbstract = isAbstract;\n    }\n\n    \/**\n     * Builds an operation signature from a method or constructor declaration.\n     *\n     * @param node The node\n     *\n     * @return The signature of the parameter\n     *\/\n    public static OperationSignature buildFor(ASTMethodOrConstructorDeclaration node) {\n        int code = code(Visibility.get(node), Role.get(node), node.isAbstract());\n        if (!POOL.containsKey(code)) {\n            POOL.put(code, new OperationSignature(Visibility.get(node), Role.get(node), node.isAbstract()));\n        }\n        return POOL.get(code);\n    }\n\n    \/** Used internally by the pooler. *\/\n    private static int code(Visibility visibility, Role role, boolean isAbstract) {\n        return visibility.hashCode() * 31 + role.hashCode() * 2 + (isAbstract ? 1 : 0);\n    }\n\n    @Override\n    public boolean equals(Object o) {\n        return o instanceof OperationSignature && super.equals(o) && role == ((OperationSignature) o).role\n            && isAbstract == ((OperationSignature) o).isAbstract;\n    }\n\n    @Override\n    public int hashCode() {\n        return super.hashCode() * 2 + role.hashCode() * 4 + (isAbstract ? 1 : 0);\n    }\n\n    \/**\n     * Role of an operation.\n     *\/\n    public enum Role {\n        GETTER_OR_SETTER, CONSTRUCTOR, METHOD, STATIC;\n\n        private static final Pattern NAME_PATTERN = Pattern.compile(\"(?:get|set|is|increment|decrement)\\\\w*\");\n\n\n        public static Role get(ASTMethodOrConstructorDeclaration node) {\n            return node instanceof ASTConstructorDeclaration ? CONSTRUCTOR : get((ASTMethodDeclaration) node);\n        }\n\n\n        private static Role get(ASTMethodDeclaration node) {\n            if (node.isStatic()) {\n                return STATIC;\n            } else if (isGetterOrSetter(node)) {\n                return GETTER_OR_SETTER;\n            } else {\n                return METHOD;\n            }\n        }\n\n        private static boolean isGetterOrSetter(ASTMethodDeclaration node) {\n            String name = node.getName();\n            if (NAME_PATTERN.matcher(name).matches()) {\n                return true;\n            }\n\n            if (node.isAbstract()) {\n                return false;\n            }\n\n            int length = node.getEndLine() - node.getBeginLine();\n\n            if (length > 6) {\n                return false;\n            } else if (length > 4 && node.getFirstDescendantOfType(ASTIfStatement.class) == null) {\n                return false;\n            }\n\n            ClassScope scope = node.getScope().getEnclosingScope(ClassScope.class);\n\n            \/\/ fields names mapped to their types\n            Map<String, String> fieldNames = new HashMap<>();\n\n            for (Map.Entry<VariableNameDeclaration, List<NameOccurrence>> decl\n                : scope.getVariableDeclarations().entrySet()) {\n\n                ASTFieldDeclaration field = decl.getKey().getNode()\n                                                .getFirstParentOfType(ASTFieldDeclaration.class);\n\n                fieldNames.put(field.getVariableName(), field.getFirstChildOfType(ASTType.class).getTypeImage());\n            }\n\n            return isGetter(node, fieldNames) || isSetter(node, fieldNames);\n        }\n\n        \/** Attempts to determine if the method is a getter. *\/\n        private static boolean isGetter(ASTMethodDeclaration node, Map<String, String> fieldNames) {\n\n\n            List<ASTReturnStatement> returnStatements\n                = node.getBlock().findDescendantsOfType(ASTReturnStatement.class);\n\n            for (ASTReturnStatement st : returnStatements) {\n                ASTName name = st.getFirstDescendantOfType(ASTName.class);\n                if (name == null) {\n                    continue;\n                }\n\n                if (fieldNames.containsKey(name.getImage().split(\"\\\\.\")[0])) {\n                    return true;\n                }\n            }\n\n            return false;\n        }\n\n        \/** Attempts to determine if the method is a setter. *\/\n        private static boolean isSetter(ASTMethodDeclaration node, Map<String, String> fieldNames) {\n\n            if (node.getFirstDescendantOfType(ASTFormalParameters.class).jjtGetNumChildren() != 1) {\n                return false;\n            }\n\n            List<ASTStatementExpression> statementExpressions\n                = node.getBlock().findDescendantsOfType(ASTStatementExpression.class);\n            Set<String> namesToCheck = new HashSet<>();\n\n            for (ASTStatementExpression st : statementExpressions) {\n                ASTName name = st.getFirstDescendantOfType(ASTName.class);\n                if (name == null) {\n                    \/\/ not an assignment, check for method\n                    ASTPrimaryExpression prim = st.getFirstChildOfType(ASTPrimaryExpression.class);\n                    ASTPrimaryPrefix prefix = prim.getFirstChildOfType(ASTPrimaryPrefix.class);\n\n                    if (prefix.usesThisModifier() || prefix.usesSuperModifier()) {\n                        namesToCheck.add(prim.getFirstChildOfType(ASTPrimarySuffix.class).getImage());\n                    } else {\n                        namesToCheck.add(prefix.getImage().split(\"\\\\.\")[0]);\n                    }\n                } else {\n                    \/\/ this is a direct assignment\n                    namesToCheck.add(name.getImage().split(\"\\\\.\")[0]);\n                }\n            }\n\n            for (String name : namesToCheck) {\n                if (fieldNames.containsKey(name)) {\n                    return true;\n                }\n            }\n            return false;\n        }\n    }\n}\n","lang_cluster":"Java","length":198,"code_uid":"ab54ec0520df4e12a3ce093f65076ea8"}
{"diff_hunk":"@@ -125,13 +125,13 @@ public class TransactionValidatorProviderTest {\n   public void validatorsAtHeadContractCallIsCached() {\n     final List<Address> validators =\n         Lists.newArrayList(Address.fromHexString(\"5\"), Address.fromHexString(\"6\"));\n-    when(validatorContractController.getValidators(3)).thenReturn(validators);\n+    when(validatorContractController.getValidators(3, CONTRACT_ADDRESS)).thenReturn(validators);\n \n     final TransactionValidatorProvider validatorProvider =\n-        new TransactionValidatorProvider(blockChain, validatorContractController);\n+        new TransactionValidatorProvider(blockChain, validatorContractController, forksSchedule);\n \n     assertThat(validatorProvider.getValidatorsAtHead()).containsExactlyElementsOf(validators);\n-    verify(validatorContractController).getValidators(3);\n+    verify(validatorContractController).getValidators(3, CONTRACT_ADDRESS);\n \n     assertThat(validatorProvider.getValidatorsAtHead()).containsExactlyElementsOf(validators);\n     verifyNoMoreInteractions(validatorContractController);","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.consensus.qbft.validator;\n\nimport static java.util.Collections.emptyList;\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.hyperledger.besu.ethereum.core.InMemoryKeyValueStorageProvider.createInMemoryBlockchain;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.verify;\nimport static org.mockito.Mockito.verifyNoMoreInteractions;\nimport static org.mockito.Mockito.when;\n\nimport org.hyperledger.besu.datatypes.Address;\nimport org.hyperledger.besu.datatypes.Hash;\nimport org.hyperledger.besu.ethereum.chain.MutableBlockchain;\nimport org.hyperledger.besu.ethereum.core.AddressHelpers;\nimport org.hyperledger.besu.ethereum.core.Block;\nimport org.hyperledger.besu.ethereum.core.BlockBody;\nimport org.hyperledger.besu.ethereum.core.BlockHeaderTestFixture;\n\nimport java.util.Collection;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\nimport com.google.common.collect.Lists;\nimport org.apache.tuweni.bytes.Bytes;\nimport org.junit.Before;\nimport org.junit.Test;\n\npublic class TransactionValidatorProviderTest {\n  private final ValidatorContractController validatorContractController =\n      mock(ValidatorContractController.class);\n\n  protected MutableBlockchain blockChain;\n  protected Block genesisBlock;\n  protected Block block_1;\n  protected Block block_2;\n  private Block block_3;\n\n  private final BlockHeaderTestFixture headerBuilder = new BlockHeaderTestFixture();\n\n  @Before\n  public void setup() {\n    genesisBlock = createEmptyBlock(0, Hash.ZERO);\n    blockChain = createInMemoryBlockchain(genesisBlock);\n    headerBuilder.extraData(Bytes.wrap(new byte[32]));\n\n    block_1 = createEmptyBlock(1, genesisBlock.getHeader().getHash());\n    block_2 = createEmptyBlock(2, block_1.getHeader().getHash());\n    block_3 = createEmptyBlock(3, block_2.getHeader().getHash());\n\n    blockChain.appendBlock(block_1, emptyList());\n    blockChain.appendBlock(block_2, emptyList());\n    blockChain.appendBlock(block_3, emptyList());\n  }\n\n  private Block createEmptyBlock(final long blockNumber, final Hash parentHash) {\n    headerBuilder.number(blockNumber).parentHash(parentHash).coinbase(AddressHelpers.ofValue(0));\n    return new Block(headerBuilder.buildHeader(), new BlockBody(emptyList(), emptyList()));\n  }\n\n  @Test\n  public void validatorsAfterBlockAreRetrievedUsingContractController() {\n    final List<Address> validatorsAt2 =\n        Lists.newArrayList(Address.fromHexString(\"5\"), Address.fromHexString(\"6\"));\n    final List<Address> validatorsAt3 =\n        Lists.newArrayList(\n            Address.fromHexString(\"5\"), Address.fromHexString(\"6\"), Address.fromHexString(\"7\"));\n    when(validatorContractController.getValidators(2)).thenReturn(validatorsAt2);\n    when(validatorContractController.getValidators(3)).thenReturn(validatorsAt3);\n\n    final TransactionValidatorProvider validatorProvider =\n        new TransactionValidatorProvider(blockChain, validatorContractController);\n\n    assertThat(validatorProvider.getValidatorsAfterBlock(block_2.getHeader()))\n        .containsExactlyElementsOf(validatorsAt2);\n    assertThat(validatorProvider.getValidatorsAfterBlock(block_3.getHeader()))\n        .containsExactlyElementsOf(validatorProvider.getValidatorsAfterBlock(block_3.getHeader()));\n  }\n\n  @Test\n  public void validatorsForBlockAreRetrievedUsingContractController() {\n    final List<Address> validatorsAt2 =\n        Lists.newArrayList(Address.fromHexString(\"5\"), Address.fromHexString(\"6\"));\n    final List<Address> validatorsAt3 =\n        Lists.newArrayList(\n            Address.fromHexString(\"5\"), Address.fromHexString(\"6\"), Address.fromHexString(\"7\"));\n    when(validatorContractController.getValidators(2)).thenReturn(validatorsAt2);\n    when(validatorContractController.getValidators(3)).thenReturn(validatorsAt3);\n\n    final TransactionValidatorProvider validatorProvider =\n        new TransactionValidatorProvider(blockChain, validatorContractController);\n\n    assertThat(validatorProvider.getValidatorsForBlock(block_2.getHeader()))\n        .containsExactlyElementsOf(validatorsAt2);\n    assertThat(validatorProvider.getValidatorsForBlock(block_3.getHeader()))\n        .containsExactlyElementsOf(validatorProvider.getValidatorsForBlock(block_3.getHeader()));\n  }\n\n  @Test\n  public void validatorsAtHeadAreRetrievedUsingContractController() {\n    final List<Address> validators =\n        Lists.newArrayList(Address.fromHexString(\"5\"), Address.fromHexString(\"6\"));\n    when(validatorContractController.getValidators(3)).thenReturn(validators);\n\n    final TransactionValidatorProvider validatorProvider =\n        new TransactionValidatorProvider(blockChain, validatorContractController);\n\n    assertThat(validatorProvider.getValidatorsAtHead()).containsExactlyElementsOf(validators);\n  }\n\n  @Test\n  public void validatorsAtHeadContractCallIsCached() {\n    final List<Address> validators =\n        Lists.newArrayList(Address.fromHexString(\"5\"), Address.fromHexString(\"6\"));\n    when(validatorContractController.getValidators(3)).thenReturn(validators);\n\n    final TransactionValidatorProvider validatorProvider =\n        new TransactionValidatorProvider(blockChain, validatorContractController);\n\n    assertThat(validatorProvider.getValidatorsAtHead()).containsExactlyElementsOf(validators);\n    verify(validatorContractController).getValidators(3);\n\n    assertThat(validatorProvider.getValidatorsAtHead()).containsExactlyElementsOf(validators);\n    verifyNoMoreInteractions(validatorContractController);\n  }\n\n  @Test\n  public void validatorsAfterBlockContractCallIsCached() {\n    final List<Address> validators =\n        Lists.newArrayList(Address.fromHexString(\"5\"), Address.fromHexString(\"6\"));\n    when(validatorContractController.getValidators(2)).thenReturn(validators);\n\n    final TransactionValidatorProvider validatorProvider =\n        new TransactionValidatorProvider(blockChain, validatorContractController);\n\n    final Collection<Address> result =\n        validatorProvider.getValidatorsAfterBlock(block_2.getHeader());\n    assertThat(result).containsExactlyElementsOf(validators);\n    verify(validatorContractController).getValidators(2);\n\n    final Collection<Address> resultCached =\n        validatorProvider.getValidatorsAfterBlock(block_2.getHeader());\n    assertThat(resultCached).containsExactlyElementsOf(validators);\n    verifyNoMoreInteractions(validatorContractController);\n  }\n\n  @Test\n  public void validatorsMustBeSorted() {\n    final List<Address> validators =\n        Lists.newArrayList(\n            Address.fromHexString(\"9\"), Address.fromHexString(\"8\"), Address.fromHexString(\"7\"));\n    when(validatorContractController.getValidators(3)).thenReturn(validators);\n\n    final TransactionValidatorProvider validatorProvider =\n        new TransactionValidatorProvider(blockChain, validatorContractController);\n\n    final Collection<Address> result = validatorProvider.getValidatorsAtHead();\n    final List<Address> expectedValidators =\n        validators.stream().sorted().collect(Collectors.toList());\n    assertThat(result).containsExactlyElementsOf(expectedValidators);\n  }\n\n  @Test\n  public void voteProviderIsEmpty() {\n    TransactionValidatorProvider transactionValidatorProvider =\n        new TransactionValidatorProvider(blockChain, validatorContractController);\n\n    assertThat(transactionValidatorProvider.getVoteProviderAtHead()).isEmpty();\n  }\n}\n","lang_cluster":"Java","length":183,"code_uid":"d2f0e741f33644a988f81004b09a3835"}
{"diff_hunk":"@@ -14,13 +14,17 @@\n  *\/\n package org.hyperledger.besu.plugin.services.storage.rocksdb.unsegmented;\n \n+import static java.util.stream.Collectors.toUnmodifiableSet;\n+\n import org.hyperledger.besu.plugin.services.MetricsSystem;\n import org.hyperledger.besu.plugin.services.exception.StorageException;\n import org.hyperledger.besu.plugin.services.metrics.OperationTimer;\n import org.hyperledger.besu.plugin.services.storage.KeyValueStorage;\n import org.hyperledger.besu.plugin.services.storage.KeyValueStorageTransaction;\n+import org.hyperledger.besu.plugin.services.storage.rocksdb.RocksDBExceptionAdapter;\n import org.hyperledger.besu.plugin.services.storage.rocksdb.RocksDBMetrics;\n import org.hyperledger.besu.plugin.services.storage.rocksdb.RocksDBMetricsFactory;\n+import org.hyperledger.besu.plugin.services.storage.rocksdb.RocksDbKeyIterator;\n import org.hyperledger.besu.plugin.services.storage.rocksdb.RocksDbUtil;\n import org.hyperledger.besu.plugin.services.storage.rocksdb.configuration.RocksDBConfiguration;\n import org.hyperledger.besu.services.kvstore.KeyValueStorageTransactionTransitionValidatorDecorator;","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.plugin.services.storage.rocksdb.unsegmented;\n\nimport org.hyperledger.besu.plugin.services.MetricsSystem;\nimport org.hyperledger.besu.plugin.services.exception.StorageException;\nimport org.hyperledger.besu.plugin.services.metrics.OperationTimer;\nimport org.hyperledger.besu.plugin.services.storage.KeyValueStorage;\nimport org.hyperledger.besu.plugin.services.storage.KeyValueStorageTransaction;\nimport org.hyperledger.besu.plugin.services.storage.rocksdb.RocksDBMetrics;\nimport org.hyperledger.besu.plugin.services.storage.rocksdb.RocksDBMetricsFactory;\nimport org.hyperledger.besu.plugin.services.storage.rocksdb.RocksDbUtil;\nimport org.hyperledger.besu.plugin.services.storage.rocksdb.configuration.RocksDBConfiguration;\nimport org.hyperledger.besu.services.kvstore.KeyValueStorageTransactionTransitionValidatorDecorator;\n\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Predicate;\n\nimport com.google.common.collect.Sets;\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\nimport org.rocksdb.BlockBasedTableConfig;\nimport org.rocksdb.LRUCache;\nimport org.rocksdb.Options;\nimport org.rocksdb.RocksDBException;\nimport org.rocksdb.RocksIterator;\nimport org.rocksdb.Statistics;\nimport org.rocksdb.TransactionDB;\nimport org.rocksdb.TransactionDBOptions;\nimport org.rocksdb.WriteOptions;\n\npublic class RocksDBKeyValueStorage implements KeyValueStorage {\n\n  static {\n    RocksDbUtil.loadNativeLibrary();\n  }\n\n  private static final Logger LOG = LogManager.getLogger();\n\n  private final Options options;\n  private final TransactionDBOptions txOptions;\n  private final TransactionDB db;\n  private final AtomicBoolean closed = new AtomicBoolean(false);\n  private final RocksDBMetrics rocksDBMetrics;\n\n  public RocksDBKeyValueStorage(\n      final RocksDBConfiguration configuration,\n      final MetricsSystem metricsSystem,\n      final RocksDBMetricsFactory rocksDBMetricsFactory) {\n\n    try {\n      final Statistics stats = new Statistics();\n      options =\n          new Options()\n              .setCreateIfMissing(true)\n              .setMaxOpenFiles(configuration.getMaxOpenFiles())\n              .setTableFormatConfig(createBlockBasedTableConfig(configuration))\n              .setMaxBackgroundCompactions(configuration.getMaxBackgroundCompactions())\n              .setStatistics(stats);\n      options.getEnv().setBackgroundThreads(configuration.getBackgroundThreadCount());\n\n      txOptions = new TransactionDBOptions();\n      db = TransactionDB.open(options, txOptions, configuration.getDatabaseDir().toString());\n      rocksDBMetrics = rocksDBMetricsFactory.create(metricsSystem, configuration, db, stats);\n    } catch (final RocksDBException e) {\n      throw new StorageException(e);\n    }\n  }\n\n  @Override\n  public void clear() throws StorageException {\n    try (final RocksIterator rocksIterator = db.newIterator()) {\n      rocksIterator.seekToFirst();\n      if (rocksIterator.isValid()) {\n        final byte[] firstKey = rocksIterator.key();\n        rocksIterator.seekToLast();\n        if (rocksIterator.isValid()) {\n          final byte[] lastKey = rocksIterator.key();\n          db.deleteRange(firstKey, lastKey);\n          db.delete(lastKey);\n        }\n      }\n    } catch (final RocksDBException e) {\n      throw new StorageException(e);\n    }\n  }\n\n  @Override\n  public boolean containsKey(final byte[] key) throws StorageException {\n    return get(key).isPresent();\n  }\n\n  @Override\n  public Optional<byte[]> get(final byte[] key) throws StorageException {\n    throwIfClosed();\n\n    try (final OperationTimer.TimingContext ignored =\n        rocksDBMetrics.getReadLatency().startTimer()) {\n      return Optional.ofNullable(db.get(key));\n    } catch (final RocksDBException e) {\n      throw new StorageException(e);\n    }\n  }\n\n  @Override\n  public long removeAllKeysUnless(final Predicate<byte[]> retainCondition) throws StorageException {\n    long removedNodeCounter = 0;\n    try (final RocksIterator rocksIterator = db.newIterator()) {\n      for (rocksIterator.seekToFirst(); rocksIterator.isValid(); rocksIterator.next()) {\n        final byte[] key = rocksIterator.key();\n        if (!retainCondition.test(key)) {\n          removedNodeCounter++;\n          db.delete(key);\n        }\n      }\n    } catch (final RocksDBException e) {\n      throw new StorageException(e);\n    }\n    return removedNodeCounter;\n  }\n\n  @Override\n  public Set<byte[]> getAllKeysThat(final Predicate<byte[]> returnCondition) {\n    final Set<byte[]> returnedKeys = Sets.newIdentityHashSet();\n    try (final RocksIterator rocksIterator = db.newIterator()) {\n      for (rocksIterator.seekToFirst(); rocksIterator.isValid(); rocksIterator.next()) {\n        final byte[] key = rocksIterator.key();\n        if (returnCondition.test(key)) {\n          returnedKeys.add(key);\n        }\n      }\n    }\n    return returnedKeys;\n  }\n\n  @Override\n  public KeyValueStorageTransaction startTransaction() throws StorageException {\n    throwIfClosed();\n    final WriteOptions options = new WriteOptions();\n    return new KeyValueStorageTransactionTransitionValidatorDecorator(\n        new RocksDBTransaction(db.beginTransaction(options), options, rocksDBMetrics));\n  }\n\n  @Override\n  public void close() {\n    if (closed.compareAndSet(false, true)) {\n      txOptions.close();\n      options.close();\n      db.close();\n    }\n  }\n\n  private BlockBasedTableConfig createBlockBasedTableConfig(final RocksDBConfiguration config) {\n    final LRUCache cache = new LRUCache(config.getCacheCapacity());\n    return new BlockBasedTableConfig().setBlockCache(cache);\n  }\n\n  private void throwIfClosed() {\n    if (closed.get()) {\n      LOG.error(\"Attempting to use a closed RocksDBKeyValueStorage\");\n      throw new IllegalStateException(\"Storage has been closed\");\n    }\n  }\n}\n","lang_cluster":"Java","length":178,"code_uid":"2415c5fdb9fc4588a69ab4d8b9f69c44"}
{"diff_hunk":"@@ -134,15 +134,15 @@ public class BigIntegerModularExponentiationPrecompiledContract\n     }\n   }\n \n-  private static final BigInteger baseLength(final Bytes input) {\n+  public static final BigInteger baseLength(final Bytes input) {\n     return extractParameter(input, BASE_LENGTH_OFFSET, PARAMETER_LENGTH);\n   }\n \n-  private static final BigInteger exponentLength(final Bytes input) {\n+  public static final BigInteger exponentLength(final Bytes input) {\n     return extractParameter(input, EXPONENT_LENGTH_OFFSET, PARAMETER_LENGTH);\n   }\n \n-  private static final BigInteger modulusLength(final Bytes input) {\n+  public static final BigInteger modulusLength(final Bytes input) {\n     return extractParameter(input, MODULUS_LENGTH_OFFSET, PARAMETER_LENGTH);\n   }\n ","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.ethereum.mainnet.precompiles;\n\nimport org.hyperledger.besu.ethereum.core.Gas;\nimport org.hyperledger.besu.ethereum.mainnet.AbstractPrecompiledContract;\nimport org.hyperledger.besu.ethereum.vm.GasCalculator;\nimport org.hyperledger.besu.ethereum.vm.MessageFrame;\n\nimport java.math.BigInteger;\nimport java.util.Arrays;\n\nimport org.apache.tuweni.bytes.Bytes;\nimport org.apache.tuweni.bytes.MutableBytes;\n\n\/\/ The big integer modular exponentiation precompiled contract defined in EIP-198.\npublic class BigIntegerModularExponentiationPrecompiledContract\n    extends AbstractPrecompiledContract {\n\n  private static final BigInteger WORD_SIZE = BigInteger.valueOf(32);\n  private static final BigInteger BITS_IN_BYTE = BigInteger.valueOf(8);\n  private static final BigInteger BASE_OFFSET = BigInteger.valueOf(96);\n  private static final BigInteger MAX_FIRST_EXPONENT_BYTES = BigInteger.valueOf(32);\n  private static final BigInteger GQUADDIVISOR = BigInteger.valueOf(20);\n  private static final int PARAMETER_LENGTH = 32;\n  private static final int BASE_LENGTH_OFFSET = 0;\n  private static final int EXPONENT_LENGTH_OFFSET = 32;\n  private static final int MODULUS_LENGTH_OFFSET = 64;\n  private static final int MAX_GAS_BITS = 255;\n\n  private static final BigInteger BIGINT_4 = BigInteger.valueOf(4);\n  private static final BigInteger BIGINT_16 = BigInteger.valueOf(16);\n  private static final BigInteger BIGINT_64 = BigInteger.valueOf(64);\n  private static final BigInteger BIGINT_96 = BigInteger.valueOf(96);\n  private static final BigInteger BIGINT_480 = BigInteger.valueOf(480);\n  private static final BigInteger BIGINT_1024 = BigInteger.valueOf(1_024L);\n  private static final BigInteger BIGINT_3072 = BigInteger.valueOf(3_072L);\n  private static final BigInteger BIGINT_199680 = BigInteger.valueOf(199_680L);\n\n  public BigIntegerModularExponentiationPrecompiledContract(final GasCalculator gasCalculator) {\n    super(\"BigIntModExp\", gasCalculator);\n  }\n\n  @Override\n  public Gas gasRequirement(final Bytes input) {\n    \/\/ Typically gas calculations are delegated to a GasCalculator instance,\n    \/\/ but the complexity and coupling with other parts of the precompile seem\n    \/\/ like reasonable reasons to do the math here instead.\n    final BigInteger baseLength = baseLength(input);\n    final BigInteger exponentLength = exponentLength(input);\n    final BigInteger modulusLength = modulusLength(input);\n    final BigInteger exponentOffset = BASE_OFFSET.add(baseLength);\n    final int firstExponentBytesCap = exponentLength.min(MAX_FIRST_EXPONENT_BYTES).intValue();\n    final BigInteger firstExpBytes = extractParameter(input, exponentOffset, firstExponentBytesCap);\n    final BigInteger adjustedExponentLength = adjustedExponentLength(exponentLength, firstExpBytes);\n    final BigInteger multiplicationComplexity =\n        multiplicationComplexity(baseLength.max(modulusLength));\n    final BigInteger gasRequirement =\n        multiplicationComplexity\n            .multiply(adjustedExponentLength.max(BigInteger.ONE))\n            .divide(GQUADDIVISOR);\n\n    \/\/ Gas price is so large it will not fit in a Gas type, so an\n    \/\/ very very very unlikely high gas price is used instead.\n    if (gasRequirement.bitLength() > MAX_GAS_BITS) {\n      return Gas.of(Long.MAX_VALUE);\n    } else {\n      return Gas.of(gasRequirement);\n    }\n  }\n\n  @Override\n  public Bytes compute(final Bytes input, final MessageFrame messageFrame) {\n    final BigInteger baseLength = baseLength(input);\n    final BigInteger exponentLength = exponentLength(input);\n    final BigInteger modulusLength = modulusLength(input);\n    final BigInteger exponentOffset = BASE_OFFSET.add(baseLength);\n    final BigInteger modulusOffset = exponentOffset.add(exponentLength);\n    final BigInteger base = extractParameter(input, BASE_OFFSET, baseLength.intValue());\n    final BigInteger exp = extractParameter(input, exponentOffset, exponentLength.intValue());\n    final BigInteger mod = extractParameter(input, modulusOffset, modulusLength.intValue());\n\n    final Bytes modExp;\n    \/\/ Result must be the length of the modulus.\n    final MutableBytes result = MutableBytes.create(modulusLength.intValue());\n    if (mod.compareTo(BigInteger.ZERO) == 0) {\n      modExp = MutableBytes.EMPTY;\n    } else {\n      \/\/ BigInteger zero-pads positive values whose most significant bit is a 1 if\n      \/\/ the padding was not there.\n      modExp = Bytes.wrap(base.modPow(exp, mod).toByteArray()).trimLeadingZeros();\n    }\n\n    modExp.copyTo(result, result.size() - modExp.size());\n    return result;\n  }\n\n  \/\/ Equation to estimate the multiplication complexity.\n  private static BigInteger multiplicationComplexity(final BigInteger x) {\n    if (x.compareTo(BIGINT_64) <= 0) {\n      return square(x);\n    } else if (x.compareTo(BIGINT_1024) <= 0) {\n      return square(x).divide(BIGINT_4).add(BIGINT_96.multiply(x)).subtract(BIGINT_3072);\n    } else {\n      return square(x).divide(BIGINT_16).add(BIGINT_480.multiply(x)).subtract(BIGINT_199680);\n    }\n  }\n\n  private static BigInteger bitLength(final BigInteger n) {\n    return n.compareTo(BigInteger.ZERO) == 0\n        ? BigInteger.ZERO\n        : BigInteger.valueOf(n.bitLength() - 1);\n  }\n\n  private static BigInteger adjustedExponentLength(\n      final BigInteger exponentLength, final BigInteger firstExpBytes) {\n    final BigInteger bitLength = bitLength(firstExpBytes);\n    if (exponentLength.compareTo(WORD_SIZE) <= 0) {\n      return bitLength;\n    } else {\n      return BITS_IN_BYTE.multiply(exponentLength.subtract(WORD_SIZE)).add(bitLength);\n    }\n  }\n\n  private static final BigInteger baseLength(final Bytes input) {\n    return extractParameter(input, BASE_LENGTH_OFFSET, PARAMETER_LENGTH);\n  }\n\n  private static final BigInteger exponentLength(final Bytes input) {\n    return extractParameter(input, EXPONENT_LENGTH_OFFSET, PARAMETER_LENGTH);\n  }\n\n  private static final BigInteger modulusLength(final Bytes input) {\n    return extractParameter(input, MODULUS_LENGTH_OFFSET, PARAMETER_LENGTH);\n  }\n\n  private static BigInteger extractParameter(\n      final Bytes input, final int offset, final int length) {\n    if (offset > input.size() || length == 0) {\n      return BigInteger.ZERO;\n    }\n    final byte[] raw = Arrays.copyOfRange(input.toArray(), offset, offset + length);\n    return new BigInteger(1, raw);\n  }\n\n  private static BigInteger extractParameter(\n      final Bytes input, final BigInteger offset, final int length) {\n    if (BigInteger.valueOf(input.size()).compareTo(offset) <= 0) {\n      return BigInteger.ZERO;\n    }\n    return extractParameter(input, offset.intValue(), length);\n  }\n\n  private static BigInteger square(final BigInteger n) {\n    return n.multiply(n);\n  }\n}\n","lang_cluster":"Java","length":169,"code_uid":"011c4331f2ab4509a877e0b32e8a7620"}
{"diff_hunk":"@@ -19,6 +19,7 @@ package azkaban.utils;\n import java.util.Collection;\n \n public class AbstractMailer {\n+  private static int MB_IN_BYTES = 1048576;\n   private String clientHostname;\n   private int clientPort;\n   private boolean usesSSL;","old_code":"\/*\n * Copyright 2012 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n *\/\n\npackage azkaban.utils;\n\nimport java.util.Collection;\n\npublic class AbstractMailer {\n  private String clientHostname;\n  private int clientPort;\n  private boolean usesSSL;\n\n  private String mailHost;\n  private String mailUser;\n  private String mailPassword;\n  private String mailSender;\n  private String azkabanName;\n\n  private String referenceURL;\n\n  public AbstractMailer(Props props) {\n    this.azkabanName = props.getString(\"azkaban.name\", \"azkaban\");\n    this.mailHost = props.getString(\"mail.host\", \"localhost\");\n    this.mailUser = props.getString(\"mail.user\", \"\");\n    this.mailPassword = props.getString(\"mail.password\", \"\");\n    this.mailSender = props.getString(\"mail.sender\", \"\");\n\n    this.clientHostname = props.get(\"server.hostname\");\n    this.clientPort = props.getInt(\"server.port\");\n    this.usesSSL = props.getBoolean(\"server.useSSL\");\n\n    if (usesSSL) {\n      referenceURL =\n          \"https:\/\/\" + clientHostname\n              + (clientPort == 443 ? \"\/\" : \":\" + clientPort + \"\/\");\n    } else {\n      referenceURL =\n          \"http:\/\/\" + clientHostname\n              + (clientPort == 80 ? \"\/\" : \":\" + clientPort + \"\/\");\n    }\n  }\n\n  public String getReferenceURL() {\n    return referenceURL;\n  }\n\n  protected EmailMessage createEmailMessage(String subject, String mimetype,\n      Collection<String> emailList) {\n    EmailMessage message = new EmailMessage(mailHost, mailUser, mailPassword);\n    message.setFromAddress(mailSender);\n    message.addAllToAddress(emailList);\n    message.setMimeType(mimetype);\n    message.setSubject(subject);\n\n    return message;\n  }\n\n  public EmailMessage prepareEmailMessage(String subject, String mimetype,\n      Collection<String> emailList) {\n    return createEmailMessage(subject, mimetype, emailList);\n  }\n\n  public String getAzkabanName() {\n    return azkabanName;\n  }\n\n  public String getMailHost() {\n    return mailHost;\n  }\n\n  public String getMailUser() {\n    return mailUser;\n  }\n\n  public String getMailPassword() {\n    return mailPassword;\n  }\n\n  public String getMailSender() {\n    return mailSender;\n  }\n}\n","lang_cluster":"Java","length":95,"code_uid":"cf4c0bbc409a4bd08e929de875332ca6"}
{"diff_hunk":"@@ -1,6 +1,6 @@\n import options from '.\/options';\n import { defer } from '.\/util';\n-import { renderComponent } from '.\/vdom\/component';\n+import { renderComponent, catchErrorInComponent } from '.\/vdom\/component';\n \n \/** Managed queue of dirty components to be re-rendered *\/\n ","old_code":"import options from '.\/options';\nimport { defer } from '.\/util';\nimport { renderComponent } from '.\/vdom\/component';\n\n\/** Managed queue of dirty components to be re-rendered *\/\n\nlet items = [];\n\nexport function enqueueRender(component) {\n\tif (!component._dirty && (component._dirty = true) && items.push(component)==1) {\n\t\t(options.debounceRendering || defer)(rerender);\n\t}\n}\n\nexport function rerender() {\n\tlet p, list = items;\n\titems = [];\n\twhile ( (p = list.pop()) ) {\n\t\tif (p._dirty) renderComponent(p);\n\t}\n}\n","lang_cluster":"Javascript","length":21,"code_uid":"b7afbfeb1d1e4f38bc7ac9872bc75a32"}
{"diff_hunk":"@@ -123,6 +123,25 @@ describe('Core.setCellMeta', () => {\n \n     hot.setCellMeta(0, 1, 'className', className);\n \n+    expect(beforeSetCellMeta).toHaveBeenCalledWith(0, 1, 'className', className, undefined, undefined);\n     expect(afterSetCellMeta).toHaveBeenCalledWith(0, 1, 'className', className, undefined, undefined);\n   });\n+\n+  it('should NOT call the `afterSetCellMeta` hook, if the `beforeSetCellMeta` returned false', () => {\n+    const className = 'htCenter htMiddle';\n+    const afterSetCellMeta = jasmine.createSpy('afterSetCellMeta');\n+    const hot = handsontable({\n+      minRows: 5,\n+      minCols: 5,\n+      beforeSetCellMeta: () => false,\n+      afterSetCellMeta\n+    });\n+\n+    hot.rowIndexMapper.setIndexesSequence([4, 3, 2, 1, 0]);\n+    hot.columnIndexMapper.setIndexesSequence([4, 3, 2, 1, 0]);\n+\n+    hot.setCellMeta(0, 1, 'className', className);\n+\n+    expect(afterSetCellMeta).not.toHaveBeenCalled();\n+  });\n });","old_code":"describe('Core.setCellMeta', () => {\n  const id = 'testContainer';\n\n  beforeEach(function() {\n    this.$container = $(`<div id=\"${id}\"><\/div>`).appendTo('body');\n  });\n\n  afterEach(function() {\n    if (this.$container) {\n      destroy();\n      this.$container.remove();\n    }\n  });\n\n  it('should set correct meta className for cell', () => {\n\n    const className = 'htCenter htMiddle';\n\n    handsontable({\n      afterCellMetaReset() {\n        this.setCellMeta(0, 0, 'className', className);\n      }\n    });\n\n    const cellMeta = getCellMeta(0, 0);\n\n    expect(cellMeta.className).not.toBeUndefined();\n    expect(cellMeta.className).toEqual(className);\n  });\n\n  it('should set proper cell meta when indexes was modified', () => {\n    const hot = handsontable({\n      minRows: 5,\n      minCols: 5\n    });\n\n    hot.rowIndexMapper.setIndexesSequence([4, 3, 2, 1, 0]);\n    hot.columnIndexMapper.setIndexesSequence([4, 3, 2, 1, 0]);\n\n    setCellMeta(0, 1, 'key', 'value');\n\n    expect(getCellMeta(0, 1).key).toEqual('value');\n  });\n\n  it('should set correct meta className for non existed cell', () => {\n    const className = 'htCenter htMiddle';\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 5),\n      afterCellMetaReset() {\n        this.setCellMeta(100, 100, 'className', className);\n      }\n    });\n\n    const cellMeta = getCellMeta(100, 100);\n\n    expect(cellMeta.className).not.toBeUndefined();\n    expect(cellMeta.className).toEqual(className);\n  });\n\n  it('should set correct meta classNames for cells using cell in configuration', () => {\n    const classNames = [\n      'htCenter htTop',\n      'htRight htBottom'\n    ];\n\n    handsontable({\n      cell: [\n        { row: 0, col: 0, className: classNames[0] },\n        { row: 1, col: 1, className: classNames[1] }\n      ]\n    });\n\n    expect(spec().$container.find('tbody tr:eq(0) td:eq(0)')[0].className).toEqual(classNames[0]);\n    expect(spec().$container.find('tbody tr:eq(1) td:eq(1)')[0].className).toEqual(classNames[1]);\n  });\n\n  it('should change cell meta data with updateSettings when the cell option is defined', () => {\n    const classNames = [\n      'htCenter htTop',\n      'htRight htBottom'\n    ];\n\n    handsontable({\n      cell: [\n        { row: 0, col: 0, className: classNames[0] },\n        { row: 1, col: 1, className: classNames[1] }\n      ]\n    });\n\n    expect(spec().$container.find('tbody tr:eq(0) td:eq(0)')[0].className).toEqual(classNames[0]);\n    expect(spec().$container.find('tbody tr:eq(1) td:eq(1)')[0].className).toEqual(classNames[1]);\n\n    updateSettings({\n      cell: []\n    });\n\n    expect(spec().$container.find('tbody tr:eq(0) td:eq(0)')[0].className).toEqual('');\n    expect(spec().$container.find('tbody tr:eq(1) td:eq(1)')[0].className).toEqual('');\n\n    updateSettings({\n      cell: [\n        { row: 0, col: 0, className: classNames[1] },\n        { row: 1, col: 1, className: classNames[0] }\n      ]\n    });\n\n    expect(spec().$container.find('tbody tr:eq(0) td:eq(0)')[0].className).toEqual(classNames[1]);\n    expect(spec().$container.find('tbody tr:eq(1) td:eq(1)')[0].className).toEqual(classNames[0]);\n  });\n\n  it('should call `afterSetCellMeta` plugin hook with visual indexes as parameters', () => {\n    const className = 'htCenter htMiddle';\n    const afterSetCellMeta = jasmine.createSpy('afterSetCellMeta');\n    const hot = handsontable({\n      minRows: 5,\n      minCols: 5,\n      afterSetCellMeta\n    });\n\n    hot.rowIndexMapper.setIndexesSequence([4, 3, 2, 1, 0]);\n    hot.columnIndexMapper.setIndexesSequence([4, 3, 2, 1, 0]);\n\n    hot.setCellMeta(0, 1, 'className', className);\n\n    expect(afterSetCellMeta).toHaveBeenCalledWith(0, 1, 'className', className, undefined, undefined);\n  });\n});\n","lang_cluster":"Javascript","length":128,"code_uid":"9a58d5a8e7ec413cb58bb89dd856b06d"}
{"diff_hunk":"@@ -51,10 +51,6 @@ function DashboardBounceRateWidget() {\n \t} = useSelect( ( select ) => {\n \t\tconst store = select( STORE_NAME );\n \n-\t\tconst accountID = store.getAccountID();\n-\t\tconst profileID = store.getProfileID();\n-\t\tconst internalWebPropertyID = store.getInternalWebPropertyID();\n-\n \t\tconst args = {\n \t\t\tdateRange: select( CORE_USER ).getDateRange(),\n \t\t\tmultiDateRange: 1,","old_code":"\/**\n * DashboardAllTrafficWidget component.\n *\n * Site Kit by Google, Copyright 2020 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * WordPress dependencies\n *\/\nimport { __, _x } from '@wordpress\/i18n';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport { STORE_NAME } from '..\/..\/datastore\/constants';\nimport { STORE_NAME as CORE_SITE } from '..\/..\/..\/..\/googlesitekit\/datastore\/site\/constants';\nimport { STORE_NAME as CORE_USER } from '..\/..\/..\/..\/googlesitekit\/datastore\/user\/constants';\nimport whenActive from '..\/..\/..\/..\/util\/when-active';\nimport PreviewBlock from '..\/..\/..\/..\/components\/PreviewBlock';\nimport DataBlock from '..\/..\/..\/..\/components\/data-block';\nimport Sparkline from '..\/..\/..\/..\/components\/Sparkline';\nimport AnalyticsInactiveCTA from '..\/..\/..\/..\/components\/AnalyticsInactiveCTA';\nimport { changeToPercent } from '..\/..\/..\/..\/util';\nimport applyEntityToReportPath from '..\/..\/util\/applyEntityToReportPath';\nimport ReportError from '..\/..\/..\/..\/components\/ReportError';\nimport ReportZero from '..\/..\/..\/..\/components\/ReportZero';\nimport parseDimensionStringToDate from '..\/..\/util\/parseDimensionStringToDate';\nimport { isZeroReport } from '..\/..\/util';\n\nconst { useSelect } = Data;\n\nfunction DashboardBounceRateWidget() {\n\tconst {\n\t\tdata,\n\t\terror,\n\t\tloading,\n\t\tserviceURL,\n\t} = useSelect( ( select ) => {\n\t\tconst store = select( STORE_NAME );\n\n\t\tconst accountID = store.getAccountID();\n\t\tconst profileID = store.getProfileID();\n\t\tconst internalWebPropertyID = store.getInternalWebPropertyID();\n\n\t\tconst args = {\n\t\t\tdateRange: select( CORE_USER ).getDateRange(),\n\t\t\tmultiDateRange: 1,\n\t\t\tdimensions: 'ga:date',\n\t\t\tmetrics: [\n\t\t\t\t{\n\t\t\t\t\texpression: 'ga:bounceRate',\n\t\t\t\t\talias: 'Bounce Rate',\n\t\t\t\t},\n\t\t\t],\n\t\t};\n\n\t\tconst url = select( CORE_SITE ).getCurrentEntityURL();\n\t\tif ( url ) {\n\t\t\targs.url = url;\n\t\t}\n\t\treturn {\n\t\t\tdata: store.getReport( args ),\n\t\t\terror: store.getErrorForSelector( 'getReport', [ args ] ),\n\t\t\tloading: ! store.hasFinishedResolution( 'getReport', [ args ] ),\n\t\t\tserviceURL: store.getServiceURL(\n\t\t\t\t{\n\t\t\t\t\tpath: applyEntityToReportPath( url, `\/report\/visitors-overview\/a${ accountID }w${ internalWebPropertyID }p${ profileID }\/` ),\n\t\t\t\t}\n\t\t\t),\n\t\t};\n\t} );\n\n\tif ( loading ) {\n\t\treturn <PreviewBlock width=\"100%\" height=\"202px\" \/>;\n\t}\n\n\tif ( error ) {\n\t\treturn <ReportError moduleSlug=\"analytics\" error={ error } \/>;\n\t}\n\n\tif ( isZeroReport( data ) ) {\n\t\treturn <ReportZero moduleSlug=\"analytics\" \/>;\n\t}\n\n\tconst sparkLineData = [\n\t\t[\n\t\t\t{ type: 'date', label: 'Day' },\n\t\t\t{ type: 'number', label: 'Bounce Rate' },\n\t\t],\n\t];\n\n\tconst dataRows = data[ 0 ].data.rows;\n\t\/\/ We only want half the date range, having `multiDateRange` in the query doubles the range.\n\tfor ( let i = Math.ceil( dataRows.length \/ 2 ); i < dataRows.length; i++ ) {\n\t\tconst { values } = dataRows[ i ].metrics[ 0 ];\n\t\tconst dateString = dataRows[ i ].dimensions[ 0 ];\n\t\tconst date = parseDimensionStringToDate( dateString );\n\t\tsparkLineData.push( [\n\t\t\tdate,\n\t\t\tvalues[ 0 ],\n\t\t] );\n\t}\n\n\tconst { totals } = data[ 0 ].data;\n\tconst lastMonth = totals[ 0 ].values;\n\tconst previousMonth = totals[ 1 ].values;\n\tconst averageBounceRate = lastMonth[ 0 ];\n\tconst averageBounceRateChange = changeToPercent( previousMonth[ 0 ], lastMonth[ 0 ] );\n\n\treturn (\n\t\t<DataBlock\n\t\t\tclassName=\"overview-bounce-rate\"\n\t\t\ttitle={ __( 'Bounce Rate', 'google-site-kit' ) }\n\t\t\tdatapoint={ Number( averageBounceRate ).toFixed( 2 ) }\n\t\t\tdatapointUnit=\"%\"\n\t\t\tchange={ averageBounceRateChange }\n\t\t\tchangeDataUnit=\"%\"\n\t\t\tinvertChangeColor\n\t\t\tsource={ {\n\t\t\t\tname: _x( 'Analytics', 'Service name', 'google-site-kit' ),\n\t\t\t\tlink: serviceURL,\n\t\t\t\texternal: true,\n\t\t\t} }\n\t\t\tsparkline={\n\t\t\t\tsparkLineData &&\n\t\t\t\t\t<Sparkline\n\t\t\t\t\t\tdata={ sparkLineData }\n\t\t\t\t\t\tchange={ averageBounceRateChange }\n\t\t\t\t\t\/>\n\t\t\t}\n\t\t\/>\n\t);\n}\n\nexport default whenActive( {\n\tmoduleName: 'analytics',\n\tfallbackComponent: AnalyticsInactiveCTA,\n} )( DashboardBounceRateWidget );\n","lang_cluster":"Javascript","length":151,"code_uid":"43238cf275a34d9f97610b60261d30f3"}
{"diff_hunk":"@@ -27,16 +27,16 @@ fi_FI.strings = {\n   dashboardWindowTitle: 'Tiedoston latausikkuna (Paina Esc sulkeaksesi)',\n   dataUploadedOfTotal: '%{complete} \/ %{total}',\n   done: 'Valmis',\n-  dropHereOr: 'Pudota tiedostot t\u00e4h\u00e4n tai %{browse}',\n-  dropHint: 'Pudota tiedostot t\u00e4h\u00e4n',\n-  dropPaste: 'Pudota tiedostot t\u00e4h\u00e4n, liit\u00e4 tai %{browse}',\n-  dropPasteImport: 'Pudota tiedostot t\u00e4h\u00e4n, liit\u00e4, %{browse} tai tuo',\n+  dropHereOr: 'Raahaa tiedostot t\u00e4h\u00e4n tai %{browse}',\n+  dropHint: 'Raahaa tiedostot t\u00e4h\u00e4n',\n+  dropPaste: 'Raahaa tiedostot t\u00e4h\u00e4n, liit\u00e4 tai %{browse}',\n+  dropPasteImport: 'Raahaa tiedostot t\u00e4h\u00e4n, liit\u00e4, %{browse} tai tuo',\n   edit: 'Muokkaa',\n   editFile: 'Muokkaa tiedostoa',\n   editing: 'Muokataan %{file}',\n   emptyFolderAdded: 'Ei lis\u00e4tty tiedostoja tyhj\u00e4st\u00e4 kansiosta',\n   encoding: 'Koodataan...',\n-  enterCorrectUrl: 'Ep\u00e4kelpo osoita: Varmista, ett\u00e4 osoite osoittaa suoraan tiedostoon',\n+  enterCorrectUrl: 'Ep\u00e4kelpo osoite: Varmista, ett\u00e4 osoite osoittaa suoraan tiedostoon',\n   enterUrlToImport: 'Anna osoite tuodaksesi tiedoston',\n   exceedsSize: 'Tiedoston koko ylitt\u00e4\u00e4 sallitun maksimin',\n   failedToFetch: 'Companion ei voinut ladata tiedostoa osoitteesta, onko osoite varmasti oikea?',","old_code":"const fi_FI = {}\n\nfi_FI.strings = {\n  addMore: 'Lis\u00e4\u00e4',\n  addMoreFiles: 'Lis\u00e4\u00e4 tiedostoja',\n  addingMoreFiles: 'Lis\u00e4t\u00e4\u00e4n tiedostoja',\n  allowAccessDescription: 'Jotta voit ottaa kuvia tai videota kamerallasi, sinun tulee antaa t\u00e4lle sivustolle oikeus k\u00e4ytt\u00e4\u00e4 kameraasi.',\n  allowAccessTitle: 'Salli kameran k\u00e4ytt\u00f6, kiitos',\n  authenticateWith: 'Mene %{pluginName}',\n  authenticateWithTitle: '%{pluginName} vaadittu tunnistautumiseen, jotta voit valita tiedostoja',\n  back: 'Takaisin',\n  browse: 'selaa',\n  cancel: 'Peruuta',\n  cancelUpload: 'Peruuta l\u00e4hetys',\n  chooseFiles: 'Valitse tiedostot',\n  closeModal: 'Sulje ikkuna',\n  companionAuthError: 'K\u00e4ytt\u00f6oikeus vaadittu',\n  companionError: 'Yhdist\u00e4minen Companioniin ep\u00e4onnistui',\n  complete: 'Valmis',\n  connectedToInternet: 'Yhdistetty Internettiin',\n  copyLink: 'Kopioi linkki',\n  copyLinkToClipboardFallback: 'Kopioi alla oleva linkki',\n  copyLinkToClipboardSuccess: 'Linkki kopioitu leikep\u00f6yd\u00e4lle',\n  creatingAssembly: 'Valmistellaan l\u00e4hetyst\u00e4...',\n  creatingAssemblyFailed: 'Transloadit: Assemblyn luonti ep\u00e4onnistui',\n  dashboardTitle: 'Tiedoston Lataaja',\n  dashboardWindowTitle: 'Tiedoston latausikkuna (Paina Esc sulkeaksesi)',\n  dataUploadedOfTotal: '%{complete} \/ %{total}',\n  done: 'Valmis',\n  dropHereOr: 'Pudota tiedostot t\u00e4h\u00e4n tai %{browse}',\n  dropHint: 'Pudota tiedostot t\u00e4h\u00e4n',\n  dropPaste: 'Pudota tiedostot t\u00e4h\u00e4n, liit\u00e4 tai %{browse}',\n  dropPasteImport: 'Pudota tiedostot t\u00e4h\u00e4n, liit\u00e4, %{browse} tai tuo',\n  edit: 'Muokkaa',\n  editFile: 'Muokkaa tiedostoa',\n  editing: 'Muokataan %{file}',\n  emptyFolderAdded: 'Ei lis\u00e4tty tiedostoja tyhj\u00e4st\u00e4 kansiosta',\n  encoding: 'Koodataan...',\n  enterCorrectUrl: 'Ep\u00e4kelpo osoita: Varmista, ett\u00e4 osoite osoittaa suoraan tiedostoon',\n  enterUrlToImport: 'Anna osoite tuodaksesi tiedoston',\n  exceedsSize: 'Tiedoston koko ylitt\u00e4\u00e4 sallitun maksimin',\n  failedToFetch: 'Companion ei voinut ladata tiedostoa osoitteesta, onko osoite varmasti oikea?',\n  failedToUpload: 'Ei voitu l\u00e4hett\u00e4\u00e4 tiedostoa %{file}',\n  fileSource: 'Tiedoston l\u00e4hde: %{name}',\n  filesUploadedOfTotal: {\n    '0': '%{complete} \/ %{smart_count} tiedostosta l\u00e4hetetty',\n    '1': '%{complete} \/ %{smart_count} tiedostoa l\u00e4hetetty',\n    '2': '%{complete} \/ %{smart_count} tiedostoa l\u00e4hetetty'\n  },\n  filter: 'Suodata',\n  finishEditingFile: 'Lopeta tiedoston muokkaus',\n  folderAdded: {\n    '0': 'Lis\u00e4tty %{smart_count} tiedosto kansiosta %{folder}',\n    '1': 'Lis\u00e4tty %{smart_count} tiedostoa kansiosta %{folder}',\n    '2': 'Lis\u00e4tty %{smart_count} tiedostoa kansiosta %{folder}'\n  },\n  import: 'Tuo',\n  importFrom: 'Tuo kohteesta %{name}',\n  link: 'Linkki',\n  loading: 'Ladataan...',\n  logOut: 'Kirjaudu ulos',\n  myDevice: 'Minun laite',\n  noFilesFound: 'Sinulla ei ole tiedostoja tai kansioita t\u00e4\u00e4ll\u00e4',\n  noInternetConnection: 'Ei Internet-yhteytt\u00e4',\n  openFolderNamed: 'Avaa kansio %{name}',\n  pause: 'Pys\u00e4yt\u00e4',\n  pauseUpload: 'Pys\u00e4yt\u00e4 l\u00e4hetys',\n  paused: 'Pys\u00e4ytetty',\n  poweredBy: 'Powered by',\n  preparingUpload: 'Valmistellaan l\u00e4hetyst\u00e4...',\n  processingXFiles: {\n    '0': 'K\u00e4sitell\u00e4\u00e4n %{smart_count} tiedostoa',\n    '1': 'K\u00e4sitell\u00e4\u00e4n %{smart_count} tiedostoa',\n    '2': 'K\u00e4sitell\u00e4\u00e4n %{smart_count} tiedostoa'\n  },\n  removeFile: 'Poista tiedosto',\n  resetFilter: 'Resetoi suodatin',\n  resume: 'Jatka',\n  resumeUpload: 'Jatka l\u00e4hetyst\u00e4',\n  retry: 'Yrit\u00e4 uudelleen',\n  retryUpload: 'Yrit\u00e4 uudelleen l\u00e4hetyst\u00e4',\n  saveChanges: 'Tallenna muutokset',\n  selectAllFilesFromFolderNamed: 'Valitse kaikki tiedostot kansiosta %{name}',\n  selectFileNamed: 'Valitse tiedosto %{name}',\n  selectX: {\n    '0': 'Valitse %{smart_count}',\n    '1': 'Valitse %{smart_count}',\n    '2': 'Valitse %{smart_count}'\n  },\n  smile: 'Hymyile!',\n  startRecording: 'Aloita videon tallennus',\n  stopRecording: 'Pys\u00e4yt\u00e4 videon tallennus',\n  takePicture: 'Ota kuva',\n  timedOut: 'L\u00e4hetys jumittunut %{seconds} sekunniksi, keskeytet\u00e4\u00e4n.',\n  unselectAllFilesFromFolderNamed: 'Poista tiedostojen valinta kansiossa %{name}',\n  unselectFileNamed: 'Poista valinta tiedostosta %{name}',\n  upload: 'L\u00e4het\u00e4',\n  uploadComplete: 'L\u00e4hetys valmis',\n  uploadFailed: 'L\u00e4hetys ep\u00e4onnistui',\n  uploadPaused: 'L\u00e4hetys pys\u00e4ytetty',\n  uploadXFiles: {\n    '0': 'L\u00e4het\u00e4 %{smart_count} tiedosto',\n    '1': 'L\u00e4het\u00e4 %{smart_count} tiedostoa',\n    '2': 'L\u00e4het\u00e4 %{smart_count} tiedostoa'\n  },\n  uploadXNewFiles: {\n    '0': 'L\u00e4het\u00e4 +%{smart_count} tiedosto',\n    '1': 'L\u00e4het\u00e4 +%{smart_count} tiedostoa',\n    '2': 'L\u00e4het\u00e4 +%{smart_count} tiedostoa'\n  },\n  uploading: 'Uploading',\n  uploadingXFiles: {\n    '0': 'L\u00e4hetet\u00e4\u00e4n %{smart_count} tiedosto',\n    '1': 'L\u00e4hetet\u00e4\u00e4n %{smart_count} tiedostoa',\n    '2': 'L\u00e4hetet\u00e4\u00e4n %{smart_count} tiedostoa'\n  },\n  xFilesSelected: {\n    '0': '%{smart_count} tiedosto valittu',\n    '1': '%{smart_count} tiedostoa valittu',\n    '2': '%{smart_count} tiedostoa valittu'\n  },\n  xMoreFilesAdded: {\n    '0': '%{smart_count} tiedosto added',\n    '1': '%{smart_count} tiedostoa added',\n    '2': '%{smart_count} tiedostoa added'\n  },\n  xTimeLeft: '%{time} j\u00e4ljell\u00e4',\n  youCanOnlyUploadFileTypes: 'Voit l\u00e4hett\u00e4\u00e4 vain: %{types}',\n  youCanOnlyUploadX: {\n    '0': 'Voit l\u00e4hett\u00e4\u00e4 vain %{smart_count} tiedosto',\n    '1': 'Voit l\u00e4hett\u00e4\u00e4 vain %{smart_count} tiedostoa',\n    '2': 'Voit l\u00e4hett\u00e4\u00e4 vain %{smart_count} tiedostoa'\n  },\n  youHaveToAtLeastSelectX: {\n    '0': 'Sinun pit\u00e4\u00e4 valita v\u00e4hint\u00e4\u00e4n %{smart_count} tiedosto',\n    '1': 'Sinun pit\u00e4\u00e4 valita v\u00e4hint\u00e4\u00e4n %{smart_count} tiedostoa',\n    '2': 'Sinun pit\u00e4\u00e4 valita v\u00e4hint\u00e4\u00e4n %{smart_count} tiedostoa'\n  }\n}\n\nfi_FI.pluralize = function (n) {\n  if (n === 1) {\n    return 0\n  }\n  return 1\n}\n\nif (typeof window !== 'undefined' && typeof window.Uppy !== 'undefined') {\n  window.Uppy.locales.fi_FI = fi_FI\n}\n\nmodule.exports = fi_FI\n","lang_cluster":"Javascript","length":152,"code_uid":"c085bc0b7c7f4850b10417bbb6e27b67"}
{"diff_hunk":"@@ -93,5 +93,27 @@ describe('Transactions', function() {\n         });\n       }\n     });\n+\n+    it('should not error if transactions are supported', {\n+      metadata: { requires: { topology: ['sharded'], mongodb: '>=4.1.0' } },\n+      test: function(done) {\n+        const configuration = this.configuration;\n+        const client = configuration.newClient(configuration.url());\n+\n+        client.connect((err, client) => {\n+          const session = client.startSession();\n+          const db = client.db(configuration.db);\n+          const coll = db.collection('transaction_error_test');\n+          coll.insertOne({ a: 1 }, err => {\n+            expect(err).to.not.exist;\n+            expect(() => session.startTransaction()).to.not.throw();\n+\n+            session.endSession(() => {\n+              client.close(done);\n+            });\n+          });\n+        });\n+      }\n+    });\n   });\n });","old_code":"'use strict';\n\nconst chai = require('chai');\nconst expect = chai.expect;\nconst core = require('..\/..\/lib\/core');\nconst sessions = core.Sessions;\nconst TestRunnerContext = require('.\/runner').TestRunnerContext;\nconst gatherTestSuites = require('.\/runner').gatherTestSuites;\nconst generateTopologyTests = require('.\/runner').generateTopologyTests;\n\ndescribe('Transactions', function() {\n  const testContext = new TestRunnerContext();\n\n  [\n    { name: 'spec tests', specPath: `${__dirname}\/spec\/transactions` },\n    {\n      name: 'withTransaction spec tests',\n      specPath: `${__dirname}\/spec\/transactions\/convenient-api`\n    }\n  ].forEach(suiteSpec => {\n    describe(suiteSpec.name, function() {\n      const testSuites = gatherTestSuites(suiteSpec.specPath);\n      after(() => testContext.teardown());\n      before(function() {\n        return testContext.setup(this.configuration);\n      });\n\n      function testFilter(spec) {\n        const SKIP_TESTS = [\n          \/\/ commitTransaction retry seems to be swallowed by mongos in these three cases\n          'commitTransaction retry succeeds on new mongos',\n          'commitTransaction retry fails on new mongos',\n          'unpin after transient error within a transaction and commit',\n          'count'\n        ];\n\n        return SKIP_TESTS.indexOf(spec.description) === -1;\n      }\n\n      generateTopologyTests(testSuites, testContext, testFilter);\n    });\n  });\n\n  describe('withTransaction', function() {\n    let session, sessionPool;\n    beforeEach(() => {\n      const topology = new core.Server();\n      sessionPool = new sessions.ServerSessionPool(topology);\n      session = new sessions.ClientSession(topology, sessionPool);\n    });\n\n    afterEach(() => {\n      sessionPool.endAllPooledSessions();\n    });\n\n    it('should provide a useful error if a Promise is not returned', {\n      metadata: { requires: { topology: ['replicaset', 'sharded'], mongodb: '>=4.1.5' } },\n      test: function(done) {\n        function fnThatDoesntReturnPromise() {\n          return false;\n        }\n\n        expect(() => session.withTransaction(fnThatDoesntReturnPromise)).to.throw(\n          \/must return a Promise\/\n        );\n\n        session.endSession(done);\n      }\n    });\n  });\n\n  describe('startTransaction', function() {\n    it('should error if transactions are not supported', {\n      metadata: { requires: { topology: ['sharded'], mongodb: '>4.0.0' } },\n      test: function(done) {\n        const configuration = this.configuration;\n        const client = configuration.newClient(configuration.writeConcernMax(), { poolSize: 1 });\n\n        client.connect((err, client) => {\n          const session = client.startSession();\n          const db = client.db(configuration.db);\n          const coll = db.collection('transaction_error_test');\n          coll.insertOne({ a: 1 }, err => {\n            expect(err).to.not.exist;\n            expect(() => session.startTransaction()).to.throw(\n              'Transactions are not supported on sharded clusters in MongoDB < 4.2.'\n            );\n\n            session.endSession(() => {\n              client.close(done);\n            });\n          });\n        });\n      }\n    });\n  });\n});\n","lang_cluster":"Javascript","length":97,"code_uid":"d5cc12a614694294a39ae1fbfcb4107b"}
{"diff_hunk":"@@ -14,6 +14,8 @@ module.exports = class RequestClient {\n     this.uppy = uppy\n     this.opts = opts\n     this.onReceiveResponse = this.onReceiveResponse.bind(this)\n+    this.allowedHeaders = []\n+    this.preflightDone = false\n   }\n \n   get hostname () {","old_code":"'use strict'\n\nconst AuthError = require('.\/AuthError')\n\n\/\/ Remove the trailing slash so we can always safely append \/xyz.\nfunction stripSlash (url) {\n  return url.replace(\/\\\/$\/, '')\n}\n\nmodule.exports = class RequestClient {\n  static VERSION = require('..\/package.json').version\n\n  constructor (uppy, opts) {\n    this.uppy = uppy\n    this.opts = opts\n    this.onReceiveResponse = this.onReceiveResponse.bind(this)\n  }\n\n  get hostname () {\n    const { companion } = this.uppy.getState()\n    const host = this.opts.companionUrl\n    return stripSlash(companion && companion[host] ? companion[host] : host)\n  }\n\n  get defaultHeaders () {\n    return {\n      'Accept': 'application\/json',\n      'Content-Type': 'application\/json'\n    }\n  }\n\n  headers () {\n    return Promise.resolve(Object.assign({}, this.defaultHeaders, this.opts.serverHeaders || {}))\n  }\n\n  _getPostResponseFunc (skip) {\n    return (response) => {\n      if (!skip) {\n        return this.onReceiveResponse(response)\n      }\n\n      return response\n    }\n  }\n\n  onReceiveResponse (response) {\n    const state = this.uppy.getState()\n    const companion = state.companion || {}\n    const host = this.opts.companionUrl\n    const headers = response.headers\n    \/\/ Store the self-identified domain name for the Companion instance we just hit.\n    if (headers.has('i-am') && headers.get('i-am') !== companion[host]) {\n      this.uppy.setState({\n        companion: Object.assign({}, companion, {\n          [host]: headers.get('i-am')\n        })\n      })\n    }\n    return response\n  }\n\n  _getUrl (url) {\n    if (\/^(https?:|)\\\/\\\/\/.test(url)) {\n      return url\n    }\n    return `${this.hostname}\/${url}`\n  }\n\n  _json (res) {\n    if (res.status === 401) {\n      throw new AuthError()\n    }\n\n    if (res.status < 200 || res.status > 300) {\n      throw new Error(`Failed request to ${res.url}. ${res.statusText}`)\n    }\n    return res.json()\n  }\n\n  get (path, skipPostResponse) {\n    return new Promise((resolve, reject) => {\n      this.headers().then((headers) => {\n        fetch(this._getUrl(path), {\n          method: 'get',\n          headers: headers,\n          credentials: 'same-origin'\n        })\n          .then(this._getPostResponseFunc(skipPostResponse))\n          .then((res) => this._json(res).then(resolve))\n          .catch((err) => {\n            err = err.isAuthError ? err : new Error(`Could not get ${this._getUrl(path)}. ${err}`)\n            reject(err)\n          })\n      })\n    })\n  }\n\n  post (path, data, skipPostResponse) {\n    return new Promise((resolve, reject) => {\n      this.headers().then((headers) => {\n        fetch(this._getUrl(path), {\n          method: 'post',\n          headers: headers,\n          credentials: 'same-origin',\n          body: JSON.stringify(data)\n        })\n          .then(this._getPostResponseFunc(skipPostResponse))\n          .then((res) => this._json(res).then(resolve))\n          .catch((err) => {\n            err = err.isAuthError ? err : new Error(`Could not post ${this._getUrl(path)}. ${err}`)\n            reject(err)\n          })\n      })\n    })\n  }\n\n  delete (path, data, skipPostResponse) {\n    return new Promise((resolve, reject) => {\n      this.headers().then((headers) => {\n        fetch(`${this.hostname}\/${path}`, {\n          method: 'delete',\n          headers: headers,\n          credentials: 'same-origin',\n          body: data ? JSON.stringify(data) : null\n        })\n          .then(this._getPostResponseFunc(skipPostResponse))\n          .then((res) => this._json(res).then(resolve))\n          .catch((err) => {\n            err = err.isAuthError ? err : new Error(`Could not delete ${this._getUrl(path)}. ${err}`)\n            reject(err)\n          })\n      })\n    })\n  }\n}\n","lang_cluster":"Javascript","length":135,"code_uid":"432855e485bb42578f81bcad88ed869a"}
{"diff_hunk":"@@ -80,7 +80,7 @@ define(['loading', 'libraryMenu', 'globalize', 'emby-checkbox', 'emby-select'],\n \n     function showAlertText(options) {\n         return new Promise(function (resolve, reject) {\n-            require(['alert'], function (alert) {\n+            import('alert').then(({default: alert}) => {\n                 alert(options).then(resolve, reject);\n             });\n         });","old_code":"define(['loading', 'libraryMenu', 'globalize', 'emby-checkbox', 'emby-select'], function (loading, libraryMenu, globalize) {\n    'use strict';\n\n    function onSubmit(e) {\n        var form = this;\n        var localAddress = form.querySelector('#txtLocalAddress').value;\n        var enableUpnp = form.querySelector('#chkEnableUpnp').checked;\n        confirmSelections(localAddress, enableUpnp, function () {\n            var validationResult = getValidationAlert(form);\n\n            if (validationResult) {\n                showAlertText(validationResult);\n                return;\n            }\n\n            validateHttps(form).then(function () {\n                loading.show();\n                ApiClient.getServerConfiguration().then(function (config) {\n                    config.LocalNetworkSubnets = form.querySelector('#txtLanNetworks').value.split(',').map(function (s) {\n                        return s.trim();\n                    }).filter(function (s) {\n                        return s.length > 0;\n                    });\n                    config.RemoteIPFilter = form.querySelector('#txtExternalAddressFilter').value.split(',').map(function (s) {\n                        return s.trim();\n                    }).filter(function (s) {\n                        return s.length > 0;\n                    });\n                    config.IsRemoteIPFilterBlacklist = 'blacklist' === form.querySelector('#selectExternalAddressFilterMode').value;\n                    config.PublicPort = form.querySelector('#txtPublicPort').value;\n                    config.PublicHttpsPort = form.querySelector('#txtPublicHttpsPort').value;\n                    config.HttpServerPortNumber = form.querySelector('#txtPortNumber').value;\n                    config.HttpsPortNumber = form.querySelector('#txtHttpsPort').value;\n                    config.EnableHttps = form.querySelector('#chkEnableHttps').checked;\n                    config.RequireHttps = form.querySelector('#chkRequireHttps').checked;\n                    config.EnableUPnP = enableUpnp;\n                    config.BaseUrl = form.querySelector('#txtBaseUrl').value;\n                    config.EnableRemoteAccess = form.querySelector('#chkRemoteAccess').checked;\n                    config.CertificatePath = form.querySelector('#txtCertificatePath').value || null;\n                    config.CertificatePassword = form.querySelector('#txtCertPassword').value || null;\n                    config.LocalNetworkAddresses = localAddress ? [localAddress] : [];\n                    ApiClient.updateServerConfiguration(config).then(Dashboard.processServerConfigurationUpdateResult, Dashboard.processErrorResponse);\n                });\n            });\n        });\n        e.preventDefault();\n    }\n\n    function triggerChange(select) {\n        var evt = document.createEvent('HTMLEvents');\n        evt.initEvent('change', false, true);\n        select.dispatchEvent(evt);\n    }\n\n    function getValidationAlert(form) {\n        if (form.querySelector('#txtPublicPort').value === form.querySelector('#txtPublicHttpsPort').value) {\n            return 'The public http and https ports must be different.';\n        }\n\n        if (form.querySelector('#txtPortNumber').value === form.querySelector('#txtHttpsPort').value) {\n            return 'The http and https ports must be different.';\n        }\n\n        return null;\n    }\n\n    function validateHttps(form) {\n        var certPath = form.querySelector('#txtCertificatePath').value || null;\n        var httpsEnabled = form.querySelector('#chkEnableHttps').checked;\n\n        if (httpsEnabled && !certPath) {\n            return showAlertText({\n                title: globalize.translate('TitleHostingSettings'),\n                text: globalize.translate('HttpsRequiresCert')\n            }).then(Promise.reject);\n        }\n\n        return Promise.resolve();\n    }\n\n    function showAlertText(options) {\n        return new Promise(function (resolve, reject) {\n            require(['alert'], function (alert) {\n                alert(options).then(resolve, reject);\n            });\n        });\n    }\n\n    function confirmSelections(localAddress, enableUpnp, callback) {\n        if (localAddress || !enableUpnp) {\n            showAlertText({\n                title: globalize.translate('TitleHostingSettings'),\n                text: globalize.translate('SettingsWarning')\n            }).then(callback);\n        } else {\n            callback();\n        }\n    }\n\n    return function (view, params) {\n        function loadPage(page, config) {\n            page.querySelector('#txtPortNumber').value = config.HttpServerPortNumber;\n            page.querySelector('#txtPublicPort').value = config.PublicPort;\n            page.querySelector('#txtPublicHttpsPort').value = config.PublicHttpsPort;\n            page.querySelector('#txtLocalAddress').value = config.LocalNetworkAddresses[0] || '';\n            page.querySelector('#txtLanNetworks').value = (config.LocalNetworkSubnets || []).join(', ');\n            page.querySelector('#txtExternalAddressFilter').value = (config.RemoteIPFilter || []).join(', ');\n            page.querySelector('#selectExternalAddressFilterMode').value = config.IsRemoteIPFilterBlacklist ? 'blacklist' : 'whitelist';\n            page.querySelector('#chkRemoteAccess').checked = null == config.EnableRemoteAccess || config.EnableRemoteAccess;\n            page.querySelector('#txtHttpsPort').value = config.HttpsPortNumber;\n            page.querySelector('#chkEnableHttps').checked = config.EnableHttps;\n            page.querySelector('#chkRequireHttps').checked = config.RequireHttps;\n            page.querySelector('#txtBaseUrl').value = config.BaseUrl || '';\n            var txtCertificatePath = page.querySelector('#txtCertificatePath');\n            txtCertificatePath.value = config.CertificatePath || '';\n            page.querySelector('#txtCertPassword').value = config.CertificatePassword || '';\n            page.querySelector('#chkEnableUpnp').checked = config.EnableUPnP;\n            triggerChange(page.querySelector('#chkRemoteAccess'));\n            loading.hide();\n        }\n\n        view.querySelector('#chkRemoteAccess').addEventListener('change', function () {\n            if (this.checked) {\n                view.querySelector('.fldExternalAddressFilter').classList.remove('hide');\n                view.querySelector('.fldExternalAddressFilterMode').classList.remove('hide');\n                view.querySelector('.fldPublicPort').classList.remove('hide');\n                view.querySelector('.fldPublicHttpsPort').classList.remove('hide');\n                view.querySelector('.fldEnableUpnp').classList.remove('hide');\n            } else {\n                view.querySelector('.fldExternalAddressFilter').classList.add('hide');\n                view.querySelector('.fldExternalAddressFilterMode').classList.add('hide');\n                view.querySelector('.fldPublicPort').classList.add('hide');\n                view.querySelector('.fldPublicHttpsPort').classList.add('hide');\n                view.querySelector('.fldEnableUpnp').classList.add('hide');\n            }\n        });\n        view.querySelector('#btnSelectCertPath').addEventListener('click', function () {\n            require(['directorybrowser'], function (directoryBrowser) {\n                var picker = new directoryBrowser();\n                picker.show({\n                    includeFiles: true,\n                    includeDirectories: true,\n                    callback: function (path) {\n                        if (path) {\n                            view.querySelector('#txtCertificatePath').value = path;\n                        }\n\n                        picker.close();\n                    },\n                    header: globalize.translate('HeaderSelectCertificatePath')\n                });\n            });\n        });\n        view.querySelector('.dashboardHostingForm').addEventListener('submit', onSubmit);\n        view.addEventListener('viewshow', function (e) {\n            loading.show();\n            ApiClient.getServerConfiguration().then(function (config) {\n                loadPage(view, config);\n            });\n        });\n    };\n});\n","lang_cluster":"Javascript","length":162,"code_uid":"9b85ae2c2cfc44c59002524d2f358612"}
{"diff_hunk":"@@ -61,6 +61,24 @@ function colorContrastEvaluate(node, options, virtualNode) {\n \t\t: contrastRatio.large;\n \tconst isValid = contrast > expected;\n \n+\t\/\/ if element or a parent has pseudo content then we need to mark\n+\t\/\/ as needs review\n+\tlet parentNode = node.parentElement;\n+\twhile (parentNode) {\n+\t\tif (\n+\t\t\thasPsuedoElement(parentNode, ':before') ||\n+\t\t\thasPsuedoElement(parentNode, ':after')\n+\t\t) {\n+\t\t\tthis.data({\n+\t\t\t\tmessageKey: 'pseudoContent'\n+\t\t\t});\n+\t\t\tthis.relatedNodes(parentNode);\n+\t\t\treturn undefined;\n+\t\t}\n+\n+\t\tparentNode = parentNode.parentElement;\n+\t}\n+\n \t\/\/ ratio is outside range\n \tif (\n \t\t(typeof minThreshold === 'number' && contrast < minThreshold) ||","old_code":"import { isVisible } from '..\/..\/commons\/dom';\nimport {\n\tvisibleVirtual,\n\thasUnicode,\n\tsanitize,\n\tremoveUnicode\n} from '..\/..\/commons\/text';\nimport {\n\tgetBackgroundColor,\n\tgetForegroundColor,\n\tincompleteData,\n\tgetContrast\n} from '..\/..\/commons\/color';\n\nfunction colorContrastEvaluate(node, options, virtualNode) {\n\tif (!isVisible(node, false)) {\n\t\treturn true;\n\t}\n\n\tconst {\n\t\tignoreUnicode,\n\t\tignoreLength,\n\t\tboldValue,\n\t\tboldTextPt,\n\t\tlargeTextPt,\n\t\tcontrastRatio\n\t} = options;\n\n\tconst visibleText = visibleVirtual(virtualNode, false, true);\n\tconst textContainsOnlyUnicode =\n\t\thasUnicode(visibleText, {\n\t\t\tnonBmp: true\n\t\t}) &&\n\t\tsanitize(\n\t\t\tremoveUnicode(visibleText, {\n\t\t\t\tnonBmp: true\n\t\t\t})\n\t\t) === '';\n\n\tif (textContainsOnlyUnicode && ignoreUnicode) {\n\t\tthis.data({ messageKey: 'nonBmp' });\n\t\treturn undefined;\n\t}\n\n\tconst bgNodes = [];\n\tconst bgColor = getBackgroundColor(node, bgNodes);\n\tconst fgColor = getForegroundColor(node, false, bgColor);\n\n\tconst nodeStyle = window.getComputedStyle(node);\n\tconst fontSize = parseFloat(nodeStyle.getPropertyValue('font-size'));\n\tconst fontWeight = nodeStyle.getPropertyValue('font-weight');\n\tconst bold = parseFloat(fontWeight) >= boldValue || fontWeight === 'bold';\n\n\tconst contrast = getContrast(bgColor, fgColor);\n\tconst ptSize = Math.ceil(fontSize * 72) \/ 96;\n\tconst isSmallFont =\n\t\t(bold && ptSize < boldTextPt) || (!bold && ptSize < largeTextPt);\n\n\tconst { expected, minThreshold, maxThreshold } = isSmallFont\n\t\t? contrastRatio.normal\n\t\t: contrastRatio.large;\n\tconst isValid = contrast > expected;\n\n\t\/\/ ratio is outside range\n\tif (\n\t\t(typeof minThreshold === 'number' && contrast < minThreshold) ||\n\t\t(typeof maxThreshold === 'number' && contrast > maxThreshold)\n\t) {\n\t\treturn true;\n\t}\n\n\t\/\/ truncate ratio to three digits while rounding down\n\t\/\/ 4.499 = 4.49, 4.019 = 4.01\n\tconst truncatedResult = Math.floor(contrast * 100) \/ 100;\n\n\t\/\/ if fgColor or bgColor are missing, get more information.\n\tlet missing;\n\tif (bgColor === null) {\n\t\tmissing = incompleteData.get('bgColor');\n\t}\n\n\tconst equalRatio = truncatedResult === 1;\n\tconst shortTextContent = visibleText.length === 1;\n\tif (equalRatio) {\n\t\tmissing = incompleteData.set('bgColor', 'equalRatio');\n\t} else if (shortTextContent && !ignoreLength) {\n\t\t\/\/ Check that the text content is a single character long\n\t\tmissing = 'shortTextContent';\n\t}\n\n\t\/\/ need both independently in case both are missing\n\tconst data = {\n\t\tfgColor: fgColor ? fgColor.toHexString() : undefined,\n\t\tbgColor: bgColor ? bgColor.toHexString() : undefined,\n\t\tcontrastRatio: truncatedResult,\n\t\tfontSize: `${((fontSize * 72) \/ 96).toFixed(1)}pt (${fontSize}px)`,\n\t\tfontWeight: bold ? 'bold' : 'normal',\n\t\tmessageKey: missing,\n\t\texpectedContrastRatio: expected + ':1'\n\t};\n\n\tthis.data(data);\n\n\t\/\/ We don't know, so we'll put it into Can't Tell\n\tif (\n\t\tfgColor === null ||\n\t\tbgColor === null ||\n\t\tequalRatio ||\n\t\t(shortTextContent && !ignoreLength && !isValid)\n\t) {\n\t\tmissing = null;\n\t\tincompleteData.clear();\n\t\tthis.relatedNodes(bgNodes);\n\t\treturn undefined;\n\t}\n\n\tif (!isValid) {\n\t\tthis.relatedNodes(bgNodes);\n\t}\n\n\treturn isValid;\n}\n\nexport default colorContrastEvaluate;\n","lang_cluster":"Javascript","length":124,"code_uid":"5ce907d97b494667a2a7bc7895d5bc0a"}
{"diff_hunk":"@@ -1,5 +1,8 @@\n 'use strict';\n \n+const parsePackageVersion = require('..\/..\/utils').parsePackageVersion;\n+const emitWarningOnce = require('..\/..\/utils').emitWarningOnce;\n+\n const require_optional = require('optional-require')(require);\n \n function debugOptions(debugFields, options) {","old_code":"'use strict';\n\nconst require_optional = require('optional-require')(require);\n\nfunction debugOptions(debugFields, options) {\n  const finaloptions = {};\n  debugFields.forEach(function(n) {\n    finaloptions[n] = options[n];\n  });\n\n  return finaloptions;\n}\n\nfunction retrieveBSON() {\n  const BSON = require('bson');\n  BSON.native = false;\n\n  const optionalBSON = require_optional('bson-ext');\n  if (optionalBSON) {\n    optionalBSON.native = true;\n    return optionalBSON;\n  }\n\n  return BSON;\n}\n\n\/\/ Throw an error if an attempt to use Snappy is made when Snappy is not installed\nfunction noSnappyWarning() {\n  throw new Error(\n    'Attempted to use Snappy compression, but Snappy is not installed. Install or disable Snappy compression and try again.'\n  );\n}\n\n\/\/ Facilitate loading Snappy optionally\nfunction retrieveSnappy() {\n  let snappy = require_optional('snappy');\n  if (!snappy) {\n    snappy = {\n      compress: noSnappyWarning,\n      uncompress: noSnappyWarning,\n      compressSync: noSnappyWarning,\n      uncompressSync: noSnappyWarning\n    };\n  }\n  return snappy;\n}\n\nmodule.exports = {\n  debugOptions,\n  retrieveBSON,\n  retrieveSnappy\n};\n","lang_cluster":"Javascript","length":52,"code_uid":"c948778c1a9c46e69e945175c14c2bb5"}
{"diff_hunk":"@@ -33,6 +33,7 @@ import { ESCAPE } from '@wordpress\/keycodes';\n  *\/\n import Data from 'googlesitekit-data';\n import { CORE_MODULES } from '..\/..\/..\/googlesitekit\/modules\/datastore\/constants';\n+import { CORE_SITE } from '..\/..\/..\/googlesitekit\/datastore\/site\/constants';\n import { clearWebStorage } from '..\/..\/..\/util';\n import Dialog from '..\/..\/Dialog';\n const { useSelect, useDispatch } = Data;","old_code":"\/**\n * ConfirmDisconnect component for SettingsActiveModule.\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * External dependencies\n *\/\nimport PropTypes from 'prop-types';\n\n\/**\n * WordPress dependencies\n *\/\nimport { __, sprintf } from '@wordpress\/i18n';\nimport { useState, useEffect, useCallback } from '@wordpress\/element';\nimport { ESCAPE } from '@wordpress\/keycodes';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport { CORE_MODULES } from '..\/..\/..\/googlesitekit\/modules\/datastore\/constants';\nimport { clearWebStorage } from '..\/..\/..\/util';\nimport Dialog from '..\/..\/Dialog';\nconst { useSelect, useDispatch } = Data;\n\nexport default function ConfirmDisconnect( { slug, handleDialog } ) {\n\tconst [ isDeactivating, setIsDeactivating ] = useState( false );\n\n\tconst dependentModules = useSelect( ( select ) => select( CORE_MODULES ).getModuleDependantNames( slug ) );\n\tconst provides = useSelect( ( select ) => select( CORE_MODULES ).getModuleFeatures( slug ) );\n\tconst module = useSelect( ( select ) => select( CORE_MODULES ).getModule( slug ) );\n\tconst moduleStoreName = useSelect( ( select ) => select( CORE_MODULES ).getModuleStoreName( slug ) );\n\tconst adminReauthURL = useSelect( ( select ) => select( moduleStoreName )?.getAdminReauthURL( false ) );\n\n\tuseEffect( () => {\n\t\tconst onKeyPress = ( e ) => {\n\t\t\tif ( ESCAPE === e.keyCode ) {\n\t\t\t\thandleDialog();\n\t\t\t}\n\t\t};\n\n\t\tglobal.addEventListener( 'keydown', onKeyPress );\n\t\treturn () => {\n\t\t\tglobal.removeEventListener( 'keydown', onKeyPress );\n\t\t};\n\t}, [ handleDialog ] );\n\n\tconst { deactivateModule } = useDispatch( CORE_MODULES );\n\tconst handleDisconnect = useCallback( async () => {\n\t\tif ( module.forceActive ) {\n\t\t\treturn;\n\t\t}\n\n\t\tsetIsDeactivating( true );\n\t\tconst { error } = await deactivateModule( slug );\n\t\tsetIsDeactivating( false );\n\n\t\tif ( ! error ) {\n\t\t\tclearWebStorage();\n\t\t\tglobal.location.assign( adminReauthURL );\n\t\t}\n\t}, [ module?.slug ] );\n\n\tif ( ! module ) {\n\t\treturn null;\n\t}\n\n\tconst { name } = module;\n\n\tconst title = sprintf(\n\t\t\/* translators: %s: module name *\/\n\t\t__( 'Disconnect %s from Site Kit?', 'google-site-kit' ),\n\t\tname,\n\t);\n\n\tconst subtitle = sprintf(\n\t\t\/* translators: %s: module name *\/\n\t\t__( 'By disconnecting the %s module from Site Kit, you will no longer have access to:', 'google-site-kit' ),\n\t\tname,\n\t);\n\n\tlet dependentModulesText = null;\n\tif ( dependentModules.length > 0 ) {\n\t\tdependentModulesText = sprintf(\n\t\t\t\/* translators: %1$s: module name, %2$s: list of dependent modules *\/\n\t\t\t__( 'these active modules depend on %1$s and will also be disconnected: %2$s', 'google-site-kit' ),\n\t\t\tname,\n\t\t\tdependentModules,\n\t\t);\n\t}\n\n\treturn (\n\t\t<Dialog\n\t\t\tdialogActive\n\t\t\thandleDialog={ handleDialog }\n\t\t\ttitle={ title }\n\t\t\tsubtitle={ subtitle }\n\t\t\tprovides={ provides }\n\t\t\thandleConfirm={ handleDisconnect }\n\t\t\tdependentModules={ dependentModulesText }\n\t\t\tinProgress={ isDeactivating }\n\t\t\tdanger\n\t\t\/>\n\t);\n}\n\nConfirmDisconnect.propTypes = {\n\tslug: PropTypes.string.isRequired,\n\thandleDialog: PropTypes.func.isRequired,\n};\n","lang_cluster":"Javascript","length":124,"code_uid":"2babde780f2d4f85b5056b9b168fb144"}
{"diff_hunk":"@@ -132,7 +132,7 @@\n                 $scope.expandableRow = {};\n \n                 $scope.expandableRow.shouldRenderExpand = function () {\n-                  var ret = $scope.colContainer.name === 'body' &&  $scope.row.isExpanded && (!$scope.grid.isScrollingVertically || $scope.row.expandedRendered);\n+                  var ret = $scope.colContainer.name === 'body' &&  $scope.grid.options.enableExpandable !== false && $scope.row.isExpanded && (!$scope.grid.isScrollingVertically || $scope.row.expandedRendered);\n                   return ret;\n                 };\n ","old_code":"(function () {\n  'use strict';\n\n  var module = angular.module('ui.grid.expandable', ['ui.grid']);\n\n  module.service('uiGridExpandableService', ['gridUtil', '$log', '$compile', function (gridUtil, $log, $compile) {\n    var service = {\n      initializeGrid: function (grid) {\n        var publicApi = {\n          events: {\n            expandable: {\n              rowExpandedStateChanged: function (scope, row) {\n              }\n            }\n          },\n          methods: {\n            expandable: {\n              toggleRowExpansion: function (rowEntity) {\n                var row = grid.getRow(rowEntity);\n                if (row !== null) {\n                  service.toggleRowExpansion(grid, row);\n                }\n              },\n              expandAllRows: function() {\n                service.expandAllRows(grid);\n              },\n              collapseAllRows: function() {\n                service.collapseAllRows(grid);\n              }\n            }\n          }\n        };\n        grid.api.registerEventsFromObject(publicApi.events);\n        grid.api.registerMethodsFromObject(publicApi.methods);\n      },\n      toggleRowExpansion: function (grid, row) {\n        row.isExpanded = !row.isExpanded;\n\n        if (row.isExpanded) {\n          row.height = row.grid.options.rowHeight + grid.options.expandable.expandableRowHeight;\n        }\n        else {\n          row.height = row.grid.options.rowHeight;\n        }\n\n        grid.api.expandable.raise.rowExpandedStateChanged(row);\n      },\n      expandAllRows: function(grid, $scope) {\n        angular.forEach(grid.renderContainers.body.visibleRowCache, function(row) {\n          if (!row.isExpanded) {\n            service.toggleRowExpansion(grid, row);\n          }\n        });\n        grid.refresh();\n      },\n      collapseAllRows: function(grid) {\n        angular.forEach(grid.renderContainers.body.visibleRowCache, function(row) {\n          if (row.isExpanded) {\n            service.toggleRowExpansion(grid, row);\n          }\n        });\n        grid.refresh();\n      }\n    };\n    return service;\n  }]);\n\n  module.directive('uiGridExpandable', ['$log', 'uiGridExpandableService', '$templateCache',\n    function ($log, uiGridExpandableService, $templateCache) {\n      return {\n        replace: true,\n        priority: 0,\n        require: '^uiGrid',\n        scope: false,\n        compile: function () {\n          return {\n            pre: function ($scope, $elm, $attrs, uiGridCtrl) {\n              if (uiGridCtrl.grid.options.expandable.enableExpandableRowHeader ) {\n                var expandableRowHeaderColDef = {name: 'expandableButtons', width: 40};\n                expandableRowHeaderColDef.cellTemplate = $templateCache.get('ui-grid\/expandableRowHeader');\n                uiGridCtrl.grid.addRowHeaderColumn(expandableRowHeaderColDef);\n              }\n              uiGridExpandableService.initializeGrid(uiGridCtrl.grid);\n            },\n            post: function ($scope, $elm, $attrs, uiGridCtrl) {\n            }\n          };\n        }\n      };\n    }]);\n\n  module.directive('uiGridExpandableRow',\n  ['uiGridExpandableService', '$timeout', '$log', '$compile', 'uiGridConstants','gridUtil','$interval',\n    function (uiGridExpandableService, $timeout, $log, $compile, uiGridConstants, gridUtil, $interval) {\n\n      return {\n        replace: false,\n        priority: 0,\n        scope: false,\n\n        compile: function () {\n          return {\n            pre: function ($scope, $elm, $attrs, uiGridCtrl) {\n              gridUtil.getTemplate($scope.grid.options.expandable.rowExpandableTemplate).then(\n                function (template) {\n                  var expandedRowElement = $compile(template)($scope);\n                  $elm.append(expandedRowElement);\n                  $scope.row.expandedRendered = true;\n              });\n            },\n\n            post: function ($scope, $elm, $attrs, uiGridCtrl) {\n              $scope.$on('$destroy', function() {\n                $scope.row.expandedRendered = false;\n              });\n            }\n          };\n        }\n      };\n    }]);\n\n  module.directive('uiGridRow',\n    ['$compile', '$log', '$templateCache',\n      function ($compile, $log, $templateCache) {\n        return {\n          priority: -200,\n          scope: false,\n          compile: function ($elm, $attrs) {\n            return {\n              pre: function ($scope, $elm, $attrs, controllers) {\n\n                $scope.expandableRow = {};\n\n                $scope.expandableRow.shouldRenderExpand = function () {\n                  var ret = $scope.colContainer.name === 'body' &&  $scope.row.isExpanded && (!$scope.grid.isScrollingVertically || $scope.row.expandedRendered);\n                  return ret;\n                };\n\n                $scope.expandableRow.shouldRenderFiller = function () {\n                  var ret = $scope.row.isExpanded && ( $scope.colContainer.name !== 'body' || ($scope.grid.isScrollingVertically && !$scope.row.expandedRendered));\n                  return ret;\n                };\n\n                  function updateRowContainerWidth() {\n                      var grid = $scope.grid;\n                      var colWidth = grid.getColumn('expandableButtons').width;\n                      return '.grid' + grid.id + ' .ui-grid-pinned-container-' + $scope.colContainer.name + ', .grid' + grid.id +\n                          ' .ui-grid-pinned-container-' + $scope.colContainer.name + ' .ui-grid-render-container-' + $scope.colContainer.name +\n                          ' .ui-grid-viewport .ui-grid-canvas .ui-grid-row { width: ' + colWidth + 'px; }';\n                  }\n\n                  if ($scope.colContainer.name === 'left') {\n                      $scope.grid.registerStyleComputation({\n                          priority: 15,\n                          func: updateRowContainerWidth\n                      });\n                  }\n\n              },\n              post: function ($scope, $elm, $attrs, controllers) {\n              }\n            };\n          }\n        };\n      }]);\n\n  module.directive('uiGridViewport',\n    ['$compile', '$log', '$templateCache',\n      function ($compile, $log, $templateCache) {\n        return {\n          priority: -200,\n          scope: false,\n          compile: function ($elm, $attrs) {\n            var rowRepeatDiv = angular.element($elm.children().children()[0]);\n            var expandedRowFillerElement = $templateCache.get('ui-grid\/expandableScrollFiller');\n            var expandedRowElement = $templateCache.get('ui-grid\/expandableRow');\n            rowRepeatDiv.append(expandedRowElement);\n            rowRepeatDiv.append(expandedRowFillerElement);\n            return {\n              pre: function ($scope, $elm, $attrs, controllers) {\n              },\n              post: function ($scope, $elm, $attrs, controllers) {\n              }\n            };\n          }\n        };\n      }]);\n\n})();\n","lang_cluster":"Javascript","length":189,"code_uid":"48d435363d3e4e089bd05baf0ebb4cdb"}
{"diff_hunk":"@@ -98,7 +98,7 @@ const WPDashboardPopularPages = ( { WidgetReportZero, WidgetReportError } ) => {\n \t\t\t<\/h2>\n \t\t\t<TableOverflowContainer>\n \t\t\t\t<ReportTable\n-\t\t\t\t\trows={ data[ 0 ].data.rows }\n+\t\t\t\t\trows={ rows }\n \t\t\t\t\tcolumns={ tableColumns }\n \t\t\t\t\tlimit={ 5 }\n \t\t\t\t\/>","old_code":"\/**\n * WPDashboardPopularPages component.\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * WordPress dependencies\n *\/\nimport { __ } from '@wordpress\/i18n';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport {\n\tMODULES_ANALYTICS,\n\tDATE_RANGE_OFFSET,\n} from '..\/..\/modules\/analytics\/datastore\/constants';\nimport { CORE_USER } from '..\/..\/googlesitekit\/datastore\/user\/constants';\nimport PreviewTable from '..\/..\/components\/PreviewTable';\nimport TableOverflowContainer from '..\/..\/components\/TableOverflowContainer';\nimport { isZeroReport } from '..\/..\/modules\/analytics\/util\/is-zero-report';\nimport ReportTable from '..\/ReportTable';\nimport DetailsPermaLinks from '..\/DetailsPermaLinks';\nimport { numFmt } from '..\/..\/util';\nconst { useSelect } = Data;\n\nconst WPDashboardPopularPages = ( { WidgetReportZero, WidgetReportError } ) => {\n\tconst dateRangeDates = useSelect( ( select ) =>\n\t\tselect( CORE_USER ).getDateRangeDates( {\n\t\t\tcompare: true,\n\t\t\toffsetDays: DATE_RANGE_OFFSET,\n\t\t} )\n\t);\n\n\tconst reportArgs = {\n\t\t...dateRangeDates,\n\t\tmetrics: [\n\t\t\t{\n\t\t\t\texpression: 'ga:pageviews',\n\t\t\t\talias: 'Pageviews',\n\t\t\t},\n\t\t],\n\t\tdimensions: [ 'ga:pageTitle', 'ga:pagePath' ],\n\t\torderby: [\n\t\t\t{\n\t\t\t\tfieldName: 'ga:pageviews',\n\t\t\t\tsortOrder: 'DESCENDING',\n\t\t\t},\n\t\t],\n\t\tlimit: 5,\n\t};\n\n\tconst data = useSelect( ( select ) =>\n\t\tselect( MODULES_ANALYTICS ).getReport( reportArgs )\n\t);\n\tconst error = useSelect( ( select ) =>\n\t\tselect( MODULES_ANALYTICS ).getErrorForSelector( 'getReport', [\n\t\t\treportArgs,\n\t\t] )\n\t);\n\tconst loading = useSelect(\n\t\t( select ) =>\n\t\t\t! select( MODULES_ANALYTICS ).hasFinishedResolution( 'getReport', [\n\t\t\t\treportArgs,\n\t\t\t] )\n\t);\n\n\tif ( loading ) {\n\t\treturn <PreviewTable rows={ 6 } \/>;\n\t}\n\n\tif ( error ) {\n\t\treturn <WidgetReportError moduleSlug=\"analytics\" error={ error } \/>;\n\t}\n\n\tif ( isZeroReport( data ) ) {\n\t\treturn <WidgetReportZero moduleSlug=\"analytics\" \/>;\n\t}\n\n\treturn (\n\t\t<div className=\"googlesitekit-search-console-widget\">\n\t\t\t<h2 className=\"googlesitekit-search-console-widget__title\">\n\t\t\t\t{ __( 'Top content over the last 28 days', 'google-site-kit' ) }\n\t\t\t<\/h2>\n\t\t\t<TableOverflowContainer>\n\t\t\t\t<ReportTable\n\t\t\t\t\trows={ data[ 0 ].data.rows }\n\t\t\t\t\tcolumns={ tableColumns }\n\t\t\t\t\tlimit={ 5 }\n\t\t\t\t\/>\n\t\t\t<\/TableOverflowContainer>\n\t\t<\/div>\n\t);\n};\n\nconst tableColumns = [\n\t{\n\t\ttitle: __( 'Title', 'google-site-kit' ),\n\t\tdescription: __( 'Page Title', 'google-site-kit' ),\n\t\tprimary: true,\n\t\tComponent: ( { row } ) => {\n\t\t\tconst [ title, path ] = row.dimensions;\n\t\t\treturn <DetailsPermaLinks title={ title } path={ path } \/>;\n\t\t},\n\t},\n\t{\n\t\ttitle: __( 'Pageviews', 'google-site-kit' ),\n\t\tdescription: __( 'Pageviews', 'google-site-kit' ),\n\t\tfield: 'metrics.0.values.0',\n\t\tComponent: ( { fieldValue } ) => (\n\t\t\t<span>{ numFmt( fieldValue, { style: 'decimal' } ) }<\/span>\n\t\t),\n\t},\n];\n\nexport default WPDashboardPopularPages;\n","lang_cluster":"Javascript","length":130,"code_uid":"30322facfefc41008209e8c47ad3f93c"}
{"diff_hunk":"@@ -128,7 +128,7 @@ class ConditionUpdateObserver {\n     }\n \n     const visibleDataFactory = curry((curriedConditionsBefore, curriedColumn, conditionsStack = []) => {\n-      const splitConditionCollection = new ConditionCollection();\n+      const splitConditionCollection = new ConditionCollection(new IndexToValueMap().init(this.getNumberOfColumns()));\n       const curriedConditionsBeforeArray = [].concat(curriedConditionsBefore, conditionsStack);\n \n       \/\/ Create new condition collection to determine what rows should be visible in \"filter by value\" box","old_code":"import { arrayEach, arrayMap, arrayFilter } from '..\/..\/helpers\/array';\nimport { mixin, objectEach } from '..\/..\/helpers\/object';\nimport { curry } from '..\/..\/helpers\/function';\nimport localHooks from '..\/..\/mixins\/localHooks';\nimport ConditionCollection from '.\/conditionCollection';\nimport DataFilter from '.\/dataFilter';\nimport { createArrayAssertion } from '.\/utils';\n\n\/**\n * Class which is designed for observing changes in condition collection. When condition is changed by user at specified\n * column it's necessary to update all conditions defined after this edited one.\n *\n * Object fires `update` hook for every column conditions change.\n *\n * @class ConditionUpdateObserver\n * @plugin Filters\n *\/\nclass ConditionUpdateObserver {\n  constructor(conditionCollection, columnDataFactory = () => []) {\n    \/**\n     * Reference to the instance of {@link ConditionCollection}.\n     *\n     * @type {ConditionCollection}\n     *\/\n    this.conditionCollection = conditionCollection;\n    \/**\n     * Function which provide source data factory for specified column.\n     *\n     * @type {Function}\n     *\/\n    this.columnDataFactory = columnDataFactory;\n    \/**\n     * Collected changes when grouping is enabled.\n     *\n     * @type {Array}\n     * @default []\n     *\/\n    this.changes = [];\n    \/**\n     * Flag which determines if grouping events is enabled.\n     *\n     * @type {boolean}\n     *\/\n    this.grouping = false;\n    \/**\n     * The latest known position of edited conditions at specified column index.\n     *\n     * @type {number}\n     * @default -1\n     *\/\n    this.latestEditedColumnPosition = -1;\n    \/**\n     * The latest known order of conditions stack.\n     *\n     * @type {Array}\n     *\/\n    this.latestOrderStack = [];\n\n    this.conditionCollection.addLocalHook('beforeRemove', column => this._onConditionBeforeModify(column));\n    this.conditionCollection.addLocalHook('afterAdd', column => this.updateStatesAtColumn(column));\n    this.conditionCollection.addLocalHook('afterClear', column => this.updateStatesAtColumn(column));\n    this.conditionCollection.addLocalHook('beforeClean', () => this._onConditionBeforeClean());\n    this.conditionCollection.addLocalHook('afterClean', () => this._onConditionAfterClean());\n  }\n\n  \/**\n   * Enable grouping changes. Grouping is helpful in situations when a lot of conditions is added in one moment. Instead of\n   * trigger `update` hook for every condition by adding\/removing you can group this changes and call `flush` method to trigger\n   * it once.\n   *\/\n  groupChanges() {\n    this.grouping = true;\n  }\n\n  \/**\n   * Flush all collected changes. This trigger `update` hook for every previously collected change from condition collection.\n   *\/\n  flush() {\n    this.grouping = false;\n\n    arrayEach(this.changes, (column) => {\n      this.updateStatesAtColumn(column);\n    });\n    this.changes.length = 0;\n  }\n\n  \/**\n   * On before modify condition (add or remove from collection),.\n   *\n   * @param {number} column Column index.\n   * @private\n   *\/\n  _onConditionBeforeModify(column) {\n    this.latestEditedColumnPosition = this.conditionCollection.orderStack.indexOf(column);\n  }\n\n  \/**\n   * Update all related states which should be changed after invoking changes applied to current column.\n   *\n   * @param {number} column The column index.\n   * @param {object} conditionArgsChange Object describing condition changes which can be handled by filters on `update` hook.\n   * It contains keys `conditionKey` and `conditionValue` which refers to change specified key of condition to specified value\n   * based on referred keys.\n   *\/\n  updateStatesAtColumn(column, conditionArgsChange) {\n    if (this.grouping) {\n      if (this.changes.indexOf(column) === -1) {\n        this.changes.push(column);\n      }\n\n      return;\n    }\n    const allConditions = this.conditionCollection.exportAllConditions();\n    let editedColumnPosition = this.conditionCollection.orderStack.indexOf(column);\n\n    if (editedColumnPosition === -1) {\n      editedColumnPosition = this.latestEditedColumnPosition;\n    }\n\n    \/\/ Collection of all conditions defined before currently edited `column` (without edited one)\n    const conditionsBefore = allConditions.slice(0, editedColumnPosition);\n    \/\/ Collection of all conditions defined after currently edited `column` (without edited one)\n    const conditionsAfter = allConditions.slice(editedColumnPosition);\n\n    \/\/ Make sure that conditionAfter doesn't contain edited column conditions\n    if (conditionsAfter.length && conditionsAfter[0].column === column) {\n      conditionsAfter.shift();\n    }\n\n    const visibleDataFactory = curry((curriedConditionsBefore, curriedColumn, conditionsStack = []) => {\n      const splitConditionCollection = new ConditionCollection();\n      const curriedConditionsBeforeArray = [].concat(curriedConditionsBefore, conditionsStack);\n\n      \/\/ Create new condition collection to determine what rows should be visible in \"filter by value\" box\n      \/\/ in the next conditions in the chain\n      splitConditionCollection.importAllConditions(curriedConditionsBeforeArray);\n\n      const allRows = this.columnDataFactory(curriedColumn);\n      let visibleRows;\n\n      if (splitConditionCollection.isEmpty()) {\n        visibleRows = allRows;\n      } else {\n        visibleRows = (new DataFilter(\n          splitConditionCollection,\n          columnData => this.columnDataFactory(columnData)\n        )).filter();\n      }\n      visibleRows = arrayMap(visibleRows, rowData => rowData.meta.visualRow);\n\n      const visibleRowsAssertion = createArrayAssertion(visibleRows);\n\n      return arrayFilter(allRows, rowData => visibleRowsAssertion(rowData.meta.visualRow));\n    })(conditionsBefore);\n\n    const editedConditions = [].concat(this.conditionCollection.getConditions(column));\n\n    this.runLocalHooks('update', {\n      editedConditionStack: { column, conditions: editedConditions },\n      dependentConditionStacks: conditionsAfter,\n      filteredRowsFactory: visibleDataFactory,\n      conditionArgsChange\n    });\n  }\n\n  \/**\n   * On before conditions clean listener.\n   *\n   * @private\n   *\/\n  _onConditionBeforeClean() {\n    this.latestOrderStack = [].concat(this.conditionCollection.orderStack);\n  }\n\n  \/**\n   * On after conditions clean listener.\n   *\n   * @private\n   *\/\n  _onConditionAfterClean() {\n    arrayEach(this.latestOrderStack, (column) => {\n      this.updateStatesAtColumn(column);\n    });\n  }\n\n  \/**\n   * Destroy instance.\n   *\/\n  destroy() {\n    this.clearLocalHooks();\n\n    objectEach(this, (value, property) => {\n      this[property] = null;\n    });\n  }\n}\n\nmixin(ConditionUpdateObserver, localHooks);\n\nexport default ConditionUpdateObserver;\n","lang_cluster":"Javascript","length":200,"code_uid":"f8ea2f44564c4f5f9756b82bbeef6da4"}
{"diff_hunk":"@@ -19,12 +19,13 @@\n \/**\n  * External dependencies\n  *\/\n+import classnames from 'classnames';\n import PropTypes from 'prop-types';\n \n \/**\n  * WordPress dependencies\n  *\/\n-import { useCallback } from '@wordpress\/element';\n+import { useCallback, useContext } from '@wordpress\/element';\n import { __, sprintf } from '@wordpress\/i18n';\n \n \/**","old_code":"\/**\n * GA4 Property Select component.\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * External dependencies\n *\/\nimport PropTypes from 'prop-types';\n\n\/**\n * WordPress dependencies\n *\/\nimport { useCallback } from '@wordpress\/element';\nimport { __, sprintf } from '@wordpress\/i18n';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport { Select, Option } from '..\/..\/..\/..\/material-components';\nimport ProgressBar from '..\/..\/..\/..\/components\/ProgressBar';\nimport {\n\tMODULES_ANALYTICS_4,\n\tPROPERTY_CREATE,\n} from '..\/..\/datastore\/constants';\nimport { MODULES_ANALYTICS } from '..\/..\/..\/analytics\/datastore\/constants';\nimport { isValidAccountID } from '..\/..\/..\/analytics\/util';\nimport { trackEvent } from '..\/..\/..\/..\/util';\nconst { useSelect, useDispatch } = Data;\n\nexport default function PropertySelect( { label } ) {\n\t\/\/ TODO: Update this select hook to pull accountID from the modules\/analytics-4 datastore when GA4 module becomes separated from the Analytics one\n\tconst accountID = useSelect( ( select ) =>\n\t\tselect( MODULES_ANALYTICS ).getAccountID()\n\t);\n\tconst properties = useSelect(\n\t\t( select ) =>\n\t\t\tselect( MODULES_ANALYTICS_4 ).getProperties( accountID ) || []\n\t);\n\tconst propertyID = useSelect( ( select ) =>\n\t\tselect( MODULES_ANALYTICS_4 ).getPropertyID()\n\t);\n\tconst isLoading = useSelect(\n\t\t( select ) =>\n\t\t\t! select( MODULES_ANALYTICS ).hasFinishedResolution(\n\t\t\t\t'getAccounts'\n\t\t\t) ||\n\t\t\t! select(\n\t\t\t\tMODULES_ANALYTICS_4\n\t\t\t).hasFinishedResolution( 'getProperties', [ accountID ] )\n\t);\n\n\tconst { selectProperty } = useDispatch( MODULES_ANALYTICS_4 );\n\n\tconst onChange = useCallback(\n\t\t( index, item ) => {\n\t\t\tconst newPropertyID = item.dataset.value;\n\t\t\tif ( propertyID !== newPropertyID ) {\n\t\t\t\tselectProperty( newPropertyID );\n\t\t\t\ttrackEvent(\n\t\t\t\t\t'analytics_setup',\n\t\t\t\t\t'property_change',\n\t\t\t\t\tnewPropertyID\n\t\t\t\t);\n\t\t\t}\n\t\t},\n\t\t[ propertyID, selectProperty ]\n\t);\n\n\tif ( ! isValidAccountID( accountID ) ) {\n\t\treturn null;\n\t}\n\n\tif ( isLoading ) {\n\t\treturn <ProgressBar small \/>;\n\t}\n\n\treturn (\n\t\t<Select\n\t\t\tclassName=\"googlesitekit-analytics__select-property\"\n\t\t\tlabel={ label || __( 'Property', 'google-site-kit' ) }\n\t\t\tvalue={ propertyID }\n\t\t\tonEnhancedChange={ onChange }\n\t\t\tdisabled={ ! isValidAccountID( accountID ) }\n\t\t\tenhanced\n\t\t\toutlined\n\t\t>\n\t\t\t{ ( properties || [] )\n\t\t\t\t.concat( {\n\t\t\t\t\t_id: PROPERTY_CREATE,\n\t\t\t\t\tdisplayName: __(\n\t\t\t\t\t\t'Set up a new property',\n\t\t\t\t\t\t'google-site-kit'\n\t\t\t\t\t),\n\t\t\t\t} )\n\t\t\t\t.map( ( { _id, displayName }, index ) => (\n\t\t\t\t\t<Option key={ index } value={ _id }>\n\t\t\t\t\t\t{ _id === PROPERTY_CREATE\n\t\t\t\t\t\t\t? displayName\n\t\t\t\t\t\t\t: sprintf(\n\t\t\t\t\t\t\t\t\t\/* translators: 1: Property name. 2: Property ID. *\/\n\t\t\t\t\t\t\t\t\t__( '%1$s (%2$s)', 'google-site-kit' ),\n\t\t\t\t\t\t\t\t\tdisplayName,\n\t\t\t\t\t\t\t\t\t_id\n\t\t\t\t\t\t\t  ) }\n\t\t\t\t\t<\/Option>\n\t\t\t\t) ) }\n\t\t<\/Select>\n\t);\n}\n\nPropertySelect.propTypes = {\n\tlabel: PropTypes.string,\n};\n","lang_cluster":"Javascript","length":128,"code_uid":"ed05b3d6411c443a8e1e22b5d9b18a68"}
{"diff_hunk":"@@ -10,6 +10,7 @@ export default {\n   ...baseConfig,\n   entry: {\n     main: [\n+      'whatwg-fetch',\n       'react-hot-loader\/patch',\n       'webpack-dev-server\/client?http:\/\/localhost:4872',\n       'webpack\/hot\/only-dev-server',","old_code":"import webpack from 'webpack';\nimport HTMLWebpackPlugin from 'html-webpack-plugin';\nimport FriendlyErrorsPlugin from 'friendly-errors-webpack-plugin';\nimport baseConfig from '.\/webpack.config';\nimport env from '..\/src\/config\/env';\nimport StyleLintPlugin from 'stylelint-webpack-plugin';\nimport getPackageVersion from '.\/getPackageVersion';\n\nexport default {\n  ...baseConfig,\n  entry: {\n    main: [\n      'react-hot-loader\/patch',\n      'webpack-dev-server\/client?http:\/\/localhost:4872',\n      'webpack\/hot\/only-dev-server',\n      `${env.SRC_ROOT}\/webui\/src\/index.js`,\n    ],\n  },\n\n  output: {\n    ...baseConfig.output,\n    publicPath: '\/',\n  },\n\n  plugins: [\n    new webpack.DefinePlugin({\n      __DEBUG__: true,\n      'process.env.NODE_ENV': '\"development\"',\n      __APP_VERSION__: `\"${getPackageVersion()}\"`,\n    }),\n    new HTMLWebpackPlugin({\n      title: 'Verdaccio',\n      filename: 'index.html',\n      verdaccioURL: '\/\/localhost:4873',\n      template: `${env.SRC_ROOT}\/webui\/template\/index.html`,\n      debug: true,\n      inject: true,\n    }),\n    new webpack.HotModuleReplacementPlugin(),\n    new webpack.NoEmitOnErrorsPlugin(),\n    new FriendlyErrorsPlugin(),\n    new StyleLintPlugin({\n      files: ['src\/**\/*.scss'],\n      failOnError: false,\n      emitErrors: false,\n      syntax: 'scss',\n    }),\n  ],\n};\n","lang_cluster":"Javascript","length":49,"code_uid":"3364e82a41ef435bbab350d5c081ded8"}
{"diff_hunk":"@@ -41,11 +41,12 @@ module.exports = class Provider {\n         'Content-Type': 'application\/json'\n       }\n     })\n+    .then(this.onReceiveResponse)\n     .then((res) => res.json())\n   }\n \n   logout (redirect = location.href) {\n-    return fetch(`${this.opts.host}\/${this.id}\/logout?redirect=${redirect}`, {\n+    return fetch(`${this.hostname}\/${this.id}\/logout?redirect=${redirect}`, {\n       method: 'get',\n       credentials: 'include',\n       headers: {","old_code":"'use strict'\n\nrequire('whatwg-fetch')\n\nconst _getName = (id) => {\n  return id.split('-').map((s) => s.charAt(0).toUpperCase() + s.slice(1)).join(' ')\n}\n\nmodule.exports = class Provider {\n  constructor (opts) {\n    this.opts = opts\n    this.provider = opts.provider\n    this.id = this.provider\n    this.authProvider = opts.authProvider || this.provider\n    this.name = this.opts.name || _getName(this.id)\n  }\n\n  auth () {\n    return fetch(`${this.opts.host}\/${this.id}\/auth`, {\n      method: 'get',\n      credentials: 'include',\n      headers: {\n        'Accept': 'application\/json',\n        'Content-Type': 'application\/json'\n      }\n    })\n    .then((res) => {\n      return res.json()\n      .then((payload) => {\n        return payload.authenticated\n      })\n    })\n  }\n\n  list (directory) {\n    return fetch(`${this.opts.host}\/${this.id}\/list\/${directory || ''}`, {\n      method: 'get',\n      credentials: 'include',\n      headers: {\n        'Accept': 'application\/json',\n        'Content-Type': 'application\/json'\n      }\n    })\n    .then((res) => res.json())\n  }\n\n  logout (redirect = location.href) {\n    return fetch(`${this.opts.host}\/${this.id}\/logout?redirect=${redirect}`, {\n      method: 'get',\n      credentials: 'include',\n      headers: {\n        'Accept': 'application\/json',\n        'Content-Type': 'application\/json'\n      }\n    })\n  }\n}\n","lang_cluster":"Javascript","length":57,"code_uid":"50b64b7455d941549605e56cc626a358"}
{"diff_hunk":"@@ -123,7 +123,6 @@ OptIn.propTypes = {\n \tid: PropTypes.string,\n \tname: PropTypes.string,\n \tclassName: PropTypes.string,\n-\toptinAction: PropTypes.string,\n };\n \n OptIn.defaultProps = {","old_code":"\/**\n * OptIn component.\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * External dependencies\n *\/\nimport PropTypes from 'prop-types';\nimport classnames from 'classnames';\n\n\/**\n * WordPress dependencies\n *\/\nimport { useCallback, createInterpolateElement } from '@wordpress\/element';\nimport { __ } from '@wordpress\/i18n';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport { CORE_USER } from '..\/googlesitekit\/datastore\/user\/constants';\nimport { toggleTracking, trackEvent } from '..\/util\/tracking';\nimport Checkbox from '.\/Checkbox';\nimport Link from '.\/Link';\nconst { useSelect, useDispatch } = Data;\n\nexport default function OptIn( { id, name, className, optinAction } ) {\n\tconst enabled = useSelect( ( select ) =>\n\t\tselect( CORE_USER ).isTrackingEnabled()\n\t);\n\tconst saving = useSelect( ( select ) =>\n\t\tselect( CORE_USER ).isSavingTrackingEnabled()\n\t);\n\tconst error = useSelect( ( select ) =>\n\t\tselect( CORE_USER ).getErrorForAction( 'setTrackingEnabled', [\n\t\t\t! enabled,\n\t\t] )\n\t);\n\n\tconst { setTrackingEnabled } = useDispatch( CORE_USER );\n\tconst handleOptIn = useCallback(\n\t\tasync ( e ) => {\n\t\t\tconst { response, error: responseError } = await setTrackingEnabled(\n\t\t\t\t!! e.target.checked\n\t\t\t);\n\n\t\t\tif ( ! responseError ) {\n\t\t\t\ttoggleTracking( response.enabled );\n\t\t\t\tif ( response.enabled ) {\n\t\t\t\t\ttrackEvent( 'tracking_plugin', optinAction );\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t[ optinAction, setTrackingEnabled ]\n\t);\n\n\tif ( enabled === undefined ) {\n\t\treturn null;\n\t}\n\n\treturn (\n\t\t<div className={ classnames( 'googlesitekit-opt-in', className ) }>\n\t\t\t<Checkbox\n\t\t\t\tid={ id }\n\t\t\t\tname={ name }\n\t\t\t\tvalue=\"1\"\n\t\t\t\tchecked={ enabled }\n\t\t\t\tdisabled={ saving }\n\t\t\t\tonChange={ handleOptIn }\n\t\t\t>\n\t\t\t\t<span>\n\t\t\t\t\t{ __(\n\t\t\t\t\t\t'Help us improve Site Kit by sharing anonymous usage data.',\n\t\t\t\t\t\t'google-site-kit'\n\t\t\t\t\t) }{ ' ' }\n\t\t\t\t<\/span>\n\t\t\t\t<span>\n\t\t\t\t\t{ createInterpolateElement(\n\t\t\t\t\t\t__(\n\t\t\t\t\t\t\t'All collected data is treated in accordance with the <a>Google Privacy Policy.<\/a>',\n\t\t\t\t\t\t\t'google-site-kit'\n\t\t\t\t\t\t),\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\ta: (\n\t\t\t\t\t\t\t\t<Link\n\t\t\t\t\t\t\t\t\tkey=\"link\"\n\t\t\t\t\t\t\t\t\thref={\n\t\t\t\t\t\t\t\t\t\t'https:\/\/policies.google.com\/privacy'\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\texternal\n\t\t\t\t\t\t\t\t\tinherit\n\t\t\t\t\t\t\t\t\/>\n\t\t\t\t\t\t\t),\n\t\t\t\t\t\t}\n\t\t\t\t\t) }\n\t\t\t\t<\/span>\n\t\t\t<\/Checkbox>\n\n\t\t\t{ error?.message && (\n\t\t\t\t<div className=\"googlesitekit-error-text\">\n\t\t\t\t\t{ error?.message }\n\t\t\t\t<\/div>\n\t\t\t) }\n\t\t<\/div>\n\t);\n}\n\nOptIn.propTypes = {\n\tid: PropTypes.string,\n\tname: PropTypes.string,\n\tclassName: PropTypes.string,\n\toptinAction: PropTypes.string,\n};\n\nOptIn.defaultProps = {\n\tid: 'googlesitekit-opt-in',\n\tname: 'optIn',\n};\n","lang_cluster":"Javascript","length":132,"code_uid":"6a6d838d37414a2997e60029f8c887f9"}
{"diff_hunk":"@@ -1,13 +1,13 @@\n const Uppy = require('..\/..\/src\/core')\n const Dashboard = require('..\/..\/src\/plugins\/Dashboard')\n-\/\/ const GoogleDrive = require('..\/..\/src\/plugins\/GoogleDrive')\n+const GoogleDrive = require('..\/..\/src\/plugins\/GoogleDrive')\n const Dropbox = require('..\/..\/src\/plugins\/Dropbox')\n const Instagram = require('..\/..\/src\/plugins\/Instagram')\n const Webcam = require('..\/..\/src\/plugins\/Webcam')\n const Tus = require('..\/..\/src\/plugins\/Tus')\n \/\/ const XHRUpload = require('..\/..\/src\/plugins\/XHRUpload')\n \/\/ const FileInput = require('..\/..\/src\/plugins\/FileInput')\n-const MetaData = require('..\/..\/src\/plugins\/MetaData')\n+\/\/ const MetaData = require('..\/..\/src\/plugins\/MetaData')\n \/\/ const Informer = require('..\/..\/src\/plugins\/Informer')\n \/\/ const StatusBar = require('..\/..\/src\/plugins\/StatusBar')\n \/\/ const DragDrop = require('..\/..\/src\/plugins\/DragDrop')","old_code":"const Uppy = require('..\/..\/src\/core')\nconst Dashboard = require('..\/..\/src\/plugins\/Dashboard')\n\/\/ const GoogleDrive = require('..\/..\/src\/plugins\/GoogleDrive')\nconst Dropbox = require('..\/..\/src\/plugins\/Dropbox')\nconst Instagram = require('..\/..\/src\/plugins\/Instagram')\nconst Webcam = require('..\/..\/src\/plugins\/Webcam')\nconst Tus = require('..\/..\/src\/plugins\/Tus')\n\/\/ const XHRUpload = require('..\/..\/src\/plugins\/XHRUpload')\n\/\/ const FileInput = require('..\/..\/src\/plugins\/FileInput')\nconst MetaData = require('..\/..\/src\/plugins\/MetaData')\n\/\/ const Informer = require('..\/..\/src\/plugins\/Informer')\n\/\/ const StatusBar = require('..\/..\/src\/plugins\/StatusBar')\n\/\/ const DragDrop = require('..\/..\/src\/plugins\/DragDrop')\n\/\/ const GoldenRetriever = require('..\/..\/src\/plugins\/GoldenRetriever')\n\nconst PROTOCOL = location.protocol === 'https:' ? 'https' : 'http'\nconst TUS_ENDPOINT = PROTOCOL + ':\/\/master.tus.io\/files\/'\n\nconst uppy = Uppy({\n  debug: true,\n  autoProceed: false,\n  meta: {\n    username: 'John'\n  }\n  \/\/ restrictions: {\n  \/\/   maxFileSize: 300000,\n  \/\/   maxNumberOfFiles: 10,\n  \/\/   minNumberOfFiles: 2,\n  \/\/   allowedFileTypes: ['image\/*', 'video\/*']\n  \/\/ }\n  \/\/ onBeforeFileAdded: (currentFile, files) => {\n  \/\/   if (currentFile.name === 'pitercss-IMG_0616.jpg') {\n  \/\/     return Promise.resolve()\n  \/\/   }\n  \/\/   return Promise.reject('this is not the file I was looking for')\n  \/\/ },\n  \/\/ onBeforeUpload: (files) => {\n  \/\/   if (Object.keys(files).length < 2) {\n  \/\/     return Promise.reject('too few files')\n  \/\/   }\n  \/\/   return Promise.resolve()\n  \/\/ }\n})\n  .use(Dashboard, {\n    trigger: '#uppyModalOpener',\n    \/\/ maxWidth: 350,\n    \/\/ maxHeight: 400,\n    inline: false,\n    \/\/ disableStatusBar: true,\n    \/\/ disableInformer: true,\n    getMetaFromForm: true,\n    \/\/ replaceTargetContent: true,\n    \/\/ target: '.MyForm',\n    hideUploadButton: false,\n    closeModalOnClickOutside: false,\n    locale: {\n      strings: {browse: 'browse'}\n    }\n    \/\/ note: 'Images and video only, 300kb or less'\n  })\n  \/\/ .use(GoogleDrive, {target: Dashboard, host: 'http:\/\/localhost:3020'})\n  .use(Dropbox, {target: Dashboard, host: 'http:\/\/localhost:3020'})\n  .use(Instagram, {target: Dashboard, host: 'http:\/\/localhost:3020'})\n  .use(Webcam, {target: Dashboard})\n  .use(Tus, {endpoint: TUS_ENDPOINT, resume: true})\n  .use(MetaData, {\n    fields: [\n      { id: 'license', name: 'License', value: 'Creative Commons', placeholder: 'specify license' },\n      { id: 'caption', name: 'Caption', value: 'none', placeholder: 'describe what the image is about' }\n    ]\n  })\n  \/\/ .use(GoldenRetriever, {serviceWorker: true})\n  .run()\n\nuppy.on('core:complete', ({ successful, failed }) => {\n  if (failed.length === 0) {\n    console.log('UPLOAD SUCCESSFUL!!!')\n  } else {\n    console.warn('UPLOAD FAILED!!!')\n  }\n  console.log('successful files:', successful)\n  console.log('failed files:', failed)\n})\n\nif ('serviceWorker' in navigator) {\n  navigator.serviceWorker\n    .register('\/sw.js')\n    .then((registration) => {\n      console.log('ServiceWorker registration successful with scope: ', registration.scope)\n    })\n    .catch((error) => {\n      console.log('Registration failed with ' + error)\n    })\n}\n\n\/\/ uppy.emit('informer', 'Smile!', 'info', 2000)\n\nvar modalTrigger = document.querySelector('#uppyModalOpener')\nif (modalTrigger) modalTrigger.click()\n","lang_cluster":"Javascript","length":99,"code_uid":"c0cc1a31f9354ecea2f4f9d9a38f9071"}
{"diff_hunk":"@@ -42,11 +42,15 @@ export default function useExistingTagEffect() {\n \t\tselect( MODULES_TAGMANAGER ).hasExistingTagPermission()\n \t);\n \t\/\/ Set the accountID and containerID if there is an existing tag.\n-\tconst { selectAccount, selectContainerByID } = useDispatch(\n+\tconst { selectAccount, selectContainerByID, setUseSnippet } = useDispatch(\n \t\tMODULES_TAGMANAGER\n \t);\n \tuseEffect( () => {\n \t\t( async () => {\n+\t\t\tif ( hasExistingTag && existingTag === containerID ) {\n+\t\t\t\t\/\/ Disable the plugin snippet as we already show the tag via other means.\n+\t\t\t\tsetUseSnippet( false );\n+\t\t\t}\n \t\t\tif ( hasExistingTag && hasExistingTagPermission ) {\n \t\t\t\tawait selectAccount( existingTagPermission.accountID );\n \t\t\t\tawait selectContainerByID( existingTag );","old_code":"\/**\n * Tag Manager useExistingTag custom hook.\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * WordPress dependencies\n *\/\nimport { useEffect } from '@wordpress\/element';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport { MODULES_TAGMANAGER } from '..\/datastore\/constants';\nconst { useSelect, useDispatch } = Data;\n\nexport default function useExistingTagEffect() {\n\tconst hasExistingTag = useSelect( ( select ) =>\n\t\tselect( MODULES_TAGMANAGER ).hasExistingTag()\n\t);\n\tconst existingTag = useSelect( ( select ) =>\n\t\tselect( MODULES_TAGMANAGER ).getExistingTag()\n\t);\n\tconst existingTagPermission = useSelect( ( select ) =>\n\t\tselect( MODULES_TAGMANAGER ).getTagPermission( existingTag )\n\t);\n\tconst hasExistingTagPermission = useSelect( ( select ) =>\n\t\tselect( MODULES_TAGMANAGER ).hasExistingTagPermission()\n\t);\n\t\/\/ Set the accountID and containerID if there is an existing tag.\n\tconst { selectAccount, selectContainerByID } = useDispatch(\n\t\tMODULES_TAGMANAGER\n\t);\n\tuseEffect( () => {\n\t\t( async () => {\n\t\t\tif ( hasExistingTag && hasExistingTagPermission ) {\n\t\t\t\tawait selectAccount( existingTagPermission.accountID );\n\t\t\t\tawait selectContainerByID( existingTag );\n\t\t\t}\n\t\t} )();\n\t}, [\n\t\thasExistingTag,\n\t\texistingTag,\n\t\thasExistingTagPermission,\n\t\texistingTagPermission,\n\t\tselectAccount,\n\t\tselectContainerByID,\n\t] );\n}\n","lang_cluster":"Javascript","length":63,"code_uid":"5522710a50f84c8fb4f8a25c1a98f553"}
{"diff_hunk":"@@ -4,6 +4,7 @@ import android.app.NotificationChannel;\n import android.app.NotificationManager;\n import android.content.Context;\n import android.support.v4.app.NotificationCompat;\n+import android.net.Uri;\n \n \n import android.util.Log;","old_code":"package com.getcapacitor.plugin;\n\nimport android.app.NotificationChannel;\nimport android.app.NotificationManager;\nimport android.content.Context;\nimport android.support.v4.app.NotificationCompat;\n\n\nimport android.util.Log;\nimport com.getcapacitor.Bridge;\nimport com.getcapacitor.JSArray;\nimport com.getcapacitor.JSObject;\nimport com.getcapacitor.NativePlugin;\nimport com.getcapacitor.Plugin;\nimport com.getcapacitor.PluginCall;\nimport com.getcapacitor.PluginHandle;\nimport com.getcapacitor.PluginMethod;\nimport com.google.android.gms.tasks.OnSuccessListener;\nimport com.google.firebase.iid.FirebaseInstanceId;\nimport com.google.firebase.iid.InstanceIdResult;\nimport com.google.firebase.messaging.FirebaseMessaging;\nimport com.google.firebase.messaging.RemoteMessage;\n\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\n@NativePlugin()\npublic class PushNotifications extends Plugin {\n\n  public static String CHANNEL_ID = \"id\";\n  public static String CHANNEL_NAME = \"name\";\n  public static String CHANNEL_DESCRIPTION = \"description\";\n  public static String CHANNEL_IMPORTANCE = \"importance\";\n  public static String CHANNEL_VISIBILITY = \"visibility\";\n\n  public static Bridge staticBridge = null;\n  public static RemoteMessage lastMessage = null;\n  public NotificationManager notificationManager;\n\n\n  private static final String EVENT_TOKEN_CHANGE = \"registration\";\n\n  public void load() {\n    notificationManager = (NotificationManager)getActivity()\n            .getSystemService(Context.NOTIFICATION_SERVICE);\n    staticBridge = this.bridge;\n    if (lastMessage != null) {\n      fireNotification(lastMessage);\n      lastMessage = null;\n    }\n  }\n\n  @PluginMethod()\n  public void register(PluginCall call) {\n    FirebaseMessaging.getInstance().setAutoInitEnabled(true);\n    FirebaseInstanceId.getInstance().getInstanceId().addOnSuccessListener(getActivity(),  new OnSuccessListener<InstanceIdResult>() {\n      @Override\n      public void onSuccess(InstanceIdResult instanceIdResult) {\n        sendToken(instanceIdResult.getToken());\n      }\n    });\n    call.success();\n  }\n\n  @PluginMethod()\n  public void getDeliveredNotifications(PluginCall call) {\n    call.unimplemented();\n  }\n\n  @PluginMethod()\n  public void removeDeliveredNotifications(PluginCall call) {\n    call.unimplemented();\n  }\n\n  @PluginMethod()\n  public void removeAllDeliveredNotifications(PluginCall call) {\n    call.unimplemented();\n  }\n\n  @PluginMethod()\n  public void createChannel(PluginCall call) {\n    if (android.os.Build.VERSION.SDK_INT  >= android.os.Build.VERSION_CODES.O) {\n      JSObject channel = new JSObject();\n      channel.put(CHANNEL_ID, call.getString(CHANNEL_ID));\n      channel.put(CHANNEL_NAME, call.getString(CHANNEL_NAME));\n      channel.put(CHANNEL_DESCRIPTION, call.getString(CHANNEL_DESCRIPTION, \"\"));\n      channel.put(CHANNEL_VISIBILITY,  call.getInt(CHANNEL_VISIBILITY, NotificationCompat.VISIBILITY_PUBLIC));\n      channel.put(CHANNEL_IMPORTANCE, call.getInt(CHANNEL_IMPORTANCE));\n      createChannel(channel);\n      call.success();\n    } else {\n      call.unavailable();\n    }\n  }\n\n  @PluginMethod()\n  public void deleteChannel(PluginCall call) {\n    if (android.os.Build.VERSION.SDK_INT  >= android.os.Build.VERSION_CODES.O) {\n      String channelId = call.getString(\"id\");\n      notificationManager.deleteNotificationChannel(channelId);\n      call.success();\n    } else {\n      call.unavailable();\n    }\n  }\n\n  @PluginMethod()\n  public void listChannels(PluginCall call) {\n    if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.O) {\n      List<NotificationChannel> notificationChannels = notificationManager.getNotificationChannels();\n      JSArray channels = new JSArray();\n      for (NotificationChannel notificationChannel : notificationChannels) {\n        JSObject channel = new JSObject();\n        channel.put(CHANNEL_ID, notificationChannel.getId());\n        channel.put(CHANNEL_NAME, notificationChannel.getName());\n        channel.put(CHANNEL_DESCRIPTION, notificationChannel.getDescription());\n        channel.put(CHANNEL_IMPORTANCE, notificationChannel.getImportance());\n        channel.put(CHANNEL_VISIBILITY, notificationChannel.getLockscreenVisibility());\n        Log.d(getLogTag(), \"visibility \" + notificationChannel.getLockscreenVisibility());\n        Log.d(getLogTag(), \"importance \" + notificationChannel.getImportance());\n        channels.put(channel);\n      }\n      JSObject result = new JSObject();\n      result.put(\"channels\", channels);\n      call.success(result);\n    } else {\n      call.unavailable();\n    }\n  }\n\n  private void createChannel(JSObject channel) {\n    if (android.os.Build.VERSION.SDK_INT >= android.os.Build.VERSION_CODES.O) {\n      NotificationChannel notificationChannelChannel = new NotificationChannel(channel.getString(CHANNEL_ID), channel.getString(CHANNEL_NAME), channel.getInteger(CHANNEL_IMPORTANCE));\n      notificationChannelChannel.setDescription(channel.getString(CHANNEL_DESCRIPTION, \"\"));\n      notificationChannelChannel.setLockscreenVisibility(0);\n      notificationManager.createNotificationChannel(notificationChannelChannel);\n    }\n  }\n\n  public void sendToken(String token) {\n    JSObject data = new JSObject();\n    data.put(\"value\", token);\n    notifyListeners(EVENT_TOKEN_CHANGE, data, true);\n  }\n\n  public static void onNewToken(String newToken) {\n    PushNotifications pushPlugin = PushNotifications.getPushNotificationsInstance();\n    if (pushPlugin != null) {\n      pushPlugin.sendToken(newToken);\n    }\n  }\n\n  public static void sendRemoteMessage(RemoteMessage remoteMessage) {\n    PushNotifications pushPlugin = PushNotifications.getPushNotificationsInstance();\n    if (pushPlugin != null) {\n      pushPlugin.fireNotification(remoteMessage);\n    } else {\n      lastMessage = remoteMessage;\n    }\n  }\n\n  public void fireNotification(RemoteMessage remoteMessage) {\n    Map<String, Object> data = new HashMap<String, Object>();\n    for (String key : remoteMessage.getData().keySet()) {\n      Object value = remoteMessage.getData().get(key);\n      data.put(key, value);\n    }\n    JSObject remoteMessageData = new JSObject();\n    remoteMessageData.put(\"data\", data);\n    notifyListeners(\"pushNotificationReceived\", remoteMessageData, true);\n  }\n\n  public static PushNotifications getPushNotificationsInstance() {\n    if (staticBridge != null && staticBridge.getWebView() != null) {\n      PluginHandle handle = staticBridge.getPlugin(\"PushNotifications\");\n      if (handle == null) {\n        return null;\n      }\n      return (PushNotifications) handle.getInstance();\n    }\n    return null;\n  }\n\n}\n","lang_cluster":"Javascript","length":185,"code_uid":"494c1395f40a4fc4a5a2927358498576"}
{"diff_hunk":"@@ -1,5 +1,4 @@\n import options from '.\/options';\n-import { assign } from '.\/util';\n \n \/**\n  * Create an virtual node (used for JSX)","old_code":"import options from '.\/options';\nimport { assign } from '.\/util';\n\n\/**\n * Create an virtual node (used for JSX)\n * @param {import('.\/internal').VNode[\"type\"]} type The node name or Component\n * constructor for this virtual node\n * @param {object | null | undefined} [props] The properties of the virtual node\n * @param {Array<import('.').ComponentChildren>} [children] The children of the virtual node\n * @returns {import('.\/internal').VNode}\n *\/\nexport function createElement(type, props, children) {\n\tprops = assign({}, props);\n\n\tif (arguments.length > 3) {\n\t\tchildren = [children];\n\t\t\/\/ https:\/\/github.com\/preactjs\/preact\/issues\/1916\n\t\tfor (let i = 3; i < arguments.length; i++) {\n\t\t\tchildren.push(arguments[i]);\n\t\t}\n\t}\n\tif (children != null) {\n\t\tprops.children = children;\n\t}\n\n\t\/\/ \"type\" may be undefined during development. The check is needed so that\n\t\/\/ we can display a nice error message with our debug helpers\n\tif (type != null && type.defaultProps != null) {\n\t\tfor (let i in type.defaultProps) {\n\t\t\tif (props[i] === undefined) props[i] = type.defaultProps[i];\n\t\t}\n\t}\n\tlet ref = props.ref;\n\tlet key = props.key;\n\tif (ref != null) delete props.ref;\n\tif (key != null) delete props.key;\n\n\treturn createVNode(type, props, key, ref);\n}\n\n\/**\n * Create a VNode (used internally by Preact)\n * @param {import('.\/internal').VNode[\"type\"]} type The node name or Component\n * Constructor for this virtual node\n * @param {object | string | number | null} props The properties of this virtual node.\n * If this virtual node represents a text node, this is the text of the node (string or number).\n * @param {string | number | null} key The key for this virtual node, used when\n * diffing it against its children\n * @param {import('.\/internal').VNode[\"ref\"]} ref The ref property that will\n * receive a reference to its created child\n * @returns {import('.\/internal').VNode}\n *\/\nexport function createVNode(type, props, key, ref) {\n\t\/\/ V8 seems to be better at detecting type shapes if the object is allocated from the same call site\n\t\/\/ Do not inline into createElement and coerceToVNode!\n\tconst vnode = {\n\t\ttype,\n\t\tprops,\n\t\tkey,\n\t\tref,\n\t\t_children: null,\n\t\t_parent: null,\n\t\t_depth: 0,\n\t\t_dom: null,\n\t\t_lastDomChild: null,\n\t\t_component: null,\n\t\tconstructor: undefined\n\t};\n\n\tif (options.vnode) options.vnode(vnode);\n\n\treturn vnode;\n}\n\nexport function createRef() {\n\treturn {};\n}\n\nexport function Fragment(props) {\n\treturn props.children;\n}\n\n\/**\n * Check if a the argument is a valid Preact VNode.\n * @param {*} vnode\n * @returns {vnode is import('.\/internal').VNode}\n *\/\nexport const isValidElement = vnode =>\n\tvnode != null && vnode.constructor === undefined;\n","lang_cluster":"Javascript","length":89,"code_uid":"0184ee3f43cc469e8401ce4f248ea8aa"}
{"diff_hunk":"@@ -23,17 +23,17 @@ define([\"jQuery\", \"loading\", \"libraryMenu\"], function($, loading, libraryMenu) {\n         }, {\n             href: \"streamingsettings.html\",\n             name: Globalize.translate(\"TabStreaming\")\n-        }]\n+        }];\n     }\n \n-    $(document).on(\"pageinit\", \"#playbackConfigurationPage\", function() {\n+    $(document).on(\"pageinit\", \"#playbackConfigurationPage\", function () {\n         $(\".playbackConfigurationForm\").off(\"submit\", onSubmit).on(\"submit\", onSubmit)\n-    }).on(\"pageshow\", \"#playbackConfigurationPage\", function() {\n+    }).on(\"pageshow\", \"#playbackConfigurationPage\", function () {\n         loading.show();\n         libraryMenu.setTabs(\"playback\", 1, getTabs);\n         var page = this;\n-        ApiClient.getServerConfiguration().then(function(config) {\n-            loadPage(page, config)\n-        })\n-    })\n-});\n+        ApiClient.getServerConfiguration().then(function (config) {\n+            loadPage(page, config);\n+        });\n+    });\n+});","old_code":"define([\"jQuery\", \"loading\", \"libraryMenu\"], function($, loading, libraryMenu) {\n    \"use strict\";\n\n    function loadPage(page, config) {\n        $(\"#txtMinResumePct\", page).val(config.MinResumePct), $(\"#txtMaxResumePct\", page).val(config.MaxResumePct), $(\"#txtMinResumeDuration\", page).val(config.MinResumeDurationSeconds), loading.hide()\n    }\n\n    function onSubmit() {\n        loading.show();\n        var form = this;\n        return ApiClient.getServerConfiguration().then(function(config) {\n            config.MinResumePct = $(\"#txtMinResumePct\", form).val(), config.MaxResumePct = $(\"#txtMaxResumePct\", form).val(), config.MinResumeDurationSeconds = $(\"#txtMinResumeDuration\", form).val(), ApiClient.updateServerConfiguration(config).then(Dashboard.processServerConfigurationUpdateResult)\n        }), !1\n    }\n\n    function getTabs() {\n        return [{\n            href: \"encodingsettings.html\",\n            name: Globalize.translate(\"Transcoding\")\n        }, {\n            href: \"playbackconfiguration.html\",\n            name: Globalize.translate(\"TabResumeSettings\")\n        }, {\n            href: \"streamingsettings.html\",\n            name: Globalize.translate(\"TabStreaming\")\n        }]\n    }\n\n    $(document).on(\"pageinit\", \"#playbackConfigurationPage\", function() {\n        $(\".playbackConfigurationForm\").off(\"submit\", onSubmit).on(\"submit\", onSubmit)\n    }).on(\"pageshow\", \"#playbackConfigurationPage\", function() {\n        loading.show();\n        libraryMenu.setTabs(\"playback\", 1, getTabs);\n        var page = this;\n        ApiClient.getServerConfiguration().then(function(config) {\n            loadPage(page, config)\n        })\n    })\n});","lang_cluster":"Javascript","length":39,"code_uid":"aee1ca7a3aa341a285690a799e1abffa"}
{"diff_hunk":"@@ -58,6 +58,16 @@ class ReplSetFixture {\n       })\n     ];\n \n+    this.secondSecondaryStates = [\n+      Object.assign({}, this.defaultFields, {\n+        ismaster: false,\n+        secondary: true,\n+        me: this.secondSecondaryServer.uri(),\n+        primary: this.primaryServer.uri(),\n+        tags: { loc: 'la' }\n+      })\n+    ];\n+\n     this.arbiterStates = [\n       Object.assign({}, this.defaultFields, {\n         ismaster: false,","old_code":"'use strict';\n\nconst mock = require('mongodb-mock-server');\nconst ObjectId = require('bson').ObjectId;\nconst Timestamp = require('bson').Timestamp;\nconst Binary = require('bson').Binary;\nconst Buffer = require('safe-buffer').Buffer;\n\nclass ReplSetFixture {\n  constructor() {\n    this.electionIds = [new ObjectId(), new ObjectId()];\n  }\n\n  setup(options) {\n    options = options || {};\n    const ismaster = options.ismaster ? options.ismaster : mock.DEFAULT_ISMASTER_36;\n\n    return Promise.all([mock.createServer(), mock.createServer(), mock.createServer()]).then(\n      servers => {\n        this.servers = servers;\n        this.primaryServer = servers[0];\n        this.firstSecondaryServer = servers[1];\n        this.arbiterServer = servers[2];\n\n        this.defaultFields = Object.assign({}, ismaster, {\n          __nodejs_mock_server__: true,\n          setName: 'rs',\n          setVersion: 1,\n          electionId: this.electionIds[0],\n          hosts: this.servers.map(server => server.uri()),\n          arbiters: [this.arbiterServer.uri()]\n        });\n\n        this.defineReplSetStates();\n        this.configureMessageHandlers();\n      }\n    );\n  }\n\n  defineReplSetStates() {\n    this.primaryStates = [\n      Object.assign({}, this.defaultFields, {\n        ismaster: true,\n        secondary: false,\n        me: this.primaryServer.uri(),\n        primary: this.primaryServer.uri(),\n        tags: { loc: 'ny' }\n      })\n    ];\n\n    this.firstSecondaryStates = [\n      Object.assign({}, this.defaultFields, {\n        ismaster: false,\n        secondary: true,\n        me: this.firstSecondaryServer.uri(),\n        primary: this.primaryServer.uri(),\n        tags: { loc: 'sf' }\n      })\n    ];\n\n    this.arbiterStates = [\n      Object.assign({}, this.defaultFields, {\n        ismaster: false,\n        secondary: false,\n        arbiterOnly: true,\n        me: this.arbiterServer.uri(),\n        primary: this.primaryServer.uri()\n      })\n    ];\n  }\n\n  configureMessageHandlers() {\n    this.primaryServer.setMessageHandler(request => {\n      var doc = request.document;\n      if (doc.ismaster) {\n        request.reply(this.primaryStates[0]);\n      }\n    });\n\n    this.firstSecondaryServer.setMessageHandler(request => {\n      var doc = request.document;\n      if (doc.ismaster) {\n        request.reply(this.firstSecondaryStates[0]);\n      }\n    });\n\n    this.arbiterServer.setMessageHandler(request => {\n      var doc = request.document;\n      if (doc.ismaster) {\n        request.reply(this.arbiterStates[0]);\n      }\n    });\n  }\n}\n\nclass MongosFixture {\n  setup(options) {\n    options = options || {};\n    const ismaster = options.ismaster ? options.ismaster : mock.DEFAULT_ISMASTER;\n    return Promise.all([mock.createServer(), mock.createServer()]).then(servers => {\n      this.servers = servers;\n      this.defaultFields = Object.assign({}, ismaster, {\n        msg: 'isdbgrid'\n      });\n    });\n  }\n}\n\n\/**\n * Creates a cluster time for use in unit testing cluster time gossiping and\n * causal consistency.\n *\n * @param {Number} time the logical time\n * @returns a cluster time according to the driver sessions specification\n *\/\nfunction genClusterTime(time) {\n  return {\n    clusterTime: new Timestamp(time),\n    signature: {\n      hash: new Binary(Buffer.from('testing')),\n      keyId: 42\n    }\n  };\n}\n\nfunction sessionCleanupHandler(session, sessionPool, done) {\n  return err => {\n    if (session == null) {\n      sessionPool.endAllPooledSessions();\n      done();\n      return;\n    }\n\n    if (session.hasEnded) {\n      sessionPool.endAllPooledSessions();\n      done(err);\n      return;\n    }\n\n    session.endSession(() => {\n      sessionPool.endAllPooledSessions();\n      done(err);\n    });\n  };\n}\n\nmodule.exports = {\n  ReplSetFixture: ReplSetFixture,\n  MongosFixture: MongosFixture,\n  genClusterTime: genClusterTime,\n  sessionCleanupHandler\n};\n","lang_cluster":"Javascript","length":152,"code_uid":"264b33f5b1d942d78136f54b2b387538"}
{"diff_hunk":"@@ -30,7 +30,6 @@ import { STORE_NAME } from '..\/..\/datastore\/constants';\n import { STORE_NAME as CORE_SITE } from '..\/..\/..\/..\/googlesitekit\/datastore\/site\/constants';\n import Switch from '..\/..\/..\/..\/components\/switch';\n import Link from '..\/..\/..\/..\/components\/link';\n-import { sanitizeHTML } from '..\/..\/..\/..\/util';\n \n const { useSelect, useDispatch } = Data;\n ","old_code":"\/**\n * Analytics Anonymize IP Switch component.\n *\n * Site Kit by Google, Copyright 2020 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * WordPress dependencies\n *\/\nimport { useCallback } from '@wordpress\/element';\nimport { __ } from '@wordpress\/i18n';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport { STORE_NAME } from '..\/..\/datastore\/constants';\nimport { STORE_NAME as CORE_SITE } from '..\/..\/..\/..\/googlesitekit\/datastore\/site\/constants';\nimport Switch from '..\/..\/..\/..\/components\/switch';\nimport Link from '..\/..\/..\/..\/components\/link';\nimport { sanitizeHTML } from '..\/..\/..\/..\/util';\n\nconst { useSelect, useDispatch } = Data;\n\nexport default function AnonymizeIPSwitch() {\n\tconst anonymizeIP = useSelect( ( select ) => select( STORE_NAME ).getAnonymizeIP() );\n\tconst useSnippet = useSelect( ( select ) => select( STORE_NAME ).getUseSnippet() );\n\tconst ampMode = useSelect( ( select ) => select( CORE_SITE ).getAMPMode() );\n\n\tconst { setAnonymizeIP } = useDispatch( STORE_NAME );\n\tconst onChange = useCallback( () => {\n\t\tsetAnonymizeIP( ! anonymizeIP );\n\t}, [ anonymizeIP ] );\n\n\tif ( ! useSnippet || ampMode === 'primary' || anonymizeIP === undefined ) {\n\t\treturn null;\n\t}\n\n\treturn (\n\t\t<div className=\"googlesitekit-analytics-anonymizeip\">\n\t\t\t<Switch\n\t\t\t\tlabel={ __( 'Anonymize IP addresses', 'google-site-kit' ) }\n\t\t\t\tonClick={ onChange }\n\t\t\t\tchecked={ anonymizeIP }\n\t\t\t\thideLabel={ false }\n\t\t\t\/>\n\t\t\t<p>\n\t\t\t\t{ anonymizeIP\n\t\t\t\t\t? __( 'IP addresses will be anonymized.', 'google-site-kit' )\n\t\t\t\t\t: __( 'IP addresses will not be anonymized.', 'google-site-kit' )\n\t\t\t\t}\n\t\t\t\t{ ' ' }\n\t\t\t\t<Link\n\t\t\t\t\thref=\"https:\/\/support.google.com\/analytics\/answer\/2763052\"\n\t\t\t\t\texternal\n\t\t\t\t\tinherit\n\t\t\t\t\tdangerouslySetInnerHTML={ sanitizeHTML(\n\t\t\t\t\t\t__( 'Learn more<span class=\"screen-reader-text\"> about IP anonymization.<\/span>', 'google-site-kit' ),\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tALLOWED_TAGS: [ 'span' ],\n\t\t\t\t\t\t\tALLOWED_ATTR: [ 'class' ],\n\t\t\t\t\t\t}\n\t\t\t\t\t) }\n\t\t\t\t\/>\n\t\t\t<\/p>\n\t\t<\/div>\n\t);\n}\n","lang_cluster":"Javascript","length":80,"code_uid":"81b55376b1b74ee5aaf8511e0e28c75b"}
{"diff_hunk":"@@ -24,6 +24,7 @@ import PropTypes from 'prop-types';\n \/**\n  * WordPress dependencies\n  *\/\n+import { __ } from '@wordpress\/i18n';\n import { Icon, closeSmall } from '@wordpress\/icons';\n \n \/**","old_code":"\/**\n * SurveyHeader component.\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * External dependencies\n *\/\nimport PropTypes from 'prop-types';\n\n\/**\n * WordPress dependencies\n *\/\nimport { Icon, closeSmall } from '@wordpress\/icons';\n\n\/**\n * Internal dependencies\n *\/\nimport Button from '..\/..\/components\/Button';\nimport Logo from '..\/..\/..\/svg\/logo-g.svg';\n\nconst SurveyHeader = ( { title, dismissSurvey } ) => (\n\t<div className=\"googlesitekit-survey__header\">\n\t\t<div className=\"googlesitekit-survey__header-logo\">\n\t\t\t<Logo width={ 24 } height={ 24 } \/>\n\t\t<\/div>\n\n\t\t<div className=\"googlesitekit-survey__header-details\">\n\t\t\t<h3>{ title }<\/h3>\n\n\t\t\t<Button\n\t\t\t\ticon={ <Icon icon={ closeSmall } size={ 40 } \/> }\n\t\t\t\tonClick={ dismissSurvey }\n\t\t\t\tclassName=\"googlesitekit-survey__header-close\"\n\t\t\t\/>\n\t\t<\/div>\n\t<\/div>\n);\n\nSurveyHeader.propTypes = {\n\ttitle: PropTypes.string.isRequired,\n\tdismissSurvey: PropTypes.func.isRequired,\n};\n\nexport default SurveyHeader;\n","lang_cluster":"Javascript","length":58,"code_uid":"50f927516d4544c1a86f940ec94a8977"}
{"diff_hunk":"@@ -34,6 +34,12 @@ const test = (suite, buildConfig = config.defaultBuildConfig, options) => {\n \n   \/\/ Build the tests\n   util.run('ninja', ['-C', config.outputDir, suite], config.defaultOptions)\n+\n+  const run_brave_installer_unitests = suite === 'brave_unit_tests'\n+  if (run_brave_installer_unitests) {\n+    util.run('ninja', ['-C', config.outputDir, 'brave_installer_unittests'], config.defaultOptions)\n+  }\n+\n   if (config.targetOS === 'ios') {\n     util.run(path.join(config.outputDir, \"iossim\"), [\n       path.join(config.outputDir, `${suite}.app`),","old_code":"const path = require('path')\n\nconst config = require('..\/lib\/config')\nconst util = require('..\/lib\/util')\n\nconst test = (suite, buildConfig = config.defaultBuildConfig, options) => {\n  config.buildConfig = buildConfig\n  config.update(options)\n\n  const braveArgs = [\n    '--enable-logging',\n    '--v=' + options.v,\n  ]\n\n  if (options.filter) {\n    braveArgs.push('--gtest_filter=' + options.filter)\n  }\n\n  if (options.output) {\n    braveArgs.push('--gtest_output=xml:' + options.output)\n  }\n\n  if (options.disable_brave_extension) {\n    braveArgs.push('--disable-brave-extension')\n  }\n\n  if (options.single_process) {\n    braveArgs.push('--single_process')\n  }\n\n  if (options.test_launcher_jobs) {\n    braveArgs.push('--test-launcher-jobs=' + options.test_launcher_jobs)\n  }\n\n  \/\/ Build the tests\n  util.run('ninja', ['-C', config.outputDir, suite], config.defaultOptions)\n  if (config.targetOS === 'ios') {\n    util.run(path.join(config.outputDir, \"iossim\"), [\n      path.join(config.outputDir, `${suite}.app`),\n      path.join(config.outputDir, `${suite}.app\/PlugIns\/${suite}_module.xctest`)\n    ], config.defaultOptions)\n  } else {\n    util.run('ninja', ['-C', config.outputDir, \"fix_brave_test_install_name\"], config.defaultOptions)\n    util.run('ninja', ['-C', config.outputDir, \"fix_brave_test_install_name_adblock\"], config.defaultOptions)\n\n    let testBinary;\n    if (process.platform === 'win32') {\n      testBinary = `${suite}.exe`\n    } else {\n      testBinary = suite\n    }\n\n    \/\/ Run the tests\n    util.run(path.join(config.outputDir, testBinary), braveArgs, config.defaultOptions)\n  }\n}\n\nmodule.exports = test\n","lang_cluster":"Javascript","length":58,"code_uid":"d1917c7e7b524b6e9f16a557718f3c73"}
{"diff_hunk":"@@ -93,7 +93,7 @@ export default function SetupAccountCreate() {\n \t\t\t\t) }\n \t\t\t\t{ ' ' }\n \t\t\t\t<Link\n-\t\t\t\t\thref=\"https:\/\/support.google.com\/adsense\/answer\/2659101\"\n+\t\t\t\t\thref={ supportURL }\n \t\t\t\t\tinherit\n \t\t\t\t\texternal\n \t\t\t\t\taria-label={ __( 'Learn more about adding a user to an existing AdSense account', 'google-site-kit' ) }","old_code":"\/**\n * AdSense Setup Account Create component.\n *\n * Site Kit by Google, Copyright 2020 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * WordPress dependencies\n *\/\nimport { Fragment, useCallback } from '@wordpress\/element';\nimport { __, sprintf } from '@wordpress\/i18n';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport Button from '..\/..\/..\/..\/components\/Button';\nimport Link from '..\/..\/..\/..\/components\/Link';\nimport { trackEvent } from '..\/..\/..\/..\/util';\nimport { parseAccountID } from '..\/..\/util\/parsing';\nimport { STORE_NAME } from '..\/..\/datastore\/constants';\nimport { STORE_NAME as siteStoreName } from '..\/..\/..\/..\/googlesitekit\/datastore\/site\/constants';\nimport { STORE_NAME as userStoreName } from '..\/..\/..\/..\/googlesitekit\/datastore\/user\/constants';\nimport {\n\tErrorNotices,\n\tUserProfile,\n} from '..\/common';\nconst { useSelect } = Data;\n\nexport default function SetupAccountCreate() {\n\tconst siteURL = useSelect( ( select ) => select( siteStoreName ).getReferenceSiteURL() );\n\tconst userEmail = useSelect( ( select ) => select( userStoreName ).getEmail() );\n\tconst existingTag = useSelect( ( select ) => select( STORE_NAME ).getExistingTag() );\n\tconst signUpURL = useSelect( ( select ) => select( STORE_NAME ).getServiceCreateAccountURL() );\n\n\tconst createAccountHandler = useCallback( async ( event ) => {\n\t\tevent.preventDefault();\n\t\tawait trackEvent( 'adsense_setup', 'create_adsense_account' );\n\t\tglobal.open( signUpURL, '_blank' );\n\t}, [ signUpURL ] );\n\n\tif ( ! siteURL || ! userEmail || undefined === existingTag ) {\n\t\treturn null;\n\t}\n\n\treturn (\n\t\t<Fragment>\n\t\t\t<h3 className=\"googlesitekit-heading-4 googlesitekit-setup-module__title\">\n\t\t\t\t{ __( 'Create your AdSense account', 'google-site-kit' ) }\n\t\t\t<\/h3>\n\n\t\t\t<ErrorNotices \/>\n\n\t\t\t<p>\n\t\t\t\t{ __( 'Site Kit will place AdSense code on every page across your site. This means Google will automatically place ads for you in all the best places.', 'google-site-kit' ) }\n\t\t\t<\/p>\n\n\t\t\t<UserProfile \/>\n\n\t\t\t<div className=\"googlesitekit-setup-module__action\">\n\t\t\t\t<Button\n\t\t\t\t\tonClick={ createAccountHandler }\n\t\t\t\t\thref={ signUpURL }\n\t\t\t\t>\n\t\t\t\t\t{ __( 'Create AdSense Account', 'google-site-kit' ) }\n\t\t\t\t<\/Button>\n\t\t\t<\/div>\n\n\t\t\t<p className=\"googlesitekit-setup-module__footer-text\">\n\t\t\t\t{ existingTag && sprintf(\n\t\t\t\t\t\/* translators: 1: client ID, 2: user email address, 3: account ID *\/\n\t\t\t\t\t__( 'Site Kit detected AdSense code %1$s on your page. We recommend you remove that code or add %2$s as a user to the AdSense account %3$s.', 'google-site-kit' ),\n\t\t\t\t\texistingTag,\n\t\t\t\t\tuserEmail,\n\t\t\t\t\tparseAccountID( existingTag )\n\t\t\t\t) }\n\t\t\t\t{ ! existingTag && sprintf(\n\t\t\t\t\t\/* translators: %s: user email address *\/\n\t\t\t\t\t__( 'Already use AdSense? Add %s as a user to an existing AdSense account.', 'google-site-kit' ),\n\t\t\t\t\tuserEmail\n\t\t\t\t) }\n\t\t\t\t{ ' ' }\n\t\t\t\t<Link\n\t\t\t\t\thref=\"https:\/\/support.google.com\/adsense\/answer\/2659101\"\n\t\t\t\t\tinherit\n\t\t\t\t\texternal\n\t\t\t\t\taria-label={ __( 'Learn more about adding a user to an existing AdSense account', 'google-site-kit' ) }\n\t\t\t\t>\n\t\t\t\t\t{ __( 'Learn more', 'google-site-kit' ) }\n\t\t\t\t<\/Link>\n\t\t\t<\/p>\n\t\t<\/Fragment>\n\t);\n}\n","lang_cluster":"Javascript","length":106,"code_uid":"89af216bbe9b4bd5b4b2c2094843abac"}
{"diff_hunk":"@@ -9,7 +9,7 @@ var plugin = {},\n     async = require('async'),\n     log = common.log('compare:api');\n \n-(function(plugin) {\n+(function() {\n \n     plugins.register('\/o\/compare\/events', function(ob) {\n         var params = ob.params;","old_code":"var plugin = {},\n    plugins = require('..\/..\/pluginManager.js'),\n    countlyModel = require('..\/..\/..\/api\/lib\/countly.model.js'),\n    countlySession = countlyModel.load(\"users\"),\n    countlyCommon = require('..\/..\/..\/api\/lib\/countly.common.js'),\n    common = require('..\/..\/..\/api\/utils\/common.js'),\n    fetch = require('..\/..\/..\/api\/parts\/data\/fetch.js'),\n    crypto = require('crypto'),\n    async = require('async'),\n    log = common.log('compare:api');\n\n(function(plugin) {\n\n    plugins.register('\/o\/compare\/events', function(ob) {\n        var params = ob.params;\n\n        if (params.qstring.events) {\n            try {\n                params.qstring.events = JSON.parse(params.qstring.events);\n            }\n            catch (SyntaxError) {\n                log.w('Parse \/o\/compare\/events JSON failed');\n            }\n        }\n\n        if (!params.qstring.events || params.qstring.events.length == 0) {\n            return common.returnMessage(params, 400, 'Missing parameter: events');\n        }\n\n        if (params.qstring.events.length > 10) {\n            return common.returnMessage(params, 400, 'Maximum length for parameter events is 10');\n        }\n\n        ob.validateUserForDataReadAPI(params, function() {\n            var eventKeysArr = params.qstring.events;\n            var collectionNames = [];\n\n            for (var i = 0; i < eventKeysArr.length; i++) {\n                collectionNames.push(\n                    \"events\" + crypto.createHash('sha1').update(eventKeysArr[i] + params.app_id).digest('hex')\n                );\n            }\n\n            async.map(collectionNames, getEventData, function(err, allEventData) {\n                var outputObj = {};\n\n                for (var i = 0; i < allEventData.length; i++) {\n                    outputObj[eventKeysArr[i]] = allEventData[i];\n                }\n\n                common.returnOutput(params, outputObj);\n            });\n        });\n\n        function getEventData(collectionName, callback) {\n            fetch.getTimeObjForEvents(collectionName, params, function(output) {\n                callback(null, output || {});\n            });\n        }\n\n        return true;\n    });\n\n    plugins.register('\/o\/compare\/apps', function(ob) {\n        var params = ob.params;\n\n        if (params.qstring.apps) {\n            try {\n                params.qstring.apps = JSON.parse(params.qstring.apps);\n            }\n            catch (SyntaxError) {\n                log.w('Parse \/o\/compare\/apps JSON failed');\n            }\n        }\n\n        if (!params.qstring.apps || params.qstring.apps.length == 0) {\n            return common.returnMessage(params, 400, 'Missing parameter: apps');\n        }\n\n        if (params.qstring.apps.length > 10) {\n            return common.returnMessage(params, 400, 'Maximum length for parameter apps is 10');\n        }\n\n        var appsToFetch = params.qstring.apps;\n\n        for (var i = 0; i < appsToFetch.length; i++) {\n            if (appsToFetch[0].length != 24) {\n                return common.returnMessage(params, 400, 'Invalid app id length in apps parameter, each app id should be 24 characters long');\n            }\n        }\n\n        params.qstring.app_id = appsToFetch[0];\n\n        ob.validateUserForDataReadAPI(params, function() {\n            if (!params.member.global_admin) {\n                for (var i = 0; i < appsToFetch.length; i++) {\n                    if (params.member && params.member.user_of) {\n                        if (params.member.user_of.indexOf(appsToFetch[i]) == -1) {\n                            return common.returnMessage(params, 401, 'User does not have view rights for one or more apps provided in apps parameter');\n                        }\n                    }\n                    else {\n                        return common.returnMessage(params, 401, 'User does not have view rights for one or more apps provided in apps parameter');\n                    }\n                }\n            }\n\n            for (var i = 0; i < appsToFetch.length; i++) {\n                appsToFetch[i] = common.db.ObjectID(appsToFetch[i]);\n            }\n\n            common.db.collection(\"apps\").find({_id: {$in: appsToFetch}}, {_id: 1, name: 1}).toArray(function(err, apps) {\n\n                function extractData(db, props) {\n                    var chartData = [\n                            { data: [], label: \"\", color: '#333933' }\n                        ],\n                        dataProps = [];\n                    dataProps.push(props);\n                    return countlyCommon.extractChartData(db, countlySession.clearObject, chartData, dataProps).chartDP[0].data;\n                }\n\n                function setAppId(inAppId) {\n                    params.app_id = inAppId + \"\";\n                }\n\n                countlyCommon.setTimezone(params.appTimezone);\n\n                async.map(apps, function(app, callback) {\n                    console.log(JSON.stringify(app));\n                    setAppId(app._id);\n\n                    fetch.getTimeObj('users', params, function(usersDoc) {\n\n                        \/\/ We need to set app_id once again here because after the callback\n                        \/\/ it is reset to it's original value\n                        setAppId(app._id);\n\n                        fetch.getTotalUsersObj(\"users\", params, function(dbTotalUsersObj) {\n                            countlySession.setDb(usersDoc || {});\n                            countlySession.setTotalUsersObj(fetch.formatTotalUsersObj(dbTotalUsersObj));\n\n                            var sessionData = countlySession.getSessionData();\n                            var charts = {\n                                \"total-users\": extractData(usersDoc || {}, {\n                                    name: \"t\",\n                                    func: function(dataObj) {\n                                        return dataObj.u;\n                                    }\n                                }),\n                                \"new-users\": extractData(usersDoc || {}, { name: \"n\" }),\n                                \"total-sessions\": extractData(usersDoc || {}, { name: \"t\" }),\n                                \"time-spent\": extractData(usersDoc || {}, {\n                                    name: \"average\",\n                                    func: function(dataObj) {\n                                        return ((dataObj.t == 0) ? 0 : ((dataObj.d \/ dataObj.t) \/ 60).toFixed(1));\n                                    }\n                                }),\n                                \"total-time-spent\": extractData(usersDoc || {}, {\n                                    name: \"t\",\n                                    func: function(dataObj) {\n                                        return ((dataObj.d \/ 60).toFixed(1));\n                                    }\n                                }),\n                                \"avg-events-served\": extractData(usersDoc || {}, {\n                                    name: \"average\",\n                                    func: function(dataObj) {\n                                        return ((dataObj.u == 0) ? 0 : ((dataObj.e \/ dataObj.u).toFixed(1)));\n                                    }\n                                })\n                            };\n\n                            var data = {id: app._id, name: app.name, sessions: sessionData.total_sessions, users: sessionData.total_users, newusers: sessionData.new_users, duration: sessionData.total_time, avgduration: sessionData.avg_time, charts: charts};\n\n                            callback(null, data);\n                        });\n                    });\n                },\n                function(err, res) {\n                    common.returnOutput(params, res);\n                });\n            });\n        });\n\n        return true;\n    });\n\n}(plugin));\n\nmodule.exports = plugin;","lang_cluster":"Javascript","length":190,"code_uid":"c2f046959ec34221a38699c954f00c9d"}
{"diff_hunk":"@@ -55,7 +55,7 @@ var LinkTypesObjectSchema = {\n     properties: [\n         {name: 'objectCol',  type: 'TestObject'},\n         {name: 'objectCol1', type: Realm.Types.OBJECT, objectType: 'TestObject'},\n-        {name: 'arrayCol',   type: Realm.Types.ARRAY, objectType: 'TestObject'},\n+        {name: 'arrayCol',   type: Realm.Types.LIST, objectType: 'TestObject'},\n     ]\n };\n ","old_code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/\n\/\/ Copyright 2015 Realm Inc.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n'use strict';\n\nvar TestObjectSchema = {\n  name: 'TestObject',\n  properties: [\n    {name: 'doubleCol', type: Realm.Types.DOUBLE},\n  ]\n};\n\nfunction PersonObject() {}\nPersonObject.prototype.schema = {\n  name: 'PersonObject',\n  properties: [\n    {name: 'name', type: Realm.Types.STRING},\n    {name: 'age',  type: Realm.Types.DOUBLE},\n  ]\n};\nPersonObject.prototype.description = function() {\n    return this.name + ' ' + this.age;\n};\n\nvar BasicTypesObjectSchema = {\n    name: 'BasicTypesObject',\n    properties: [\n        {name: 'boolCol',   type: Realm.Types.BOOL},\n        {name: 'intCol',    type: Realm.Types.INT},\n        {name: 'floatCol',  type: Realm.Types.FLOAT},\n        {name: 'doubleCol', type: Realm.Types.DOUBLE},\n        {name: 'stringCol', type: Realm.Types.STRING},\n        {name: 'dateCol',   type: Realm.Types.DATE},\n        {name: 'dataCol',   type: Realm.Types.DATA},\n    ]\n};\n\nvar LinkTypesObjectSchema = {\n    name: 'LinkTypesObject',\n    properties: [\n        {name: 'objectCol',  type: 'TestObject'},\n        {name: 'objectCol1', type: Realm.Types.OBJECT, objectType: 'TestObject'},\n        {name: 'arrayCol',   type: Realm.Types.ARRAY, objectType: 'TestObject'},\n    ]\n};\n\nvar IntPrimaryObjectSchema = {\n  name: 'IntPrimaryObject',\n  primaryKey: 'primaryCol',\n  properties: [\n    {name: 'primaryCol', type: Realm.Types.INT},\n    {name: 'valueCol',   type: Realm.Types.STRING},\n  ]\n};\n\nvar AllTypesObjectSchema = {\n  name: 'AllTypesObject',\n  primaryKey: 'primaryCol',\n  properties: [\n    {name: 'primaryCol',type: Realm.Types.STRING},\n    {name: 'boolCol',   type: Realm.Types.BOOL},\n    {name: 'intCol',    type: Realm.Types.INT},\n    {name: 'floatCol',  type: Realm.Types.FLOAT},\n    {name: 'doubleCol', type: Realm.Types.DOUBLE},\n    {name: 'stringCol', type: Realm.Types.STRING},\n    {name: 'dateCol',   type: Realm.Types.DATE},\n    {name: 'dataCol',   type: Realm.Types.DATA}, \n    {name: 'objectCol', type: 'TestObject'},\n    {name: 'arrayCol',  type: Realm.Types.ARRAY, objectType: 'TestObject'}, \n  ]\n};\n\nvar DefaultValuesObjectSchema = {\n  name: 'DefaultValuesObject',\n  properties: [\n    {name: 'boolCol',   type: Realm.Types.BOOL,   default: true},\n    {name: 'intCol',    type: Realm.Types.INT,    default: -1},\n    {name: 'floatCol',  type: Realm.Types.FLOAT,  default: -1.1},\n    {name: 'doubleCol', type: Realm.Types.DOUBLE, default: -1.11},\n    {name: 'stringCol', type: Realm.Types.STRING, default: 'defaultString'},\n    {name: 'dateCol',   type: Realm.Types.DATE,   default: new Date(1.111)},\n    {name: 'dataCol',   type: Realm.Types.DATA,   default: 'defaultData'}, \n    {name: 'objectCol', type: 'TestObject',     default: [1]},\n    {name: 'nullObjectCol', type: 'TestObject', default: null},\n    {name: 'arrayCol',  type: Realm.Types.ARRAY, objectType: 'TestObject', default: [[2]]}, \n  ]\n};\n\n","lang_cluster":"Javascript","length":103,"code_uid":"9f045221d8904df78339bf0a45cdeba5"}
{"diff_hunk":"@@ -1,5 +1,5 @@\n \/*global helpers *\/\n-function normalizeRelatedNodes(node, xpath) {\n+function normalizeRelatedNodes(node, options) {\n \t'use strict';\n \t['any','all','none'].forEach((type) => {\n \t\tif (!Array.isArray(node[type])) {","old_code":"\/*global helpers *\/\nfunction normalizeRelatedNodes(node, xpath) {\n\t'use strict';\n\t['any','all','none'].forEach((type) => {\n\t\tif (!Array.isArray(node[type])) {\n\t\t\treturn;\n\t\t}\n\t\tnode[type].filter((checkRes) => Array.isArray(checkRes.relatedNodes))\n\t\t.forEach((checkRes) => {\n\t\t\tcheckRes.relatedNodes = checkRes.relatedNodes.map((relatedNode) => {\n\t\t\t\tvar res = {\n\t\t\t\t\thtml: relatedNode.source,\n\t\t\t\t\ttarget: relatedNode.selector\n\t\t\t\t};\n\t\t\t\tif (xpath) {\n\t\t\t\t\tres.xpath = relatedNode.xpath;\n\t\t\t\t}\n\t\t\t\treturn res;\n\t\t\t});\n\t\t});\n\n\t});\n}\n\nvar resultKeys = axe.constants.resultGroups;\nhelpers.processAggregate = function (results, options) {\n\tvar resultObject = axe.utils.aggregateResult(results);\n\n\tresultObject.timestamp = new Date().toISOString();\n\tresultObject.url = window.location.href;\n\n\tresultKeys.forEach(function (key) {\n\t\tresultObject[key] = (resultObject[key] || []).map(function (ruleResult) {\n\t\t\truleResult = Object.assign({}, ruleResult);\n\n\t\t\tif (Array.isArray(ruleResult.nodes) && ruleResult.nodes.length > 0) {\n\t\t\t\truleResult.nodes = ruleResult.nodes.map(function (subResult) {\n\t\t\t\t\tif (typeof subResult.node === 'object') {\n\t\t\t\t\t\tsubResult.html = subResult.node.source;\n\t\t\t\t\t\tsubResult.target = subResult.node.selector;\n\n\t\t\t\t\t\tif (options.xpath) {\n\t\t\t\t\t\t\tsubResult.xpath = subResult.node.xpath;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tdelete subResult.result;\n\t\t\t\t\tdelete subResult.node;\n\n\t\t\t\t\tnormalizeRelatedNodes(subResult, options.xpath);\n\n\t\t\t\t\treturn subResult;\n\t\t\t\t});\n\t\t\t}\n\n\t\t\tresultKeys.forEach((key) => delete ruleResult[key]);\n\t\t\tdelete ruleResult.pageLevel;\n\t\t\tdelete ruleResult.result;\n\n\t\t\treturn ruleResult;\n\t\t});\n\t});\n\n\treturn resultObject;\n};\n","lang_cluster":"Javascript","length":64,"code_uid":"1ed7642c83634cd4a9874b993c5bb7ed"}
{"diff_hunk":"@@ -65,7 +65,7 @@ const Header = ( { children } ) => {\n \t\t\t\t<\/section>\n \t\t\t<\/header>\n \t\t\t<LegacyErrorNotification \/>\n-\t\t\t{ featureFlags.storeErrorNotifications.enabled && <ErrorNotifications \/> }\n+\t\t\t{ storeErrorNotifications && <ErrorNotifications \/> }\n \t\t<\/Fragment>\n \t);\n };","old_code":"\/**\n * Header component.\n *\n * Site Kit by Google, Copyright 2019 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * WordPress dependencies\n *\/\nimport { Fragment } from '@wordpress\/element';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport Logo from '.\/Logo';\nimport UserMenu from '.\/UserMenu';\nimport LegacyErrorNotification from '.\/legacy-notifications\/error-notification';\nimport ErrorNotifications from '.\/notifications\/ErrorNotifications';\nimport { STORE_NAME as CORE_USER } from '..\/googlesitekit\/datastore\/user\/constants';\nconst { useSelect } = Data;\n\nconst Header = ( { children } ) => {\n\tconst isAuthenticated = useSelect( ( select ) => select( CORE_USER ).isAuthenticated() );\n\n\treturn (\n\t\t<Fragment>\n\t\t\t<header className=\"googlesitekit-header\">\n\t\t\t\t<section className=\"mdc-layout-grid\">\n\t\t\t\t\t<div className=\"mdc-layout-grid__inner\">\n\t\t\t\t\t\t<div className=\"\n\t\t\t\t\t\t\tgooglesitekit-header__logo\n\t\t\t\t\t\t\tmdc-layout-grid__cell\n\t\t\t\t\t\t\tmdc-layout-grid__cell--align-middle\n\t\t\t\t\t\t\tmdc-layout-grid__cell--span-1-phone\n\t\t\t\t\t\t\tmdc-layout-grid__cell--span-2-tablet\n\t\t\t\t\t\t\tmdc-layout-grid__cell--span-4-desktop\n\t\t\t\t\t\t\">\n\t\t\t\t\t\t\t<Logo \/>\n\t\t\t\t\t\t<\/div>\n\t\t\t\t\t\t<div className=\"\n\t\t\t\t\t\t\tmdc-layout-grid__cell\n\t\t\t\t\t\t\tmdc-layout-grid__cell--align-middle\n\t\t\t\t\t\t\tmdc-layout-grid__cell--align-right-phone\n\t\t\t\t\t\t\tmdc-layout-grid__cell--span-3-phone\n\t\t\t\t\t\t\tmdc-layout-grid__cell--span-6-tablet\n\t\t\t\t\t\t\tmdc-layout-grid__cell--span-8-desktop\n\t\t\t\t\t\t\">\n\t\t\t\t\t\t\t{ isAuthenticated && children }\n\t\t\t\t\t\t\t{ isAuthenticated && <UserMenu \/> }\n\t\t\t\t\t\t<\/div>\n\t\t\t\t\t<\/div>\n\t\t\t\t<\/section>\n\t\t\t<\/header>\n\t\t\t<LegacyErrorNotification \/>\n\t\t\t{ featureFlags.storeErrorNotifications.enabled && <ErrorNotifications \/> }\n\t\t<\/Fragment>\n\t);\n};\n\nexport default Header;\n","lang_cluster":"Javascript","length":73,"code_uid":"0574cdc31639479b8186fbc191fadf27"}
{"diff_hunk":"@@ -45,14 +45,14 @@ public class VirtualAuthenticatorTest extends JUnit4TestBase {\n     + \"      id: \\\"localhost\\\",\"\n     + \"      name: \\\"Selenium WebDriver Test\\\",\"\n     + \"    },\"\n-    + \"    challenge: Uint8Array.from(\\\"challenge\\\"),\"\n+    + \"    challenge: Int8Array.from(\\\"challenge\\\"),\"\n     + \"    pubKeyCredParams: [\"\n     + \"      {type: \\\"public-key\\\", alg: -7},\"\n     + \"    ],\"\n     + \"    user: {\"\n     + \"      name: \\\"name\\\",\"\n     + \"      displayName: \\\"displayName\\\",\"\n-    + \"      id: Uint8Array.from([1]),\"\n+    + \"      id: Int8Array.from([1]),\"\n     + \"    },\"\n     + \"  }, options);\"\n ","old_code":"\/\/ Licensed to the Software Freedom Conservancy (SFC) under one\n\/\/ or more contributor license agreements.  See the NOTICE file\n\/\/ distributed with this work for additional information\n\/\/ regarding copyright ownership.  The SFC licenses this file\n\/\/ to you under the Apache License, Version 2.0 (the\n\/\/ \"License\"); you may not use this file except in compliance\n\/\/ with the License.  You may obtain a copy of the License at\n\/\/\n\/\/   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing,\n\/\/ software distributed under the License is distributed on an\n\/\/ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\/\/ KIND, either express or implied.  See the License for the\n\/\/ specific language governing permissions and limitations\n\/\/ under the License.\n\npackage org.openqa.selenium.virtualauthenticator;\n\nimport static org.assertj.core.api.Assumptions.assumeThat;\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.openqa.selenium.By;\nimport org.openqa.selenium.JavascriptExecutor;\nimport org.openqa.selenium.environment.webserver.Page;\nimport org.openqa.selenium.testing.JUnit4TestBase;\nimport org.openqa.selenium.testing.NotYetImplemented;\nimport org.openqa.selenium.virtualauthenticator.HasVirtualAuthenticator;\nimport org.openqa.selenium.virtualauthenticator.VirtualAuthenticator;\nimport org.openqa.selenium.virtualauthenticator.VirtualAuthenticatorOptions;\n\nimport java.util.Map;\n\npublic class VirtualAuthenticatorTest extends JUnit4TestBase {\n\n  private final String script =\n      \"async function registerCredential(options = {}) {\"\n    + \"  options = Object.assign({\"\n    + \"    authenticatorSelection: {\"\n    + \"      requireResidentKey: false,\"\n    + \"    },\"\n    + \"    rp: {\"\n    + \"      id: \\\"localhost\\\",\"\n    + \"      name: \\\"Selenium WebDriver Test\\\",\"\n    + \"    },\"\n    + \"    challenge: Uint8Array.from(\\\"challenge\\\"),\"\n    + \"    pubKeyCredParams: [\"\n    + \"      {type: \\\"public-key\\\", alg: -7},\"\n    + \"    ],\"\n    + \"    user: {\"\n    + \"      name: \\\"name\\\",\"\n    + \"      displayName: \\\"displayName\\\",\"\n    + \"      id: Uint8Array.from([1]),\"\n    + \"    },\"\n    + \"  }, options);\"\n\n    + \"  try {\"\n    + \"    const credential = await navigator.credentials.create({publicKey: options});\"\n    + \"    return {\"\n    + \"      status: \\\"OK\\\",\"\n    + \"      credential: {\"\n    + \"        id: credential.id,\"\n    + \"        rawId: Array.from(new Uint8Array(credential.rawId)),\"\n    + \"        transports: credential.response.getTransports(),\"\n    + \"      }\"\n    + \"    };\"\n    + \"  } catch (error) {\"\n    + \"    return {status: error.toString()};\"\n    + \"  }\"\n    + \"}\"\n\n    + \"async function getCredential(credential, options = {}) {\"\n    + \"  options = Object.assign({\"\n    + \"    challenge: Uint8Array.from(\\\"Winter is Coming\\\"),\"\n    + \"    rpId: \\\"localhost\\\",\"\n    + \"    allowCredentials: [credential],\"\n    + \"    userVerification: \\\"preferred\\\",\"\n    + \"  }, options);\"\n\n    + \"  try {\"\n    + \"    const attestation = await navigator.credentials.get({publicKey: options});\"\n    + \"    return {\"\n    + \"      status: \\\"OK\\\",\"\n    + \"      attestation,\"\n    + \"    };\"\n    + \"  } catch (error) {\"\n    + \"    return {status: error.toString()};\"\n    + \"  }\"\n    + \"}\";\n\n  @Before\n  public void setup() {\n    assumeThat(driver).isInstanceOf(HasVirtualAuthenticator.class);\n    driver.get(appServer.create(new Page()\n        .withTitle(\"Virtual Authenticator Test\")\n        .withScripts(script)));\n  }\n\n  @Test\n  public void testCreateAuthenticator() {\n    \/\/ Register a credential on the Virtual Authenticator.\n    VirtualAuthenticatorOptions options = new VirtualAuthenticatorOptions();\n    ((HasVirtualAuthenticator) driver).addVirtualAuthenticator(options);\n    Map<String, Object> response = (Map<String, Object>)\n      ((JavascriptExecutor) driver).executeAsyncScript(\n        \"registerCredential().then(arguments[arguments.length - 1]);\");\n    assertThat(response.get(\"status\")).isEqualTo(\"OK\");\n\n    \/\/ Attempt to use the credential to get an assertion.\n    Object credentialId = ((Map<String, Object>) response.get(\"credential\")).get(\"rawId\");\n    response = (Map<String, Object>)\n      ((JavascriptExecutor) driver).executeAsyncScript(\n        \"getCredential({\"\n      + \"  \\\"type\\\": \\\"public-key\\\",\"\n      + \"  \\\"id\\\": Uint8Array.from(arguments[0]),\"\n      + \"}).then(arguments[arguments.length - 1]);\", credentialId);\n\n    assertThat(response.get(\"status\")).isEqualTo(\"OK\");\n  }\n\n  @Test\n  public void testRemoveAuthenticator() {\n    VirtualAuthenticatorOptions options = new VirtualAuthenticatorOptions();\n    VirtualAuthenticator authenticator =\n      ((HasVirtualAuthenticator) driver).addVirtualAuthenticator(options);\n    ((HasVirtualAuthenticator) driver).removeVirtualAuthenticator(authenticator);\n    \/\/ no exceptions.\n  }\n}\n","lang_cluster":"Javascript","length":131,"code_uid":"11cb80e2235e4e598b59fc0e6c3a20f8"}
{"diff_hunk":"@@ -73,8 +73,13 @@\n      * @param arr\n      * @returns {String}\n      *\/\n-    stringify: function (arr) {\n-      var r, rLen, c, cLen, str = '', val;\n+    stringify(arr) {\n+      let r;\n+      let rLen;\n+      let c;\n+      let cLen;\n+      let str = '';\n+      let val;\n \n       for (r = 0, rLen = arr.length; r < rLen; r += 1) {\n         cLen = arr[r].length;","old_code":"\/**\n * SheetClip - Spreadsheet Clipboard Parser\n * version 0.2\n *\n * This tiny library transforms JavaScript arrays to strings that are pasteable by LibreOffice, OpenOffice,\n * Google Docs and Microsoft Excel.\n *\n * Copyright 2012, Marcin Warpechowski\n * Licensed under the MIT license.\n * http:\/\/github.com\/warpech\/sheetclip\/\n *\/\n\/*jslint white: true*\/\n(function (global) {\n  \"use strict\";\n\n  function countQuotes(str) {\n    return str.split('\"').length - 1;\n  }\n\n  var SheetClip = {\n    \/**\n     * Decode spreadsheet string into array\n     *\n     * @param {String} str\n     * @returns {Array}\n     *\/\n    parse: function (str) {\n      var r, rLen, rows, arr = [], a = 0, c, cLen, multiline, last;\n\n      rows = str.replace(\/\\r\\n|\\r\/g, '\\n').split('\\n');\n\n      if (rows.length > 1 && rows[rows.length - 1] === '') {\n        rows.pop();\n      }\n      for (r = 0, rLen = rows.length; r < rLen; r += 1) {\n        rows[r] = rows[r].split('\\t');\n\n        for (c = 0, cLen = rows[r].length; c < cLen; c += 1) {\n          if (!arr[a]) {\n            arr[a] = [];\n          }\n          if (multiline && c === 0) {\n            last = arr[a].length - 1;\n            arr[a][last] = arr[a][last] + '\\n' + rows[r][0];\n\n            if (multiline && (countQuotes(rows[r][0]) & 1)) { \/\/& 1 is a bitwise way of performing mod 2\n              multiline = false;\n              arr[a][last] = arr[a][last].substring(0, arr[a][last].length - 1).replace(\/\"\"\/g, '\"');\n            }\n          }\n          else {\n            if (c === cLen - 1 && rows[r][c].indexOf('\"') === 0 && (countQuotes(rows[r][c]) & 1)) {\n              arr[a].push(rows[r][c].substring(1).replace(\/\"\"\/g, '\"'));\n              multiline = true;\n            }\n            else {\n              arr[a].push(rows[r][c].replace(\/\"\"\/g, '\"'));\n              multiline = false;\n            }\n          }\n        }\n        if (!multiline) {\n          a += 1;\n        }\n      }\n\n      return arr;\n    },\n\n    \/**\n     * Encode array into valid spreadsheet string\n     *\n     * @param arr\n     * @returns {String}\n     *\/\n    stringify: function (arr) {\n      var r, rLen, c, cLen, str = '', val;\n\n      for (r = 0, rLen = arr.length; r < rLen; r += 1) {\n        cLen = arr[r].length;\n\n        for (c = 0; c < cLen; c += 1) {\n          if (c > 0) {\n            str += '\\t';\n          }\n          val = arr[r][c];\n\n          if (typeof val === 'string') {\n            if (val.indexOf('\\n') > -1) {\n              str += '\"' + val.replace(\/\"\/g, '\"\"') + '\"';\n            }\n            else {\n              str += val;\n            }\n          }\n          else if (val === null || val === void 0) { \/\/ void 0 resolves to undefined\n            str += '';\n          }\n          else {\n            str += val;\n          }\n        }\n\n        if (r !== rLen - 1) {\n          str += '\\n';\n        }\n      }\n\n      return str;\n    }\n  };\n\n  if (typeof exports !== 'undefined') {\n    exports.parse = SheetClip.parse;\n    exports.stringify = SheetClip.stringify;\n  } else {\n    global.SheetClip = SheetClip;\n  }\n}(window));\n","lang_cluster":"Javascript","length":119,"code_uid":"60dc2e154aac4222a0fc9abbf12dcfc6"}
{"diff_hunk":"@@ -84,6 +84,19 @@ export const reducer = Data.addInitializeReducer(\n \t\treport.reducer,\n \t\ttags.reducer,\n \t\turlchannels.reducer,\n+\t\tsettings.reducer,\n+\t\tadblocker.reducer,\n+\t\t\/\/ TODO: Revisit better way to handle and retrieve errors.\n+\t\t( state, { type, payload } ) => {\n+\t\t\tif ( 'RECEIVE_ERROR' === type ) {\n+\t\t\t\tconst { error } = payload;\n+\t\t\t\treturn {\n+\t\t\t\t\t...state,\n+\t\t\t\t\terror,\n+\t\t\t\t};\n+\t\t\t}\n+\t\t\treturn { ...state };\n+\t\t},\n \t)\n );\n ","old_code":"\/**\n * modules\/adsense data store\n *\n * Site Kit by Google, Copyright 2020 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * External dependencies\n *\/\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport Modules from 'googlesitekit-modules';\nimport accounts from '.\/accounts';\nimport alerts from '.\/alerts';\nimport clients from '.\/clients';\nimport report from '.\/report';\nimport tags from '.\/tags';\nimport urlchannels from '.\/urlchannels';\n\nconst baseModuleStore = Modules.createModuleStore( 'adsense', {\n\tsettingSlugs: [\n\t\t'accountID',\n\t\t'clientID',\n\t\t'useSnippet',\n\t\t'accountStatus',\n\t\t'siteStatus',\n\t],\n} );\n\nexport const STORE_NAME = baseModuleStore.STORE_NAME;\n\nexport const INITIAL_STATE = Data.collectState(\n\tbaseModuleStore.INITIAL_STATE,\n\taccounts.INITIAL_STATE,\n\talerts.INITIAL_STATE,\n\tclients.INITIAL_STATE,\n\treport.INITIAL_STATE,\n\ttags.INITIAL_STATE,\n\turlchannels.INITIAL_STATE,\n);\n\nexport const actions = Data.addInitializeAction( Data.collectActions(\n\tbaseModuleStore.actions,\n\taccounts.actions,\n\talerts.actions,\n\tclients.actions,\n\treport.actions,\n\ttags.actions,\n\turlchannels.actions,\n) );\n\nexport const controls = Data.collectControls(\n\tbaseModuleStore.controls,\n\taccounts.controls,\n\talerts.controls,\n\tclients.controls,\n\treport.controls,\n\ttags.controls,\n\turlchannels.controls,\n);\n\nexport const reducer = Data.addInitializeReducer(\n\tINITIAL_STATE,\n\tData.collectReducers(\n\t\tbaseModuleStore.reducer,\n\t\taccounts.reducer,\n\t\talerts.reducer,\n\t\tclients.reducer,\n\t\treport.reducer,\n\t\ttags.reducer,\n\t\turlchannels.reducer,\n\t)\n);\n\nexport const resolvers = Data.collectResolvers(\n\tbaseModuleStore.resolvers,\n\taccounts.resolvers,\n\talerts.resolvers,\n\tclients.resolvers,\n\treport.resolvers,\n\ttags.resolvers,\n\turlchannels.resolvers,\n);\n\nexport const selectors = Data.collectSelectors(\n\tbaseModuleStore.selectors,\n\taccounts.selectors,\n\talerts.selectors,\n\tclients.selectors,\n\treport.selectors,\n\ttags.selectors,\n\turlchannels.selectors,\n\t{\n\t\t\/\/ TODO: Revisit better way to handle and retrieve errors.\n\t\tgetError( state ) {\n\t\t\tconst { error } = state;\n\n\t\t\treturn error;\n\t\t},\n\t},\n);\n\nconst store = {\n\tactions,\n\tcontrols,\n\treducer,\n\tresolvers,\n\tselectors,\n};\n\n\/\/ Register this store on the global registry.\nData.registerStore( STORE_NAME, store );\n\nexport default store;\n","lang_cluster":"Javascript","length":129,"code_uid":"af67bbe289934cba8b974f9b5dede93c"}
{"diff_hunk":"@@ -66,8 +66,8 @@ function missingRequiredChildren(node, childRoles, all, role) {\n \t\/\/ combobox exceptions\n \tif (role === 'combobox') {\n \t\t\/\/ remove 'textbox' from missing roles if combobox is a native text-type input or owns a 'searchbox'\n-\t\tvar textboxIndex = missing.indexOf('textbox');\n-\t\tvar textTypeInputs = ['text', 'search', 'email', 'url', 'tel'];\n+\t\tconst textboxIndex = missing.indexOf('textbox');\n+\t\tconst textTypeInputs = ['text', 'search', 'email', 'url', 'tel'];\n \t\tif (\n \t\t\t(textboxIndex >= 0 &&\n \t\t\t\t(node.nodeName.toUpperCase() === 'INPUT' &&","old_code":"const requiredOwned = axe.commons.aria.requiredOwned;\nconst implicitNodes = axe.commons.aria.implicitNodes;\nconst matchesSelector = axe.utils.matchesSelector;\nconst idrefs = axe.commons.dom.idrefs;\nconst hasContentVirtual = axe.commons.dom.hasContentVirtual;\nconst reviewEmpty =\n\toptions && Array.isArray(options.reviewEmpty) ? options.reviewEmpty : [];\n\nfunction owns(node, virtualTree, role, ariaOwned) {\n\tif (node === null) {\n\t\treturn false;\n\t}\n\tvar implicit = implicitNodes(role),\n\t\tselector = ['[role=\"' + role + '\"]'];\n\n\tif (implicit) {\n\t\tselector = selector.concat(\n\t\t\timplicit.map(implicitSelector => implicitSelector + ':not([role])')\n\t\t);\n\t}\n\n\tselector = selector.join(',');\n\treturn ariaOwned\n\t\t? matchesSelector(node, selector) ||\n\t\t\t\t!!axe.utils.querySelectorAll(virtualTree, selector)[0]\n\t\t: !!axe.utils.querySelectorAll(virtualTree, selector)[0];\n}\n\nfunction ariaOwns(nodes, role) {\n\tvar index, length;\n\n\tfor (index = 0, length = nodes.length; index < length; index++) {\n\t\tif (nodes[index] === null) {\n\t\t\tcontinue;\n\t\t}\n\t\tconst virtualTree = axe.utils.getNodeFromTree(nodes[index]);\n\t\tif (owns(nodes[index], virtualTree, role, true)) {\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nfunction missingRequiredChildren(node, childRoles, all, role) {\n\tvar index,\n\t\tlength = childRoles.length,\n\t\tmissing = [],\n\t\townedElements = idrefs(node, 'aria-owns');\n\n\tfor (index = 0; index < length; index++) {\n\t\tvar childRole = childRoles[index];\n\t\tif (\n\t\t\towns(node, virtualNode, childRole) ||\n\t\t\tariaOwns(ownedElements, childRole)\n\t\t) {\n\t\t\tif (!all) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t} else {\n\t\t\tif (all) {\n\t\t\t\tmissing.push(childRole);\n\t\t\t}\n\t\t}\n\t}\n\n\t\/\/ combobox exceptions\n\tif (role === 'combobox') {\n\t\t\/\/ remove 'textbox' from missing roles if combobox is a native text-type input or owns a 'searchbox'\n\t\tvar textboxIndex = missing.indexOf('textbox');\n\t\tvar textTypeInputs = ['text', 'search', 'email', 'url', 'tel'];\n\t\tif (\n\t\t\t(textboxIndex >= 0 &&\n\t\t\t\t(node.nodeName.toUpperCase() === 'INPUT' &&\n\t\t\t\t\ttextTypeInputs.includes(node.type))) ||\n\t\t\t(owns(node, virtualNode, 'searchbox') ||\n\t\t\t\tariaOwns(ownedElements, 'searchbox'))\n\t\t) {\n\t\t\tmissing.splice(textboxIndex, 1);\n\t\t}\n\n\t\t\/\/ remove 'listbox' from missing roles if combobox is collapsed\n\t\tvar listboxIndex = missing.indexOf('listbox');\n\t\tvar expanded = node.getAttribute('aria-expanded');\n\t\tif (listboxIndex >= 0 && (!expanded || expanded === 'false')) {\n\t\t\tmissing.splice(listboxIndex, 1);\n\t\t}\n\t}\n\n\tif (missing.length) {\n\t\treturn missing;\n\t}\n\tif (!all && childRoles.length) {\n\t\treturn childRoles;\n\t}\n\treturn null;\n}\n\nfunction hasDecendantWithRole(node) {\n\treturn (\n\t\tnode.children &&\n\t\tnode.children.some(child => {\n\t\t\tconst role = axe.commons.aria.getRole(child);\n\t\t\treturn (\n\t\t\t\t!['presentation', 'none', null].includes(role) ||\n\t\t\t\thasDecendantWithRole(child)\n\t\t\t);\n\t\t})\n\t);\n}\n\nvar role = node.getAttribute('role');\nvar required = requiredOwned(role);\n\nif (!required) {\n\treturn true;\n}\n\nvar all = false;\nvar childRoles = required.one;\nif (!childRoles) {\n\tvar all = true;\n\tchildRoles = required.all;\n}\n\nvar missing = missingRequiredChildren(node, childRoles, all, role);\n\nif (!missing) {\n\treturn true;\n}\n\nthis.data(missing);\n\n\/\/ Only review empty nodes when a node is both empty and does not have an aria-owns relationship\nif (\n\treviewEmpty.includes(role) &&\n\t!hasContentVirtual(virtualNode, false, true) &&\n\t!hasDecendantWithRole(virtualNode) &&\n\tidrefs(node, 'aria-owns').length === 0\n) {\n\treturn undefined;\n} else {\n\treturn false;\n}\n","lang_cluster":"Javascript","length":143,"code_uid":"f01a64c420024c1eb548e6aa4fe968e9"}
{"diff_hunk":"@@ -1,7 +1,10 @@\n 'use strict';\n \n const { expect } = require('chai');\n+const { MongoClient } = require('..\/..\/..\/src');\n const { AggregateOperation } = require('..\/..\/..\/src\/operations\/aggregate');\n+const { isHello } = require('..\/..\/..\/src\/utils');\n+const { HELLO, cleanup, createServer } = require('..\/..\/tools\/mongodb-mock');\n \n describe('AggregateOperation', function () {\n   const db = 'test';","old_code":"'use strict';\n\nconst { expect } = require('chai');\nconst { AggregateOperation } = require('..\/..\/..\/src\/operations\/aggregate');\n\ndescribe('AggregateOperation', function () {\n  const db = 'test';\n\n  describe('#constructor', function () {\n    context('when out is in the options', function () {\n      const operation = new AggregateOperation(db, [], { out: 'test', dbName: db });\n\n      it('sets trySecondaryWrite to true', function () {\n        expect(operation.trySecondaryWrite).to.be.true;\n      });\n    });\n\n    context('when $out is the last stage', function () {\n      const operation = new AggregateOperation(db, [{ $out: 'test' }], { dbName: db });\n\n      it('sets trySecondaryWrite to true', function () {\n        expect(operation.trySecondaryWrite).to.be.true;\n      });\n    });\n\n    context('when $out is not the last stage', function () {\n      const operation = new AggregateOperation(db, [{ $out: 'test' }, { $project: { name: 1 } }], {\n        dbName: db\n      });\n\n      it('sets trySecondaryWrite to false', function () {\n        expect(operation.trySecondaryWrite).to.be.false;\n      });\n    });\n\n    context('when $merge is the last stage', function () {\n      const operation = new AggregateOperation(db, [{ $merge: { into: 'test' } }], { dbName: db });\n\n      it('sets trySecondaryWrite to true', function () {\n        expect(operation.trySecondaryWrite).to.be.true;\n      });\n    });\n\n    context('when $merge is not the last stage', function () {\n      const operation = new AggregateOperation(\n        db,\n        [{ $merge: { into: 'test' } }, { $project: { name: 1 } }],\n        { dbName: db }\n      );\n\n      it('sets trySecondaryWrite to false', function () {\n        expect(operation.trySecondaryWrite).to.be.false;\n      });\n    });\n\n    context('when no writable stages in empty pipeline', function () {\n      const operation = new AggregateOperation(db, [], { dbName: db });\n\n      it('sets trySecondaryWrite to false', function () {\n        expect(operation.trySecondaryWrite).to.be.false;\n      });\n    });\n\n    context('when no writable stages', function () {\n      const operation = new AggregateOperation(db, [{ $project: { name: 1 } }], { dbName: db });\n\n      it('sets trySecondaryWrite to false', function () {\n        expect(operation.trySecondaryWrite).to.be.false;\n      });\n    });\n  });\n});\n","lang_cluster":"Javascript","length":72,"code_uid":"d7a7268ee3334181b1940d6df353b7b2"}
{"diff_hunk":"@@ -69,14 +69,10 @@ describe( 'core\/site reset', () => {\n \t\t\tit( 'does not require any params', () => {\n \t\t\t\texpect( async () => {\n \t\t\t\t\tconst response = true;\n-\t\t\t\t\tfetch\n-\t\t\t\t\t\t.doMockOnceIf(\n-\t\t\t\t\t\t\t\/^\\\/google-site-kit\\\/v1\\\/core\\\/site\\\/data\\\/reset\/\n-\t\t\t\t\t\t)\n-\t\t\t\t\t\t.mockResponseOnce(\n-\t\t\t\t\t\t\tJSON.stringify( response ),\n-\t\t\t\t\t\t\t{ status: 200 }\n-\t\t\t\t\t\t);\n+\t\t\t\t\tfetchMock.once(\n+\t\t\t\t\t\t\/^\\\/google-site-kit\\\/v1\\\/core\\\/site\\\/data\\\/reset\/,\n+\t\t\t\t\t\t{ body: JSON.stringify( response ), status: 200 }\n+\t\t\t\t\t);\n \n \t\t\t\t\tawait registry.dispatch( STORE_NAME ).reset();\n \t\t\t\t} ).not.toThrow();","old_code":"\/**\n * core\/site data store: reset connection tests.\n *\n * Site Kit by Google, Copyright 2020 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * WordPress dependencies\n *\/\nimport apiFetch from '@wordpress\/api-fetch';\n\n\/**\n * Internal dependencies\n *\/\nimport {\n\tcreateTestRegistry,\n\tmuteConsole,\n\tsubscribeUntil,\n\tunsubscribeFromAll,\n} from 'tests\/js\/utils';\nimport { STORE_NAME } from '.\/constants';\n\ndescribe( 'core\/site reset', () => {\n\tlet apiFetchSpy;\n\tlet registry;\n\n\tbeforeEach( () => {\n\t\tregistry = createTestRegistry();\n\n\t\tapiFetchSpy = jest.spyOn( { apiFetch }, 'apiFetch' );\n\t} );\n\n\tafterEach( () => {\n\t\tunsubscribeFromAll( registry );\n\t\tapiFetchSpy.mockRestore();\n\t} );\n\n\tdescribe( 'actions', () => {\n\t\tdescribe( 'fetchReset', () => {\n\t\t\tit( 'sets isDoingReset ', async () => {\n\t\t\t\tconst response = true;\n\t\t\t\tfetch\n\t\t\t\t\t.doMockOnceIf(\n\t\t\t\t\t\t\/^\\\/google-site-kit\\\/v1\\\/core\\\/site\\\/data\\\/reset\/\n\t\t\t\t\t)\n\t\t\t\t\t.mockResponseOnce(\n\t\t\t\t\t\tJSON.stringify( response ),\n\t\t\t\t\t\t{ status: 200 }\n\t\t\t\t\t);\n\n\t\t\t\tregistry.dispatch( STORE_NAME ).fetchReset();\n\t\t\t\texpect( registry.select( STORE_NAME ).isDoingReset() ).toEqual( true );\n\t\t\t} );\n\t\t} );\n\n\t\tdescribe( 'reset', () => {\n\t\t\tit( 'does not require any params', () => {\n\t\t\t\texpect( async () => {\n\t\t\t\t\tconst response = true;\n\t\t\t\t\tfetch\n\t\t\t\t\t\t.doMockOnceIf(\n\t\t\t\t\t\t\t\/^\\\/google-site-kit\\\/v1\\\/core\\\/site\\\/data\\\/reset\/\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.mockResponseOnce(\n\t\t\t\t\t\t\tJSON.stringify( response ),\n\t\t\t\t\t\t\t{ status: 200 }\n\t\t\t\t\t\t);\n\n\t\t\t\t\tawait registry.dispatch( STORE_NAME ).reset();\n\t\t\t\t} ).not.toThrow();\n\t\t\t} );\n\n\t\t\tit( 'resets connection ', async () => {\n\t\t\t\tconst response = true;\n\t\t\t\tfetch\n\t\t\t\t\t.doMockOnceIf(\n\t\t\t\t\t\t\/^\\\/google-site-kit\\\/v1\\\/core\\\/site\\\/data\\\/reset\/\n\t\t\t\t\t)\n\t\t\t\t\t.mockResponseOnce(\n\t\t\t\t\t\tJSON.stringify( response ),\n\t\t\t\t\t\t{ status: 200 }\n\t\t\t\t\t);\n\n\t\t\t\tregistry\n\t\t\t\t\t.dispatch( STORE_NAME )\n\t\t\t\t\t.receiveConnection( { connected: true, resettable: true } );\n\n\t\t\t\tawait registry.dispatch( STORE_NAME ).reset();\n\t\t\t\texpect( fetch ).toHaveBeenCalledTimes( 1 );\n\n\t\t\t\tfetch\n\t\t\t\t\t.doMockOnceIf(\n\t\t\t\t\t\t\/^\\\/google-site-kit\\\/v1\\\/core\\\/site\\\/data\\\/connection\/\n\t\t\t\t\t)\n\t\t\t\t\t.mockResponseOnce(\n\t\t\t\t\t\tJSON.stringify( { connected: false, resettable: false } ),\n\t\t\t\t\t\t{ status: 200 }\n\t\t\t\t\t);\n\n\t\t\t\t\/\/ After a successful reset, `connection` should be `undefined` again.\n\t\t\t\tconst connection = await registry.select( STORE_NAME ).getConnection();\n\t\t\t\texpect( connection ).toEqual( undefined );\n\t\t\t} );\n\n\t\t\tit( 'does not reset local connection if reset request fails', async () => {\n\t\t\t\t\/\/ Make sure there is existing data in the store so we can ensure\n\t\t\t\t\/\/ it isn't reset.\n\t\t\t\tregistry.dispatch( STORE_NAME ).receiveConnection(\n\t\t\t\t\t{ connected: true, resettable: true }\n\t\t\t\t);\n\n\t\t\t\tconst response = {\n\t\t\t\t\tcode: 'internal_server_error',\n\t\t\t\t\tmessage: 'Internal server error',\n\t\t\t\t\tdata: { status: 500 },\n\t\t\t\t};\n\t\t\t\tfetch\n\t\t\t\t\t.doMockOnceIf(\n\t\t\t\t\t\t\/^\\\/google-site-kit\\\/v1\\\/core\\\/site\\\/data\\\/reset\/\n\t\t\t\t\t)\n\t\t\t\t\t.mockResponseOnce(\n\t\t\t\t\t\tJSON.stringify( response ),\n\t\t\t\t\t\t{ status: 500 }\n\t\t\t\t\t);\n\n\t\t\t\tmuteConsole( 'error' );\n\t\t\t\tregistry.dispatch( STORE_NAME ).reset();\n\t\t\t\tawait subscribeUntil( registry, () => registry.select( STORE_NAME ).isDoingReset() === false );\n\n\t\t\t\texpect( fetch ).toHaveBeenCalledTimes( 1 );\n\n\t\t\t\t\/\/ After a failed reset, `connection` should still exist.\n\t\t\t\tconst connection = registry.select( STORE_NAME ).getConnection();\n\t\t\t\texpect( connection ).toEqual( { connected: true, resettable: true } );\n\t\t\t} );\n\t\t} );\n\t} );\n} );\n","lang_cluster":"Javascript","length":150,"code_uid":"804dfdcfcc224c70b46498d9914e0902"}
{"diff_hunk":"@@ -20,7 +20,6 @@\n  * WordPress dependencies\n  *\/\n import { Fragment } from '@wordpress\/element';\n-import { __ } from '@wordpress\/i18n';\n \n \/**\n  * External dependencies","old_code":"\/**\n * ModuleApp component.\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * WordPress dependencies\n *\/\nimport { Fragment } from '@wordpress\/element';\nimport { __ } from '@wordpress\/i18n';\n\n\/**\n * External dependencies\n *\/\nimport PropTypes from 'prop-types';\nimport classNames from 'classnames';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport Header from '..\/Header';\nimport Alert from '..\/Alert';\nimport ModuleHeader from '.\/ModuleHeader';\nimport WidgetContextRenderer from '..\/..\/googlesitekit\/widgets\/components\/WidgetContextRenderer';\nimport HelpMenu from '..\/help\/HelpMenu';\nimport DateRangeSelector from '..\/DateRangeSelector';\nimport HelpMenuLink from '..\/help\/HelpMenuLink';\nimport { CORE_MODULES } from '..\/..\/googlesitekit\/modules\/datastore\/constants';\n\nconst { useSelect } = Data;\n\nfunction ModuleApp( { moduleSlug } ) {\n\tconst screenWidgetContext = useSelect( ( select ) =>\n\t\tselect( CORE_MODULES ).getScreenWidgetContext( moduleSlug )\n\t);\n\tconst moduleConnected = useSelect( ( select ) =>\n\t\tselect( CORE_MODULES ).isModuleConnected( moduleSlug )\n\t);\n\tconst getModuleHeader = () => <ModuleHeader moduleSlug={ moduleSlug } \/>;\n\n\treturn (\n\t\t<Fragment>\n\t\t\t<Header>\n\t\t\t\t<HelpMenu>\n\t\t\t\t\t{ moduleSlug === 'adsense' && (\n\t\t\t\t\t\t<HelpMenuLink\n\t\t\t\t\t\t\tgaEventLabel=\"adsense_help\"\n\t\t\t\t\t\t\thref=\"https:\/\/support.google.com\/adsense\/\"\n\t\t\t\t\t\t>\n\t\t\t\t\t\t\t{ __( 'Get help with AdSense', 'google-site-kit' ) }\n\t\t\t\t\t\t<\/HelpMenuLink>\n\t\t\t\t\t) }\n\t\t\t\t<\/HelpMenu>\n\t\t\t\t{ moduleConnected && <DateRangeSelector \/> }\n\t\t\t<\/Header>\n\t\t\t<Alert module={ moduleSlug } \/>\n\t\t\t<WidgetContextRenderer\n\t\t\t\tslug={ screenWidgetContext }\n\t\t\t\tclassName={ classNames( [\n\t\t\t\t\t'googlesitekit-module-page',\n\t\t\t\t\t`googlesitekit-module-page--${ moduleSlug }`,\n\t\t\t\t] ) }\n\t\t\t\tHeader={ getModuleHeader }\n\t\t\t\/>\n\t\t<\/Fragment>\n\t);\n}\n\nModuleApp.propTypes = {\n\tmoduleSlug: PropTypes.string,\n};\n\nexport default ModuleApp;\n","lang_cluster":"Javascript","length":87,"code_uid":"a7d8aff00d2e48de9adeaada01d5a8ed"}
{"diff_hunk":"@@ -1,5 +1,15 @@\n-define(['events', 'globalize', 'dom', 'date-fns', 'dfnshelper', 'userSettings', 'serverNotifications', 'connectionManager', 'emby-button', 'listViewStyle'], function (events, globalize, dom, datefns, dfnshelper, userSettings, serverNotifications, connectionManager) {\n-    'use strict';\n+import events from 'events';\n+import globalize from 'globalize';\n+import dom from 'dom';\n+import * as datefns from 'date-fns';\n+import dfnshelper from 'dfnshelper';\n+import userSettings from 'userSettings';\n+import serverNotifications from 'serverNotifications';\n+import connectionManager from 'connectionManager';\n+import 'emby-button';\n+import 'listViewStyle';\n+\n+\/* eslint-disable indent *\/\n \n     function getEntryHtml(entry, apiClient) {\n         var html = '';","old_code":"define(['events', 'globalize', 'dom', 'date-fns', 'dfnshelper', 'userSettings', 'serverNotifications', 'connectionManager', 'emby-button', 'listViewStyle'], function (events, globalize, dom, datefns, dfnshelper, userSettings, serverNotifications, connectionManager) {\n    'use strict';\n\n    function getEntryHtml(entry, apiClient) {\n        var html = '';\n        html += '<div class=\"listItem listItem-border\">';\n        var color = '#00a4dc';\n        var icon = 'notifications';\n\n        if ('Error' == entry.Severity || 'Fatal' == entry.Severity || 'Warn' == entry.Severity) {\n            color = '#cc0000';\n            icon = 'notification_important';\n        }\n\n        if (entry.UserId && entry.UserPrimaryImageTag) {\n            html += '<span class=\"listItemIcon material-icons dvr\" style=\"width:2em!important;height:2em!important;padding:0;color:transparent;background-color:' + color + \";background-image:url('\" + apiClient.getUserImageUrl(entry.UserId, {\n                type: 'Primary',\n                tag: entry.UserPrimaryImageTag\n            }) + \"');background-repeat:no-repeat;background-position:center center;background-size: cover;\\\"><\/span>\";\n        } else {\n            html += '<span class=\"listItemIcon material-icons ' + icon + '\" style=\"background-color:' + color + '\"><\/span>';\n        }\n\n        html += '<div class=\"listItemBody three-line\">';\n        html += '<div class=\"listItemBodyText\">';\n        html += entry.Name;\n        html += '<\/div>';\n        html += '<div class=\"listItemBodyText secondary\">';\n        html += datefns.formatRelative(Date.parse(entry.Date), Date.parse(new Date()), { locale: dfnshelper.getLocale() });\n        html += '<\/div>';\n        html += '<div class=\"listItemBodyText secondary listItemBodyText-nowrap\">';\n        html += entry.ShortOverview || '';\n        html += '<\/div>';\n        html += '<\/div>';\n\n        if (entry.Overview) {\n            html += `<button type=\"button\" is=\"paper-icon-button-light\" class=\"btnEntryInfo\" data-id=\"${entry.Id}\" title=\"${globalize.translate('Info')}\">\n                       <span class=\"material-icons info\"><\/span>\n                    <\/button>`;\n        }\n\n        html += '<\/div>';\n\n        return html;\n    }\n\n    function renderList(elem, apiClient, result, startIndex, limit) {\n        elem.innerHTML = result.Items.map(function (i) {\n            return getEntryHtml(i, apiClient);\n        }).join('');\n    }\n\n    function reloadData(instance, elem, apiClient, startIndex, limit) {\n        if (null == startIndex) {\n            startIndex = parseInt(elem.getAttribute('data-activitystartindex') || '0');\n        }\n\n        limit = limit || parseInt(elem.getAttribute('data-activitylimit') || '7');\n        var minDate = new Date();\n        var hasUserId = 'false' !== elem.getAttribute('data-useractivity');\n\n        if (hasUserId) {\n            minDate.setTime(minDate.getTime() - 24 * 60 * 60 * 1000); \/\/ one day back\n        } else {\n            minDate.setTime(minDate.getTime() - 7 * 24 * 60 * 60 * 1000); \/\/ one week back\n        }\n\n        ApiClient.getJSON(ApiClient.getUrl('System\/ActivityLog\/Entries', {\n            startIndex: startIndex,\n            limit: limit,\n            minDate: minDate.toISOString(),\n            hasUserId: hasUserId\n        })).then(function (result) {\n            elem.setAttribute('data-activitystartindex', startIndex);\n            elem.setAttribute('data-activitylimit', limit);\n            if (!startIndex) {\n                var activityContainer = dom.parentWithClass(elem, 'activityContainer');\n\n                if (activityContainer) {\n                    if (result.Items.length) {\n                        activityContainer.classList.remove('hide');\n                    } else {\n                        activityContainer.classList.add('hide');\n                    }\n                }\n            }\n\n            instance.items = result.Items;\n            renderList(elem, apiClient, result, startIndex, limit);\n        });\n    }\n\n    function onActivityLogUpdate(e, apiClient, data) {\n        var options = this.options;\n\n        if (options && options.serverId === apiClient.serverId()) {\n            reloadData(this, options.element, apiClient);\n        }\n    }\n\n    function onListClick(e) {\n        var btnEntryInfo = dom.parentWithClass(e.target, 'btnEntryInfo');\n\n        if (btnEntryInfo) {\n            var id = btnEntryInfo.getAttribute('data-id');\n            var items = this.items;\n\n            if (items) {\n                var item = items.filter(function (i) {\n                    return i.Id.toString() === id;\n                })[0];\n\n                if (item) {\n                    showItemOverview(item);\n                }\n            }\n        }\n    }\n\n    function showItemOverview(item) {\n        require(['alert'], function (alert) {\n            alert({\n                text: item.Overview\n            });\n        });\n    }\n\n    function ActivityLog(options) {\n        this.options = options;\n        var element = options.element;\n        element.classList.add('activityLogListWidget');\n        element.addEventListener('click', onListClick.bind(this));\n        var apiClient = connectionManager.getApiClient(options.serverId);\n        reloadData(this, element, apiClient);\n        var onUpdate = onActivityLogUpdate.bind(this);\n        this.updateFn = onUpdate;\n        events.on(serverNotifications, 'ActivityLogEntry', onUpdate);\n        apiClient.sendMessage('ActivityLogEntryStart', '0,1500');\n    }\n\n    ActivityLog.prototype.destroy = function () {\n        var options = this.options;\n\n        if (options) {\n            options.element.classList.remove('activityLogListWidget');\n            connectionManager.getApiClient(options.serverId).sendMessage('ActivityLogEntryStop', '0,1500');\n        }\n\n        var onUpdate = this.updateFn;\n\n        if (onUpdate) {\n            events.off(serverNotifications, 'ActivityLogEntry', onUpdate);\n        }\n\n        this.items = null;\n        this.options = null;\n    };\n\n    return ActivityLog;\n});\n","lang_cluster":"Javascript","length":160,"code_uid":"19500af902f542b1822c1716e4fdb908"}
{"diff_hunk":"@@ -23,6 +23,10 @@ import 'css!.\/style';\n             source = entry;\n         }\n \n+        if (!entry.target.classList.contains('blurhashed')) {\n+            itemBlurhashing(entry);\n+        }\n+\n         if (entry.intersectionRatio > 0) {\n             if (source) fillImageElement(entry.target, source);\n         } else if (!source) {","old_code":"import * as lazyLoader from 'lazyLoader';\nimport * as userSettings from 'userSettings';\nimport 'css!.\/style';\n\/* eslint-disable indent *\/\n\n    export function lazyImage(elem, source = elem.getAttribute('data-src')) {\n        if (!source) {\n            return;\n        }\n\n        fillImageElement(elem, source);\n    }\n\n    export function fillImage(entry) {\n        if (!entry) {\n            throw new Error('entry cannot be null');\n        }\n\n        var source = undefined;\n        if (entry.target) {\n            source = entry.target.getAttribute('data-src');\n        } else {\n            source = entry;\n        }\n\n        if (entry.intersectionRatio > 0) {\n            if (source) fillImageElement(entry.target, source);\n        } else if (!source) {\n            emptyImageElement(entry.target);\n        }\n    }\n\n    function fillImageElement(elem, url) {\n        if (url === undefined) {\n            throw new Error('url cannot be undefined');\n        }\n\n        let preloaderImg = new Image();\n        preloaderImg.src = url;\n\n        preloaderImg.addEventListener('load', () => {\n            if (elem.tagName !== 'IMG') {\n                elem.style.backgroundImage = \"url('\" + url + \"')\";\n            } else {\n                elem.setAttribute('src', url);\n            }\n\n            if (userSettings.enableFastFadein()) {\n                elem.classList.add('lazy-image-fadein-fast');\n            } else {\n                elem.classList.add('lazy-image-fadein');\n            }\n\n            elem.removeAttribute('data-src');\n        });\n    }\n\n    function emptyImageElement(elem) {\n        var url;\n\n        if (elem.tagName !== 'IMG') {\n            url = elem.style.backgroundImage.slice(4, -1).replace(\/\"\/g, '');\n            elem.style.backgroundImage = 'none';\n        } else {\n            url = elem.getAttribute('src');\n            elem.setAttribute('src', '');\n        }\n\n        elem.setAttribute('data-src', url);\n\n        elem.classList.remove('lazy-image-fadein-fast');\n        elem.classList.remove('lazy-image-fadein');\n    }\n\n    export function lazyChildren(elem) {\n        lazyLoader.lazyChildren(elem, fillImage);\n    }\n\n    export function getPrimaryImageAspectRatio(items) {\n\n        var values = [];\n\n        for (var i = 0, length = items.length; i < length; i++) {\n\n            var ratio = items[i].PrimaryImageAspectRatio || 0;\n\n            if (!ratio) {\n                continue;\n            }\n\n            values[values.length] = ratio;\n        }\n\n        if (!values.length) {\n            return null;\n        }\n\n        \/\/ Use the median\n        values.sort(function (a, b) {\n            return a - b;\n        });\n\n        var half = Math.floor(values.length \/ 2);\n\n        var result;\n\n        if (values.length % 2) {\n            result = values[half];\n        } else {\n            result = (values[half - 1] + values[half]) \/ 2.0;\n        }\n\n        \/\/ If really close to 2:3 (poster image), just return 2:3\n        var aspect2x3 = 2 \/ 3;\n        if (Math.abs(aspect2x3 - result) <= 0.15) {\n            return aspect2x3;\n        }\n\n        \/\/ If really close to 16:9 (episode image), just return 16:9\n        var aspect16x9 = 16 \/ 9;\n        if (Math.abs(aspect16x9 - result) <= 0.2) {\n            return aspect16x9;\n        }\n\n        \/\/ If really close to 1 (square image), just return 1\n        if (Math.abs(1 - result) <= 0.15) {\n            return 1;\n        }\n\n        \/\/ If really close to 4:3 (poster image), just return 2:3\n        var aspect4x3 = 4 \/ 3;\n        if (Math.abs(aspect4x3 - result) <= 0.15) {\n            return aspect4x3;\n        }\n\n        return result;\n    }\n\n    export function fillImages(elems) {\n\n        for (var i = 0, length = elems.length; i < length; i++) {\n            var elem = elems[0];\n            fillImage(elem);\n        }\n    }\n\n\/* eslint-enable indent *\/\nexport default {\n    fillImages: fillImages,\n    fillImage: fillImage,\n    lazyImage: lazyImage,\n    lazyChildren: lazyChildren,\n    getPrimaryImageAspectRatio: getPrimaryImageAspectRatio\n};\n","lang_cluster":"Javascript","length":154,"code_uid":"ead54d2363dc48ff88ec08833d00789a"}
{"diff_hunk":"@@ -8,7 +8,6 @@ const EditorPanel = require('.\/EditorPanel')\n const PanelTopBar = require('.\/PickerPanelTopBar')\n const FileCard = require('.\/FileCard')\n const Slide = require('.\/Slide')\n-const isDragDropSupported = require('@uppy\/utils\/lib\/isDragDropSupported')\n \n \/\/ http:\/\/dev.edenspiekermann.com\/2016\/02\/11\/introducing-accessible-modal-dialog\n \/\/ https:\/\/github.com\/ghosh\/micromodal","old_code":"const { h } = require('preact')\nconst classNames = require('classnames')\nconst FileList = require('.\/FileList')\nconst AddFiles = require('.\/AddFiles')\nconst AddFilesPanel = require('.\/AddFilesPanel')\nconst PickerPanelContent = require('.\/PickerPanelContent')\nconst EditorPanel = require('.\/EditorPanel')\nconst PanelTopBar = require('.\/PickerPanelTopBar')\nconst FileCard = require('.\/FileCard')\nconst Slide = require('.\/Slide')\nconst isDragDropSupported = require('@uppy\/utils\/lib\/isDragDropSupported')\n\n\/\/ http:\/\/dev.edenspiekermann.com\/2016\/02\/11\/introducing-accessible-modal-dialog\n\/\/ https:\/\/github.com\/ghosh\/micromodal\n\nconst WIDTH_XL = 900\nconst WIDTH_LG = 700\nconst WIDTH_MD = 576\nconst HEIGHT_MD = 400\n\nmodule.exports = function Dashboard (props) {\n  const noFiles = props.totalFileCount === 0\n  const isSizeMD = props.containerWidth > WIDTH_MD\n\n  const wrapperClassName = classNames({\n    'uppy-Root': props.isTargetDOMEl,\n  })\n\n  const dashboardClassName = classNames({\n    'uppy-Dashboard': true,\n    'uppy-Dashboard--isDisabled': props.disabled,\n    'uppy-Dashboard--animateOpenClose': props.animateOpenClose,\n    'uppy-Dashboard--isClosing': props.isClosing,\n    'uppy-Dashboard--isDraggingOver': props.isDraggingOver,\n    'uppy-Dashboard--modal': !props.inline,\n    'uppy-size--md': props.containerWidth > WIDTH_MD,\n    'uppy-size--lg': props.containerWidth > WIDTH_LG,\n    'uppy-size--xl': props.containerWidth > WIDTH_XL,\n    'uppy-size--height-md': props.containerHeight > HEIGHT_MD,\n    'uppy-Dashboard--isAddFilesPanelVisible': props.showAddFilesPanel,\n    'uppy-Dashboard--isInnerWrapVisible': props.areInsidesReadyToBeVisible,\n  })\n\n  \/\/ Important: keep these in sync with the percent width values in `src\/components\/FileItem\/index.scss`.\n  let itemsPerRow = 1 \/\/ mobile\n  if (props.containerWidth > WIDTH_XL) {\n    itemsPerRow = 5\n  } else if (props.containerWidth > WIDTH_LG) {\n    itemsPerRow = 4\n  } else if (props.containerWidth > WIDTH_MD) {\n    itemsPerRow = 3\n  }\n\n  const showFileList = props.showSelectedFiles && !noFiles\n\n  const numberOfFilesForRecovery = props.recoveredState ? Object.keys(props.recoveredState.files).length : null\n  const numberOfGhosts = props.files ? Object.keys(props.files).filter((fileID) => props.files[fileID].isGhost).length : null\n\n  const renderRestoredText = () => {\n    if (numberOfGhosts > 0) {\n      return props.i18n('recoveredXFiles', {\n        smart_count: numberOfGhosts,\n      })\n    }\n\n    return props.i18n('recoveredAllFiles')\n  }\n\n  const dashboard = (\n    <div\n      className={dashboardClassName}\n      data-uppy-theme={props.theme}\n      data-uppy-num-acquirers={props.acquirers.length}\n      data-uppy-drag-drop-supported={!props.disableLocalFiles && isDragDropSupported()}\n      aria-hidden={props.inline ? 'false' : props.isHidden}\n      aria-disabled={props.disabled}\n      aria-label={!props.inline ? props.i18n('dashboardWindowTitle') : props.i18n('dashboardTitle')}\n      onPaste={props.handlePaste}\n      onDragOver={props.handleDragOver}\n      onDragLeave={props.handleDragLeave}\n      onDrop={props.handleDrop}\n    >\n      <div\n        className=\"uppy-Dashboard-overlay\"\n        tabIndex={-1}\n        onClick={props.handleClickOutside}\n      \/>\n\n      <div\n        className=\"uppy-Dashboard-inner\"\n        aria-modal={!props.inline && 'true'}\n        role={!props.inline && 'dialog'}\n        style={{\n          width: props.inline && props.width ? props.width : '',\n          height: props.inline && props.height ? props.height : '',\n        }}\n      >\n\n        {!props.inline ? (\n          <button\n            className=\"uppy-u-reset uppy-Dashboard-close\"\n            type=\"button\"\n            aria-label={props.i18n('closeModal')}\n            title={props.i18n('closeModal')}\n            onClick={props.closeModal}\n          >\n            <span aria-hidden=\"true\">&times;<\/span>\n          <\/button>\n        ) : null}\n\n        <div className=\"uppy-Dashboard-innerWrap\">\n          <div className=\"uppy-Dashboard-dropFilesHereHint\">\n            {props.i18n('dropHint')}\n          <\/div>\n\n          {showFileList && <PanelTopBar {...props} \/>}\n\n          {numberOfFilesForRecovery && (\n            <div className=\"uppy-Dashboard-serviceMsg\">\n              <svg className=\"uppy-Dashboard-serviceMsg-icon\" aria-hidden=\"true\" focusable=\"false\" width=\"21\" height=\"16\" viewBox=\"0 0 24 19\">\n                <g transform=\"translate(0 -1)\" fill=\"none\" fillRule=\"evenodd\">\n                  <path d=\"M12.857 1.43l10.234 17.056A1 1 0 0122.234 20H1.766a1 1 0 01-.857-1.514L11.143 1.429a1 1 0 011.714 0z\" fill=\"#FFD300\" \/>\n                  <path fill=\"#000\" d=\"M11 6h2l-.3 8h-1.4z\" \/>\n                  <circle fill=\"#000\" cx=\"12\" cy=\"17\" r=\"1\" \/>\n                <\/g>\n              <\/svg>\n              <strong className=\"uppy-Dashboard-serviceMsg-title\">\n                {props.i18n('sessionRestored')}\n              <\/strong>\n              <div class=\"uppy-Dashboard-serviceMsg-text\">\n                {renderRestoredText()}\n              <\/div>\n            <\/div>\n          )}\n\n          {showFileList ? (\n            <FileList\n              {...props}\n              itemsPerRow={itemsPerRow}\n            \/>\n          ) : (\n            <AddFiles {...props} isSizeMD={isSizeMD} \/>\n          )}\n\n          <Slide>\n            {props.showAddFilesPanel ? <AddFilesPanel key=\"AddFiles\" {...props} isSizeMD={isSizeMD} \/> : null}\n          <\/Slide>\n\n          <Slide>\n            {props.fileCardFor ? <FileCard key=\"FileCard\" {...props} \/> : null}\n          <\/Slide>\n\n          <Slide>\n            {props.activePickerPanel ? <PickerPanelContent key=\"Picker\" {...props} \/> : null}\n          <\/Slide>\n\n          <Slide>\n            {props.showFileEditor ? <EditorPanel key=\"Editor\" {...props} \/> : null}\n          <\/Slide>\n\n          <div className=\"uppy-Dashboard-progressindicators\">\n            {props.progressindicators.map((target) => {\n              return props.getPlugin(target.id).render(props.state)\n            })}\n          <\/div>\n        <\/div>\n      <\/div>\n    <\/div>\n  )\n\n  return (\n    \/\/ Wrap it for RTL language support\n    <div className={wrapperClassName} dir={props.direction}>\n      {dashboard}\n    <\/div>\n  )\n}\n","lang_cluster":"Javascript","length":177,"code_uid":"b96954263cd346e7bf422dd004a77eff"}
{"diff_hunk":"@@ -9,7 +9,9 @@ Page({\n     ],\n     fileList3: [{ url: 'https:\/\/img.yzcdn.cn\/vant\/sand.jpg' }],\n     fileList4: [],\n-    fileList5: []\n+    fileList5: [],\n+    fileList6: [],\n+    cloudPath: []\n   },\n \n   beforeRead(event) {","old_code":"import Page from '..\/..\/common\/page';\n\nPage({\n  data: {\n    fileList: [],\n    fileList2: [\n      { url: 'https:\/\/img.yzcdn.cn\/vant\/leaf.jpg' },\n      { url: 'https:\/\/img.yzcdn.cn\/vant\/tree.jpg' }\n    ],\n    fileList3: [{ url: 'https:\/\/img.yzcdn.cn\/vant\/sand.jpg' }],\n    fileList4: [],\n    fileList5: []\n  },\n\n  beforeRead(event) {\n    const { file, callback = () => {} } = event.detail;\n    if (file.path.indexOf('jpeg') < 0) {\n      wx.showToast({ title: '\u8bf7\u9009\u62e9jpg\u56fe\u7247\u4e0a\u4f20', icon: 'none' });\n      callback(false);\n      return;\n    }\n    callback(true);\n  },\n\n  afterRead(event) {\n    const { file, name } = event.detail;\n    const fileList = this.data[`fileList${name}`];\n\n    this.setData({ [`fileList${name}`]: fileList.concat(file) });\n  },\n\n  oversize() {\n    wx.showToast({ title: '\u6587\u4ef6\u8d85\u51fa\u5927\u5c0f\u9650\u5236', icon: 'none' });\n  },\n\n  delete(event) {\n    const { index, name } = event.detail;\n    const fileList = this.data[`fileList${name}`];\n    fileList.splice(index, 1);\n    this.setData({ [`fileList${name}`]: fileList });\n  },\n\n  clickPreview() {}\n});\n","lang_cluster":"Javascript","length":44,"code_uid":"838c3dfa55da421682b2b8844d6928f2"}
{"diff_hunk":"@@ -15,15 +15,18 @@ function PanelTopBar (props) {\n       <div class=\"uppy-DashboardContent-title\" role=\"heading\" aria-level=\"h1\">\n         <DashboardContentTitle {...props} \/>\n       <\/div>\n-      <button class=\"uppy-DashboardContent-addMore\"\n-        type=\"button\"\n-        aria-label={props.i18n('addMoreFiles')}\n-        title={props.i18n('addMoreFiles')}\n-        onclick={() => props.toggleAddFilesPanel(true)}>\n-        <svg class=\"UppyIcon\" width=\"15\" height=\"15\" viewBox=\"0 0 13 13\" version=\"1.1\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\">\n-          <path d=\"M7,6 L13,6 L13,7 L7,7 L7,13 L6,13 L6,7 L0,7 L0,6 L6,6 L6,0 L7,0 L7,6 Z\" \/>\n-        <\/svg>\n-      <\/button>\n+      { notOverFileLimit &&\n+        <button class=\"uppy-DashboardContent-addMore\"\n+          type=\"button\"\n+          aria-label={props.i18n('addMoreFiles')}\n+          title={props.i18n('addMoreFiles')}\n+          onclick={() => props.toggleAddFilesPanel(true)}>\n+          <svg class=\"UppyIcon\" width=\"15\" height=\"15\" viewBox=\"0 0 13 13\" version=\"1.1\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\">\n+            <path d=\"M7,6 L13,6 L13,7 L7,7 L7,13 L6,13 L6,7 L0,7 L0,6 L6,6 L6,0 L7,0 L7,6 Z\" \/>\n+          <\/svg>\n+        <\/button>\n+      }\n+\n     <\/div>\n   )\n }","old_code":"const { h } = require('preact')\n\nfunction DashboardContentTitle (props) {\n  if (props.newFiles.length) {\n    return props.i18n('xFilesSelected', { smart_count: props.newFiles.length })\n  }\n}\n\nfunction PanelTopBar (props) {\n  return (\n    <div class=\"uppy-DashboardContent-bar\">\n      <button class=\"uppy-DashboardContent-back\"\n        type=\"button\"\n        onclick={props.cancelAll}>{props.i18n('cancel')}<\/button>\n      <div class=\"uppy-DashboardContent-title\" role=\"heading\" aria-level=\"h1\">\n        <DashboardContentTitle {...props} \/>\n      <\/div>\n      <button class=\"uppy-DashboardContent-addMore\"\n        type=\"button\"\n        aria-label={props.i18n('addMoreFiles')}\n        title={props.i18n('addMoreFiles')}\n        onclick={() => props.toggleAddFilesPanel(true)}>\n        <svg class=\"UppyIcon\" width=\"15\" height=\"15\" viewBox=\"0 0 13 13\" version=\"1.1\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\">\n          <path d=\"M7,6 L13,6 L13,7 L7,7 L7,13 L6,13 L6,7 L0,7 L0,6 L6,6 L6,0 L7,0 L7,6 Z\" \/>\n        <\/svg>\n      <\/button>\n    <\/div>\n  )\n}\n\nmodule.exports = PanelTopBar\n","lang_cluster":"Javascript","length":31,"code_uid":"553ea9c3d72b4ff0952d45458b61263c"}
{"diff_hunk":"@@ -21,4 +21,4 @@ exports.createAdminUser = function () {\n             password\n         };\n     });\n-}\n+}","old_code":"'use strict';\nconst require_method = require;\nfunction node_require(module) {\n    return require_method(module);\n}\n\nconst Realm = node_require('realm');\n\nconst adminName = \"realm-admin\"\nconst password = '';\n\nexports.createAdminUser = function () {\n    const credentials = Realm.Sync.Credentials.usernamePassword(adminName, password);\n    return Realm.Sync.User.login('http:\/\/127.0.0.1:9080', credentials).then((user) => {\n        if (!user.isAdmin) {\n            throw new Error(`${adminName} user is not an admin user on this server`);\n        }\n\n        return {\n            username: adminName,\n            password\n        };\n    });\n}\n","lang_cluster":"Javascript","length":24,"code_uid":"d7a4cdfe878e4a3283c0f6a831910f70"}
{"diff_hunk":"@@ -52,9 +52,6 @@ function DashboardUniqueVisitorsWidget() {\n \t} = useSelect( ( select ) => {\n \t\tconst store = select( STORE_NAME );\n \n-\t\tconst accountID = store.getAccountID();\n-\t\tconst profileID = store.getProfileID();\n-\t\tconst internalWebPropertyID = store.getInternalWebPropertyID();\n \t\tconst commonArgs = {\n \t\t\tdateRange: select( CORE_USER ).getDateRange(),\n \t\t};","old_code":"\/**\n * DashboardAllTrafficWidget component.\n *\n * Site Kit by Google, Copyright 2020 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * WordPress dependencies\n *\/\nimport { __, _x } from '@wordpress\/i18n';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport { STORE_NAME } from '..\/..\/datastore\/constants';\nimport { STORE_NAME as CORE_SITE } from '..\/..\/..\/..\/googlesitekit\/datastore\/site\/constants';\nimport { STORE_NAME as CORE_USER } from '..\/..\/..\/..\/googlesitekit\/datastore\/user\/constants';\nimport whenActive from '..\/..\/..\/..\/util\/when-active';\nimport PreviewBlock from '..\/..\/..\/..\/components\/PreviewBlock';\nimport DataBlock from '..\/..\/..\/..\/components\/data-block';\nimport Sparkline from '..\/..\/..\/..\/components\/Sparkline';\nimport AnalyticsInactiveCTA from '..\/..\/..\/..\/components\/AnalyticsInactiveCTA';\nimport { changeToPercent, readableLargeNumber } from '..\/..\/..\/..\/util';\nimport ReportError from '..\/..\/..\/..\/components\/ReportError';\nimport ReportZero from '..\/..\/..\/..\/components\/ReportZero';\nimport parseDimensionStringToDate from '..\/..\/util\/parseDimensionStringToDate';\nimport applyEntityToReportPath from '..\/..\/util\/applyEntityToReportPath';\nimport { isZeroReport } from '..\/..\/util';\n\nconst { useSelect } = Data;\n\nfunction DashboardUniqueVisitorsWidget() {\n\tconst {\n\t\tloading,\n\t\terror,\n\t\tsparkData,\n\t\tserviceURL,\n\t\tvisitorsData,\n\t} = useSelect( ( select ) => {\n\t\tconst store = select( STORE_NAME );\n\n\t\tconst accountID = store.getAccountID();\n\t\tconst profileID = store.getProfileID();\n\t\tconst internalWebPropertyID = store.getInternalWebPropertyID();\n\t\tconst commonArgs = {\n\t\t\tdateRange: select( CORE_USER ).getDateRange(),\n\t\t};\n\n\t\tconst url = select( CORE_SITE ).getCurrentEntityURL();\n\t\tif ( url ) {\n\t\t\tcommonArgs.url = url;\n\t\t}\n\t\tconst sparklineArgs = {\n\t\t\tdimensions: 'ga:date',\n\t\t\tmetrics: [\n\t\t\t\t{\n\t\t\t\t\texpression: 'ga:users',\n\t\t\t\t\talias: 'Users',\n\t\t\t\t},\n\t\t\t],\n\t\t\t...commonArgs,\n\t\t};\n\n\t\t\/\/ This request needs to be separate from the sparkline request because it would result in a different total if it included the ga:date dimension.\n\t\tconst args = {\n\t\t\tmultiDateRange: 1,\n\t\t\tmetrics: [\n\t\t\t\t{\n\t\t\t\t\texpression: 'ga:users',\n\t\t\t\t\talias: 'Total Users',\n\t\t\t\t},\n\t\t\t],\n\t\t\t...commonArgs,\n\t\t};\n\n\t\treturn {\n\t\t\tloading: ! store.hasFinishedResolution( 'getReport', [ sparklineArgs ] ) || ! store.hasFinishedResolution( 'getReport', [ args ] ),\n\t\t\terror: store.getErrorForSelector( 'getReport', [ sparklineArgs ] ) || store.getErrorForSelector( 'getReport', [ args ] ),\n\t\t\t\/\/ Due to the nature of these queries, we need to run them separately.\n\t\t\tsparkData: store.getReport( sparklineArgs ),\n\t\t\tserviceURL: store.getServiceURL(\n\t\t\t\t{\n\t\t\t\t\tpath: applyEntityToReportPath( url, `\/report\/visitors-overview\/a${ accountID }w${ internalWebPropertyID }p${ profileID }\/` ),\n\t\t\t\t}\n\t\t\t),\n\t\t\tvisitorsData: store.getReport( args ),\n\t\t};\n\t} );\n\n\tif ( loading ) {\n\t\treturn <PreviewBlock width=\"100%\" height=\"202px\" \/>;\n\t}\n\n\tif ( error ) {\n\t\treturn <ReportError moduleSlug=\"analytics\" error={ error } \/>;\n\t}\n\n\tif ( isZeroReport( sparkData ) || isZeroReport( visitorsData ) ) {\n\t\treturn <ReportZero moduleSlug=\"analytics\" \/>;\n\t}\n\n\tconst sparkLineData = [\n\t\t[\n\t\t\t{ type: 'date', label: 'Day' },\n\t\t\t{ type: 'number', label: 'Bounce Rate' },\n\t\t],\n\t];\n\tconst dataRows = sparkData[ 0 ].data.rows;\n\n\t\/\/ Loop the rows to build the chart data.\n\tfor ( let i = 0; i < dataRows.length; i++ ) {\n\t\tconst { values } = dataRows[ i ].metrics[ 0 ];\n\t\tconst dateString = dataRows[ i ].dimensions[ 0 ];\n\t\tconst date = parseDimensionStringToDate( dateString );\n\t\tsparkLineData.push( [\n\t\t\tdate,\n\t\t\tvalues[ 0 ],\n\t\t] );\n\t}\n\n\tconst { totals } = visitorsData[ 0 ].data;\n\tconst totalUsers = totals[ 0 ].values;\n\tconst previousTotalUsers = totals[ 1 ].values;\n\tconst totalUsersChange = changeToPercent( previousTotalUsers, totalUsers );\n\n\treturn (\n\t\t<DataBlock\n\t\t\tclassName=\"overview-total-users\"\n\t\t\ttitle={ __( 'Unique Visitors', 'google-site-kit' ) }\n\t\t\tdatapoint={ readableLargeNumber( totalUsers ) }\n\t\t\tchange={ totalUsersChange }\n\t\t\tchangeDataUnit=\"%\"\n\t\t\tsource={ {\n\t\t\t\tname: _x( 'Analytics', 'Service name', 'google-site-kit' ),\n\t\t\t\tlink: serviceURL,\n\t\t\t\texternal: true,\n\t\t\t} }\n\t\t\tsparkline={\n\t\t\t\tsparkLineData &&\n\t\t\t\t\t<Sparkline\n\t\t\t\t\t\tdata={ sparkLineData }\n\t\t\t\t\t\tchange={ totalUsersChange }\n\t\t\t\t\t\/>\n\t\t\t}\n\t\t\/>\n\t);\n}\n\nexport default whenActive( {\n\tmoduleName: 'analytics',\n\tfallbackComponent: AnalyticsInactiveCTA,\n} )( DashboardUniqueVisitorsWidget );\n","lang_cluster":"Javascript","length":165,"code_uid":"ee1ded20b6994c5cb75f34ad78815e11"}
{"diff_hunk":"@@ -23,9 +23,4 @@ describe('truncateString', () => {\n     expect(truncateString('hello world', 100)).toEqual('hello world')\n     expect(truncateString('hello world', 11)).toEqual('hello world')\n   })\n-\n-  it('should not truncate the string if it is too short to be meaningfully truncated', () => {\n-    expect(truncateString('abc', 2)).toEqual('ab')\n-    expect(truncateString('abc', 1)).toEqual('a')\n-  })\n })","old_code":"const truncateString = require('.\/truncateString')\n\ndescribe('truncateString', () => {\n  it('should truncate the string to the length', () => {\n    expect(truncateString('abcdefghijkl', 14)).toEqual('abcdefghijkl')\n    expect(truncateString('abcdefghijkl', 13)).toEqual('abcdefghijkl')\n    expect(truncateString('abcdefghijkl', 12)).toEqual('abcdefghijkl')\n    expect(truncateString('abcdefghijkl', 11)).toEqual('abcd...ijkl')\n    expect(truncateString('abcdefghijkl', 10)).toEqual('abcd...jkl')\n    expect(truncateString('abcdefghijkl', 9)).toEqual('abc...jkl')\n    expect(truncateString('abcdefghijkl', 8)).toEqual('abc...kl')\n    expect(truncateString('abcdefghijkl', 7)).toEqual('ab...kl')\n    expect(truncateString('abcdefghijkl', 6)).toEqual('ab...l')\n    expect(truncateString('abcdefghijkl', 5)).toEqual('a...l')\n    expect(truncateString('abcdefghijkl', 4)).toEqual('a...')\n    expect(truncateString('abcdefghijkl', 3)).toEqual('abc')\n    expect(truncateString('abcdefghijkl', 2)).toEqual('ab')\n    expect(truncateString('abcdefghijkl', 1)).toEqual('a')\n    expect(truncateString('abcdefghijkl', 0)).toEqual('')\n  })\n\n  it('should not truncate the string if it is already short enough', () => {\n    expect(truncateString('hello world', 100)).toEqual('hello world')\n    expect(truncateString('hello world', 11)).toEqual('hello world')\n  })\n\n  it('should not truncate the string if it is too short to be meaningfully truncated', () => {\n    expect(truncateString('abc', 2)).toEqual('ab')\n    expect(truncateString('abc', 1)).toEqual('a')\n  })\n})\n","lang_cluster":"Javascript","length":31,"code_uid":"c59e8a72e58a4d3a9c1911c1f6ffd9d9"}
{"diff_hunk":"@@ -14,7 +14,7 @@ import localHooks from '.\/..\/mixins\/localHooks';\n class Transformation {\n   constructor(range, options) {\n     \/**\n-     * Instance of the SelectionRange, holder for coordinates applied to the table.\n+     * Instance of the SelectionRange, holder for visual coordinates applied to the table.\n      *\n      * @type {SelectionRange}\n      *\/","old_code":"import { CellCoords } from '.\/..\/3rdparty\/walkontable\/src';\nimport { mixin } from '.\/..\/helpers\/object';\nimport localHooks from '.\/..\/mixins\/localHooks';\n\n\/**\n * The Transformation class implements algorithms for transforming coordinates based on current settings\n * passed to the Handsontable.\n *\n * Transformation is always applied relative to the current selection.\n *\n * @class Transformation\n * @util\n *\/\nclass Transformation {\n  constructor(range, options) {\n    \/**\n     * Instance of the SelectionRange, holder for coordinates applied to the table.\n     *\n     * @type {SelectionRange}\n     *\/\n    this.range = range;\n    \/**\n     * Additional options which define the state of the settings which can infer transformation.\n     *\n     * @type {Object}\n     *\/\n    this.options = options;\n  }\n\n  \/**\n   * Selects cell relative to current cell (if possible).\n   *\n   * @param {Number} rowDelta Rows number to move, value can be passed as negative number.\n   * @param {Number} colDelta Columns number to move, value can be passed as negative number.\n   * @param {Boolean} force If `true` the new rows\/columns will be created if necessary. Otherwise, row\/column will\n   *                        be created according to `minSpareRows\/minSpareCols` settings of Handsontable.\n   * @returns {CellCoords}\n   *\/\n  transformStart(rowDelta, colDelta, force) {\n    const delta = new CellCoords(rowDelta, colDelta);\n\n    this.runLocalHooks('beforeTransformStart', delta);\n\n    let totalRows = this.options.countRows();\n    let totalCols = this.options.countCols();\n    const fixedRowsBottom = this.options.fixedRowsBottom();\n    const minSpareRows = this.options.minSpareRows();\n    const minSpareCols = this.options.minSpareCols();\n    const autoWrapRow = this.options.autoWrapRow();\n    const autoWrapCol = this.options.autoWrapCol();\n    const highlightCoords = this.range.current().highlight;\n\n    if (highlightCoords.row + rowDelta > totalRows - 1) {\n      if (force && minSpareRows > 0 && !(fixedRowsBottom && highlightCoords.row >= totalRows - fixedRowsBottom - 1)) {\n        this.runLocalHooks('insertRowRequire', totalRows);\n        totalRows = this.options.countRows();\n\n      } else if (autoWrapCol) {\n        delta.row = 1 - totalRows;\n        delta.col = highlightCoords.col + delta.col === totalCols - 1 ? 1 - totalCols : 1;\n      }\n    } else if (autoWrapCol && highlightCoords.row + delta.row < 0 && highlightCoords.col + delta.col >= 0) {\n      delta.row = totalRows - 1;\n      delta.col = highlightCoords.col + delta.col === 0 ? totalCols - 1 : -1;\n    }\n\n    if (highlightCoords.col + delta.col > totalCols - 1) {\n      if (force && minSpareCols > 0) {\n        this.runLocalHooks('insertColRequire', totalCols);\n        totalCols = this.options.countCols();\n\n      } else if (autoWrapRow) {\n        delta.row = highlightCoords.row + delta.row === totalRows - 1 ? 1 - totalRows : 1;\n        delta.col = 1 - totalCols;\n      }\n    } else if (autoWrapRow && highlightCoords.col + delta.col < 0 && highlightCoords.row + delta.row >= 0) {\n      delta.row = highlightCoords.row + delta.row === 0 ? totalRows - 1 : -1;\n      delta.col = totalCols - 1;\n    }\n\n    const coords = new CellCoords(highlightCoords.row + delta.row, highlightCoords.col + delta.col);\n    let rowTransformDir = 0;\n    let colTransformDir = 0;\n\n    if (coords.row < 0) {\n      rowTransformDir = -1;\n      coords.row = 0;\n\n    } else if (coords.row > 0 && coords.row >= totalRows) {\n      rowTransformDir = 1;\n      coords.row = totalRows - 1;\n    }\n\n    if (coords.col < 0) {\n      colTransformDir = -1;\n      coords.col = 0;\n\n    } else if (coords.col > 0 && coords.col >= totalCols) {\n      colTransformDir = 1;\n      coords.col = totalCols - 1;\n    }\n    this.runLocalHooks('afterTransformStart', coords, rowTransformDir, colTransformDir);\n\n    return coords;\n  }\n\n  \/**\n   * Sets selection end cell relative to current selection end cell (if possible).\n   *\n   * @param {Number} rowDelta Rows number to move, value can be passed as negative number.\n   * @param {Number} colDelta Columns number to move, value can be passed as negative number.\n   * @returns {CellCoords}\n   *\/\n  transformEnd(rowDelta, colDelta) {\n    const delta = new CellCoords(rowDelta, colDelta);\n\n    this.runLocalHooks('beforeTransformEnd', delta);\n\n    const totalRows = this.options.countRows();\n    const totalCols = this.options.countCols();\n    const cellRange = this.range.current();\n    const coords = new CellCoords(cellRange.to.row + delta.row, cellRange.to.col + delta.col);\n    let rowTransformDir = 0;\n    let colTransformDir = 0;\n\n    if (coords.row < 0) {\n      rowTransformDir = -1;\n      coords.row = 0;\n\n    } else if (coords.row > 0 && coords.row >= totalRows) {\n      rowTransformDir = 1;\n      coords.row = totalRows - 1;\n    }\n\n    if (coords.col < 0) {\n      colTransformDir = -1;\n      coords.col = 0;\n\n    } else if (coords.col > 0 && coords.col >= totalCols) {\n      colTransformDir = 1;\n      coords.col = totalCols - 1;\n    }\n    this.runLocalHooks('afterTransformEnd', coords, rowTransformDir, colTransformDir);\n\n    return coords;\n  }\n}\n\nmixin(Transformation, localHooks);\n\nexport default Transformation;\n","lang_cluster":"Javascript","length":151,"code_uid":"a34cc0fe27ff4aed81ffc5fe48b60c44"}
{"diff_hunk":"@@ -60,6 +60,42 @@ if (process.env.NODE_ENV !== 'production') {\n \t\t\t'onRenderControls',\n \t\t\tcreateDocUrl('onRenderControls')\n \t\t);\n+\n+\t\tif (props.onRenderActions) {\n+\t\t\trenderFunctionReturnContentsLackDisplayName(\n+\t\t\t\tCOMPONENT,\n+\t\t\t\t'onRenderActions',\n+\t\t\t\tprops.onRenderActions(),\n+\t\t\t\tPAGE_HEADER_CONTROL,\n+\t\t\t\ttrue\n+\t\t\t);\n+\t\t} else if (props.contentRight) {\n+\t\t\trenderFunctionReturnContentsLackDisplayName(\n+\t\t\t\tCOMPONENT,\n+\t\t\t\t'contentRight',\n+\t\t\t\tprops.contentRight,\n+\t\t\t\tPAGE_HEADER_CONTROL,\n+\t\t\t\ttrue\n+\t\t\t);\n+\t\t}\n+\n+\t\tif (props.onRenderControls) {\n+\t\t\trenderFunctionReturnContentsLackDisplayName(\n+\t\t\t\tCOMPONENT,\n+\t\t\t\t'onRenderControls',\n+\t\t\t\tprops.onRenderControls(),\n+\t\t\t\tPAGE_HEADER_CONTROL,\n+\t\t\t\ttrue\n+\t\t\t);\n+\t\t} else if (props.navRight) {\n+\t\t\trenderFunctionReturnContentsLackDisplayName(\n+\t\t\t\tCOMPONENT,\n+\t\t\t\t'navRight',\n+\t\t\t\tprops.navRight,\n+\t\t\t\tPAGE_HEADER_CONTROL,\n+\t\t\t\ttrue\n+\t\t\t);\n+\t\t}\n \t};\n }\n ","old_code":"\/* Copyright (c) 2015-present, salesforce.com, inc. All rights reserved *\/\n\/* Licensed under BSD 3-Clause - see LICENSE.txt or git.io\/sfdc-license *\/\n\/* eslint-disable import\/no-mutable-exports *\/\n\nimport deprecatedPropertyValue from '..\/..\/utilities\/warning\/deprecated-property-value';\nimport deprecatedProperty from '..\/..\/utilities\/warning\/deprecated-property';\nimport getComponentDocFn from '..\/..\/utilities\/get-component-doc';\n\nlet checkProps = function checkPropsFunction() {};\n\nif (process.env.NODE_ENV !== 'production') {\n\tcheckProps = function checkPropsFunction(COMPONENT, props, jsonDoc) {\n\t\tconst createDocUrl = getComponentDocFn(jsonDoc);\n\t\tif (props.variant === 'objectHome') {\n\t\t\tdeprecatedPropertyValue(\n\t\t\t\tCOMPONENT,\n\t\t\t\t{\n\t\t\t\t\tpropAsString: 'variant',\n\t\t\t\t\tpropValue: props.variant,\n\t\t\t\t\tdeprecatedPropValue: 'objectHome',\n\t\t\t\t\treplacementPropAsValue: 'object-home',\n\t\t\t\t},\n\t\t\t\t`Using value of variants in camelCase is deprecated. Use kebab-case ('object-home') instead. ${createDocUrl(\n\t\t\t\t\t'variant'\n\t\t\t\t)}`\n\t\t\t);\n\t\t}\n\t\tif (props.variant === 'recordHome') {\n\t\t\tdeprecatedPropertyValue(\n\t\t\t\tCOMPONENT,\n\t\t\t\t{\n\t\t\t\t\tpropAsString: 'variant',\n\t\t\t\t\tpropValue: props.variant,\n\t\t\t\t\tdeprecatedPropValue: 'recordHome',\n\t\t\t\t\treplacementPropAsValue: 'record-home',\n\t\t\t\t},\n\t\t\t\t`Using value of variants in camelCase is deprecated. Use kebab-case ('record-home') instead. ${createDocUrl(\n\t\t\t\t\t'variant'\n\t\t\t\t)}`\n\t\t\t);\n\t\t}\n\t\tif (props.variant === 'relatedList') {\n\t\t\tdeprecatedPropertyValue(\n\t\t\t\tCOMPONENT,\n\t\t\t\t{\n\t\t\t\t\tpropAsString: 'variant',\n\t\t\t\t\tpropValue: props.variant,\n\t\t\t\t\tdeprecatedPropValue: 'relatedList',\n\t\t\t\t\treplacementPropAsValue: 'related-list',\n\t\t\t\t},\n\t\t\t\t`Using value of variants in camelCase is deprecated. Use kebab-case ('related-list') instead. ${createDocUrl(\n\t\t\t\t\t'variant'\n\t\t\t\t)}`\n\t\t\t);\n\t\t}\n\t\tdeprecatedProperty(\n\t\t\tCOMPONENT,\n\t\t\tprops.navRight,\n\t\t\t'navRight',\n\t\t\t'onRenderControls',\n\t\t\tcreateDocUrl('onRenderControls')\n\t\t);\n\t};\n}\n\nexport default checkProps;\n","lang_cluster":"Javascript","length":66,"code_uid":"666e569622fc427c9ad27ebc09430bdb"}
{"diff_hunk":"@@ -129,6 +129,10 @@ beforeAll( async() => {\n \tcapturePageEventsForTearDown();\n \tenablePageDialogAccept();\n \tobserveConsoleLogging();\n+\tif ( process.env.DEBUG_REST ) {\n+\t\tpage.on( 'request', observeRestRequest );\n+\t\tpage.on( 'response', observeRestResponse );\n+\t}\n \tawait setBrowserViewport( 'large' );\n } );\n ","old_code":"\/**\n * External dependencies\n *\/\nimport { get } from 'lodash';\n\n\/**\n * WordPress dependencies\n *\/\nimport {\n\tclearLocalStorage,\n\tenablePageDialogAccept,\n\tsetBrowserViewport,\n} from '@wordpress\/e2e-test-utils';\n\n\/**\n * Environment variables\n *\/\nconst { PUPPETEER_TIMEOUT } = process.env;\n\n\/**\n * Set of console logging types observed to protect against unexpected yet\n * handled (i.e. not catastrophic) errors or warnings. Each key corresponds\n * to the Puppeteer ConsoleMessage type, its value the corresponding function\n * on the console global object.\n *\n * @type {Object<string,string>}\n *\/\nconst OBSERVED_CONSOLE_MESSAGE_TYPES = {\n\twarning: 'warn',\n\terror: 'error',\n};\n\n\/**\n * Array of page event tuples of [ eventName, handler ].\n *\n * @type {Array}\n *\/\nconst pageEvents = [];\n\n\/\/ The Jest timeout is increased because these tests are a bit slow\njest.setTimeout( PUPPETEER_TIMEOUT || 100000 );\n\n\/**\n * Adds an event listener to the page to handle additions of page event\n * handlers, to assure that they are removed at test teardown.\n *\/\nfunction capturePageEventsForTearDown() {\n\tpage.on( 'newListener', ( eventName, listener ) => {\n\t\tpageEvents.push( [ eventName, listener ] );\n\t} );\n}\n\n\/**\n * Removes all bound page event handlers.\n *\/\nfunction removePageEvents() {\n\tpageEvents.forEach( ( [ eventName, handler ] ) => {\n\t\tpage.removeListener( eventName, handler );\n\t} );\n}\n\n\/**\n * Adds a page event handler to emit uncaught exception to process if one of\n * the observed console logging types is encountered.\n *\/\nfunction observeConsoleLogging() {\n\tpage.on( 'console', ( message ) => {\n\t\tconst type = message.type();\n\t\tif ( ! OBSERVED_CONSOLE_MESSAGE_TYPES.hasOwnProperty( type ) ) {\n\t\t\treturn;\n\t\t}\n\n\t\tlet text = message.text();\n\n\t\t\/\/ An exception is made for _blanket_ deprecation warnings: Those\n\t\t\/\/ which log regardless of whether a deprecated feature is in use.\n\t\tif ( text.includes( 'This is a global warning' ) ) {\n\t\t\treturn;\n\t\t}\n\n\t\t\/\/ Viewing posts on the front end can result in this error, which\n\t\t\/\/ has nothing to do with Gutenberg.\n\t\tif ( text.includes( 'net::ERR_UNKNOWN_URL_SCHEME' ) ) {\n\t\t\treturn;\n\t\t}\n\n\t\t\/\/ A bug present in WordPress 5.2 will produce console warnings when\n\t\t\/\/ loading the Dashicons font. These can be safely ignored, as they do\n\t\t\/\/ not otherwise regress on application behavior. This logic should be\n\t\t\/\/ removed once the associated ticket has been closed.\n\t\t\/\/\n\t\t\/\/ See: https:\/\/core.trac.wordpress.org\/ticket\/47183\n\t\tif (\n\t\t\ttext.startsWith( 'Failed to decode downloaded font:' ) ||\n\t\t\ttext.startsWith( 'OTS parsing error:' )\n\t\t) {\n\t\t\treturn;\n\t\t}\n\n\t\tconst logFunction = OBSERVED_CONSOLE_MESSAGE_TYPES[ type ];\n\n\t\t\/\/ As of Puppeteer 1.6.1, `message.text()` wrongly returns an object of\n\t\t\/\/ type JSHandle for error logging, instead of the expected string.\n\t\t\/\/\n\t\t\/\/ See: https:\/\/github.com\/GoogleChrome\/puppeteer\/issues\/3397\n\t\t\/\/\n\t\t\/\/ The recommendation there to asynchronously resolve the error value\n\t\t\/\/ upon a console event may be prone to a race condition with the test\n\t\t\/\/ completion, leaving a possibility of an error not being surfaced\n\t\t\/\/ correctly. Instead, the logic here synchronously inspects the\n\t\t\/\/ internal object shape of the JSHandle to find the error text. If it\n\t\t\/\/ cannot be found, the default text value is used instead.\n\t\ttext = get( message.args(), [ 0, '_remoteObject', 'description' ], text );\n\n\t\t\/\/ Disable reason: We intentionally bubble up the console message\n\t\t\/\/ which, unless the test explicitly anticipates the logging via\n\t\t\/\/ @wordpress\/jest-console matchers, will cause the intended test\n\t\t\/\/ failure.\n\n\t\t\/\/ eslint-disable-next-line no-console\n\t\tconsole[ logFunction ]( text );\n\t} );\n}\n\n\/\/ Before every test suite run, delete all content created by the test. This ensures\n\/\/ other posts\/comments\/etc. aren't dirtying tests and tests don't depend on\n\/\/ each other's side-effects.\nbeforeAll( async() => {\n\tcapturePageEventsForTearDown();\n\tenablePageDialogAccept();\n\tobserveConsoleLogging();\n\tawait setBrowserViewport( 'large' );\n} );\n\nafterEach( async() => {\n\tawait clearLocalStorage();\n\tawait setBrowserViewport( 'large' );\n} );\n\nafterAll( () => {\n\tremovePageEvents();\n} );\n","lang_cluster":"Javascript","length":142,"code_uid":"6a31f2e030d544d98a8669dba63aca5a"}
{"diff_hunk":"@@ -95,4 +95,5 @@ export default function ModulePopularPagesWidget( { Widget, WidgetReportError }\n ModulePopularPagesWidget.propTypes = {\n \tWidget: PropTypes.elementType.isRequired,\n \tWidgetReportError: PropTypes.elementType.isRequired,\n+\tWidgetReportZero: PropTypes.elementType.isRequired,\n };","old_code":"\/**\n * ModulePopularPagesWidget component.\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * External dependencies\n *\/\nimport PropTypes from 'prop-types';\n\n\/**\n * Internal dependencies\n *\/\nimport Data from 'googlesitekit-data';\nimport { CORE_USER } from '..\/..\/..\/..\/..\/googlesitekit\/datastore\/user\/constants';\nimport { DATE_RANGE_OFFSET, MODULES_ANALYTICS } from '..\/..\/..\/datastore\/constants';\nimport PreviewTable from '..\/..\/..\/..\/..\/components\/PreviewTable';\nimport Header from '.\/Header';\nimport Table from '.\/Table';\nimport Footer from '.\/Footer';\nconst { useSelect } = Data;\n\nexport default function ModulePopularPagesWidget( { Widget, WidgetReportError } ) {\n\tconst { startDate, endDate } = useSelect( ( select ) => select( CORE_USER ).getDateRangeDates( {\n\t\toffsetDays: DATE_RANGE_OFFSET,\n\t} ) );\n\n\tconst args = {\n\t\tstartDate,\n\t\tendDate,\n\t\tdimensions: [\n\t\t\t'ga:pageTitle',\n\t\t\t'ga:pagePath',\n\t\t],\n\t\tmetrics: [\n\t\t\t{\n\t\t\t\texpression: 'ga:pageviews',\n\t\t\t\talias: 'Pageviews',\n\t\t\t},\n\t\t\t{\n\t\t\t\texpression: 'ga:uniquePageviews',\n\t\t\t\talias: 'Unique Pageviews',\n\t\t\t},\n\t\t\t{\n\t\t\t\texpression: 'ga:bounceRate',\n\t\t\t\talias: 'Bounce rate',\n\t\t\t},\n\t\t],\n\t\torderby: [\n\t\t\t{\n\t\t\t\tfieldName: 'ga:pageviews',\n\t\t\t\tsortOrder: 'DESCENDING',\n\t\t\t},\n\t\t],\n\t\tlimit: 10,\n\t};\n\n\tconst report = useSelect( ( select ) => select( MODULES_ANALYTICS ).getReport( args ) );\n\tconst loaded = useSelect( ( select ) => select( MODULES_ANALYTICS ).hasFinishedResolution( 'getReport', [ args ] ) );\n\tconst error = useSelect( ( select ) => select( MODULES_ANALYTICS ).getErrorForSelector( 'getReport', [ args ] ) );\n\n\tif ( error ) {\n\t\treturn <WidgetReportError error={ error } \/>;\n\t}\n\n\treturn (\n\t\t<Widget\n\t\t\tHeader={ Header }\n\t\t\tFooter={ Footer }\n\t\t\tnoPadding\n\t\t>\n\t\t\t{ ! loaded && (\n\t\t\t\t<PreviewTable padding \/>\n\t\t\t) }\n\t\t\t{ loaded && (\n\t\t\t\t<Table report={ report } \/>\n\t\t\t) }\n\t\t<\/Widget>\n\t);\n}\n\nModulePopularPagesWidget.propTypes = {\n\tWidget: PropTypes.elementType.isRequired,\n\tWidgetReportError: PropTypes.elementType.isRequired,\n};\n","lang_cluster":"Javascript","length":98,"code_uid":"176ca03f640b4b66b7525849b9503d2f"}
{"diff_hunk":"@@ -82,7 +82,7 @@ class Router implements \\Laminas\\Log\\LoggerAwareInterface\n      * @return string|bool\n      *\/\n     public function getUrl(RecordDriver $driver, $size = 'small',\n-        $resolveDynamic = true\n+        $resolveDynamic = true, $testLoadImage = false\n     ) {\n         \/\/ Try to build thumbnail:\n         $thumb = $driver->tryMethod('getThumbnail', [$size]);","old_code":"<?php\n\/**\n * Cover image router\n *\n * PHP version 7\n *\n * Copyright (C) Villanova University 2016.\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License version 2,\n * as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\n *\n * @category VuFind\n * @package  Cover_Generator\n * @author   Demian Katz <demian.katz@villanova.edu>\n * @license  http:\/\/opensource.org\/licenses\/gpl-2.0.php GNU General Public License\n * @link     https:\/\/vufind.org\/wiki\/configuration:external_content Wiki\n *\/\nnamespace VuFind\\Cover;\n\nuse VuFind\\Cover\\Loader as CoverLoader;\nuse VuFind\\RecordDriver\\AbstractBase as RecordDriver;\n\n\/**\n * Cover image router\n *\n * @category VuFind\n * @package  Cover_Generator\n * @author   Demian Katz <demian.katz@villanova.edu>\n * @license  http:\/\/opensource.org\/licenses\/gpl-2.0.php GNU General Public License\n * @link     https:\/\/vufind.org\/wiki\/configuration:external_content Wiki\n *\/\nclass Router implements \\Laminas\\Log\\LoggerAwareInterface\n{\n    use \\VuFind\\Log\\LoggerAwareTrait;\n\n    \/**\n     * Base URL for dynamic cover images.\n     *\n     * @var string\n     *\/\n    protected $dynamicUrl;\n\n    \/**\n     * Cover loader\n     *\n     * @var CoverLoader\n     *\/\n    protected $coverLoader;\n\n    \/**\n     * Constructor\n     *\n     * @param string      $url         Base URL for dynamic cover images.\n     * @param CoverLoader $coverLoader Cover loader\n     *\/\n    public function __construct($url, CoverLoader $coverLoader)\n    {\n        $this->dynamicUrl = $url;\n        $this->coverLoader = $coverLoader;\n    }\n\n    \/**\n     * Generate a thumbnail URL (return false if unsupported; return null to indicate\n     * that a subsequent AJAX check is needed).\n     *\n     * @param RecordDriver $driver         Record driver\n     * @param string       $size           Size of thumbnail (small, medium or large;\n     * small is default).\n     * @param bool         $resolveDynamic Should we resolve dynamic cover data into\n     * a URL (true) or simply return false (false)?\n     *\n     * @return string|bool\n     *\/\n    public function getUrl(RecordDriver $driver, $size = 'small',\n        $resolveDynamic = true\n    ) {\n        \/\/ Try to build thumbnail:\n        $thumb = $driver->tryMethod('getThumbnail', [$size]);\n\n        \/\/ No thumbnail?  Return false:\n        if (empty($thumb)) {\n            return false;\n        }\n\n        \/\/ Array? It's parameters to send to the cover generator:\n        if (is_array($thumb)) {\n            if (!$resolveDynamic) {\n                return null;\n            }\n            $dynamicUrl =  $this->dynamicUrl . '?' . http_build_query($thumb);\n        } else {\n            return $thumb;\n        }\n\n        $settings = is_array($thumb) ? array_merge($thumb, ['size' => $size])\n            : ['size' => $size];\n        $handlers = $this->coverLoader->getHandlers();\n        $ids = $this->coverLoader->getIdentifiersForSettings($settings);\n\n        foreach ($handlers as $handler) {\n            try {\n                \/\/ Is the current provider appropriate for the available data?\n                if ($handler['handler']->supports($ids)\n                    && $handler['handler']->useDirectUrls()\n                ) {\n                    $nextDirectUrl = $handler['handler']\n                        ->getUrl($handler['key'], $size, $ids);\n                    if ($nextDirectUrl !== false) {\n                        $directUrl = $nextDirectUrl;\n                        break;\n                    }\n                }\n            } catch (\\Exception $e) {\n                $this->debug(\n                    get_class($e) . ' during processing of '\n                    . get_class($handler['handler']) . ': ' . $e->getMessage()\n                );\n            }\n        }\n        return $directUrl ?? $dynamicUrl ?? false;\n    }\n}\n","lang_cluster":"PHP","length":132,"code_uid":"9160b7e5f8ad4557aba6f43dd97b0e14"}
{"diff_hunk":"@@ -29,7 +29,7 @@ return [\n     |\n     *\/\n \n-    'lifetime' => 120,\n+    'lifetime' => env('SESSION_LIFETIME', 120),\n \n     'expire_on_close' => false,\n ","old_code":"<?php\n\nreturn [\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Default Session Driver\n    |--------------------------------------------------------------------------\n    |\n    | This option controls the default session \"driver\" that will be used on\n    | requests. By default, we will use the lightweight native driver but\n    | you may specify any of the other wonderful drivers provided here.\n    |\n    | Supported: \"file\", \"cookie\", \"database\", \"apc\",\n    |            \"memcached\", \"redis\", \"array\"\n    |\n    *\/\n\n    'driver' => 'file',\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Session Lifetime\n    |--------------------------------------------------------------------------\n    |\n    | Here you may specify the number of minutes that you wish the session\n    | to be allowed to remain idle for it is expired. If you want them\n    | to immediately expire when the browser closes, set it to zero.\n    |\n    *\/\n\n    'lifetime' => 120,\n\n    'expire_on_close' => false,\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Session Encryption\n    |--------------------------------------------------------------------------\n    |\n    | This option allows you to easily specify that all of your session data\n    | should be encrypted before it is stored. All encryption will be run\n    | automatically by Laravel and you can use the Session like normal.\n    |\n    *\/\n\n    'encrypt' => false,\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Session File Location\n    |--------------------------------------------------------------------------\n    |\n    | When using the native session driver, we need a location where session\n    | files may be stored. A default has been set for you but a different\n    | location may be specified. This is only needed for file sessions.\n    |\n    *\/\n\n    'files' => storage_path('framework\/sessions'),\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Session Database Connection\n    |--------------------------------------------------------------------------\n    |\n    | When using the \"database\" session driver, you may specify the database\n    | connection that should be used to manage your sessions. This should\n    | correspond to a connection in your \"database\" configuration file.\n    |\n    *\/\n\n    'connection' => null,\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Session Database Table\n    |--------------------------------------------------------------------------\n    |\n    | When using the \"database\" session driver, you may specify the table we\n    | should use to manage the sessions. Of course, a sensible default is\n    | provided for you; however, you are free to change this as needed.\n    |\n    *\/\n\n    'table' => 'sessions',\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Session Sweeping Lottery\n    |--------------------------------------------------------------------------\n    |\n    | Some session drivers must manually sweep their storage location to get\n    | rid of old sessions from storage. Here are the chances that it will\n    | happen on a given request. By default, the odds are 2 out of 100.\n    |\n    *\/\n\n    'lottery' => [2, 100],\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Session Cookie Name\n    |--------------------------------------------------------------------------\n    |\n    | Here you may change the name of the cookie used to identify a session\n    | instance by ID. The name specified here will get used every time a\n    | new session cookie is created by the framework for every driver.\n    |\n    *\/\n\n    'cookie' => 'october_session',\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Session Cookie Path\n    |--------------------------------------------------------------------------\n    |\n    | The session cookie path determines the path for which the cookie will\n    | be regarded as available. Typically, this will be the root path of\n    | your application but you are free to change this when necessary.\n    |\n    *\/\n\n    'path' => '\/',\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Session Cookie Domain\n    |--------------------------------------------------------------------------\n    |\n    | Here you may change the domain of the cookie used to identify a session\n    | in your application. This will determine which domains the cookie is\n    | available to in your application. A sensible default has been set.\n    |\n    *\/\n\n    'domain' => null,\n\n    \/*\n    |--------------------------------------------------------------------------\n    | HTTPS Only Cookies\n    |--------------------------------------------------------------------------\n    |\n    | By setting this option to true, session cookies will only be sent back\n    | to the server if the browser has a HTTPS connection. This will keep\n    | the cookie from being sent to you if it can not be done securely.\n    |\n    *\/\n\n    'secure' => false,\n\n];\n","lang_cluster":"PHP","length":153,"code_uid":"d92f3884f5b6426580fe48c171375a4c"}
{"diff_hunk":"@@ -4,8 +4,52 @@ declare(strict_types=1);\n \n namespace Shopsys\\FrameworkBundle\\Model\\Product\\Search;\n \n+use Shopsys\\FrameworkBundle\\Component\\Domain\\Domain;\n+use Shopsys\\FrameworkBundle\\Component\\Elasticsearch\\IndexDefinitionLoader;\n+use Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CurrentCustomerUser;\n+use Shopsys\\FrameworkBundle\\Model\\Product\\Elasticsearch\\ProductIndex;\n+use Shopsys\\FrameworkBundle\\Model\\Product\\Filter\\ProductFilterData;\n+\n class FilterQueryFactory\n {\n+    \/**\n+     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\Search\\ProductFilterDataToQueryTransformer\n+     *\/\n+    protected $productFilterDataToQueryTransformer;\n+\n+    \/**\n+     * @var \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CurrentCustomerUser\n+     *\/\n+    protected $currentCustomerUser;\n+\n+    \/**\n+     * @var \\Shopsys\\FrameworkBundle\\Component\\Elasticsearch\\IndexDefinitionLoader\n+     *\/\n+    protected $indexDefinitionLoader;\n+\n+    \/**\n+     * @var \\Shopsys\\FrameworkBundle\\Component\\Domain\\Domain\n+     *\/\n+    protected $domain;\n+\n+    \/**\n+     * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\Search\\ProductFilterDataToQueryTransformer $productFilterDataToQueryTransformer\n+     * @param \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CurrentCustomerUser $currentCustomerUser\n+     * @param \\Shopsys\\FrameworkBundle\\Component\\Elasticsearch\\IndexDefinitionLoader $indexDefinitionLoader\n+     * @param \\Shopsys\\FrameworkBundle\\Component\\Domain\\Domain $domain\n+     *\/\n+    public function __construct(\n+        ProductFilterDataToQueryTransformer $productFilterDataToQueryTransformer,\n+        CurrentCustomerUser $currentCustomerUser,\n+        IndexDefinitionLoader $indexDefinitionLoader,\n+        Domain $domain\n+    ) {\n+        $this->productFilterDataToQueryTransformer = $productFilterDataToQueryTransformer;\n+        $this->currentCustomerUser = $currentCustomerUser;\n+        $this->indexDefinitionLoader = $indexDefinitionLoader;\n+        $this->domain = $domain;\n+    }\n+\n     \/**\n      * @param string $indexName\n      * @return \\Shopsys\\FrameworkBundle\\Model\\Product\\Search\\FilterQuery","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Shopsys\\FrameworkBundle\\Model\\Product\\Search;\n\nclass FilterQueryFactory\n{\n    \/**\n     * @param string $indexName\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Product\\Search\\FilterQuery\n     *\/\n    public function create(string $indexName): FilterQuery\n    {\n        return new FilterQuery($indexName);\n    }\n}\n","lang_cluster":"PHP","length":17,"code_uid":"13d0c9f7b4224477bba0245861ac43d5"}
{"diff_hunk":"@@ -69,7 +69,7 @@ class TypeHintResolver\n         } elseif ($hint instanceof PhpParser\\Node\\Name\\FullyQualified) {\n             $fq_type_string = (string)$hint;\n \n-            $scanner->queueClassLikeForScanning($fq_type_string);\n+            $codebase->scanner->queueClassLikeForScanning($fq_type_string);\n             $file_storage->referenced_classlikes[strtolower($fq_type_string)] = $fq_type_string;\n         } else {\n             $lower_hint = strtolower($hint->parts[0]);","old_code":"<?php\n\nnamespace Psalm\\Internal\\PhpVisitor\\Reflector;\n\nuse PhpParser;\nuse Psalm\\Aliases;\nuse Psalm\\Internal\\Analyzer\\ClassLikeAnalyzer;\nuse Psalm\\Internal\\Codebase\\Scanner as CodebaseScanner;\nuse Psalm\\Storage\\ClassLikeStorage;\nuse Psalm\\Storage\\FileStorage;\nuse Psalm\\Type;\nuse Psalm\\Type\\Atomic\\TNull;\nuse Psalm\\Type\\Union;\nuse UnexpectedValueException;\n\nuse function implode;\nuse function strtolower;\n\n\/**\n * @internal\n *\/\nclass TypeHintResolver\n{\n    \/**\n     * @param PhpParser\\Node\\Identifier|PhpParser\\Node\\Name|PhpParser\\Node\\NullableType|PhpParser\\Node\\UnionType $hint\n     *\/\n    public static function resolve(\n        PhpParser\\NodeAbstract $hint,\n        CodebaseScanner $scanner,\n        FileStorage $file_storage,\n        ?ClassLikeStorage $classlike_storage,\n        Aliases $aliases,\n        int $analysis_php_version_id\n    ): Union {\n        if ($hint instanceof PhpParser\\Node\\UnionType) {\n            $type = null;\n\n            if (!$hint->types) {\n                throw new UnexpectedValueException('bad');\n            }\n\n            foreach ($hint->types as $atomic_typehint) {\n                $resolved_type = self::resolve(\n                    $atomic_typehint,\n                    $scanner,\n                    $file_storage,\n                    $classlike_storage,\n                    $aliases,\n                    $analysis_php_version_id\n                );\n\n                $type = Type::combineUnionTypes($resolved_type, $type);\n            }\n\n            return $type;\n        }\n\n        $is_nullable = false;\n\n        if ($hint instanceof PhpParser\\Node\\NullableType) {\n            $is_nullable = true;\n            $hint = $hint->type;\n        }\n\n        $type_string = null;\n\n        if ($hint instanceof PhpParser\\Node\\Identifier) {\n            $fq_type_string = $hint->name;\n        } elseif ($hint instanceof PhpParser\\Node\\Name\\FullyQualified) {\n            $fq_type_string = (string)$hint;\n\n            $scanner->queueClassLikeForScanning($fq_type_string);\n            $file_storage->referenced_classlikes[strtolower($fq_type_string)] = $fq_type_string;\n        } else {\n            $lower_hint = strtolower($hint->parts[0]);\n\n            if ($classlike_storage\n                && ($lower_hint === 'self' || $lower_hint === 'static')\n                && !$classlike_storage->is_trait\n            ) {\n                $fq_type_string = $classlike_storage->name;\n\n                if ($lower_hint === 'static') {\n                    $fq_type_string .= '&static';\n                }\n            } else {\n                $type_string = implode('\\\\', $hint->parts);\n                $fq_type_string = ClassLikeAnalyzer::getFQCLNFromNameObject($hint, $aliases);\n\n                $scanner->queueClassLikeForScanning($fq_type_string);\n                $file_storage->referenced_classlikes[strtolower($fq_type_string)] = $fq_type_string;\n            }\n        }\n\n        $type = Type::parseString(\n            $fq_type_string,\n            $analysis_php_version_id,\n            []\n        );\n\n        if ($type_string) {\n            $atomic_type = $type->getSingleAtomic();\n            $atomic_type->text = $type_string;\n        }\n\n        if ($is_nullable) {\n            $type->addType(new TNull);\n        }\n\n        return $type;\n    }\n}\n","lang_cluster":"PHP","length":112,"code_uid":"faa1afaea6674752b85385780cc87768"}
{"diff_hunk":"@@ -17,14 +17,26 @@ use Ergonode\\Attribute\\Domain\\Repository\\AttributeRepositoryInterface;\n use Webmozart\\Assert\\Assert;\n use Ergonode\\Workflow\\Infrastructure\\Exception\\WorkflowConditionCalculatorException;\n use Ergonode\\Attribute\\Domain\\Entity\\AbstractAttribute;\n+use Ergonode\\Product\\Infrastructure\\Calculator\\TranslationInheritanceCalculator;\n+use Ergonode\\Core\\Domain\\Query\\LanguageQueryInterface;\n+use Ergonode\\Value\\Domain\\ValueObject\\ValueInterface;\n \n class AttributeExistsWorkflowConditionCalculator implements WorkflowConditionCalculatorInterface\n {\n     private AttributeRepositoryInterface $repository;\n \n-    public function __construct(AttributeRepositoryInterface $repository)\n-    {\n+    private TranslationInheritanceCalculator $calculator;\n+\n+    private LanguageQueryInterface $languageQuery;\n+\n+    public function __construct(\n+        AttributeRepositoryInterface $repository,\n+        TranslationInheritanceCalculator $calculator,\n+        LanguageQueryInterface $languageQuery\n+    ) {\n         $this->repository = $repository;\n+        $this->calculator = $calculator;\n+        $this->languageQuery = $languageQuery;\n     }\n \n     public function supports(WorkflowConditionInterface $condition): bool","old_code":"<?php\n\/**\n * Copyright \u00a9 Ergonode Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types=1);\n\nnamespace Ergonode\\Workflow\\Infrastructure\\Condition\\Calculator;\n\nuse Ergonode\\Workflow\\Domain\\Condition\\WorkflowConditionCalculatorInterface;\nuse Ergonode\\Product\\Domain\\Entity\\AbstractProduct;\nuse Ergonode\\Core\\Domain\\ValueObject\\Language;\nuse Ergonode\\Workflow\\Domain\\Condition\\WorkflowConditionInterface;\nuse Ergonode\\Workflow\\Infrastructure\\Condition\\AttributeExistsWorkflowCondition;\nuse Ergonode\\Attribute\\Domain\\Repository\\AttributeRepositoryInterface;\nuse Webmozart\\Assert\\Assert;\nuse Ergonode\\Workflow\\Infrastructure\\Exception\\WorkflowConditionCalculatorException;\nuse Ergonode\\Attribute\\Domain\\Entity\\AbstractAttribute;\n\nclass AttributeExistsWorkflowConditionCalculator implements WorkflowConditionCalculatorInterface\n{\n    private AttributeRepositoryInterface $repository;\n\n    public function __construct(AttributeRepositoryInterface $repository)\n    {\n        $this->repository = $repository;\n    }\n\n    public function supports(WorkflowConditionInterface $condition): bool\n    {\n        return $condition instanceof AttributeExistsWorkflowCondition;\n    }\n\n    \/**\n     * @param AttributeExistsWorkflowCondition $condition\n     *\/\n    public function calculate(AbstractProduct $product, WorkflowConditionInterface $condition, Language $language): bool\n    {\n        if (!$this->supports($condition)) {\n            throw new WorkflowConditionCalculatorException(\n                sprintf(\n                    'Expected an instance of %s. %s received.',\n                    AttributeExistsWorkflowCondition::class,\n                    get_debug_type($condition)\n                )\n            );\n        }\n\n        $attributeId = $condition->getAttribute();\n\n        $attribute = $this->repository->load($attributeId);\n\n        Assert::isInstanceOf($attribute, AbstractAttribute::class);\n\n        return $product->hasAttribute($attribute->getCode());\n    }\n}\n","lang_cluster":"PHP","length":58,"code_uid":"c355b1b1cc094b59809feb0a1250b5f3"}
{"diff_hunk":"@@ -1,9 +1,16 @@\n <?php\n \n+declare(strict_types=1);\n+\n namespace Shopsys\\FrameworkBundle\\Model\\Product\\Search\\Export;\n \n+use BadMethodCallException;\n+use Doctrine\\ORM\\EntityManagerInterface;\n+use Shopsys\\FrameworkBundle\\Component\\Console\\ProgressBarFactory;\n+use Shopsys\\FrameworkBundle\\Component\\Doctrine\\SqlLoggerFacade;\n use Shopsys\\FrameworkBundle\\Model\\Product\\Search\\ProductElasticsearchConverter;\n use Shopsys\\FrameworkBundle\\Model\\Product\\Search\\ProductElasticsearchRepository;\n+use Symfony\\Component\\Console\\Style\\SymfonyStyle;\n \n class ProductSearchExporter\n {","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\Model\\Product\\Search\\Export;\n\nuse Shopsys\\FrameworkBundle\\Model\\Product\\Search\\ProductElasticsearchConverter;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\Search\\ProductElasticsearchRepository;\n\nclass ProductSearchExporter\n{\n    \/** @access protected *\/\n    const BATCH_SIZE = 100;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\Search\\Export\\ProductSearchExportRepository\n     *\/\n    protected $productSearchExportRepository;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\Search\\ProductElasticsearchRepository\n     *\/\n    protected $productElasticsearchRepository;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\Search\\ProductElasticsearchConverter\n     *\/\n    protected $productElasticsearchConverter;\n\n    \/**\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\Search\\Export\\ProductSearchExportRepository $productSearchExportRepository\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\Search\\ProductElasticsearchRepository $productElasticsearchRepository\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\Search\\ProductElasticsearchConverter $productElasticsearchConverter\n     *\/\n    public function __construct(\n        ProductSearchExportRepository $productSearchExportRepository,\n        ProductElasticsearchRepository $productElasticsearchRepository,\n        ProductElasticsearchConverter $productElasticsearchConverter\n    ) {\n        $this->productSearchExportRepository = $productSearchExportRepository;\n        $this->productElasticsearchRepository = $productElasticsearchRepository;\n        $this->productElasticsearchConverter = $productElasticsearchConverter;\n    }\n\n    \/**\n     * @param int $domainId\n     * @param string $locale\n     *\/\n    public function export(int $domainId, string $locale): void\n    {\n        $startFrom = 0;\n        $exportedIds = [];\n        do {\n            $batchExportedIds = $this->exportBatch($domainId, $locale, $startFrom);\n            $exportedIds = array_merge($exportedIds, $batchExportedIds);\n            $startFrom += static::BATCH_SIZE;\n        } while (!empty($batchExportedIds));\n        $this->removeNotUpdated((string)$domainId, $exportedIds);\n    }\n\n    \/**\n     * @param int $domainId\n     * @param string $locale\n     * @param int $startFrom\n     * @return int[]\n     *\/\n    protected function exportBatch(int $domainId, string $locale, int $startFrom): array\n    {\n        $productsData = $this->productSearchExportRepository->getProductsData($domainId, $locale, $startFrom, static::BATCH_SIZE);\n        if (count($productsData) === 0) {\n            return [];\n        }\n\n        $data = $this->productElasticsearchConverter->convertExportBulk($productsData);\n        $this->productElasticsearchRepository->bulkUpdate($domainId, $data);\n\n        return $this->productElasticsearchConverter->extractIds($productsData);\n    }\n\n    \/**\n     * @param int $domainId\n     * @param int[] $exportedIds\n     *\/\n    protected function removeNotUpdated(int $domainId, array $exportedIds): void\n    {\n        $this->productElasticsearchRepository->deleteNotPresent($domainId, $exportedIds);\n    }\n}\n","lang_cluster":"PHP","length":86,"code_uid":"18ea63dad79a4e82b94b7cb7ee156dc0"}
{"diff_hunk":"@@ -11,11 +11,6 @@ use Symfony\\Component\\Routing\\RouteCompiler;\n \n class FriendlyUrlGenerator extends BaseUrlGenerator\n {\n-    \/**\n-     * @var \\Symfony\\Component\\Routing\\RouteCompiler\n-     *\/\n-    private $routeCompiler;\n-\n     \/**\n      * @var FriendlyUrlRepository\n      *\/","old_code":"<?php\n\nnamespace Shopsys\\ShopBundle\\Component\\Router\\FriendlyUrl;\n\nuse Shopsys\\ShopBundle\\Component\\Domain\\Config\\DomainConfig;\nuse Symfony\\Component\\Routing\\Generator\\UrlGenerator as BaseUrlGenerator;\nuse Symfony\\Component\\Routing\\RequestContext;\nuse Symfony\\Component\\Routing\\Route;\nuse Symfony\\Component\\Routing\\RouteCollection;\nuse Symfony\\Component\\Routing\\RouteCompiler;\n\nclass FriendlyUrlGenerator extends BaseUrlGenerator\n{\n    \/**\n     * @var \\Symfony\\Component\\Routing\\RouteCompiler\n     *\/\n    private $routeCompiler;\n\n    \/**\n     * @var FriendlyUrlRepository\n     *\/\n    private $friendlyUrlRepository;\n\n    public function __construct(\n        RequestContext $context,\n        RouteCompiler $routeCompiler,\n        FriendlyUrlRepository $friendlyUrlRepository\n    ) {\n        parent::__construct(new RouteCollection(), $context, null);\n\n        $this->routeCompiler = $routeCompiler;\n        $this->friendlyUrlRepository = $friendlyUrlRepository;\n    }\n\n    \/**\n     * @param \\Symfony\\Component\\Routing\\RouteCollection $routeCollection\n     * @param \\Shopsys\\ShopBundle\\Component\\Domain\\Config\\DomainConfig $domainConfig\n     * @param string $routeName\n     * @param array $parameters\n     * @param int $referenceType\n     * @return string\n     *\/\n    public function generateFromRouteCollection(\n        RouteCollection $routeCollection,\n        DomainConfig $domainConfig,\n        $routeName,\n        array $parameters = [],\n        $referenceType = self::ABSOLUTE_PATH\n    ) {\n        $route = $routeCollection->get($routeName);\n        if ($route === null) {\n            $message = 'Unable to generate a URL for the named route \"' . $routeName . '\" as such route does not exist.';\n            throw new \\Symfony\\Component\\Routing\\Exception\\RouteNotFoundException($message);\n        }\n        if (!array_key_exists('id', $parameters)) {\n            $message = 'Missing mandatory parameter \"id\" for route ' . $routeName . '.';\n            throw new \\Symfony\\Component\\Routing\\Exception\\MissingMandatoryParametersException($message);\n        }\n        $entityId = $parameters['id'];\n        unset($parameters['id']);\n\n        try {\n            $friendlyUrl = $this->friendlyUrlRepository->getMainFriendlyUrl(\n                $domainConfig->getId(),\n                $routeName,\n                $entityId\n            );\n        } catch (\\Shopsys\\ShopBundle\\Component\\Router\\FriendlyUrl\\Exception\\FriendlyUrlNotFoundException $e) {\n            $message = 'Unable to generate a URL for the named route \"' . $routeName . '\" as such route does not exist.';\n            throw new \\Symfony\\Component\\Routing\\Exception\\RouteNotFoundException($message, 0, $e);\n        }\n\n        return $this->getGeneratedUrl($routeName, $route, $friendlyUrl, $parameters, $referenceType);\n    }\n\n    \/**\n     * @param string $routeName\n     * @param \\Symfony\\Component\\Routing\\Route $route\n     * @param \\Shopsys\\ShopBundle\\Component\\Router\\FriendlyUrl\\FriendlyUrl $friendlyUrl\n     * @param array $parameters\n     * @param string $referenceType\n     * @return string\n     *\/\n    public function getGeneratedUrl($routeName, Route $route, FriendlyUrl $friendlyUrl, array $parameters, $referenceType)\n    {\n        $compiledRoute = RouteCompiler::compile($route);\n\n        $tokens = [\n            [\n                0 => 'text',\n                1 => '\/' . $friendlyUrl->getSlug(),\n            ],\n        ];\n\n        return $this->doGenerate(\n            $compiledRoute->getVariables(),\n            $route->getDefaults(),\n            $route->getRequirements(),\n            $tokens,\n            $parameters,\n            $routeName,\n            $referenceType,\n            $compiledRoute->getHostTokens(),\n            $route->getSchemes()\n        );\n    }\n\n    \/**\n     * Not supported method\n     *\/\n    public function generate($routeName, $parameters = [], $referenceType = self::ABSOLUTE_PATH)\n    {\n        throw new \\Shopsys\\ShopBundle\\Component\\Router\\FriendlyUrl\\Exception\\MethodGenerateIsNotSupportedException();\n    }\n}\n","lang_cluster":"PHP","length":115,"code_uid":"57e6de554152427c8f5376729f07d39f"}
{"diff_hunk":"@@ -73,4 +73,9 @@ class CreateFileExportChannelCommand implements CreateChannelCommandInterface\n     {\n         return $this->languages;\n     }\n+\n+    public function getSegmentId(): ?SegmentId\n+    {\n+        return $this->segmentId;\n+    }\n }","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Ergonode Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types=1);\n\nnamespace Ergonode\\ExporterFile\\Domain\\Command;\n\nuse Ergonode\\Channel\\Domain\\Command\\CreateChannelCommandInterface;\nuse Ergonode\\SharedKernel\\Domain\\Aggregate\\ChannelId;\nuse Ergonode\\Core\\Domain\\ValueObject\\Language;\nuse Webmozart\\Assert\\Assert;\nuse Ergonode\\ExporterFile\\Domain\\Entity\\FileExportChannel;\n\nclass CreateFileExportChannelCommand implements CreateChannelCommandInterface\n{\n    protected ChannelId $id;\n\n    protected string $name;\n\n    protected string $format;\n\n    \/**\n     * @var Language[]\n     *\/\n    protected array $languages;\n\n    protected string $exportType;\n\n    \/**\n     * @param Language[] $languages\n     *\/\n    public function __construct(ChannelId $id, string $name, string $format, string $exportType, array $languages = [])\n    {\n        Assert::allIsInstanceOf($languages, Language::class);\n        Assert::oneOf($exportType, FileExportChannel::EXPORT_TYPES);\n\n        $this->id = $id;\n        $this->name = $name;\n        $this->format = $format;\n        $this->exportType = $exportType;\n        $this->languages = $languages;\n    }\n\n\n    public function getId(): ChannelId\n    {\n        return $this->id;\n    }\n\n    public function getName(): string\n    {\n        return $this->name;\n    }\n\n    public function getFormat(): string\n    {\n        return $this->format;\n    }\n\n    public function getExportType(): string\n    {\n        return $this->exportType;\n    }\n\n    \/**\n     * @return Language[]\n     *\/\n    public function getLanguages(): array\n    {\n        return $this->languages;\n    }\n}\n","lang_cluster":"PHP","length":76,"code_uid":"857bc881d71346fda5e335d75fbb38d0"}
{"diff_hunk":"@@ -6,7 +6,6 @@ namespace Bolt\\Controller\\Async;\n \n use Bolt\\Configuration\\Config;\n use Bolt\\Content\\MediaFactory;\n-use Bolt\\Media\\RequestHandler;\n use Cocur\\Slugify\\Slugify;\n use Doctrine\\Common\\Persistence\\ObjectManager;\n use Sirius\\Upload\\Handler;","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Bolt\\Controller\\Async;\n\nuse Bolt\\Configuration\\Config;\nuse Bolt\\Content\\MediaFactory;\nuse Bolt\\Media\\RequestHandler;\nuse Cocur\\Slugify\\Slugify;\nuse Doctrine\\Common\\Persistence\\ObjectManager;\nuse Sirius\\Upload\\Handler;\nuse Sirius\\Upload\\Result\\File;\nuse Symfony\\Component\\HttpFoundation\\Request;\nuse Symfony\\Component\\HttpFoundation\\Response;\nuse Symfony\\Component\\Routing\\Annotation\\Route;\nuse Webmozart\\PathUtil\\Path;\n\nclass Uploader\n{\n    \/** @var MediaFactory *\/\n    private $mediaFactory;\n\n    \/** @var RequestHandler *\/\n    private $requestHandler;\n\n    \/** @var ObjectManager *\/\n    private $manager;\n\n    \/** @var Config *\/\n    private $config;\n\n    public function __construct(MediaFactory $mediaFactory, RequestHandler $requestHandler, ObjectManager $manager, Config $config)\n    {\n        $this->mediaFactory = $mediaFactory;\n        $this->requestHandler = $requestHandler;\n        $this->manager = $manager;\n        $this->config = $config;\n    }\n\n    \/**\n     * @Route(\"\/upload\", name=\"bolt_upload_post\", methods={\"POST\"})\n     *\/\n    public function upload(Request $request)\n    {\n\/\/        $uploadHandler = new Handler('\/path\/to\/local_folder');\n\n        $area = $request->query->get('area', '');\n        $path = $request->query->get('path', '');\n\n        $target = $this->config->getPath($area, true, $path);\n\n        $uploadHandler = new Handler($target, [\n            Handler::OPTION_AUTOCONFIRM => true,\n            Handler::OPTION_OVERWRITE => true,\n        ]);\n\n        $uploadHandler->addRule('extension', ['allowed' => 'jpg', 'jpeg', 'png'], '{label} should be a valid image (jpg, jpeg, png)', 'Profile picture');\n        $uploadHandler->addRule('size', ['max' => '20M'], '{label} should have less than {max}', 'Profile picture');\n        $uploadHandler->setSanitizerCallback(function ($name) {\n            return $this->sanitiseFilename($name);\n        });\n\n        \/** @var File $result *\/\n        $result = $uploadHandler->process($_FILES);\n\n        if ($result->isValid()) {\n            try {\n                $media = $this->mediaFactory->createFromFilename($area, $path, $result->name);\n                $this->manager->persist($media);\n                $this->manager->flush();\n\n                return new Response($media->getFilenamePath());\n            } catch (\\Throwable $e) {\n                \/\/ something wrong happened, we don't need the uploaded files anymore\n                $result->clear();\n                throw $e;\n            }\n        } else {\n            \/\/ image was not moved to the container, where are error messages\n            $messages = $result->getMessages();\n        }\n\n        return new Response('Not OK');\n    }\n\n    private function sanitiseFilename(string $filename): string\n    {\n        $extensionSlug = new Slugify(['regexp' => '\/([^a-z0-9]|-)+\/']);\n        $filenameSlug = new Slugify(['lowercase' => false]);\n\n        $extension = $extensionSlug->slugify(Path::getExtension($filename));\n        $filename = $filenameSlug->slugify(Path::getFilenameWithoutExtension($filename));\n\n        return $filename . '.' . $extension;\n    }\n}\n","lang_cluster":"PHP","length":97,"code_uid":"1dbfa0ffe63b431e91646b5793c6377a"}
{"diff_hunk":"@@ -58,7 +58,7 @@\n \n                         <div class=\"form-group row mb-0\">\n                             <div class=\"col-md-12\">\n-                                <button type=\"submit\" class=\"btn btn-primary btn-block py-0 font-weight-bold\">\n+                                <button type=\"submit\" class=\"btn btn-primary btn-block btn-lg font-weight-bold\">\n                                     {{ __('Login') }}\n                                 <\/button>\n ","old_code":"@extends('layouts.app')\n\n@section('content')\n<div class=\"container mt-4\">\n    <div class=\"row justify-content-center\">\n        <div class=\"col-lg-5\">\n            <div class=\"\">\n                <div class=\"card-header bg-transparent p-3 text-center font-weight-bold\">{{ __('Login') }}<\/div>\n\n                <div class=\"card-body\">\n                    <form method=\"POST\" action=\"{{ route('login') }}\" class=\"px-5\">\n                        @csrf\n\n                        <div class=\"form-group row\">\n\n                            <div class=\"col-md-12\">\n                                <input id=\"email\" type=\"email\" class=\"form-control{{ $errors->has('email') ? ' is-invalid' : '' }}\" name=\"email\" value=\"{{ old('email') }}\" placeholder=\"{{__('Email')}}\" required autofocus>\n\n                                @if ($errors->has('email'))\n                                    <span class=\"invalid-feedback\">\n                                        <strong>{{ $errors->first('email') }}<\/strong>\n                                    <\/span>\n                                @endif\n                            <\/div>\n                        <\/div>\n\n                        <div class=\"form-group row\">\n\n                            <div class=\"col-md-12\">\n                                <input id=\"password\" type=\"password\" class=\"form-control{{ $errors->has('password') ? ' is-invalid' : '' }}\" name=\"password\" placeholder=\"{{__('Password')}}\" required>\n\n                                @if ($errors->has('password'))\n                                    <span class=\"invalid-feedback\">\n                                        <strong>{{ $errors->first('password') }}<\/strong>\n                                    <\/span>\n                                @endif\n                            <\/div>\n                        <\/div>\n\n                        <div class=\"form-group row\">\n                            <div class=\"col-md-12\">\n                                <div class=\"checkbox\">\n                                    <label>\n                                        <input type=\"checkbox\" name=\"remember\" {{ old('remember') ? 'checked' : '' }}> \n                                        <span class=\"font-weight-bold small ml-1 text-muted\">\n                                            {{ __('Remember Me') }}\n                                        <\/span>\n                                    <\/label>\n                                <\/div>\n                            <\/div>\n                        <\/div>\n\n                        @if(config('captcha.enabled'))\n                        <div class=\"d-flex justify-content-center mb-3\">\n                            {!! Captcha::display() !!}\n                        <\/div>\n                        @endif\n\n                        <div class=\"form-group row mb-0\">\n                            <div class=\"col-md-12\">\n                                <button type=\"submit\" class=\"btn btn-primary btn-block py-0 font-weight-bold\">\n                                    {{ __('Login') }}\n                                <\/button>\n\n                            <\/div>\n                        <\/div>\n                    <\/form>\n\n                    <hr>\n\n                    <p class=\"text-center small font-weight-bold mb-0\">\n                        <a href=\"{{ route('password.request') }}\">\n                            {{ __('Forgot Password') }}\n                        <\/a>\n                    <\/p>\n                <\/div>\n            <\/div>\n        <\/div>\n    <\/div>\n<\/div>\n@endsection\n","lang_cluster":"PHP","length":81,"code_uid":"03d41c2bfb054a6fb0f2cef1c1bdb6dc"}
{"diff_hunk":"@@ -26,21 +26,26 @@ func TestCmdSnapshot(t *testing.T) {\n \t\tcleanup()\n \t}()\n \n+\t\/\/ Ensure that there are no snapshots available before we create one\n+\targs := []string{\"snapshot\", \"--cleanup\", \"--yes\"}\n+\t_, err = exec.RunCommand(DdevBin, args)\n+\tassert.NoError(err)\n+\n \t\/\/ Ensure that a snapshot can be created\n-\targs := []string{\"snapshot\", \"--name\", \"test-snapshot\"}\n+\targs = []string{\"snapshot\", \"--name\", \"test-snapshot\"}\n \tout, err := exec.RunCommand(DdevBin, args)\n \tassert.NoError(err)\n-\tassert.Contains(string(out), \"Created snapshot test-snapshot\")\n+\tassert.Contains(out, \"Created snapshot test-snapshot\")\n \n \t\/\/ Try to delete a not existing snapshot\n \targs = []string{\"snapshot\", \"--name\", \"not-existing-snapshot\", \"--cleanup\", \"--yes\"}\n \tout, err = exec.RunCommand(DdevBin, args)\n \tassert.Error(err)\n-\tassert.Contains(string(out), \"Failed to delete snapshot\")\n+\tassert.Contains(out, \"Failed to delete snapshot\")\n \n \t\/\/ Ensure that an existing snapshot can be deleted\n-\targs = []string{\"snapshot\", \"--name\", \"test-snapshot\", \"--cleanup\"}\n+\targs = []string{\"snapshot\", \"--name\", \"test-snapshot\", \"--cleanup\", \"--yes\"}\n \tout, err = exec.RunCommand(DdevBin, args)\n \tassert.NoError(err)\n-\tassert.Contains(string(out), \"Deleted database snapshot test-snapshot\")\n+\tassert.Contains(out, \"Deleted database snapshot test-snapshot\")\n }","old_code":"package cmd\n\nimport (\n\t\"fmt\"\n\t\"github.com\/drud\/ddev\/pkg\/ddevapp\"\n\t\"github.com\/drud\/ddev\/pkg\/exec\"\n\tasrt \"github.com\/stretchr\/testify\/assert\"\n\t\"os\"\n\t\"testing\"\n)\n\n\/\/ TestCmdSnapshot runs `ddev snapshot` on the test apps\nfunc TestCmdSnapshot(t *testing.T) {\n\tassert := asrt.New(t)\n\n\ttestDir, _ := os.Getwd()\n\tfmt.Println(testDir)\n\tsite := TestSites[0]\n\tcleanup := site.Chdir()\n\tapp, err := ddevapp.NewApp(site.Dir, false, \"\")\n\tassert.NoError(err)\n\tdefer func() {\n\t\t\/\/ Make sure all databases are back to default empty\n\t\t_ = app.Stop(true, false)\n\t\t_ = app.Start()\n\t\tcleanup()\n\t}()\n\n\t\/\/ Ensure that a snapshot can be created\n\targs := []string{\"snapshot\", \"--name\", \"test-snapshot\"}\n\tout, err := exec.RunCommand(DdevBin, args)\n\tassert.NoError(err)\n\tassert.Contains(string(out), \"Created snapshot test-snapshot\")\n\n\t\/\/ Try to delete a not existing snapshot\n\targs = []string{\"snapshot\", \"--name\", \"not-existing-snapshot\", \"--cleanup\", \"--yes\"}\n\tout, err = exec.RunCommand(DdevBin, args)\n\tassert.Error(err)\n\tassert.Contains(string(out), \"Failed to delete snapshot\")\n\n\t\/\/ Ensure that an existing snapshot can be deleted\n\targs = []string{\"snapshot\", \"--name\", \"test-snapshot\", \"--cleanup\"}\n\tout, err = exec.RunCommand(DdevBin, args)\n\tassert.NoError(err)\n\tassert.Contains(string(out), \"Deleted database snapshot test-snapshot\")\n}\n","lang_cluster":"PHP","length":46,"code_uid":"e4522a8e17df4d16a8df1f25b60b33c7"}
{"diff_hunk":"@@ -1,11 +1,10 @@\n <?php\n-\n \/**\n  * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n  * See LICENSE.txt for license details.\n  *\/\n \n-declare(strict_types = 1);\n+declare(strict_types=1);\n \n namespace Ergonode\\Designer\\Domain\\ValueObject\\TemplateElement;\n ","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types = 1);\n\nnamespace Ergonode\\Designer\\Domain\\ValueObject\\TemplateElement;\n\nuse Ergonode\\Designer\\Domain\\ValueObject\\TemplateElementPropertyInterface;\nuse Ergonode\\SharedKernel\\Domain\\Aggregate\\AttributeId;\nuse JMS\\Serializer\\Annotation as JMS;\n\n\/**\n *\/\nclass AttributeTemplateElementProperty implements TemplateElementPropertyInterface\n{\n    public const VARIANT = 'attribute';\n\n    \/**\n     * @var AttributeId\n     *\n     * @JMS\\Type(\"Ergonode\\SharedKernel\\Domain\\Aggregate\\AttributeId\")\n     * @JMS\\SerializedName(\"attribute_id\")\n     *\/\n    private AttributeId $attributeId;\n\n    \/**\n     * @var bool\n     *\n     * @JMS\\Type(\"bool\")\n     *\/\n    private bool $required;\n\n    \/**\n     * @param AttributeId $attributeId\n     * @param bool        $required\n     *\/\n    public function __construct(AttributeId $attributeId, bool $required = false)\n    {\n        $this->attributeId = $attributeId;\n        $this->required = $required;\n    }\n\n    \/**\n     * {@inheritDoc}\n     *\n     * @JMS\\VirtualProperty()\n     *\/\n    public function getVariant(): string\n    {\n        return self::VARIANT;\n    }\n\n    \/**\n     * @return AttributeId\n     *\/\n    public function getAttributeId(): AttributeId\n    {\n        return $this->attributeId;\n    }\n\n    \/**\n     * @return bool\n     *\/\n    public function isRequired(): bool\n    {\n        return $this->required;\n    }\n}\n","lang_cluster":"PHP","length":72,"code_uid":"bc5ebc57cd12427abaa8d9ec71dc4b16"}
{"diff_hunk":"@@ -13,6 +13,7 @@ import (\n \t\"github.com\/drud\/ddev\/pkg\/dockerutil\"\n \t\"github.com\/drud\/ddev\/pkg\/util\"\n \t\"github.com\/drud\/ddev\/pkg\/version\"\n+\t\"github.com\/fatih\/color\"\n \n \thomedir \"github.com\/mitchellh\/go-homedir\"\n )","old_code":"package platform\n\nimport (\n\t\"bytes\"\n\t\"html\/template\"\n\t\"log\"\n\t\"os\"\n\t\"path\"\n\t\"path\/filepath\"\n\n\t\"strings\"\n\n\t\"github.com\/drud\/ddev\/pkg\/dockerutil\"\n\t\"github.com\/drud\/ddev\/pkg\/util\"\n\t\"github.com\/drud\/ddev\/pkg\/version\"\n\n\thomedir \"github.com\/mitchellh\/go-homedir\"\n)\n\nconst routerProjectName = \"ddev-router\"\n\n\/\/ RouterComposeYAMLPath returns the full filepath to the routers docker-compose yaml file.\nfunc RouterComposeYAMLPath() string {\n\tuserHome, err := homedir.Dir()\n\tif err != nil {\n\t\tlog.Fatal(\"could not get home directory for current user. is it set?\")\n\t}\n\trouterdir := path.Join(userHome, \".ddev\")\n\tdest := path.Join(routerdir, \"router-compose.yaml\")\n\n\treturn dest\n}\n\n\/\/ StopRouter stops the local router if there are no ddev containers running.\nfunc StopRouter() error {\n\n\tcontainersRunning, err := ddevContainersRunning()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif !containersRunning {\n\t\tdest := RouterComposeYAMLPath()\n\t\treturn dockerutil.ComposeCmd([]string{dest}, \"-p\", routerProjectName, \"down\", \"-v\")\n\t}\n\treturn nil\n}\n\n\/\/ StartDdevRouter ensures the router is running.\nfunc StartDdevRouter() {\n\texposedPorts := determineRouterPorts()\n\n\tdest := RouterComposeYAMLPath()\n\trouterdir := filepath.Dir(dest)\n\terr := os.MkdirAll(routerdir, 0755)\n\tif err != nil {\n\t\tlog.Fatalf(\"unable to create directory for ddev router: %s\", err)\n\t}\n\n\tvar doc bytes.Buffer\n\tf, ferr := os.Create(dest)\n\tif ferr != nil {\n\t\tlog.Fatal(ferr)\n\t}\n\tdefer util.CheckClose(f)\n\n\ttempl := template.New(\"compose template\")\n\ttempl, err = templ.Parse(DdevRouterTemplate)\n\tif err != nil {\n\t\tlog.Fatal(ferr)\n\t}\n\n\ttemplateVars := map[string]interface{}{\n\t\t\"router_image\": version.RouterImage,\n\t\t\"router_tag\":   version.RouterTag,\n\t\t\"ports\":        exposedPorts,\n\t}\n\n\terr = templ.Execute(&doc, templateVars)\n\tutil.CheckErr(err)\n\t_, err = f.WriteString(doc.String())\n\tutil.CheckErr(err)\n\n\t\/\/ run docker-compose up -d in the newly created directory\n\terr = dockerutil.ComposeCmd([]string{dest}, \"-p\", routerProjectName, \"up\", \"-d\")\n\tif err != nil {\n\t\tlog.Fatalf(\"Could not start router: %v\", err)\n\t}\n}\n\n\/\/ determineRouterPorts returns a list of port mappings retrieved from running site\n\/\/ containers defining VIRTUAL_PORT env var\nfunc determineRouterPorts() []string {\n\tvar routerPorts []string\n\tcontainers, err := dockerutil.GetDockerContainers(false)\n\tif err != nil {\n\t\tlog.Fatal(\"failed to retreive containers for determining port mappings\", err)\n\t}\n\n\t\/\/ loop through all containers with site-name label\n\tfor _, container := range containers {\n\t\tif _, ok := container.Labels[\"com.ddev.site-name\"]; ok {\n\t\t\tvar exposePorts []string\n\n\t\t\thttpPorts := dockerutil.GetContainerEnv(\"HTTP_EXPOSE\", container)\n\t\t\tif httpPorts != \"\" {\n\t\t\t\tports := strings.Split(httpPorts, \",\")\n\t\t\t\texposePorts = append(exposePorts, ports...)\n\t\t\t}\n\n\t\t\tfor _, exposePort := range exposePorts {\n\t\t\t\t\/\/ ports defined as hostPort:containerPort allow for router to configure upstreams\n\t\t\t\t\/\/ for containerPort, with server listening on hostPort. exposed ports for router\n\t\t\t\t\/\/ should be hostPort:hostPort so router can determine what port a request came from\n\t\t\t\t\/\/ and route the request to the correct upstream\n\t\t\t\tif strings.Contains(exposePort, \":\") {\n\t\t\t\t\tports := strings.Split(exposePort, \":\")\n\t\t\t\t\texposePort = ports[0]\n\t\t\t\t}\n\n\t\t\t\tvar match bool\n\t\t\t\tfor _, routerPort := range routerPorts {\n\t\t\t\t\tif exposePort == routerPort {\n\t\t\t\t\t\tmatch = true\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t\/\/ if no match, we are adding a new port mapping\n\t\t\t\tif !match {\n\t\t\t\t\trouterPorts = append(routerPorts, exposePort)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn routerPorts\n}\n","lang_cluster":"PHP","length":136,"code_uid":"a1d6d1fa79bf43e6ab345149737a9264"}
{"diff_hunk":"@@ -58,11 +58,11 @@ class Price\n      * @param \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Price $priceToAdd\n      * @return \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Price\n      *\/\n-    public function add(self $priceToAdd)\n+    public function add(self $priceToAdd): self\n     {\n         return new self(\n-            $this->priceWithoutVat + $priceToAdd->getPriceWithoutVat(),\n-            $this->priceWithVat + $priceToAdd->getPriceWithVat()\n+            $this->priceWithoutVat->add($priceToAdd->priceWithoutVat),\n+            $this->priceWithVat->add($priceToAdd->priceWithVat)\n         );\n     }\n ","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\Model\\Pricing;\n\nclass Price\n{\n    \/**\n     * @var string\n     *\/\n    protected $priceWithoutVat;\n\n    \/**\n     * @var string\n     *\/\n    protected $priceWithVat;\n\n    \/**\n     * @var string\n     *\/\n    protected $vatAmount;\n\n    \/**\n     * @param string $priceWithoutVat\n     * @param string $priceWithVat\n     *\/\n    public function __construct($priceWithoutVat, $priceWithVat)\n    {\n        $this->priceWithoutVat = $priceWithoutVat;\n        $this->priceWithVat = $priceWithVat;\n        $this->vatAmount = $priceWithVat - $priceWithoutVat;\n    }\n\n    \/**\n     * @return string\n     *\/\n    public function getPriceWithoutVat()\n    {\n        return $this->priceWithoutVat;\n    }\n\n    \/**\n     * @return string\n     *\/\n    public function getPriceWithVat()\n    {\n        return $this->priceWithVat;\n    }\n\n    \/**\n     * @return string\n     *\/\n    public function getVatAmount()\n    {\n        return $this->vatAmount;\n    }\n\n    \/**\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Price $priceToAdd\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Price\n     *\/\n    public function add(self $priceToAdd)\n    {\n        return new self(\n            $this->priceWithoutVat + $priceToAdd->getPriceWithoutVat(),\n            $this->priceWithVat + $priceToAdd->getPriceWithVat()\n        );\n    }\n\n    \/**\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Price $priceToSubtract\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Price\n     *\/\n    public function subtract(self $priceToSubtract)\n    {\n        return new self(\n            $this->priceWithoutVat - $priceToSubtract->getPriceWithoutVat(),\n            $this->priceWithVat - $priceToSubtract->getPriceWithVat()\n        );\n    }\n}\n","lang_cluster":"PHP","length":80,"code_uid":"1c6f319b7a9143ee829f31f98c681e44"}
{"diff_hunk":"@@ -11,7 +11,6 @@ namespace Ergonode\\Account\\Tests\\Domain\\Event;\n \n use Ergonode\\SharedKernel\\Domain\\Aggregate\\UserId;\n use Ergonode\\Account\\Domain\\Event\\User\\UserAvatarChangedEvent;\n-use Ergonode\\SharedKernel\\Domain\\Aggregate\\MultimediaId;\n use PHPUnit\\Framework\\MockObject\\MockObject;\n use PHPUnit\\Framework\\TestCase;\n ","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types = 1);\n\nnamespace Ergonode\\Account\\Tests\\Domain\\Event;\n\nuse Ergonode\\SharedKernel\\Domain\\Aggregate\\UserId;\nuse Ergonode\\Account\\Domain\\Event\\User\\UserAvatarChangedEvent;\nuse Ergonode\\SharedKernel\\Domain\\Aggregate\\MultimediaId;\nuse PHPUnit\\Framework\\MockObject\\MockObject;\nuse PHPUnit\\Framework\\TestCase;\n\n\/**\n *\/\nclass UserAvatarChangedEventTest extends TestCase\n{\n    \/**\n     *\/\n    public function testCreateEvent(): void\n    {\n        \/** @var UserId|MockObject $id *\/\n        $id = $this->createMock(UserId::class);\n        $multimediaId = $this->createMock(MultimediaId::class);\n\n        $event = new UserAvatarChangedEvent($id, $multimediaId);\n\n        $this->assertEquals($id, $event->getAggregateId());\n        $this->assertEquals($multimediaId, $event->getAvatarId());\n    }\n}\n","lang_cluster":"PHP","length":35,"code_uid":"065bf7e198b840819bd0c80b5e57abf6"}
{"diff_hunk":"@@ -5,6 +5,8 @@ declare(strict_types=1);\n namespace Bolt\\Twig;\n \n use Bolt\\Menu\\FrontendMenuBuilderInterface;\n+use Symfony\\Component\\HttpFoundation\\Request;\n+use Symfony\\Component\\HttpFoundation\\RequestStack;\n use Twig\\Environment;\n use Twig\\Extension\\AbstractExtension;\n use Twig\\TwigFunction;","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Bolt\\Twig;\n\nuse Bolt\\Menu\\FrontendMenuBuilderInterface;\nuse Twig\\Environment;\nuse Twig\\Extension\\AbstractExtension;\nuse Twig\\TwigFunction;\n\nclass FrontendMenuExtension extends AbstractExtension\n{\n    \/** @var FrontendMenuBuilderInterface *\/\n    private $menuBuilder;\n\n    public function __construct(FrontendMenuBuilderInterface $menuBuilder)\n    {\n        $this->menuBuilder = $menuBuilder;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getFunctions(): array\n    {\n        $safe = [\n            'is_safe' => ['html'],\n        ];\n        $env = ['needs_environment' => true];\n\n        return [\n            new TwigFunction('menu', [$this, 'renderMenu'], $env + $safe),\n            new TwigFunction('menu_array', [$this, 'getMenu'], $safe),\n        ];\n    }\n\n    public function getMenu(?string $name = null): array\n    {\n        return $this->menuBuilder->buildMenu($name);\n    }\n\n    public function renderMenu(Environment $twig, ?string $name = null, string $template = 'helpers\/_menu.html.twig', string $class = '', bool $withsubmenus = true): string\n    {\n        $context = [\n            'menu' => $this->menuBuilder->buildMenu($name),\n            'class' => $class,\n            'withsubmenus' => $withsubmenus,\n        ];\n\n        return $twig->render($template, $context);\n    }\n}\n","lang_cluster":"PHP","length":53,"code_uid":"0b6cb25803454c169a50411ec91d0ba0"}
{"diff_hunk":"@@ -73,7 +73,6 @@ class CmsObjectCollection extends CollectionBase\n             $hasComponent = false;\n \n             foreach ((array) $components as $componentName) {\n-\n                 if (!$componentAlias = $object->hasComponent($componentName)) {\n                     continue;\n                 }","old_code":"<?php namespace Cms\\Classes;\n\nuse October\\Rain\\Support\\Collection as CollectionBase;\n\n\/**\n * This class represents a collection of Cms Objects.\n *\n * @package october\\cms\n * @author Alexey Bobkov, Samuel Georges\n *\/\nclass CmsObjectCollection extends CollectionBase\n{\n    \/**\n     * Returns objects that use the supplied component.\n     * @param  string|array $components\n     * @param null|callback $callback\n     * @return static\n     *\/\n    public function withComponent($components, $callback = null)\n    {\n        return $this->filter(function ($object) use ($components, $callback) {\n\n            $hasComponent = false;\n\n            foreach ((array) $components as $componentName) {\n\n                if (!$callback && $object->hasComponent($componentName)) {\n                    $hasComponent = true;\n                }\n\n                if ($callback && ($component = $object->getComponent($componentName))) {\n                    $hasComponent = call_user_func($callback, $component) ?: $hasComponent;\n                }\n            }\n\n            return $hasComponent;\n        });\n    }\n\n    \/**\n     * Returns objects whose properties match the supplied value.\n     * @param string $property\n     * @param string $value\n     * @param bool $strict\n     * @return static\n     *\/\n    public function where($property, $value, $strict = true)\n    {\n        return $this->filter(function ($object) use ($property, $value, $strict) {\n\n            if (!array_key_exists($property, $object->settings)) {\n                return false;\n            }\n\n            return $strict\n                ? $object->settings[$property] === $value\n                : $object->settings[$property] == $value;\n        });\n    }\n\n    \/**\n     * Returns objects whose component properties match the supplied value.\n     * @param mixed $components\n     * @param string $property\n     * @param string $value\n     * @param bool $strict\n     * @return static\n     *\/\n    public function whereComponent($components, $property, $value, $strict = false)\n    {\n        return $this->filter(function ($object) use ($components, $property, $value, $strict) {\n\n            $hasComponent = false;\n\n            foreach ((array) $components as $componentName) {\n\n                if (!$componentAlias = $object->hasComponent($componentName)) {\n                    continue;\n                }\n\n                $componentSettings = array_get($object->settings, 'components', []);\n\n                if (!array_key_exists($componentAlias, $componentSettings)) {\n                    continue;\n                }\n\n                $settings = $componentSettings[$componentAlias];\n\n                if (!array_key_exists($property, $settings)) {\n                    continue;\n                }\n\n                if (\n                    ($strict && $settings[$property] === $value) ||\n                    (!$strict && $settings[$property] == $value)\n                ) {\n                    $hasComponent = true;\n                }\n            }\n\n            return $hasComponent;\n        });\n    }\n\n}\n","lang_cluster":"PHP","length":105,"code_uid":"4db8dbcb415342c1aac68fe163fe341f"}
{"diff_hunk":"@@ -29,7 +29,6 @@ $venue = sanitize_tags(get_str(\"venue\", true));\n $columns = get_str(\"cols\", true);\n $c = $columns?\"&cols=$columns\":\"\";\n check_subset($subset);\n-\n if ($action) {\n     check_tokens($user->authenticator);\n     if ($subset == \"global\") {","old_code":"<?php\n\/\/ This file is part of BOINC.\n\/\/ http:\/\/boinc.berkeley.edu\n\/\/ Copyright (C) 2008 University of California\n\/\/\n\/\/ BOINC is free software; you can redistribute it and\/or modify it\n\/\/ under the terms of the GNU Lesser General Public License\n\/\/ as published by the Free Software Foundation,\n\/\/ either version 3 of the License, or (at your option) any later version.\n\/\/\n\/\/ BOINC is distributed in the hope that it will be useful,\n\/\/ but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\/\/ See the GNU Lesser General Public License for more details.\n\/\/\n\/\/ You should have received a copy of the GNU Lesser General Public License\n\/\/ along with BOINC.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\ninclude_once(\"..\/inc\/db.inc\");\ninclude_once(\"..\/inc\/util.inc\");\ninclude_once(\"..\/inc\/prefs.inc\");\ninclude_once(\"..\/inc\/prefs_project.inc\");\n\n$user = get_logged_in_user();\n\n$action = sanitize_tags(get_str(\"action\", true));\n$subset = sanitize_tags(get_str(\"subset\"));\n$venue = sanitize_tags(get_str(\"venue\", true));\n$columns = get_str(\"cols\", true);\n$c = $columns?\"&cols=$columns\":\"\";\ncheck_subset($subset);\n\nif ($action) {\n    check_tokens($user->authenticator);\n    if ($subset == \"global\") {\n        $main_prefs = prefs_parse_global($user->global_prefs);\n        if ($venue) $prefs = $main_prefs->$venue;\n        else $prefs = $main_prefs;\n        $error = prefs_global_parse_form($prefs);\n        if ($error != false) {\n            $title = tra(\"Edit %1 preferences\", subset_name($subset));\n            if ($venue) $title = \"$title for $venue\";\n            page_head($title);\n\n            echo PREFS_FORM_DESC1;\n            echo PREFS_FORM_ERROR_DESC;\n\n            print_prefs_form(\n                \"edit\", $subset, $venue, $user, $prefs, $columns, $error\n            );\n        } else {\n            if ($venue) $main_prefs->$venue = $prefs;\n            else $main_prefs = $prefs;\n            global_prefs_update($user, $main_prefs);\n            Header(\"Location: prefs.php?subset=$subset&updated=1$c\");\n        }\n    } else {\n        $main_prefs = prefs_parse_project($user->project_prefs);\n        if ($venue) $prefs = $main_prefs->$venue;\n        else $prefs = $main_prefs;\n\n        $project_error = prefs_project_parse_form($prefs);\n        $error = prefs_resource_parse_form($prefs);\n        if ($error != false || $project_error != false) {\n            $title = tra(\"Edit %1 preferences\", subset_name($subset));\n            if ($venue) $title = tra(\"%1 for %2\", $title, $venue);\n            page_head($title);\n\n            echo PREFS_FORM_ERROR_DESC;\n\n            print_prefs_form(\n                \"edit\", $subset, $venue, $user, $prefs, $columns, $error,\n                $project_error\n            );\n        } else {\n            if ($venue) {\n                $main_prefs->$venue = $prefs;\n            } else {\n                $main_prefs = $prefs;\n                prefs_privacy_parse_form($user);\n            }\n\n            project_prefs_update($user, $main_prefs);\n\n            if (!$venue) {\n                venue_parse_form($user);\n                $user->update(\"venue='$user->venue'\");\n            }\n            Header(\"Location: prefs.php?subset=$subset&updated=1$c\");\n        }\n    }\n} else {\n    $title = tra(\"Edit %1 preferences\", subset_name($subset));\n    if ($venue) $title = tra(\"%1 for %2\", $title, $venue);\n    page_head($title);\n    checkbox_clicked_js();\n\n    if ($subset == \"global\") {\n        echo PREFS_FORM_DESC1;\n        $prefs = prefs_parse_global($user->global_prefs);\n        if ($venue) {\n            $prefs = $prefs->$venue;\n        }\n    } else {\n        $prefs = prefs_parse_project($user->project_prefs);\n        if ($venue) {\n            $prefs = $prefs->$venue;\n        }\n    }\n    print_prefs_form(\"edit\", $subset, $venue, $user, $prefs, $columns);\n}\necho \"<a href=prefs.php?subset=$subset$c>\".tra(\"Back to preferences\").\"<\/a>\\n\";\npage_tail();\n\n$cvs_version_tracker[]=\"\\$Id$\";  \/\/Generated automatically - do not edit\n?>\n","lang_cluster":"PHP","length":116,"code_uid":"78a6a55b40fd4924b4d078b75da940f7"}
{"diff_hunk":"@@ -4,16 +4,16 @@\n  * See LICENSE.txt for license details.\n  *\/\n \n-declare(strict_types = 1);\n+declare(strict_types=1);\n \n namespace Ergonode\\ImporterMagento1\\Infrastructure\\Processor\\Step;\n \n use Ergonode\\ImporterMagento1\\Infrastructure\\Model\\ProductModel;\n use Ergonode\\ImporterMagento1\\Domain\\Entity\\Magento1CsvSource;\n-use Ergonode\\Transformer\\Domain\\Entity\\Transformer;\n use Ergonode\\Attribute\\Domain\\Entity\\Attribute\\ImageAttribute;\n use Ergonode\\Core\\Domain\\ValueObject\\TranslatableString;\n use Ergonode\\Category\\Domain\\ValueObject\\CategoryCode;\n+use Ergonode\\Attribute\\Domain\\Entity\\AbstractAttribute;\n \n abstract class AbstractProductProcessor\n {","old_code":"<?php\n\/**\n * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types = 1);\n\nnamespace Ergonode\\ImporterMagento1\\Infrastructure\\Processor\\Step;\n\nuse Ergonode\\ImporterMagento1\\Infrastructure\\Model\\ProductModel;\nuse Ergonode\\ImporterMagento1\\Domain\\Entity\\Magento1CsvSource;\nuse Ergonode\\Transformer\\Domain\\Entity\\Transformer;\nuse Ergonode\\Attribute\\Domain\\Entity\\Attribute\\ImageAttribute;\nuse Ergonode\\Core\\Domain\\ValueObject\\TranslatableString;\nuse Ergonode\\Category\\Domain\\ValueObject\\CategoryCode;\n\nabstract class AbstractProductProcessor\n{\n    \/**\n     * @return CategoryCode[]\n     *\/\n    protected function getCategories(ProductModel $product): array\n    {\n        $result = [];\n\n        $default = $product->get('default');\n        if ($categories = $default['esa_categories'] ?? null) {\n            foreach (explode(',', $categories) as $category) {\n                $result[] = new CategoryCode($category);\n            }\n        }\n\n        return $result;\n    }\n\n    \/**\n     * @return string[]\n     *\/\n    protected function getAttributes(\n        Transformer $transformer,\n        Magento1CsvSource $source,\n        ProductModel $product\n    ): array {\n        $result = [];\n        $default = $product->get('default');\n\n        foreach ($default as $field => $value) {\n            $translation = [];\n            if ($transformer->hasAttribute($field)) {\n                $type = $transformer->getAttributeType($field);\n                $value = $this->format($type, $value);\n                if ($value) {\n                    $translation[$source->getDefaultLanguage()->getCode()] = $value;\n                }\n\n                foreach ($source->getLanguages() as $key => $language) {\n                    if ($product->has($key)) {\n                        $translatedVer = $product->get($key);\n                        if (array_key_exists($field, $translatedVer) && null !== $translatedVer[$field]) {\n                            $code = $language->getCode();\n                            $translation[$code] = $this->format($type, $translatedVer[$field]);\n                        }\n                    }\n                }\n\n                $result[$field] = new TranslatableString($translation);\n            }\n        }\n\n        return $result;\n    }\n\n    protected function format(\n        string $type,\n        ?string $value\n    ): ?string {\n        if ($value && ImageAttribute::TYPE === $type) {\n            $value = pathinfo($value, PATHINFO_FILENAME);\n            if ('no_selection' === $value) {\n                $value = null;\n            }\n        }\n\n        return $value;\n    }\n}\n","lang_cluster":"PHP","length":87,"code_uid":"c9489d5f3b564e72a9d2ca4ef695c94b"}
{"diff_hunk":"@@ -121,7 +121,7 @@ abstract class BaseMediaEventSubscriber implements EventSubscriber\n     }\n \n     \/**\n-     * @param \\Doctrine\\Common\\EventArgs $args\n+     * @param EventArgs $args\n      *\/\n     public function preRemove(EventArgs $args)\n     {","old_code":"<?php\n\n\/*\n * This file is part of the Sonata project.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Listener;\n\nuse Doctrine\\Common\\EventArgs;\nuse Doctrine\\Common\\EventSubscriber;\nuse Sonata\\MediaBundle\\Model\\MediaInterface;\nuse Sonata\\MediaBundle\\Provider\\Pool;\nuse Symfony\\Component\\DependencyInjection\\ContainerInterface;\n\nabstract class BaseMediaEventSubscriber implements EventSubscriber\n{\n    protected $container;\n\n    \/**\n     * @param \\Symfony\\Component\\DependencyInjection\\ContainerInterface $container\n     *\/\n    public function __construct(ContainerInterface $container)\n    {\n        $this->container = $container;\n    }\n\n    \/**\n     * @return \\Sonata\\MediaBundle\\Provider\\Pool\n     *\/\n    public function getPool()\n    {\n        return $this->container->get('sonata.media.pool');\n    }\n\n    \/**\n     * @abstract\n     *\n     * @param \\Doctrine\\Common\\EventArgs $args\n     *\/\n    abstract protected function recomputeSingleEntityChangeSet(EventArgs $args);\n\n    \/**\n     * @abstract\n     *\n     * @param \\Doctrine\\Common\\EventArgs $args\n     *\n     * @return \\Sonata\\MediaBundle\\Model\\MediaInterface\n     *\/\n    abstract protected function getMedia(EventArgs $args);\n\n    \/**\n     * @param \\Doctrine\\Common\\EventArgs $args\n     *\n     * @return \\Sonata\\MediaBundle\\Provider\\MediaProviderInterface\n     *\/\n    protected function getProvider(EventArgs $args)\n    {\n        $media = $this->getMedia($args);\n\n        if (!$media instanceof MediaInterface) {\n            return;\n        }\n\n        return $this->getPool()->getProvider($media->getProviderName());\n    }\n\n    \/**\n     * @param \\Doctrine\\Common\\EventArgs $args\n     *\/\n    public function postUpdate(EventArgs $args)\n    {\n        if (!($provider = $this->getProvider($args))) {\n            return;\n        }\n\n        $provider->postUpdate($this->getMedia($args));\n    }\n\n    \/**\n     * @param \\Doctrine\\Common\\EventArgs $args\n     *\/\n    public function postRemove(EventArgs $args)\n    {\n        if (!($provider = $this->getProvider($args))) {\n            return;\n        }\n\n        $provider->postRemove($this->getMedia($args));\n    }\n\n    \/**\n     * @param \\Doctrine\\Common\\EventArgs $args\n     *\/\n    public function postPersist(EventArgs $args)\n    {\n        if (!($provider = $this->getProvider($args))) {\n            return;\n        }\n\n        $provider->postPersist($this->getMedia($args));\n    }\n\n    \/**\n     * @param \\Doctrine\\Common\\EventArgs $args\n     *\/\n    public function preUpdate(EventArgs $args)\n    {\n        if (!($provider = $this->getProvider($args))) {\n            return;\n        }\n\n        $provider->transform($this->getMedia($args));\n        $provider->preUpdate($this->getMedia($args));\n\n        $this->recomputeSingleEntityChangeSet($args);\n    }\n\n    \/**\n     * @param \\Doctrine\\Common\\EventArgs $args\n     *\/\n    public function preRemove(EventArgs $args)\n    {\n        if (!($provider = $this->getProvider($args))) {\n            return;\n        }\n\n        $provider->preRemove($this->getMedia($args));\n    }\n\n    \/**\n     * @param \\Doctrine\\Common\\EventArgs $args\n     *\/\n    public function prePersist(EventArgs $args)\n    {\n        if (!($provider = $this->getProvider($args))) {\n            return;\n        }\n\n        $provider->transform($this->getMedia($args));\n        $provider->prePersist($this->getMedia($args));\n    }\n}\n","lang_cluster":"PHP","length":147,"code_uid":"81fbd56c2a5d4720b2000a8a38423269"}
{"diff_hunk":"@@ -46,6 +46,14 @@ class LiipImagineThumbnail implements ThumbnailInterface\n             $this->router = $cacheManager;\n         }\n         $this->cacheManager = $cacheManager;\n+        if (!$resolverRegistry instanceof ResolverRegistryInterface) {\n+            @trigger_error(sprintf(\n+                'Using %s without a %s is deprecated since version 3.16 and will no longer be possible in 4.0.',\n+                __CLASS__,\n+                ResolverRegistryInterface::class\n+            ), E_USER_DEPRECATED);\n+        }\n+        $this->resolverRegistry = $resolverRegistry;\n     }\n \n     \/**","old_code":"<?php\n\ndeclare(strict_types=1);\n\n\/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Thumbnail;\n\nuse Liip\\ImagineBundle\\Imagine\\Cache\\CacheManager;\nuse Sonata\\MediaBundle\\Model\\MediaInterface;\nuse Sonata\\MediaBundle\\Provider\\MediaProviderInterface;\nuse Symfony\\Component\\Routing\\RouterInterface;\n\nclass LiipImagineThumbnail implements ThumbnailInterface\n{\n    \/**\n     * @deprecated Since version 3.3, will be removed in 4.0.\n     *\n     * @var RouterInterface\n     *\/\n    protected $router;\n\n    \/**\n     * @var CacheManager\n     *\/\n    private $cacheManager;\n\n    \/**\n     * @param RouterInterface|CacheManager $cacheManager\n     *\/\n    public function __construct($cacheManager)\n    {\n        if ($cacheManager instanceof RouterInterface) {\n            @trigger_error(sprintf(\n                'Using an instance of %s is deprecated since version 3.3 and will be removed in 4.0. Use %s.',\n                RouterInterface::class,\n                CacheManager::class\n            ), E_USER_DEPRECATED);\n            $this->router = $cacheManager;\n        }\n        $this->cacheManager = $cacheManager;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function generatePublicUrl(MediaProviderInterface $provider, MediaInterface $media, $format)\n    {\n        $path = $provider->getReferenceImage($media);\n\n        if (MediaProviderInterface::FORMAT_ADMIN === $format || MediaProviderInterface::FORMAT_REFERENCE === $format) {\n            return $path;\n        }\n        if ($this->router instanceof RouterInterface && !($this->cacheManager instanceof CacheManager)) {\n            $path = $this->router->generate(\n                sprintf('_imagine_%s', $format),\n                ['path' => sprintf('%s\/%s_%s.jpg', $provider->generatePath($media), $media->getId(), $format)]\n            );\n        }\n\n        $path = $provider->getCdnPath($path, $media->getCdnIsFlushable());\n        if ($this->cacheManager instanceof CacheManager) {\n            $path = $this->cacheManager->getBrowserPath($path, $format);\n        }\n\n        return $path;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function generatePrivateUrl(MediaProviderInterface $provider, MediaInterface $media, $format)\n    {\n        if (MediaProviderInterface::FORMAT_REFERENCE !== $format) {\n            throw new \\RuntimeException('No private url for LiipImagineThumbnail');\n        }\n\n        $path = $provider->getReferenceImage($media);\n\n        return $path;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function generate(MediaProviderInterface $provider, MediaInterface $media)\n    {\n        \/\/ nothing to generate, as generated on demand\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function delete(MediaProviderInterface $provider, MediaInterface $media, $formats = null)\n    {\n        \/\/ feature not available\n    }\n}\n","lang_cluster":"PHP","length":105,"code_uid":"ec47a03ea3f3435d9000ea350c1357bb"}
{"diff_hunk":"@@ -127,7 +127,7 @@ final class DailyMotionProvider extends BaseVideoProvider\n     {\n         $this->fixBinaryContent($media);\n \n-        if (!$media->getBinaryContent()) {\n+        if (null === $media->getBinaryContent()) {\n             return;\n         }\n ","old_code":"<?php\n\ndeclare(strict_types=1);\n\n\/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Provider;\n\nuse Sonata\\MediaBundle\\Model\\MediaInterface;\nuse Symfony\\Component\\HttpFoundation\\RedirectResponse;\nuse Symfony\\Component\\HttpFoundation\\Response;\n\nfinal class DailyMotionProvider extends BaseVideoProvider\n{\n    public function getHelperProperties(MediaInterface $media, string $format, array $options = []): array\n    {\n        \/\/ documentation : http:\/\/www.dailymotion.com\/en\/doc\/api\/player\n\n        $defaults = [\n            \/\/ Values: 0 or 1. Default is 0. Determines if the player loads related videos when\n            \/\/ the current video begins playback.\n            'related' => 0,\n\n            \/\/ Values: 0 or 1. Default is 1. Determines if the player allows explicit content to\n            \/\/ be played. This parameter may be added to embed code by platforms which do not\n            \/\/ want explicit content to be posted by their users.\n            'explicit' => 0,\n\n            \/\/ Values: 0 or 1. Default is 0. Determines if the video will begin playing\n            \/\/ automatically when the player loads.\n            'autoPlay' => 0,\n\n            \/\/ Values: 0 or 1. Default is 0. Determines if the video will begin muted.\n            'autoMute' => 0,\n\n            \/\/ Values: 0 or 1. Default is 0. Determines if the video will unmuted on mouse over.\n            \/\/ Of course it works only if the player is on automute=1.\n            'unmuteOnMouseOver' => 0,\n\n            \/\/ Values: a number of seconds. Default is 0. Determines if the video will begin\n            \/\/ playing the video at a given time.\n            'start' => 0,\n\n            \/\/ Values: 0 or 1. Default is 0. Enable the Javascript API by setting this parameter\n            \/\/ to 1. For more information and instructions on using the Javascript API, see the\n            \/\/ JavaScript API documentation.\n            'enableApi' => 0,\n\n            \/\/ Values: 0 or 1. Default is 0. Determines if the player should display controls\n            \/\/ or not during video playback.\n            'chromeless' => 0,\n\n            \/\/ Values: 0 or 1. Default is 0. Determines if the video should be expended to fit\n            \/\/ the whole player's size.\n            'expendVideo' => 0,\n            'color2' => null,\n\n            \/\/ Player color changes may be set using color codes. A color is described by its\n            \/\/ hexadecimal value (eg: FF0000 for red).\n            'foreground' => null,\n            'background' => null,\n            'highlight' => null,\n        ];\n\n        $player_parameters = array_merge($defaults, $options['player_parameters'] ?? []);\n\n        $box = $this->getBoxHelperProperties($media, $format, $options);\n\n        $params = [\n            'player_parameters' => http_build_query($player_parameters),\n            'allowFullScreen' => $options['allowFullScreen'] ?? 'true',\n            'allowScriptAccess' => $options['allowScriptAccess'] ?? 'always',\n            'width' => $box->getWidth(),\n            'height' => $box->getHeight(),\n        ];\n\n        return $params;\n    }\n\n    public function getProviderMetadata(): MetadataInterface\n    {\n        return new Metadata($this->getName(), $this->getName().'.description', 'bundles\/sonatamedia\/dailymotion-icon.png', 'SonataMediaBundle');\n    }\n\n    public function updateMetadata(MediaInterface $media, bool $force = false): void\n    {\n        $url = sprintf('http:\/\/www.dailymotion.com\/services\/oembed?url=%s&format=json', $this->getReferenceUrl($media));\n\n        try {\n            $metadata = $this->getMetadata($media, $url);\n        } catch (\\RuntimeException $e) {\n            $media->setEnabled(false);\n            $media->setProviderStatus(MediaInterface::STATUS_ERROR);\n\n            return;\n        }\n\n        $media->setProviderMetadata($metadata);\n\n        if ($force) {\n            $media->setName($metadata['title']);\n            $media->setAuthorName($metadata['author_name']);\n        }\n\n        $media->setHeight($metadata['height']);\n        $media->setWidth($metadata['width']);\n    }\n\n    public function getDownloadResponse(MediaInterface $media, string $format, string $mode, array $headers = []): Response\n    {\n        return new RedirectResponse($this->getReferenceUrl($media), 302, $headers);\n    }\n\n    public function getReferenceUrl(MediaInterface $media): string\n    {\n        return sprintf('http:\/\/www.dailymotion.com\/video\/%s', $media->getProviderReference());\n    }\n\n    protected function doTransform(MediaInterface $media): void\n    {\n        $this->fixBinaryContent($media);\n\n        if (!$media->getBinaryContent()) {\n            return;\n        }\n\n        $media->setProviderName($this->name);\n        $media->setProviderStatus(MediaInterface::STATUS_OK);\n        $media->setProviderReference($media->getBinaryContent());\n\n        $this->updateMetadata($media, true);\n    }\n\n    private function fixBinaryContent(MediaInterface $media): void\n    {\n        if (!$media->getBinaryContent()) {\n            return;\n        }\n\n        if (preg_match('{^(?:https?:\/\/)?www.dailymotion.com\/video\/(?<video_id>[0-9a-zA-Z]*)}', $media->getBinaryContent(), $matches)) {\n            $media->setBinaryContent($matches['video_id']);\n        }\n    }\n}\n","lang_cluster":"PHP","length":151,"code_uid":"7f08a7a385b6465fa83abf224edcce97"}
{"diff_hunk":"@@ -29,13 +29,14 @@ class AttributeImportFilter\n         $this->attributeImportValidator = $attributeImportValidator;\n     }\n \n-\n     \/**\n-     * @var TranslatableString[]\n+     * @param TranslatableString[] $attributes\n+     *\n+     * @return TranslatableString[]\n      *\/\n-    public function filter(array $attributes): array\n+    public function filter(array $attributes, string $skuValue): array\n     {\n-        $attributesToRedispatch = [];\n+        $filteredAttributes = [];\n         foreach ($attributes as $codeValue => $attribute) {\n             $code = new AttributeCode($codeValue);\n             $attributeType = $this->attributeQuery->getAttributeTypeByCode($code);","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Ergonode Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types=1);\n\nnamespace Ergonode\\Importer\\Infrastructure\\Filter;\n\nuse Ergonode\\Attribute\\Domain\\Query\\AttributeQueryInterface;\nuse Ergonode\\Attribute\\Domain\\ValueObject\\AttributeCode;\nuse Ergonode\\Core\\Domain\\ValueObject\\TranslatableString;\nuse Ergonode\\Importer\\Infrastructure\\Exception\\ImportException;\nuse Ergonode\\Importer\\Infrastructure\\Validator\\AttributeImportValidator;\n\nclass AttributeImportFilter\n{\n    private AttributeQueryInterface $attributeQuery;\n\n    private AttributeImportValidator $attributeImportValidator;\n\n    public function __construct(\n        AttributeQueryInterface $attributeQuery,\n        AttributeImportValidator $attributeImportValidator\n    ) {\n        $this->attributeQuery = $attributeQuery;\n        $this->attributeImportValidator = $attributeImportValidator;\n    }\n\n\n    \/**\n     * @var TranslatableString[]\n     *\/\n    public function filter(array $attributes): array\n    {\n        $attributesToRedispatch = [];\n        foreach ($attributes as $codeValue => $attribute) {\n            $code = new AttributeCode($codeValue);\n            $attributeType = $this->attributeQuery->getAttributeTypeByCode($code);\n\n            if (!$attributeType) {\n                throw new ImportException(\n                    'Attribute with {code} code does not exist',\n                    ['{code}' => $code->getValue()]\n                );\n            }\n\n            if (!$this->attributeImportValidator->validate($attributeType, $code, $attribute)) {\n                $attributesToRedispatch[$codeValue] = $attribute;\n            }\n        }\n\n        return $attributesToRedispatch;\n    }\n}\n","lang_cluster":"PHP","length":57,"code_uid":"2fe345ae0c0948e58ba8371f2a0e0dee"}
{"diff_hunk":"@@ -11,11 +11,14 @@\n \n namespace EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Type;\n \n+use ArrayObject;\n use EasyCorp\\Bundle\\EasyAdminBundle\\Configuration\\ConfigManager;\n use EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Type\\Configurator\\TypeConfiguratorInterface;\n use EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Util\\LegacyFormHelper;\n use Symfony\\Component\\Form\\AbstractType;\n use Symfony\\Component\\Form\\FormBuilderInterface;\n+use Symfony\\Component\\Form\\FormEvent;\n+use Symfony\\Component\\Form\\FormEvents;\n use Symfony\\Component\\Form\\FormInterface;\n use Symfony\\Component\\Form\\FormView;\n use Symfony\\Component\\OptionsResolver\\Options;","old_code":"<?php\n\n\/*\n * This file is part of the EasyAdminBundle.\n *\n * (c) Javier Eguiluz <javier.eguiluz@gmail.com>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Type;\n\nuse EasyCorp\\Bundle\\EasyAdminBundle\\Configuration\\ConfigManager;\nuse EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Type\\Configurator\\TypeConfiguratorInterface;\nuse EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Util\\LegacyFormHelper;\nuse Symfony\\Component\\Form\\AbstractType;\nuse Symfony\\Component\\Form\\FormBuilderInterface;\nuse Symfony\\Component\\Form\\FormInterface;\nuse Symfony\\Component\\Form\\FormView;\nuse Symfony\\Component\\OptionsResolver\\Options;\nuse Symfony\\Component\\OptionsResolver\\OptionsResolver;\nuse Symfony\\Component\\OptionsResolver\\OptionsResolverInterface;\n\n\/**\n * Custom form type that deals with some of the logic used to render the\n * forms used to create and edit EasyAdmin entities.\n *\n * @author Maxime Steinhausser <maxime.steinhausser@gmail.com>\n *\/\nclass EasyAdminFormType extends AbstractType\n{\n    \/** @var ConfigManager *\/\n    private $configManager;\n\n    \/** @var TypeConfiguratorInterface[] *\/\n    private $configurators;\n\n    \/**\n     * @param ConfigManager               $configManager\n     * @param TypeConfiguratorInterface[] $configurators\n     *\/\n    public function __construct(ConfigManager $configManager, array $configurators = array())\n    {\n        $this->configManager = $configManager;\n        $this->configurators = $configurators;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function buildForm(FormBuilderInterface $builder, array $options)\n    {\n        $entity = $options['entity'];\n        $view = $options['view'];\n        $entityConfig = $this->configManager->getEntityConfig($entity);\n        $entityProperties = isset($entityConfig[$view]['fields']) ? $entityConfig[$view]['fields'] : array();\n        $formGroups = array();\n        $currentFormGroup = null;\n\n        foreach ($entityProperties as $name => $metadata) {\n            $formFieldOptions = $metadata['type_options'];\n\n            \/\/ Configure options using the list of registered type configurators:\n            foreach ($this->configurators as $configurator) {\n                if ($configurator->supports($metadata['fieldType'], $formFieldOptions, $metadata)) {\n                    $formFieldOptions = $configurator->configure($name, $formFieldOptions, $metadata, $builder);\n                }\n            }\n\n            $formFieldType = LegacyFormHelper::getType($metadata['fieldType']);\n\n            \/\/ if the form field is a special 'group' design element, don't add it\n            \/\/ to the form. Instead, consider it the current form group (this is\n            \/\/ applied to the form fields defined after it) and store its details\n            \/\/ in a property to get them in form template\n            if (in_array($formFieldType, array('easyadmin_group', 'EasyCorp\\\\Bundle\\\\EasyAdminBundle\\\\Form\\\\Type\\\\EasyAdminGroupType'))) {\n                $currentFormGroup = $metadata['fieldName'];\n                $formGroups[$currentFormGroup] = $metadata;\n\n                continue;\n            }\n\n            \/\/ 'divider' and 'section' are 'fake' form fields used to create the design\n            \/\/ elements of the complex form layouts: define them as unmapped and non-required\n            if (0 === strpos($metadata['property'], '_easyadmin_form_design_element_')) {\n                $formFieldOptions['mapped'] = false;\n                $formFieldOptions['required'] = false;\n            }\n\n            $formField = $builder->getFormFactory()->createNamedBuilder($name, $formFieldType, null, $formFieldOptions);\n            $formField->setAttribute('easyadmin_form_group', $currentFormGroup);\n\n            $builder->add($formField);\n        }\n\n        $builder->setAttribute('easyadmin_form_groups', $formGroups);\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function finishView(FormView $view, FormInterface $form, array $options)\n    {\n        $view->vars['easyadmin_form_groups'] = $form->getConfig()->getAttribute('easyadmin_form_groups');\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function configureOptions(OptionsResolver $resolver)\n    {\n        $configManager = $this->configManager;\n\n        $resolver\n            ->setDefaults(array(\n                'allow_extra_fields' => true,\n                'data_class' => function (Options $options) use ($configManager) {\n                    $entity = $options['entity'];\n                    $entityConfig = $configManager->getEntityConfig($entity);\n\n                    return $entityConfig['class'];\n                },\n            ))\n            ->setRequired(array('entity', 'view'));\n\n        \/\/ setNormalizer() is available since Symfony 2.6\n        if (method_exists($resolver, 'setNormalizer')) {\n            $resolver->setNormalizer('attr', $this->getAttributesNormalizer());\n        } else {\n            \/\/ BC for Symfony < 2.6\n            $resolver->setNormalizers(array('attr' => $this->getAttributesNormalizer()));\n        }\n    }\n\n    \/\/ BC for SF < 2.7\n    public function setDefaultOptions(OptionsResolverInterface $resolver)\n    {\n        $this->configureOptions($resolver);\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getBlockPrefix()\n    {\n        return 'easyadmin';\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getName()\n    {\n        return $this->getBlockPrefix();\n    }\n\n    \/**\n     * Returns a closure normalizing the form html attributes.\n     *\n     * @return \\Closure\n     *\/\n    private function getAttributesNormalizer()\n    {\n        return function (Options $options, $value) {\n            return array_replace(array(\n                'id' => sprintf('%s-%s-form', $options['view'], mb_strtolower($options['entity'])),\n            ), $value);\n        };\n    }\n}\n\nclass_alias('EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Type\\EasyAdminFormType', 'JavierEguiluz\\Bundle\\EasyAdminBundle\\Form\\Type\\EasyAdminFormType', false);\n","lang_cluster":"PHP","length":173,"code_uid":"2ddf0195cc844428b3d9ea3e2a84dab4"}
{"diff_hunk":"@@ -61,7 +61,7 @@ class FixMediaContextCommandTest extends TestCase\n      *\/\n     protected function setUp()\n     {\n-        $this->container = $this->getMockBuilder('Symfony\\Component\\DependencyInjection\\ContainerInterface')->getMock();\n+        $this->container = $this->getMockBuilder(ContainerInterface::class)->getMock();\n \n         $this->command = new FixMediaContextCommand();\n         $this->command->setContainer($this->container);","old_code":"<?php\n\n\/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Tests\\Command;\n\nuse PHPUnit\\Framework\\TestCase;\nuse Sonata\\ClassificationBundle\\Model\\ContextManagerInterface;\nuse Sonata\\MediaBundle\\Command\\FixMediaContextCommand;\nuse Sonata\\MediaBundle\\Model\\CategoryManagerInterface;\nuse Sonata\\MediaBundle\\Provider\\Pool;\nuse Symfony\\Component\\Console\\Application;\nuse Symfony\\Component\\Console\\Tester\\CommandTester;\n\nclass FixMediaContextCommandTest extends TestCase\n{\n    \/**\n     * @var \\PHPUnit_Framework_MockObject_MockObject|ContainerInterface\n     *\/\n    protected $container;\n\n    \/**\n     * @var Application\n     *\/\n    protected $application;\n\n    \/**\n     * @var ContainerAwareCommand\n     *\/\n    protected $command;\n\n    \/**\n     * @var CommandTester\n     *\/\n    protected $tester;\n\n    \/**\n     * @var \\PHPUnit_Framework_MockObject_MockObject|Pool\n     *\/\n    private $pool;\n\n    \/**\n     * @var \\PHPUnit_Framework_MockObject_MockObject|ContextManagerInterface\n     *\/\n    private $contextManger;\n\n    \/**\n     * @var \\PHPUnit_Framework_MockObject_MockObject|CategoryManagerInterface\n     *\/\n    private $categoryManger;\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    protected function setUp()\n    {\n        $this->container = $this->getMockBuilder('Symfony\\Component\\DependencyInjection\\ContainerInterface')->getMock();\n\n        $this->command = new FixMediaContextCommand();\n        $this->command->setContainer($this->container);\n\n        $this->application = new Application();\n        $this->application->add($this->command);\n\n        $this->tester = new CommandTester($this->application->find('sonata:media:fix-media-context'));\n\n        $this->pool = $pool = $this->getMockBuilder('Sonata\\MediaBundle\\Provider\\Pool')->disableOriginalConstructor()->getMock();\n\n        $this->contextManger = $contextManger = $this->getMockBuilder('Sonata\\ClassificationBundle\\Model\\ContextManagerInterface')->getMock();\n\n        $this->categoryManger = $categoryManger = $this->createMock('Sonata\\MediaBundle\\Model\\CategoryManagerInterface');\n\n        $this->container->expects($this->any())\n            ->method('get')\n            ->will($this->returnCallback(function ($id) use ($pool, $contextManger, $categoryManger) {\n                switch ($id) {\n                    case 'sonata.media.pool':\n                        return $pool;\n                    case 'sonata.classification.manager.context':\n                        return $contextManger;\n                    case 'sonata.media.manager.category':\n                        return $categoryManger;\n                }\n            }));\n    }\n\n    public function testExecuteWithDisabledClassfication()\n    {\n        $this->container->method('has')->with($this->equalTo('sonata.media.manager.category'))\n            ->will($this->returnValue(false));\n\n        $this->expectException('\\LogicException');\n\n        $this->tester->execute(['command' => $this->command->getName()]);\n    }\n\n    public function testExecuteWithExisting()\n    {\n        $context = [\n            'providers' => [],\n            'formats' => [],\n            'download' => [],\n        ];\n\n        $this->container->method('has')->with($this->equalTo('sonata.media.manager.category'))\n            ->will($this->returnValue(true));\n\n        $this->pool->expects($this->any())->method('getContexts')->will($this->returnValue(['foo' => $context]));\n\n        $contextModel = $this->getMockBuilder('Sonata\\ClassificationBundle\\Model\\ContextInterface')->getMock();\n\n        $this->contextManger->expects($this->once())->method('findOneBy')->with($this->equalTo(['id' => 'foo']))->will($this->returnValue($contextModel));\n\n        $category = $this->getMockBuilder('Sonata\\ClassificationBundle\\Model\\CategoryInterface')->getMock();\n\n        $this->categoryManger->expects($this->once())->method('getRootCategory')->with($this->equalTo($contextModel))->will($this->returnValue($category));\n\n        $output = $this->tester->execute(['command' => $this->command->getName()]);\n\n        $this->assertRegExp('@Done!@', $this->tester->getDisplay());\n\n        $this->assertSame(0, $output);\n    }\n\n    public function testExecuteWithEmptyRoot()\n    {\n        $context = [\n            'providers' => [],\n            'formats' => [],\n            'download' => [],\n        ];\n\n        $this->container->method('has')->with($this->equalTo('sonata.media.manager.category'))\n            ->will($this->returnValue(true));\n\n        $this->pool->expects($this->any())->method('getContexts')->will($this->returnValue(['foo' => $context]));\n\n        $contextModel = $this->getMockBuilder('Sonata\\ClassificationBundle\\Model\\ContextInterface')->getMock();\n\n        $this->contextManger->expects($this->once())->method('findOneBy')->with($this->equalTo(['id' => 'foo']))->will($this->returnValue($contextModel));\n\n        $category = $this->getMockBuilder('Sonata\\ClassificationBundle\\Model\\CategoryInterface')->getMock();\n\n        $this->categoryManger->expects($this->once())->method('getRootCategory')->with($this->equalTo($contextModel))->will($this->returnValue(null));\n        $this->categoryManger->expects($this->once())->method('create')->will($this->returnValue($category));\n        $this->categoryManger->expects($this->once())->method('save')->with($this->equalTo($category));\n\n        $output = $this->tester->execute(['command' => $this->command->getName()]);\n\n        $this->assertRegExp('@ > default category for \\'foo\\' is missing, creating one\\s+Done!@', $this->tester->getDisplay());\n\n        $this->assertSame(0, $output);\n    }\n\n    public function testExecuteWithNew()\n    {\n        $context = [\n            'providers' => [],\n            'formats' => [],\n            'download' => [],\n        ];\n\n        $this->container->method('has')->with($this->equalTo('sonata.media.manager.category'))\n            ->will($this->returnValue(true));\n\n        $this->pool->expects($this->any())->method('getContexts')->will($this->returnValue(['foo' => $context]));\n\n        $contextModel = $this->getMockBuilder('Sonata\\ClassificationBundle\\Model\\ContextInterface')->getMock();\n\n        $this->contextManger->expects($this->once())->method('findOneBy')->with($this->equalTo(['id' => 'foo']))->will($this->returnValue(null));\n        $this->contextManger->expects($this->once())->method('create')->will($this->returnValue($contextModel));\n        $this->contextManger->expects($this->once())->method('save')->with($this->equalTo($contextModel));\n\n        $category = $this->getMockBuilder('Sonata\\ClassificationBundle\\Model\\CategoryInterface')->getMock();\n\n        $this->categoryManger->expects($this->once())->method('getRootCategory')->with($this->equalTo($contextModel))->will($this->returnValue(null));\n        $this->categoryManger->expects($this->once())->method('create')->will($this->returnValue($category));\n        $this->categoryManger->expects($this->once())->method('save')->with($this->equalTo($category));\n\n        $output = $this->tester->execute(['command' => $this->command->getName()]);\n\n        $this->assertRegExp('@ > default category for \\'foo\\' is missing, creating one\\s+Done!@', $this->tester->getDisplay());\n\n        $this->assertSame(0, $output);\n    }\n}\n","lang_cluster":"PHP","length":193,"code_uid":"07c1dbe09f4d447eb5cdc6e92dc53333"}
{"diff_hunk":"@@ -117,5 +117,9 @@ class CustomerUserPasswordFacade\n         $encoder = $this->encoderFactory->getEncoder($customerUser);\n         $passwordHash = $encoder->encodePassword($password, null);\n         $customerUser->setPasswordHash($passwordHash);\n+\n+        $this->em->flush();\n+\n+        $this->customerUserRefreshTokenChainFacade->removeAllCustomerUserRefreshTokenChains($customerUser);\n     }\n }","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Shopsys\\FrameworkBundle\\Model\\Customer\\User;\n\nuse Doctrine\\ORM\\EntityManagerInterface;\nuse Shopsys\\FrameworkBundle\\Component\\String\\HashGenerator;\nuse Shopsys\\FrameworkBundle\\Model\\Customer\\Mail\\ResetPasswordMailFacade;\nuse Symfony\\Component\\Security\\Core\\Encoder\\EncoderFactoryInterface;\n\nclass CustomerUserPasswordFacade\n{\n    public const RESET_PASSWORD_HASH_LENGTH = 50;\n\n    \/**\n     * @var \\Doctrine\\ORM\\EntityManagerInterface\n     *\/\n    protected $em;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUserRepository\n     *\/\n    protected $customerUserRepository;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Customer\\Mail\\ResetPasswordMailFacade\n     *\/\n    protected $resetPasswordMailFacade;\n\n    \/**\n     * @var \\Symfony\\Component\\Security\\Core\\Encoder\\EncoderFactoryInterface\n     *\/\n    protected $encoderFactory;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Component\\String\\HashGenerator\n     *\/\n    protected $hashGenerator;\n\n    \/**\n     * @param \\Doctrine\\ORM\\EntityManagerInterface $em\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUserRepository $customerUserRepository\n     * @param \\Symfony\\Component\\Security\\Core\\Encoder\\EncoderFactoryInterface $encoderFactory\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Customer\\Mail\\ResetPasswordMailFacade $resetPasswordMailFacade\n     * @param \\Shopsys\\FrameworkBundle\\Component\\String\\HashGenerator $hashGenerator\n     *\/\n    public function __construct(\n        EntityManagerInterface $em,\n        CustomerUserRepository $customerUserRepository,\n        EncoderFactoryInterface $encoderFactory,\n        ResetPasswordMailFacade $resetPasswordMailFacade,\n        HashGenerator $hashGenerator\n    ) {\n        $this->em = $em;\n        $this->customerUserRepository = $customerUserRepository;\n        $this->encoderFactory = $encoderFactory;\n        $this->resetPasswordMailFacade = $resetPasswordMailFacade;\n        $this->hashGenerator = $hashGenerator;\n    }\n\n    \/**\n     * @param string $email\n     * @param int $domainId\n     *\/\n    public function resetPassword($email, $domainId)\n    {\n        $customerUser = $this->customerUserRepository->getCustomerUserByEmailAndDomain($email, $domainId);\n\n        $resetPasswordHash = $this->hashGenerator->generateHash(static::RESET_PASSWORD_HASH_LENGTH);\n        $customerUser->setResetPasswordHash($resetPasswordHash);\n\n        $this->em->flush($customerUser);\n        $this->resetPasswordMailFacade->sendMail($customerUser);\n    }\n\n    \/**\n     * @param string $email\n     * @param int $domainId\n     * @param string|null $hash\n     * @return bool\n     *\/\n    public function isResetPasswordHashValid($email, $domainId, $hash)\n    {\n        $customerUser = $this->customerUserRepository->getCustomerUserByEmailAndDomain($email, $domainId);\n\n        return $customerUser->isResetPasswordHashValid($hash);\n    }\n\n    \/**\n     * @param string $email\n     * @param int $domainId\n     * @param string|null $resetPasswordHash\n     * @param string $newPassword\n     *\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUser\n     *\/\n    public function setNewPassword(string $email, int $domainId, ?string $resetPasswordHash, string $newPassword): CustomerUser\n    {\n        $customerUser = $this->customerUserRepository->getCustomerUserByEmailAndDomain($email, $domainId);\n\n        if (!$customerUser->isResetPasswordHashValid($resetPasswordHash)) {\n            throw new \\Shopsys\\FrameworkBundle\\Model\\Customer\\Exception\\InvalidResetPasswordHashUserException();\n        }\n\n        $this->changePassword($customerUser, $newPassword);\n\n        return $customerUser;\n    }\n\n    \/**\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUser $customerUser\n     * @param string $password\n     *\/\n    public function changePassword(CustomerUser $customerUser, string $password): void\n    {\n        $encoder = $this->encoderFactory->getEncoder($customerUser);\n        $passwordHash = $encoder->encodePassword($password, null);\n        $customerUser->setPasswordHash($passwordHash);\n    }\n}\n","lang_cluster":"PHP","length":121,"code_uid":"5f6894d388c344a6aa9e8080e1aebd5e"}
{"diff_hunk":"@@ -100,9 +100,9 @@ return [\n     'required_if'          => 'The :attribute field is required when :other is :value.',\n     'required_unless'      => 'The :attribute field is required unless :other is in :values.',\n     'required_with'        => 'The :attribute field is required when :values is present.',\n-    'required_with_all'    => 'The :attribute field is required when :values are present.',\n     'required_without'     => 'The :attribute field is required when :values is not present.',\n     'required_without_all' => 'The :attribute field is required when none of :values are present.',\n+    'required_with_all'    => 'The :attribute field is required when :values are present.',\n     'same'                 => 'The :attribute and :other must match.',\n     'size'                 => [\n         'numeric' => 'The :attribute must be :size.',","old_code":"<?php\n\nreturn [\n    \/*\n    |--------------------------------------------------------------------------\n    | Validation Language Lines\n    |--------------------------------------------------------------------------\n    |\n    | The following language lines contain the default error messages used by\n    | the validator class. Some of these rules have multiple versions such\n    | as the size rules. Feel free to tweak each of these messages here.\n    |\n    *\/\n\n    'accepted'             => 'The :attribute must be accepted.',\n    'active_url'           => 'The :attribute is not a valid URL.',\n    'after'                => 'The :attribute must be a date after :date.',\n    'after_or_equal'       => 'The :attribute must be a date after or equal to :date.',\n    'alpha'                => 'The :attribute may only contain letters.',\n    'alpha_dash'           => 'The :attribute may only contain letters, numbers, dashes and underscores.',\n    'alpha_num'            => 'The :attribute may only contain letters and numbers.',\n    'array'                => 'The :attribute must be an array.',\n    'before'               => 'The :attribute must be a date before :date.',\n    'before_or_equal'      => 'The :attribute must be a date before or equal to :date.',\n    'between'              => [\n        'numeric' => 'The :attribute must be between :min and :max.',\n        'file'    => 'The :attribute must be between :min and :max kilobytes.',\n        'string'  => 'The :attribute must be between :min and :max characters.',\n        'array'   => 'The :attribute must have between :min and :max items.',\n    ],\n    'boolean'              => 'The :attribute field must be true or false.',\n    'confirmed'            => 'The :attribute confirmation does not match.',\n    'date'                 => 'The :attribute is not a valid date.',\n    'date_equals'          => 'The :attribute must be a date equal to :date.',\n    'date_format'          => 'The :attribute does not match the format :format.',\n    'different'            => 'The :attribute and :other must be different.',\n    'digits'               => 'The :attribute must be :digits digits.',\n    'digits_between'       => 'The :attribute must be between :min and :max digits.',\n    'dimensions'           => 'The :attribute has invalid image dimensions.',\n    'distinct'             => 'The :attribute field has a duplicate value.',\n    'email'                => 'The :attribute must be a valid email address.',\n    'ends_with'            => 'The :attribute must end with one of the following: :values.',\n    'exists'               => 'The selected :attribute is invalid.',\n    'file'                 => 'The :attribute must be a file.',\n    'filled'               => 'The :attribute field must have a value.',\n    'gt'                   => [\n        'numeric' => 'The :attribute must be greater than :value.',\n        'file'    => 'The :attribute must be greater than :value kilobytes.',\n        'string'  => 'The :attribute must be greater than :value characters.',\n        'array'   => 'The :attribute must have more than :value items.',\n    ],\n    'gte'                  => [\n        'numeric' => 'The :attribute must be greater than or equal :value.',\n        'file'    => 'The :attribute must be greater than or equal :value kilobytes.',\n        'string'  => 'The :attribute must be greater than or equal :value characters.',\n        'array'   => 'The :attribute must have :value items or more.',\n    ],\n    'image'                => 'The :attribute must be an image.',\n    'in'                   => 'The selected :attribute is invalid.',\n    'in_array'             => 'The :attribute field does not exist in :other.',\n    'integer'              => 'The :attribute must be an integer.',\n    'ip'                   => 'The :attribute must be a valid IP address.',\n    'ipv4'                 => 'The :attribute must be a valid IPv4 address.',\n    'ipv6'                 => 'The :attribute must be a valid IPv6 address.',\n    'json'                 => 'The :attribute must be a valid JSON string.',\n    'lt'                   => [\n        'numeric' => 'The :attribute must be less than :value.',\n        'file'    => 'The :attribute must be less than :value kilobytes.',\n        'string'  => 'The :attribute must be less than :value characters.',\n        'array'   => 'The :attribute must have less than :value items.',\n    ],\n    'lte'                  => [\n        'numeric' => 'The :attribute must be less than or equal :value.',\n        'file'    => 'The :attribute must be less than or equal :value kilobytes.',\n        'string'  => 'The :attribute must be less than or equal :value characters.',\n        'array'   => 'The :attribute must not have more than :value items.',\n    ],\n    'max'                  => [\n        'numeric' => 'The :attribute may not be greater than :max.',\n        'file'    => 'The :attribute may not be greater than :max kilobytes.',\n        'string'  => 'The :attribute may not be greater than :max characters.',\n        'array'   => 'The :attribute may not have more than :max items.',\n    ],\n    'mimes'                => 'The :attribute must be a file of type: :values.',\n    'mimetypes'            => 'The :attribute must be a file of type: :values.',\n    'min'                  => [\n        'numeric' => 'The :attribute must be at least :min.',\n        'file'    => 'The :attribute must be at least :min kilobytes.',\n        'string'  => 'The :attribute must be at least :min characters.',\n        'array'   => 'The :attribute must have at least :min items.',\n    ],\n    'multiple_of'          => 'The :attribute must be a multiple of :value',\n    'not_in'               => 'The selected :attribute is invalid.',\n    'not_regex'            => 'The :attribute format is invalid.',\n    'numeric'              => 'The :attribute must be a number.',\n    'password'             => 'The password is incorrect.',\n    'present'              => 'The :attribute field must be present.',\n    'regex'                => 'The :attribute format is invalid.',\n    'required'             => 'The :attribute field is required.',\n    'required_if'          => 'The :attribute field is required when :other is :value.',\n    'required_unless'      => 'The :attribute field is required unless :other is in :values.',\n    'required_with'        => 'The :attribute field is required when :values is present.',\n    'required_with_all'    => 'The :attribute field is required when :values are present.',\n    'required_without'     => 'The :attribute field is required when :values is not present.',\n    'required_without_all' => 'The :attribute field is required when none of :values are present.',\n    'same'                 => 'The :attribute and :other must match.',\n    'size'                 => [\n        'numeric' => 'The :attribute must be :size.',\n        'file'    => 'The :attribute must be :size kilobytes.',\n        'string'  => 'The :attribute must be :size characters.',\n        'array'   => 'The :attribute must contain :size items.',\n    ],\n    'starts_with'          => 'The :attribute must start with one of the following: :values.',\n    'string'               => 'The :attribute must be a string.',\n    'timezone'             => 'The :attribute must be a valid zone.',\n    'unique'               => 'The :attribute has already been taken.',\n    'uploaded'             => 'The :attribute failed to upload.',\n    'url'                  => 'The :attribute format is invalid.',\n    'uuid'                 => 'The :attribute must be a valid UUID.',\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Custom Validation Language Lines\n    |--------------------------------------------------------------------------\n    |\n    | Here you may specify custom validation messages for attributes using the\n    | convention \"attribute.rule\" to name the lines. This makes it quick to\n    | specify a specific custom language line for a given attribute rule.\n    |\n    *\/\n\n    'custom' => [\n        'attribute-name' => [\n            'rule-name' => 'custom-message',\n        ],\n    ],\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Custom Validation Attributes\n    |--------------------------------------------------------------------------\n    |\n    | The following language lines are used to swap our attribute placeholder\n    | with something more reader friendly such as \"E-Mail Address\" instead\n    | of \"email\". This simply helps us make our message more expressive.\n    |\n    *\/\n\n    'attributes' => [],\n];\n","lang_cluster":"PHP","length":150,"code_uid":"19b96807c121402cb2d54675573756cd"}
{"diff_hunk":"@@ -22,6 +22,18 @@ use Symfony\\Component\\Console\\Output\\OutputInterface;\n  *\/\n class MigrateToJsonTypeCommand extends BaseCommand\n {\n+    \/**\n+     * @var EntityManagerInterface\n+     *\/\n+    private $entityManager;\n+\n+    public function __construct(ManagerInterface $mediaManager, Pool $pool, ?EntityManagerInterface $entityManager = null)\n+    {\n+        parent::__construct($mediaManager, $pool);\n+\n+        $this->entityManager = $entityManager;\n+    }\n+\n     public function configure()\n     {\n         $this->setName('sonata:media:migrate-json');","old_code":"<?php\n\ndeclare(strict_types=1);\n\n\/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Command;\n\nuse Symfony\\Component\\Console\\Input\\InputInterface;\nuse Symfony\\Component\\Console\\Input\\InputOption;\nuse Symfony\\Component\\Console\\Output\\OutputInterface;\n\n\/**\n * @final since sonata-project\/media-bundle 3.21.0\n *\/\nclass MigrateToJsonTypeCommand extends BaseCommand\n{\n    public function configure()\n    {\n        $this->setName('sonata:media:migrate-json');\n        $this->addOption('table', null, InputOption::VALUE_OPTIONAL, 'Media table', 'media__media');\n        $this->addOption('column', null, InputOption::VALUE_OPTIONAL, 'Column name for provider_metadata', 'provider_metadata');\n        $this->addOption('column_id', null, InputOption::VALUE_OPTIONAL, 'Column name for id', 'id');\n        $this->setDescription('Migrate all media provider metadata to the doctrine JsonType');\n    }\n\n    public function execute(InputInterface $input, OutputInterface $output)\n    {\n        $count = 0;\n        $table = $input->getOption('table');\n        $column = $input->getOption('column');\n        $columnId = $input->getOption('column_id');\n        $connection = $this->getContainer()->get('doctrine.orm.entity_manager')->getConnection();\n        $medias = $connection->fetchAll(\"SELECT * FROM $table\");\n\n        foreach ($medias as $media) {\n            \/\/ if the row need to migrate\n            if (0 !== strpos($media[$column], '{') && '[]' !== $media[$column]) {\n                $media[$column] = json_encode(unserialize($media[$column]));\n                $connection->update($table, [$column => $media[$column]], [$columnId => $media[$columnId]]);\n                ++$count;\n            }\n        }\n\n        $output->writeln(\"Migrated $count medias\");\n\n        return 0;\n    }\n}\n","lang_cluster":"PHP","length":56,"code_uid":"bc35389ab6f84f9293b93bbf54a4cbd0"}
{"diff_hunk":"@@ -52,8 +52,15 @@ class UnitGridFactory implements GridFactoryInterface\n             ->from(Unit::class, 'u')\n             ->join('u.translations', 'ut', Join::WITH, 'ut.locale = :locale')\n             ->setParameter('locale', $this->localization->getAdminLocale());\n-        $dataSource = new QueryBuilderDataSource($queryBuilder, 'u.id');\n+        return new QueryBuilderDataSource($queryBuilder, 'u.id');\n+    }\n \n+    \/**\n+     * @param \\Shopsys\\FrameworkBundle\\Component\\Grid\\DataSourceInterface $dataSource\n+     * @return \\Shopsys\\FrameworkBundle\\Component\\Grid\\Grid\n+     *\/\n+    protected function getGridForDataSource(DataSourceInterface $dataSource): Grid\n+    {\n         $grid = $this->gridFactory->create('unitList', $dataSource);\n         $grid->setDefaultOrder('name');\n ","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\Model\\Product\\Unit;\n\nuse Doctrine\\ORM\\EntityManagerInterface;\nuse Doctrine\\ORM\\Query\\Expr\\Join;\nuse Shopsys\\FrameworkBundle\\Component\\Grid\\GridFactory;\nuse Shopsys\\FrameworkBundle\\Component\\Grid\\GridFactoryInterface;\nuse Shopsys\\FrameworkBundle\\Component\\Grid\\QueryBuilderDataSource;\nuse Shopsys\\FrameworkBundle\\Model\\Localization\\Localization;\n\nclass UnitGridFactory implements GridFactoryInterface\n{\n    \/**\n     * @var \\Doctrine\\ORM\\EntityManagerInterface\n     *\/\n    protected $em;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Component\\Grid\\GridFactory\n     *\/\n    protected $gridFactory;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Localization\\Localization\n     *\/\n    protected $localization;\n\n    \/**\n     * @param \\Doctrine\\ORM\\EntityManagerInterface $em\n     * @param \\Shopsys\\FrameworkBundle\\Component\\Grid\\GridFactory $gridFactory\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Localization\\Localization $localization\n     *\/\n    public function __construct(\n        EntityManagerInterface $em,\n        GridFactory $gridFactory,\n        Localization $localization\n    ) {\n        $this->em = $em;\n        $this->gridFactory = $gridFactory;\n        $this->localization = $localization;\n    }\n\n    \/**\n     * @return \\Shopsys\\FrameworkBundle\\Component\\Grid\\Grid\n     *\/\n    public function create()\n    {\n        $queryBuilder = $this->em->createQueryBuilder();\n        $queryBuilder\n            ->select('u, ut')\n            ->from(Unit::class, 'u')\n            ->join('u.translations', 'ut', Join::WITH, 'ut.locale = :locale')\n            ->setParameter('locale', $this->localization->getAdminLocale());\n        $dataSource = new QueryBuilderDataSource($queryBuilder, 'u.id');\n\n        $grid = $this->gridFactory->create('unitList', $dataSource);\n        $grid->setDefaultOrder('name');\n\n        $grid->addColumn('name', 'ut.name', t('Name'), true);\n\n        $grid->setActionColumnClassAttribute('table-col table-col-10');\n        $grid->addDeleteActionColumn('admin_unit_deleteconfirm', ['id' => 'u.id'])\n            ->setAjaxConfirm();\n\n        $grid->setTheme('@ShopsysFramework\/Admin\/Content\/Unit\/listGrid.html.twig');\n\n        return $grid;\n    }\n}\n","lang_cluster":"PHP","length":70,"code_uid":"34978fea0c714668afc95a1f72e45b0c"}
{"diff_hunk":"@@ -46,23 +46,28 @@ if (!is_valid_email_addr($email_addr)) {\n         \/\/ deal with the case where user hasn't set passwd\n         \/\/ (i.e. passwd is account key)\n         \/\/\n-        if ($passwd_hash != $user->passwd_hash) {\n+        if ($passwd_hash != $user->passwd_hash && !password_verify($passwd_hash, $user->passwd_hash)) {\n             $passwd = $user->authenticator;\n             $passwd_hash = md5($passwd.$user->email_addr);\n         }\n-        if ($passwd_hash != $user->passwd_hash) {\n+        if ($passwd_hash != $user->passwd_hash && !password_verify($passwd_hash, $user->passwd_hash)) {\n             echo tra(\"Invalid password.\");\n         } else {\n             $passwd_hash = md5($passwd.$email_addr);\n+            $database_passwd_hash = password_hash($passwd_hash , PASSWORD_DEFAULT);\n             $email_addr = BoincDb::escape_string($email_addr);\n+\t    $user->email_addr_change_time = time();\n             $result = $user->update(\n-                \"email_addr='$email_addr', passwd_hash='$passwd_hash', email_validated=0\"\n+                \"email_addr='$email_addr', previous_email_addr='$user->email_addr', email_addr_change_time=$user->email_addr_change_time, passwd_hash='$database_passwd_hash', email_validated=0\"\n             );\n+            $user->previous_email_addr=$user->email_addr;\n+            $user->email_addr=$email_addr;\n             if ($result) {\n                 echo tra(\"The email address of your account is now %1.\", $email_addr);\n                 if (defined(\"SHOW_NONVALIDATED_EMAIL_ADDR\")) {\n                     echo \"<p>\".tra(\"Please %1 validate this email address %2.\", \"<a href=validate_email_addr.php>\", \"<\/a>\").\"\\n\";\n                 }\n+                send_changed_email($user);\n             } else {\n                 echo tra(\"We can't update your email address due to a database problem.  Please try again later.\");\n             }","old_code":"<?php\n\/\/ This file is part of BOINC.\n\/\/ http:\/\/boinc.berkeley.edu\n\/\/ Copyright (C) 2014 University of California\n\/\/\n\/\/ BOINC is free software; you can redistribute it and\/or modify it\n\/\/ under the terms of the GNU Lesser General Public License\n\/\/ as published by the Free Software Foundation,\n\/\/ either version 3 of the License, or (at your option) any later version.\n\/\/\n\/\/ BOINC is distributed in the hope that it will be useful,\n\/\/ but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\/\/ See the GNU Lesser General Public License for more details.\n\/\/\n\/\/ You should have received a copy of the GNU Lesser General Public License\n\/\/ along with BOINC.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nrequire_once(\"..\/inc\/boinc_db.inc\");\nrequire_once(\"..\/inc\/util.inc\");\nrequire_once(\"..\/inc\/email.inc\");\nrequire_once(\"..\/inc\/user_util.inc\");\n\ncheck_get_args(array());\n\n$user = get_logged_in_user();\n\n$email_addr = strtolower(post_str(\"email_addr\"));\n$passwd = post_str(\"passwd\", true);\n\npage_head(tra(\"Change email address of account\"));\n\nif (!is_valid_email_addr($email_addr)) {\n    echo tra(\"New email address '%1' is invalid.\", $email_addr);\n} else if (is_banned_email_addr($email_addr)) {\n    echo tra(\"New email address '%1' is invalid.\", $email_addr);\n} else if ($email_addr == $user->email_addr) {\n    echo tra(\"New email address is same as existing address. Nothing is changed.\");\n} else {\n    $existing = BoincUser::lookup_email_addr($email_addr);\n    if ($existing) {\n        echo tra(\"There's already an account with that email address\");\n    } else {\n        $passwd_hash = md5($passwd.$user->email_addr);\n\n        \/\/ deal with the case where user hasn't set passwd\n        \/\/ (i.e. passwd is account key)\n        \/\/\n        if ($passwd_hash != $user->passwd_hash) {\n            $passwd = $user->authenticator;\n            $passwd_hash = md5($passwd.$user->email_addr);\n        }\n        if ($passwd_hash != $user->passwd_hash) {\n            echo tra(\"Invalid password.\");\n        } else {\n            $passwd_hash = md5($passwd.$email_addr);\n            $email_addr = BoincDb::escape_string($email_addr);\n            $result = $user->update(\n                \"email_addr='$email_addr', passwd_hash='$passwd_hash', email_validated=0\"\n            );\n            if ($result) {\n                echo tra(\"The email address of your account is now %1.\", $email_addr);\n                if (defined(\"SHOW_NONVALIDATED_EMAIL_ADDR\")) {\n                    echo \"<p>\".tra(\"Please %1 validate this email address %2.\", \"<a href=validate_email_addr.php>\", \"<\/a>\").\"\\n\";\n                }\n            } else {\n                echo tra(\"We can't update your email address due to a database problem.  Please try again later.\");\n            }\n        }\n    }\n}\n\npage_tail();\n?>\n","lang_cluster":"PHP","length":74,"code_uid":"fae0df3ce71b48eb8ea1ce21b69a0402"}
{"diff_hunk":"@@ -64,21 +64,33 @@ class EasyAdminDataCollector extends DataCollector\n         );\n     }\n \n+    \/**\n+     * @return int\n+     *\/\n     public function getNumEntities()\n     {\n         return $this->data['num_entities'];\n     }\n \n+    \/**\n+     * @return array\n+     *\/\n     public function getRequestParameters()\n     {\n         return $this->data['request_parameters'];\n     }\n \n+    \/**\n+     * @return array\n+     *\/\n     public function getCurrentEntityConfig()\n     {\n         return $this->data['current_entity_configuration'];\n     }\n \n+    \/**\n+     * @return array\n+     *\/\n     public function getBackendConfig()\n     {\n         return $this->data['backend_configuration'];","old_code":"<?php\n\n\/*\n * This file is part of the EasyAdminBundle.\n *\n * (c) Javier Eguiluz <javier.eguiluz@gmail.com>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace JavierEguiluz\\Bundle\\EasyAdminBundle\\DataCollector;\n\nuse JavierEguiluz\\Bundle\\EasyAdminBundle\\Configuration\\ConfigManager;\nuse Symfony\\Component\\HttpFoundation\\Request;\nuse Symfony\\Component\\HttpFoundation\\Response;\nuse Symfony\\Component\\HttpKernel\\DataCollector\\DataCollector;\nuse Symfony\\Component\\VarDumper\\Cloner\\VarCloner;\nuse Symfony\\Component\\VarDumper\\Dumper\\HtmlDumper;\nuse Symfony\\Component\\Yaml\\Yaml;\n\n\/**\n * Collects information about the requests related to EasyAdmin and displays\n * it both in the web debug toolbar and in the profiler.\n *\n * @author Javier Eguiluz <javier.eguiluz@gmail.com>\n *\/\nclass EasyAdminDataCollector extends DataCollector\n{\n    private $configManager;\n\n    public function __construct(ConfigManager $configManager)\n    {\n        $this->configManager = $configManager;\n    }\n\n    public function collect(Request $request, Response $response, \\Exception $exception = null)\n    {\n        $backendConfig = $this->configManager->getBackendConfig();\n        $entityName = $request->query->get('entity', null);\n        $currentEntityConfig = array_key_exists($entityName, $backendConfig['entities']) ? $backendConfig['entities'][$entityName] : array();\n\n        $this->data = array(\n            'num_entities' => count($backendConfig['entities']),\n            'request_parameters' => $this->getEasyAdminParameters($request),\n            'current_entity_configuration' => $currentEntityConfig,\n            'backend_configuration' => $backendConfig,\n        );\n    }\n\n    private function getEasyAdminParameters(Request $request)\n    {\n        \/\/ 'admin' is the deprecated route name that will be removed in version 2.0.\n        if (!in_array($request->attributes->get('_route'), array('easyadmin', 'admin'))) {\n            return;\n        }\n\n        return array(\n            'action' => $request->query->get('action'),\n            'entity' => $request->query->get('entity'),\n            'id' => $request->query->get('id'),\n            'sort_field' => $request->query->get('sortField'),\n            'sort_direction' => $request->query->get('sortDirection'),\n        );\n    }\n\n    public function getNumEntities()\n    {\n        return $this->data['num_entities'];\n    }\n\n    public function getRequestParameters()\n    {\n        return $this->data['request_parameters'];\n    }\n\n    public function getCurrentEntityConfig()\n    {\n        return $this->data['current_entity_configuration'];\n    }\n\n    public function getBackendConfig()\n    {\n        return $this->data['backend_configuration'];\n    }\n\n    \/**\n     * It dumps the contents of the given variable. It tries several dumpers in\n     * turn (VarDumper component, Yaml::dump, etc.) and if none is available, it\n     * falls back to PHP's var_export().\n     *\n     * @param mixed $variable\n     *\n     * @return string\n     *\/\n    public function dump($variable)\n    {\n        if (class_exists('Symfony\\Component\\VarDumper\\Dumper\\HtmlDumper')) {\n            $cloner = new VarCloner();\n            $dumper = new HtmlDumper();\n\n            $dumper->dump($cloner->cloneVar($variable), $output = fopen('php:\/\/memory', 'r+b'));\n            $dumpedData = stream_get_contents($output, -1, 0);\n        } elseif (class_exists('Symfony\\Component\\Yaml\\Yaml')) {\n            $dumpedData = sprintf('<pre class=\"sf-dump\">%s<\/pre>', Yaml::dump((array) $variable, 1024));\n        } else {\n            $dumpedData = sprintf('<pre class=\"sf-dump\">%s<\/pre>', var_export($variable, true));\n        }\n\n        return $dumpedData;\n    }\n\n    public function getName()\n    {\n        return 'easyadmin';\n    }\n}\n","lang_cluster":"PHP","length":117,"code_uid":"243b0d347e414b11ad9dee555d9de353"}
{"diff_hunk":"@@ -109,7 +109,11 @@ class BackendController extends ControllerBase\n             $action,\n             base_path().'\/modules'\n         )) {\n-            return $controllerObj->run($action, $controllerParams);\n+            return [\n+                'controller' => $controllerObj,\n+                'action' => $action,\n+                'params' => $controllerParams\n+            ];\n         }\n \n         \/*","old_code":"<?php namespace Backend\\Classes;\n\nuse Str;\nuse App;\nuse File;\nuse View;\nuse Config;\nuse Response;\nuse Illuminate\\Routing\\Controller as ControllerBase;\nuse October\\Rain\\Router\\Helper as RouterHelper;\nuse Closure;\nuse System\\Classes\\PluginManager;\n\n\n\/**\n * This is the master controller for all back-end pages.\n * All requests that are prefixed with the backend URI pattern are sent here,\n * then the next URI segments are analysed and the request is routed to the\n * relevant back-end controller.\n *\n * For example, a request with the URL `\/backend\/acme\/blog\/posts` will look\n * for the `Posts` controller inside the `Acme.Blog` plugin.\n *\n * @see Backend\\Classes\\Controller Base class for back-end controllers\n * @package october\\backend\n * @author Alexey Bobkov, Samuel Georges\n *\/\nclass BackendController extends ControllerBase\n{\n    use \\October\\Rain\\Extension\\ExtendableTrait;\n\n    \/**\n     * @var array Behaviors implemented by this controller.\n     *\/\n    public $implement;\n\n    \/**\n     * @var string Allows early access to page action.\n     *\/\n    public static $action;\n\n    \/**\n     * @var array Allows early access to page parameters.\n     *\/\n    public static $params;\n\n    \/**\n     * Instantiate a new BackendController instance.\n     *\/\n    public function __construct()\n    {\n        $this->extendableConstruct();\n    }\n\n    \/**\n     * Extend this object properties upon construction.\n     *\/\n    public static function extend(Closure $callback)\n    {\n        self::extendableExtendCallback($callback);\n    }\n\n    \/**\n     * Pass unhandled URLs to the CMS Controller, if it exists\n     *\n     * @param string $url\n     * @return Response\n     *\/\n    protected function passToCmsController($url)\n    {\n        if (class_exists('\\Cms\\Classes\\Controller')) {\n            return App::make('Cms\\Classes\\Controller')->run($url);\n        } else {\n            return Response::make(View::make('backend::404'), 404);\n        }\n    }\n\n    \/**\n     * Finds and serves the requested backend controller.\n     * If the controller cannot be found, returns the Cms page with the URL \/404.\n     * If the \/404 page doesn't exist, returns the system 404 page.\n     * @param string $url Specifies the requested page URL.\n     * If the parameter is omitted, the current URL used.\n     * @return string Returns the processed page content.\n     *\/\n    public function run($url = null)\n    {\n        $params = RouterHelper::segmentizeUrl($url);\n\n        \/*\n         * Database check\n         *\/\n        if (!App::hasDatabase()) {\n            return Config::get('app.debug', false)\n                ? Response::make(View::make('backend::no_database'), 200)\n                : $this->passToCmsController($url);\n        }\n\n        \/*\n         * Look for a Module controller\n         *\/\n        $module = $params[0] ?? 'backend';\n        $controller = $params[1] ?? 'index';\n        self::$action = $action = isset($params[2]) ? $this->parseAction($params[2]) : 'index';\n        self::$params = $controllerParams = array_slice($params, 3);\n        $controllerClass = '\\\\'.$module.'\\Controllers\\\\'.$controller;\n        if ($controllerObj = $this->findController(\n            $controllerClass,\n            $action,\n            base_path().'\/modules'\n        )) {\n            return $controllerObj->run($action, $controllerParams);\n        }\n\n        \/*\n         * Look for a Plugin controller\n         *\/\n        if (count($params) >= 2) {\n            list($author, $plugin) = $params;\n\n            $pluginCode = ucfirst($author) . '.' . ucfirst($plugin);\n            if (PluginManager::instance()->isDisabled($pluginCode)) {\n                return Response::make(View::make('backend::404'), 404);\n            }\n\n            $controller = $params[2] ?? 'index';\n            self::$action = $action = isset($params[3]) ? $this->parseAction($params[3]) : 'index';\n            self::$params = $controllerParams = array_slice($params, 4);\n            $controllerClass = '\\\\'.$author.'\\\\'.$plugin.'\\Controllers\\\\'.$controller;\n            if ($controllerObj = $this->findController(\n                $controllerClass,\n                $action,\n                plugins_path()\n            )) {\n                return $controllerObj->run($action, $controllerParams);\n            }\n        }\n\n        \/*\n         * Fall back on Cms controller\n         *\/\n        return $this->passToCmsController($url);\n    }\n\n    \/**\n     * This method is used internally.\n     * Finds a backend controller with a callable action method.\n     * @param string $controller Specifies a method name to execute.\n     * @param string $action Specifies a method name to execute.\n     * @param string $inPath Base path for class file location.\n     * @return ControllerBase Returns the backend controller object\n     *\/\n    protected function findController($controller, $action, $inPath)\n    {\n        \/*\n         * Workaround: Composer does not support case insensitivity.\n         *\/\n        if (!class_exists($controller)) {\n            $controller = Str::normalizeClassName($controller);\n            $controllerFile = $inPath.strtolower(str_replace('\\\\', '\/', $controller)) . '.php';\n            if ($controllerFile = File::existsInsensitive($controllerFile)) {\n                include_once $controllerFile;\n            }\n        }\n\n        if (!class_exists($controller)) {\n            return false;\n        }\n\n        $controllerObj = App::make($controller);\n\n        if ($controllerObj->actionExists($action)) {\n            return $controllerObj;\n        }\n\n        return false;\n    }\n\n    \/**\n     * Process the action name, since dashes are not supported in PHP methods.\n     * @param  string $actionName\n     * @return string\n     *\/\n    protected function parseAction($actionName)\n    {\n        if (strpos($actionName, '-') !== false) {\n            return camel_case($actionName);\n        }\n\n        return $actionName;\n    }\n}\n","lang_cluster":"PHP","length":192,"code_uid":"b84a049feb7947c38479a41920022545"}
{"diff_hunk":"@@ -15,12 +15,21 @@ import (\n \t\"github.com\/stretchr\/testify\/assert\"\n )\n \n+var appTypeSettingsLocations = map[string][]string{\n+\t\"drupal6\":  {\"sites\/default\/settings.php\", \"sites\/default\/settings.ddev.php\"},\n+\t\"drupal7\":  {\"sites\/default\/settings.php\", \"sites\/default\/settings.ddev.php\"},\n+\t\"drupal8\":  {\"sites\/default\/settings.php\", \"sites\/default\/settings.ddev.php\"},\n+\t\"backdrop\": {\"settings.php\", \"settings.ddev.php\"},\n+}\n+\n \/\/ TestWriteSettings tests writing app settings (like Drupal\n \/\/ settings.php\/settings.local.php\n func TestWriteSettings(t *testing.T) {\n \texpectations := map[string]string{\n-\t\t\"drupal7\":   \"sites\/default\/settings.php\",\n-\t\t\"drupal8\":   \"sites\/default\/settings.php\",\n+\t\t\"backdrop\":  \"settings.ddev.php\",\n+\t\t\"drupal6\":   \"sites\/default\/settings.ddev.php\",\n+\t\t\"drupal7\":   \"sites\/default\/settings.ddev.php\",\n+\t\t\"drupal8\":   \"sites\/default\/settings.ddev.php\",\n \t\t\"wordpress\": \"wp-config.php\",\n \t\t\"typo3\":     \"typo3conf\/AdditionalConfiguration.php\",\n \t}","old_code":"package ddevapp_test\n\nimport (\n\t\"path\/filepath\"\n\t\"testing\"\n\n\t\"os\"\n\n\t\"io\/ioutil\"\n\n\t. \"github.com\/drud\/ddev\/pkg\/ddevapp\"\n\t\"github.com\/drud\/ddev\/pkg\/fileutil\"\n\t\"github.com\/drud\/ddev\/pkg\/testcommon\"\n\t\"github.com\/drud\/ddev\/pkg\/util\"\n\t\"github.com\/stretchr\/testify\/assert\"\n)\n\n\/\/ TestWriteSettings tests writing app settings (like Drupal\n\/\/ settings.php\/settings.local.php\nfunc TestWriteSettings(t *testing.T) {\n\texpectations := map[string]string{\n\t\t\"drupal7\":   \"sites\/default\/settings.php\",\n\t\t\"drupal8\":   \"sites\/default\/settings.php\",\n\t\t\"wordpress\": \"wp-config.php\",\n\t\t\"typo3\":     \"typo3conf\/AdditionalConfiguration.php\",\n\t}\n\tdir := testcommon.CreateTmpDir(\"example\")\n\terr := os.MkdirAll(filepath.Join(dir, \"sites\/default\"), 0777)\n\tassert.NoError(t, err)\n\terr = os.MkdirAll(filepath.Join(dir, \"typo3conf\"), 0777)\n\tassert.NoError(t, err)\n\n\t\/\/ TYPO3 wants LocalConfiguration.php to exist in the repo ahead of time.\n\terr = ioutil.WriteFile(filepath.Join(dir, \"typo3conf\", \"LocalConfiguration.php\"), []byte(\"<?php\\n\"), 0644)\n\tassert.NoError(t, err)\n\n\tapp, err := NewApp(dir, DefaultProviderName)\n\tassert.NoError(t, err)\n\n\tfor apptype, settingsRelativePath := range expectations {\n\t\tapp.Type = apptype\n\n\t\texpectedSettingsFile := filepath.Join(dir, settingsRelativePath)\n\t\t_, err = os.Stat(expectedSettingsFile)\n\t\tassert.True(t, os.IsNotExist(err))\n\t\t\/\/ nolint: vetshadow\n\t\tcreatedFile, err := app.CreateSettingsFile()\n\t\tassert.NoError(t, err)\n\t\tassert.EqualValues(t, expectedSettingsFile, createdFile)\n\t\t_, err = os.Stat(expectedSettingsFile)\n\t\tassert.NoError(t, err)\n\t\t\/\/ nolint: vetshadow\n\t\tsignatureFound, err := fileutil.FgrepStringInFile(expectedSettingsFile, DdevFileSignature)\n\t\tassert.NoError(t, err)\n\t\tassert.True(t, signatureFound)\n\t\terr = os.Remove(expectedSettingsFile)\n\t\tassert.NoError(t, err)\n\t}\n\n\terr = os.RemoveAll(dir)\n\tassert.NoError(t, err)\n}\n\n\/\/ @todo: Take a look at drush config in general to make sure its config\n\/\/ is noted properly. Do we need it? Are we using it?\nfunc TestWriteDrushConfig(t *testing.T) {\n\n\tdir := testcommon.CreateTmpDir(\"example\")\n\n\tfile, err := ioutil.TempFile(dir, \"file\")\n\tassert.NoError(t, err)\n\n\tdrushConfig := NewDrushConfig()\n\terr = WriteDrushConfig(drushConfig, file.Name())\n\tassert.NoError(t, err)\n\n\tutil.CheckClose(file)\n\n\terr = os.Chmod(dir, 0755)\n\tassert.NoError(t, err)\n\terr = os.Chmod(file.Name(), 0666)\n\tassert.NoError(t, err)\n\n\terr = os.RemoveAll(dir)\n\tassert.NoError(t, err)\n}\n","lang_cluster":"PHP","length":86,"code_uid":"6425e053429e4a65bf389a864311aa3a"}
{"diff_hunk":"@@ -26,8 +26,6 @@ var StartCmd = &cobra.Command{\n \tShort:   \"Start the local development environment for a site.\",\n \tLong:    `Start initializes and configures the web server and database containers to provide a working environment for development.`,\n \tPreRun: func(cmd *cobra.Command, args []string) {\n-\t\tfmt.Printf(\"Starting environment for %s...\\n\", activeApp)\n-\n \t\tclient, err := platform.GetDockerClient()\n \t\tif err != nil {\n \t\t\tlog.Fatal(err)","old_code":"package cmd\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"strings\"\n\n\t\"github.com\/drud\/ddev\/pkg\/plugins\/platform\"\n\t\"github.com\/spf13\/cobra\"\n)\n\nconst netName = \"ddev_default\"\n\nvar (\n\tserviceType string\n\twebImage    string\n\tdbImage     string\n\twebImageTag string\n\tdbImageTag  string\n)\n\n\/\/ StartCmd represents the add command\nvar StartCmd = &cobra.Command{\n\tUse:     \"start\",\n\tAliases: []string{\"add\"},\n\tShort:   \"Start the local development environment for a site.\",\n\tLong:    `Start initializes and configures the web server and database containers to provide a working environment for development.`,\n\tPreRun: func(cmd *cobra.Command, args []string) {\n\t\tfmt.Printf(\"Starting environment for %s...\\n\", activeApp)\n\n\t\tclient, err := platform.GetDockerClient()\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\n\t\terr = EnsureNetwork(client, netName)\n\t\tif err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\n\t},\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tapp := platform.PluginMap[strings.ToLower(plugin)]\n\t\terr := app.Init()\n\t\tif err != nil {\n\t\t\tlog.Fatalf(\"Could not initialize application: %v\", err)\n\t\t}\n\n\t\terr = app.Start()\n\t\tif err != nil {\n\t\t\tFailed(\"Failed to start %s: %s\", app.GetName(), err)\n\t\t}\n\n\t\tfmt.Println(\"Waiting for the environment to become ready. This may take a couple of minutes...\")\n\t\tsiteURL, err := app.Wait()\n\t\tif err != nil {\n\t\t\tFailed(\"The environment for %s never became ready: %s\", activeApp, err)\n\t\t}\n\n\t\tSuccess(\"Successfully started %s\", activeApp)\n\t\tSuccess(\"Your application can be reached at: %s\", siteURL)\n\n\t},\n}\n\nfunc init() {\n\tStartCmd.Flags().StringVarP(&webImage, \"web-image\", \"\", \"\", \"Change the image used for the app's web server\")\n\tStartCmd.Flags().StringVarP(&dbImage, \"db-image\", \"\", \"\", \"Change the image used for the app's database server\")\n\tStartCmd.Flags().StringVarP(&webImageTag, \"web-image-tag\", \"\", \"\", \"Override the default web image tag\")\n\tStartCmd.Flags().StringVarP(&dbImageTag, \"db-image-tag\", \"\", \"\", \"Override the default web image tag\")\n\n\tRootCmd.AddCommand(StartCmd)\n}\n","lang_cluster":"PHP","length":73,"code_uid":"380d807ffcf348c281cbb94533013eb9"}
{"diff_hunk":"@@ -24,16 +24,19 @@ class ArrayFilterType extends FilterType\n      *\/\n     public function buildForm(FormBuilderInterface $builder, array $options)\n     {\n-        $builder->add('value', $options['value_type'], $options['value_type_options'] + [\n-            'label' => false,\n-            'choice_loader' => new DynamicChoiceLoader(),\n-        ]);\n+        $defaultOptions = ['label' => false];\n+        if (!isset($options['value_type_options']['choices']) || [] === $options['value_type_options']['choices']) {\n+            $defaultOptions += ['choice_loader' => new DynamicChoiceLoader()];\n+        }\n+        $builder->add('value', $options['value_type'], $options['value_type_options'] + $defaultOptions);\n \n         $builder->addModelTransformer(new CallbackTransformer(\n             static function ($data) { return $data; },\n             static function ($data) {\n-                if ([] === $data['value']) {\n+                if (null === $data['value'] || [] === $data['value']) {\n                     $data['comparison'] = ComparisonType::CONTAINS === $data['comparison'] ? 'IS NULL' : 'IS NOT NULL';\n+                } else {\n+                    $data['value'] = (array) $data['value'];\n                 }\n \n                 return $data;","old_code":"<?php\n\nnamespace EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Filter\\Type;\n\nuse Doctrine\\ORM\\Query\\Expr;\nuse Doctrine\\ORM\\QueryBuilder;\nuse EasyCorp\\Bundle\\EasyAdminBundle\\Form\\ChoiceList\\Loader\\DynamicChoiceLoader;\nuse EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Type\\ComparisonType;\nuse Symfony\\Component\\Form\\CallbackTransformer;\nuse Symfony\\Component\\Form\\Extension\\Core\\Type\\ChoiceType;\nuse Symfony\\Component\\Form\\FormBuilderInterface;\nuse Symfony\\Component\\Form\\FormInterface;\nuse Symfony\\Component\\OptionsResolver\\OptionsResolver;\n\n\/**\n * @author Yonel Ceruto <yonelceruto@gmail.com>\n *\/\nclass ArrayFilterType extends FilterType\n{\n    use FilterTypeTrait;\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function buildForm(FormBuilderInterface $builder, array $options)\n    {\n        $builder->add('value', $options['value_type'], $options['value_type_options'] + [\n            'label' => false,\n            'choice_loader' => new DynamicChoiceLoader(),\n        ]);\n\n        $builder->addModelTransformer(new CallbackTransformer(\n            static function ($data) { return $data; },\n            static function ($data) {\n                if ([] === $data['value']) {\n                    $data['comparison'] = ComparisonType::CONTAINS === $data['comparison'] ? 'IS NULL' : 'IS NOT NULL';\n                }\n\n                return $data;\n            }\n        ));\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function configureOptions(OptionsResolver $resolver)\n    {\n        $resolver->setDefaults([\n            'comparison_type_options' => ['type' => 'array'],\n            'value_type' => ChoiceType::class,\n            'value_type_options' => [\n                'multiple' => true,\n                'attr' => [\n                    'data-widget' => 'select2',\n                    'data-select2-tags' => 'true',\n                ],\n            ],\n        ]);\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getParent()\n    {\n        return ComparisonFilterType::class;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function filter(QueryBuilder $queryBuilder, FormInterface $form, array $metadata)\n    {\n        $alias = \\current($queryBuilder->getRootAliases());\n        $property = $metadata['property'];\n        $paramName = static::createAlias($property);\n        $useQuotes = 'simple_array' !== $metadata['dataType'];\n        $data = $form->getData();\n\n        if ([] === $data['value']) {\n            $queryBuilder->andWhere(\\sprintf('%s.%s %s', $alias, $property, $data['comparison']));\n        } else {\n            $orX = new Expr\\Orx();\n            foreach ($data['value'] as $value) {\n                $orX->add(\\sprintf('%s.%s %s :%s', $alias, $property, $data['comparison'], $paramName));\n                $queryBuilder->setParameter($paramName, $useQuotes ? '%\"'.$value.'\"%' : '%'.$value.'%');\n            }\n            if (ComparisonType::NOT_CONTAINS === $data['comparison']) {\n                $orX->add(\\sprintf('%s.%s IS NULL', $alias, $property));\n            }\n            $queryBuilder->andWhere($orX);\n        }\n    }\n}\n","lang_cluster":"PHP","length":95,"code_uid":"214cdd5c565846c6891dfb509353e7d1"}
{"diff_hunk":"@@ -18,6 +18,7 @@ use Symfony\\Component\\Form\\FormEvent;\n use Symfony\\Component\\Form\\FormEvents;\n use Symfony\\Component\\Form\\FormInterface;\n use Symfony\\Component\\Form\\FormView;\n+use Symfony\\Component\\HttpKernel\\Kernel;\n use Symfony\\Component\\OptionsResolver\\OptionsResolverInterface;\n \n use Sonata\\MediaBundle\\Provider\\Pool;","old_code":"<?php\n\n\/*\n * This file is part of the Sonata project.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Form\\Type;\n\nuse Symfony\\Component\\Form\\AbstractType;\nuse Symfony\\Component\\Form\\FormBuilderInterface;\n\nuse Symfony\\Component\\Form\\FormEvent;\nuse Symfony\\Component\\Form\\FormEvents;\nuse Symfony\\Component\\Form\\FormInterface;\nuse Symfony\\Component\\Form\\FormView;\nuse Symfony\\Component\\OptionsResolver\\OptionsResolverInterface;\n\nuse Sonata\\MediaBundle\\Provider\\Pool;\nuse Sonata\\MediaBundle\\Form\\DataTransformer\\ProviderDataTransformer;\n\nclass MediaType extends AbstractType\n{\n    protected $pool;\n\n    protected $class;\n\n    \/**\n     * @param Pool   $pool\n     * @param string $class\n     *\/\n    public function __construct(Pool $pool, $class)\n    {\n        $this->pool  = $pool;\n        $this->class = $class;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function buildForm(FormBuilderInterface $builder, array $options)\n    {\n        $builder->addModelTransformer(new ProviderDataTransformer($this->pool, $this->class, array(\n            'provider'      => $options['provider'],\n            'context'       => $options['context'],\n            'empty_on_new'  => $options['empty_on_new'],\n            'new_on_update' => $options['new_on_update'],\n        )));\n\n        $builder->addEventListener(FormEvents::BIND, function(FormEvent $event) {\n            if ($event->getForm()->get('unlink')->getData()) {\n                $event->setData(null);\n            }\n        });\n\n        $this->pool->getProvider($options['provider'])->buildMediaType($builder);\n\n        $builder->add('unlink', 'checkbox', array(\n            'mapped'   => false,\n            'data'     => false,\n            'required' => false\n        ));\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function buildView(FormView $view, FormInterface $form, array $options)\n    {\n        $view->vars['provider'] = $options['provider'];\n        $view->vars['context'] = $options['context'];\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function setDefaultOptions(OptionsResolverInterface $resolver)\n    {\n        $resolver->setDefaults(array(\n            'data_class'    => $this->class,\n            'provider'      => null,\n            'context'       => null,\n            'empty_on_new'  => true,\n            'new_on_update' => true,\n        ));\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getParent()\n    {\n        return 'form';\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getName()\n    {\n        return 'sonata_media_type';\n    }\n}\n","lang_cluster":"PHP","length":107,"code_uid":"20ccf36b3bb54a58bf960af1015df226"}
{"diff_hunk":"@@ -11,13 +11,13 @@ namespace Ergonode\\Attribute\\Infrastructure\\Provider\\Strategy;\n \n use Ergonode\\Attribute\\Domain\\Entity\\AbstractAttribute;\n use Ergonode\\Attribute\\Domain\\Entity\\Attribute\\GalleryAttribute;\n-use Ergonode\\Attribute\\Infrastructure\\Provider\\AttributeValueConstraintStrategyInterface;\n+use Ergonode\\Attribute\\Infrastructure\\Provider\\ContextAwareAttributeValueConstraintStrategyInterface;\n use Ergonode\\Multimedia\\Domain\\Query\\MultimediaQueryInterface;\n use Symfony\\Component\\Validator\\Constraint;\n use Symfony\\Component\\Validator\\Constraints\\Choice;\n use Symfony\\Component\\Validator\\Constraints\\Collection;\n \n-class GalleryAttributeValueConstraintStrategy implements AttributeValueConstraintStrategyInterface\n+class GalleryAttributeValueConstraintStrategy implements ContextAwareAttributeValueConstraintStrategyInterface\n {\n     private MultimediaQueryInterface $query;\n ","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Ergonode Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types=1);\n\nnamespace Ergonode\\Attribute\\Infrastructure\\Provider\\Strategy;\n\nuse Ergonode\\Attribute\\Domain\\Entity\\AbstractAttribute;\nuse Ergonode\\Attribute\\Domain\\Entity\\Attribute\\GalleryAttribute;\nuse Ergonode\\Attribute\\Infrastructure\\Provider\\AttributeValueConstraintStrategyInterface;\nuse Ergonode\\Multimedia\\Domain\\Query\\MultimediaQueryInterface;\nuse Symfony\\Component\\Validator\\Constraint;\nuse Symfony\\Component\\Validator\\Constraints\\Choice;\nuse Symfony\\Component\\Validator\\Constraints\\Collection;\n\nclass GalleryAttributeValueConstraintStrategy implements AttributeValueConstraintStrategyInterface\n{\n    private MultimediaQueryInterface $query;\n\n    public function __construct(MultimediaQueryInterface $query)\n    {\n        $this->query = $query;\n    }\n\n    \/**\n     * {@inheritDoc}\n     *\/\n    public function supports(AbstractAttribute $attribute): bool\n    {\n        return $attribute instanceof GalleryAttribute;\n    }\n\n    \/**\n     * {@inheritDoc}\n     *\/\n    public function get(AbstractAttribute $attribute): Constraint\n    {\n        $multimedia = $this->query->getAll();\n\n        return new Collection([\n            'value' => [\n                new Choice(['choices' => $multimedia, 'multiple' => true]),\n            ],\n        ]);\n    }\n}\n","lang_cluster":"PHP","length":50,"code_uid":"fc608c47230c4f9292a5e68ffcbd75ab"}
{"diff_hunk":"@@ -6,7 +6,15 @@ namespace Bolt\\Controller;\n \n use Bolt\\Configuration\\Config;\n use Bolt\\Entity\\Field\\TemplateselectField;\n+use Bolt\\Snippet\\RequestZone;\n+use Bolt\\Snippet\\Target;\n use Bolt\\Version;\n+use Bolt\\Widget\\BoltHeaderWidget;\n+use Bolt\\Widget\\CanonicalLinkWidget;\n+use Bolt\\Widget\\NewsWidget;\n+use Bolt\\Widget\\SnippetWidget;\n+use Bolt\\Widget\\WeatherWidget;\n+use Bolt\\Widgets;\n use Symfony\\Bundle\\FrameworkBundle\\Controller\\AbstractController;\n use Symfony\\Component\\HttpFoundation\\Response;\n use Tightenco\\Collect\\Support\\Collection;","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Bolt\\Controller;\n\nuse Bolt\\Configuration\\Config;\nuse Bolt\\Entity\\Field\\TemplateselectField;\nuse Bolt\\Version;\nuse Symfony\\Bundle\\FrameworkBundle\\Controller\\AbstractController;\nuse Symfony\\Component\\HttpFoundation\\Response;\nuse Tightenco\\Collect\\Support\\Collection;\nuse Twig\\Environment;\n\nclass TwigAwareController extends AbstractController\n{\n    \/**\n     * @var Config\n     *\/\n    protected $config;\n\n    \/**\n     * @var Environment\n     *\/\n    protected $twig;\n\n    \/**\n     * @required\n     *\/\n    public function setAutowire(Config $config, Environment $twig): void\n    {\n        $this->config = $config;\n        $this->twig = $twig;\n    }\n\n    \/**\n     * Renders a view.\n     *\n     * @final\n     *\n     * @param string|array $template\n     *\n     * @throws \\Twig_Error_Loader  When none of the templates can be found\n     * @throws \\Twig_Error_Syntax  When an error occurred during compilation\n     * @throws \\Twig_Error_Runtime When an error occurred during rendering\n     *\/\n    protected function renderTemplate($template, array $parameters = [], ?Response $response = null): Response\n    {\n        \/\/ Set config and version.\n        $parameters['config'] = $parameters['config'] ?? $this->config;\n        $parameters['version'] = $parameters['version'] ?? Version::VERSION;\n        $parameters['user'] = $parameters['user'] ?? $this->getUser();\n\n        \/\/ Resolve string|array of templates into the first one that is found.\n        if (is_array($template)) {\n            $templates = (new Collection($template))\n                ->map(function ($element): ?string {\n                    if ($element instanceof TemplateselectField) {\n                        return $element->__toString();\n                    }\n                    return $element;\n                })\n                ->filter()\n                ->toArray();\n            $template = $this->twig->resolveTemplate($templates);\n        }\n\n        $content = $this->twig->render($template, $parameters);\n\n        if ($response === null) {\n            $response = new Response();\n        }\n\n        $response->setContent($content);\n\n        return $response;\n    }\n}\n","lang_cluster":"PHP","length":78,"code_uid":"1cbd857c9b7d4490b0297412708fb3c5"}
{"diff_hunk":"@@ -27,21 +27,18 @@ class ProductTemplateRelationshipStrategy implements RelationshipStrategyInterfa\n         $this->query = $query;\n     }\n \n-    \/**\n-     * {@inheritDoc}\n-     *\/\n     public function supports(AggregateId $id): bool\n     {\n         return $id instanceof TemplateId;\n     }\n \n-    \/**\n-     * {@inheritDoc}\n-     *\/\n     public function getRelationshipGroup(AggregateId $id): RelationshipGroup\n     {\n         Assert::isInstanceOf($id, TemplateId::class);\n \n-        return new RelationshipGroup(self::MESSAGE, $this->query->findProductIdByTemplateId($id));\n+        $relations = $this->query->findProductIdByTemplateId($id);\n+        $message = count($relations) === 1 ? self::ONE_MESSAGE : self::MULTIPLE_MESSAGE;\n+\n+        return new RelationshipGroup($message, $relations);\n     }\n }","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Ergonode Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types=1);\n\nnamespace Ergonode\\Designer\\Infrastructure\\Strategy\\Relationship;\n\nuse Ergonode\\Core\\Infrastructure\\Strategy\\RelationshipStrategyInterface;\nuse Ergonode\\Designer\\Domain\\Query\\TemplateQueryInterface;\nuse Ergonode\\SharedKernel\\Domain\\Aggregate\\TemplateId;\nuse Ergonode\\SharedKernel\\Domain\\AggregateId;\nuse Webmozart\\Assert\\Assert;\nuse Ergonode\\Core\\Infrastructure\\Model\\RelationshipGroup;\n\nclass ProductTemplateRelationshipStrategy implements RelationshipStrategyInterface\n{\n    private const MESSAGE = 'Object has active relationships with product %relations%';\n\n    private TemplateQueryInterface $query;\n\n    public function __construct(TemplateQueryInterface $query)\n    {\n        $this->query = $query;\n    }\n\n    \/**\n     * {@inheritDoc}\n     *\/\n    public function supports(AggregateId $id): bool\n    {\n        return $id instanceof TemplateId;\n    }\n\n    \/**\n     * {@inheritDoc}\n     *\/\n    public function getRelationshipGroup(AggregateId $id): RelationshipGroup\n    {\n        Assert::isInstanceOf($id, TemplateId::class);\n\n        return new RelationshipGroup(self::MESSAGE, $this->query->findProductIdByTemplateId($id));\n    }\n}\n","lang_cluster":"PHP","length":47,"code_uid":"e76c5d83796a4c3d93dfaba78a3539ca"}
{"diff_hunk":"@@ -114,8 +114,17 @@ function login_with_auth($authenticator, $next_url, $perm) {\n         sleep(LOGIN_FAIL_SLEEP_SEC);\n         error_page(\"This account has been administratively disabled.\");\n     } else {\n-        Header(\"Location: $next_url\");\n-        send_cookie('auth', $authenticator, $perm);\n+\n+        \/\/ Intercept next_url if consent has not yet been given\n+        list($checkct, $ctid) = check_consent_type('ENROLL');\n+        $config = get_config();\n+        if (parse_bool($config, \"enable_login_mustagree_termsofuse\") and $checkct and check_termsofuse() and (!check_user_consent($user, 'ENROLL'))) {\n+            $next_url = consent_after_login($user, $perm, $next_url);\n+        }\n+        else {\n+            send_cookie('auth', $authenticator, $perm);\n+        }\n+        Header(\"Location: \".url_base().\"$next_url\");\n     }\n }\n ","old_code":"<?php\n\/\/ This file is part of BOINC.\n\/\/ http:\/\/boinc.berkeley.edu\n\/\/ Copyright (C) 2014 University of California\n\/\/\n\/\/ BOINC is free software; you can redistribute it and\/or modify it\n\/\/ under the terms of the GNU Lesser General Public License\n\/\/ as published by the Free Software Foundation,\n\/\/ either version 3 of the License, or (at your option) any later version.\n\/\/\n\/\/ BOINC is distributed in the hope that it will be useful,\n\/\/ but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\/\/ See the GNU Lesser General Public License for more details.\n\/\/\n\/\/ You should have received a copy of the GNU Lesser General Public License\n\/\/ along with BOINC.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n\/\/ validate a user's credentials, and log them in by sending a cookie.\n\/\/ This handles several cases:\n\/\/\n\/\/ 1) login via web form w\/ email addr and password\n\/\/ 2) login via web form w\/ authenticator\n\/\/ 3) login via a URL sent in email (e.g. to reset password)\n\nrequire_once(\"..\/inc\/boinc_db.inc\");\nrequire_once(\"..\/inc\/util.inc\");\nrequire_once(\"..\/inc\/email.inc\");\nrequire_once(\"..\/inc\/user.inc\");\nrequire_once(\"..\/inc\/ldap.inc\");\nrequire_once(\"..\/inc\/user_util.inc\");\nrequire_once(\"..\/inc\/password_compat\/password.inc\");\n\ncheck_get_args(array(\"id\", \"t\", \"h\", \"key\"));\n\n\/\/ login with email addr \/ passwd\n\/\/\nfunction login_with_email($email_addr, $passwd, $next_url, $perm) {\n    $user = BoincUser::lookup_email_addr($email_addr);\n    if (!$user) {\n        sleep(LOGIN_FAIL_SLEEP_SEC);\n        page_head(\"No such account\");\n        echo \"No account with email address <b>$email_addr<\/b> exists.\n            Please go back and try again.\n        \";\n        page_tail();\n        exit;\n    }\n    if (substr($user->authenticator, 0, 1) == 'x'){\n        sleep(LOGIN_FAIL_SLEEP_SEC);\n        error_page(\"This account has been administratively disabled.\");\n    }\n\n    \/\/ allow authenticator as password\n    \/\/\n    if ($passwd != $user->authenticator) {\n        $passwd_hash = md5($passwd.$email_addr);\n\n        if (!check_passwd_hash($user, $passwd_hash)) {\n            sleep(LOGIN_FAIL_SLEEP_SEC);\n            page_head(\"Password incorrect\");\n            echo \"The password you entered is incorrect. Please go back and try again.\\n\";\n            page_tail();\n            exit;\n        }\n    }\n    $authenticator = $user->authenticator;\n    Header(\"Location: \".url_base().\"$next_url\");\n    send_cookie('auth', $authenticator, $perm);\n}\n\n\/\/ email link case\n\/\/\nfunction login_via_link($id, $t, $h) {\n    $user = BoincUser::lookup_id($id);\n    if (!$user) {\n        sleep(LOGIN_FAIL_SLEEP_SEC);\n        error_page(\"Invalid user ID.\n            Please make sure you visited the complete URL;\n            it may have been split across lines by your email reader.\"\n        );\n    }\n    $x = $id.$user->authenticator.$t;\n    $x = md5($x);\n    $x = substr($x, 0, 16);\n    if ($x != $h) {\n        sleep(LOGIN_FAIL_SLEEP_SEC);\n        error_page(\"Invalid authenticator.\n            Please make sure you visited the complete URL;\n            it may have been split across lines by your email reader.\"\n        );\n    }\n    if (time() - $t > 86400) {\n        sleep(LOGIN_FAIL_SLEEP_SEC);\n        error_page(\"Link has expired;\n            go <a href=get_passwd.php>here<\/a> to\n            get a new login link by email.\"\n        );\n    }\n    send_cookie('auth', $user->authenticator, true);\n    Header(\"Location: \".USER_HOME);\n}\n\nfunction login_with_auth($authenticator, $next_url, $perm) {\n    $user = BoincUser::lookup_auth($authenticator);\n    if (!$user) {\n        sleep(LOGIN_FAIL_SLEEP_SEC);\n        page_head(\"Login failed\");\n        echo \"There is no account with that authenticator.\n            Please <a href=get_passwd.php>try again<\/a>.\n        \";\n        page_tail();\n    } else if (substr($user->authenticator, 0, 1) == 'x'){\n        sleep(LOGIN_FAIL_SLEEP_SEC);\n        error_page(\"This account has been administratively disabled.\");\n    } else {\n        Header(\"Location: $next_url\");\n        send_cookie('auth', $authenticator, $perm);\n    }\n}\n\nfunction login_with_ldap($uid, $passwd, $next_url, $perm) {\n    list ($ldap_user, $error_msg) = ldap_auth($uid, $passwd);\n    if ($error_msg) {\n        sleep(LOGIN_FAIL_SLEEP_SEC);\n        error_page($error_msg);\n    }\n    $x = ldap_email_string($uid);\n    $user = BoincUser::lookup_email_addr($x);\n    if (!$user) {\n        \/\/ LDAP authentication succeeded but we don't have a user record.\n        \/\/ Create one.\n        \/\/\n        $user = make_user_ldap($x, $ldap_user->name);\n    }\n    if (!$user) {\n        error_page(\"Couldn't create user\");\n    }\n    Header(\"Location: \".url_base().\"$next_url\");\n    send_cookie('auth', $user->authenticator, $perm);\n    return;\n}\n\n$id = get_int('id', true);\n$t = get_int('t', true);\n$h = get_str('h', true);\nif ($id && $t && $h) {\n    login_via_link($id, $t, $h);\n    exit;\n}\n\n$next_url = post_str(\"next_url\", true);\n$next_url = urldecode($next_url);\n$next_url = sanitize_local_url($next_url);\nif (strlen($next_url) == 0) {\n    $next_url = USER_HOME;\n}\n\n$perm = false;\nif (isset($_POST['stay_logged_in'])) {\n    $perm = $_POST['stay_logged_in'];\n}\n\n\/\/ check for account key case.\n\/\/ see if key is in URL; if not then check for POST data\n\/\/\n$authenticator = get_str(\"key\", true);\nif (!$authenticator) {\n    $authenticator = post_str(\"authenticator\", true);\n}\nif ($authenticator) {\n    login_with_auth($authenticator, $next_url, $perm);\n    exit;\n}\n\n$email_addr = strtolower(sanitize_tags(post_str(\"email_addr\", true)));\n$passwd = post_str(\"passwd\", true);\nif ($email_addr && $passwd) {\n    if (LDAP_HOST && !is_valid_email_addr($email_addr)) {\n        login_with_ldap($email_addr, $passwd, $next_url, $perm);\n    } else {\n        login_with_email($email_addr, $passwd, $next_url, $perm);\n    }\n    exit;\n}\n\nerror_page(\"You must supply an email address and password\");\n\n?>\n","lang_cluster":"PHP","length":189,"code_uid":"0a843d7cfa564a69a98f04ec5d48b3a2"}
{"diff_hunk":"@@ -169,10 +169,22 @@ class Settings extends Controller\n     }\n \n     \/**\n-     * Locates a setting item for a module or plugin\n+     * Locates a setting item for a module or plugin.\n+     *\n+     * If none of the parameters are provided, they will be auto-guessed from the URL.\n+     *\n+     * @param string|null $author\n+     * @param string|null $plugin\n+     * @param string|null $code\n+     *\n+     * @return array\n      *\/\n-    protected function findSettingItem($author, $plugin, $code)\n+    protected function findSettingItem($author = null, $plugin = null, $code = null)\n     {\n+        if (is_null($author) || is_null($plugin)) {\n+            [$author, $plugin, $code] = $this->guessSettingItem();\n+        }\n+\n         $manager = SettingsManager::instance();\n \n         $moduleOwner = $author;","old_code":"<?php namespace System\\Controllers;\n\nuse Lang;\nuse Flash;\nuse Backend;\nuse BackendMenu;\nuse System\\Classes\\SettingsManager;\nuse Backend\\Classes\\Controller;\nuse ApplicationException;\nuse Exception;\n\n\/**\n * Settings controller\n *\n * @package october\\system\n * @author Alexey Bobkov, Samuel Georges\n *\n *\/\nclass Settings extends Controller\n{\n    \/**\n     * @var WidgetBase Reference to the widget object.\n     *\/\n    protected $formWidget;\n\n    \/**\n     * @var array Permissions required to view this page.\n     *\/\n    public $requiredPermissions = [];\n\n    \/**\n     * Constructor.\n     *\/\n    public function __construct()\n    {\n        parent::__construct();\n\n        if ($this->action == 'backend_preferences') {\n            $this->requiredPermissions = ['backend.manage_preferences'];\n        }\n\n        $this->addCss('\/modules\/system\/assets\/css\/settings\/settings.css', 'core');\n\n        BackendMenu::setContext('October.System', 'system', 'settings');\n    }\n\n    public function index()\n    {\n        $this->pageTitle = 'system::lang.settings.menu_label';\n        $this->vars['items'] = SettingsManager::instance()->listItems('system');\n        $this->bodyClass = 'compact-container sidenav-tree-root';\n    }\n\n    public function mysettings()\n    {\n        BackendMenu::setContextSideMenu('mysettings');\n        $this->pageTitle = 'backend::lang.mysettings.menu_label';\n        $this->vars['items'] = SettingsManager::instance()->listItems('mysettings');\n        $this->bodyClass = 'compact-container';\n    }\n\n    \/\/\n    \/\/ Generated Form\n    \/\/\n\n    public function update($author, $plugin, $code = null)\n    {\n        SettingsManager::setContext($author.'.'.$plugin, $code);\n\n        $this->vars['parentLink'] = Backend::url('system\/settings');\n        $this->vars['parentLabel'] = Lang::get('system::lang.settings.menu_label');\n\n        try {\n            if (!$item = $this->findSettingItem($author, $plugin, $code)) {\n                throw new ApplicationException(Lang::get('system::lang.settings.not_found'));\n            }\n\n            $this->pageTitle = $item->label;\n\n            if ($item->context == 'mysettings') {\n                $this->vars['parentLink'] = Backend::url('system\/settings\/mysettings');\n                $this->vars['parentLabel'] = Lang::get('backend::lang.mysettings.menu_label');\n            }\n\n            $model = $this->createModel($item);\n            $this->initWidgets($model);\n        }\n        catch (Exception $ex) {\n            $this->handleError($ex);\n        }\n    }\n\n    public function update_onSave($author, $plugin, $code = null)\n    {\n        $item = $this->findSettingItem($author, $plugin, $code);\n        $model = $this->createModel($item);\n        $this->initWidgets($model);\n\n        $saveData = $this->formWidget->getSaveData();\n        foreach ($saveData as $attribute => $value) {\n            $model->{$attribute} = $value;\n        }\n        $model->save(null, $this->formWidget->getSessionKey());\n\n        Flash::success(Lang::get('system::lang.settings.update_success', ['name' => Lang::get($item->label)]));\n\n        \/*\n         * Handle redirect\n         *\/\n        if ($redirectUrl = post('redirect', true)) {\n            $redirectUrl = ($item->context == 'mysettings')\n                ? 'system\/settings\/mysettings'\n                : 'system\/settings';\n\n            return Backend::redirect($redirectUrl);\n        }\n    }\n\n    public function update_onResetDefault($author, $plugin, $code = null)\n    {\n        $item = $this->findSettingItem($author, $plugin, $code);\n        $model = $this->createModel($item);\n        $model->resetDefault();\n\n        Flash::success(Lang::get('backend::lang.form.reset_success'));\n\n        return Backend::redirect('system\/settings\/update\/'.$author.'\/'.$plugin.'\/'.$code);\n    }\n\n    \/**\n     * Render the form.\n     *\/\n    public function formRender($options = [])\n    {\n        if (!$this->formWidget) {\n            throw new ApplicationException(Lang::get('backend::lang.form.behavior_not_ready'));\n        }\n\n        return $this->formWidget->render($options);\n    }\n\n    \/**\n     * Prepare the widgets used by this action\n     * Model $model\n     *\/\n    protected function initWidgets($model)\n    {\n        $config = $model->getFieldConfig();\n        $config->model = $model;\n        $config->arrayName = class_basename($model);\n        $config->context = 'update';\n\n        $widget = $this->makeWidget('Backend\\Widgets\\Form', $config);\n        $widget->bindToController();\n        $this->formWidget = $widget;\n    }\n\n    \/**\n     * Internal method, prepare the list model object\n     *\/\n    protected function createModel($item)\n    {\n        if (!isset($item->class) || !strlen($item->class)) {\n            throw new ApplicationException(Lang::get('system::lang.settings.missing_model'));\n        }\n\n        $class = $item->class;\n        return $class::instance();\n    }\n\n    \/**\n     * Locates a setting item for a module or plugin\n     *\/\n    protected function findSettingItem($author, $plugin, $code)\n    {\n        $manager = SettingsManager::instance();\n\n        $moduleOwner = $author;\n        $moduleCode = $plugin;\n        $item = $manager->findSettingItem($moduleOwner, $moduleCode);\n\n        if (!$item) {\n            $pluginOwner = $author . '.' . $plugin;\n            $pluginCode = $code;\n            $item = $manager->findSettingItem($pluginOwner, $pluginCode);\n        }\n\n        return $item;\n    }\n}\n","lang_cluster":"PHP","length":190,"code_uid":"91d14b79e3614d8b9424f059ac26477e"}
{"diff_hunk":"@@ -87,6 +87,7 @@ class MailLayout extends Model\n     \/**\n      * Loops over each mail layout and ensures the system has a layout,\n      * if the layout does not exist, it will create one.\n+     *\n      * @return void\n      *\/\n     public static function createLayouts()","old_code":"<?php namespace System\\Models;\n\nuse View;\nuse Model;\nuse System\\Classes\\MailManager;\nuse October\\Rain\\Mail\\MailParser;\nuse ApplicationException;\nuse File as FileHelper;\n\n\/**\n * Mail layout\n *\n * @package october\\system\n * @author Alexey Bobkov, Samuel Georges\n *\/\nclass MailLayout extends Model\n{\n    use \\October\\Rain\\Database\\Traits\\Validation;\n\n    \/**\n     * @var string The database table used by the model.\n     *\/\n    protected $table = 'system_mail_layouts';\n\n    \/**\n     * @var array Guarded fields\n     *\/\n    protected $guarded = [];\n\n    \/**\n     * @var array Fillable fields\n     *\/\n    protected $fillable = [];\n\n    \/**\n     * @var array Validation rules\n     *\/\n    public $rules = [\n        'code'                  => 'required|unique:system_mail_layouts',\n        'name'                  => 'required',\n        'content_html'          => 'required',\n    ];\n\n    \/**\n     * @var array Options array\n     *\/\n    protected $jsonable = [\n        'options'\n    ];\n\n    public static $codeCache;\n\n    public function beforeDelete()\n    {\n        if ($this->is_locked) {\n            throw new ApplicationException('Cannot delete this template because it is locked');\n        }\n    }\n\n    public static function listCodes()\n    {\n        if (self::$codeCache !== null) {\n            return self::$codeCache;\n        }\n\n        return self::$codeCache = self::lists('id', 'code');\n    }\n\n    public static function getIdFromCode($code)\n    {\n        return array_get(self::listCodes(), $code);\n    }\n\n    public static function findOrMakeLayout($code)\n    {\n        $layout = self::whereCode($code)->first();\n\n        if (!$layout && View::exists($code)) {\n            $layout = new self;\n            $layout->code = $code;\n            $layout->fillFromView($code);\n        }\n\n        return $layout;\n    }\n\n    \/**\n     * Loops over each mail layout and ensures the system has a layout,\n     * if the layout does not exist, it will create one.\n     * @return void\n     *\/\n    public static function createLayouts()\n    {\n        $dbLayouts = self::lists('code', 'code');\n\n        $definitions = MailManager::instance()->listRegisteredLayouts();\n        foreach ($definitions as $code => $path) {\n            if (array_key_exists($code, $dbLayouts)) {\n                continue;\n            }\n\n            $layout = new static;\n            $layout->code = $code;\n            $layout->is_locked = true;\n            $layout->fillFromView($path);\n            $layout->save();\n        }\n    }\n\n    public function fillFromCode($code = null)\n    {\n        $definitions = MailManager::instance()->listRegisteredLayouts();\n\n        if ($code === null) {\n            $code = $this->code;\n        }\n\n        if (!$definition = array_get($definitions, $code)) {\n            throw new ApplicationException('Unable to find a registered layout with code: '.$code);\n        }\n\n        $this->fillFromView($definition);\n    }\n\n    public function fillFromView($path)\n    {\n        $sections = self::getTemplateSections($path);\n\n        $css = '\n@media only screen and (max-width: 600px) {\n    .inner-body {\n        width: 100% !important;\n    }\n\n    .footer {\n        width: 100% !important;\n    }\n}\n\n@media only screen and (max-width: 500px) {\n    .button {\n        width: 100% !important;\n    }\n}\n        ';\n\n        $this->name = array_get($sections, 'settings.name', '???');\n        $this->content_css = $css;\n        $this->content_html =  array_get($sections, 'html');\n        $this->content_text = array_get($sections, 'text');\n    }\n\n    protected static function getTemplateSections($code)\n    {\n        return MailParser::parse(FileHelper::get(View::make($code)->getPath()));\n    }\n}\n","lang_cluster":"PHP","length":157,"code_uid":"ab9c3992e95f49958aa5c0425e0fbe12"}
{"diff_hunk":"@@ -3,6 +3,7 @@\n namespace Shopsys\\FrameworkBundle\\DependencyInjection\\Compiler;\n \n use Doctrine\\Common\\Cache\\RedisCache;\n+use Shopsys\\FrameworkBundle\\Component\\Environment\\EnvironmentType;\n use Symfony\\Component\\DependencyInjection\\Compiler\\CompilerPassInterface;\n use Symfony\\Component\\DependencyInjection\\ContainerBuilder;\n ","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\DependencyInjection\\Compiler;\n\nuse Doctrine\\Common\\Cache\\RedisCache;\nuse Symfony\\Component\\DependencyInjection\\Compiler\\CompilerPassInterface;\nuse Symfony\\Component\\DependencyInjection\\ContainerBuilder;\n\n\/**\n * Set Redis-related services as lazy because it might not even be connected (e.g during Docker image build)\n *\/\nclass LazyRedisCompilerPass implements CompilerPassInterface\n{\n    \/**\n     * @param \\Symfony\\Component\\DependencyInjection\\ContainerBuilder $container\n     *\/\n    public function process(ContainerBuilder $container): void\n    {\n        $container->getDefinition('session')\n            ->setLazy(true);\n\n        foreach ($container->getDefinitions() as $definition) {\n            if ($definition->getClass() === RedisCache::class) {\n                $definition->setLazy(true);\n            }\n        }\n    }\n}\n","lang_cluster":"PHP","length":28,"code_uid":"6843a3c9d34b40eeaaacfeaf74a5a264"}
{"diff_hunk":"@@ -2,8 +2,8 @@\n \n namespace Shopsys\\FrameworkBundle\\Form\\Admin\\Heureka;\n \n+use Shopsys\\FrameworkBundle\\Form\\GroupType;\n use Symfony\\Component\\Form\\AbstractType;\n-use Symfony\\Component\\Form\\Extension\\Core\\Type\\FormType;\n use Symfony\\Component\\Form\\Extension\\Core\\Type\\SubmitType;\n use Symfony\\Component\\Form\\Extension\\Core\\Type\\TextareaType;\n use Symfony\\Component\\Form\\Extension\\Core\\Type\\TextType;","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\Form\\Admin\\Heureka;\n\nuse Symfony\\Component\\Form\\AbstractType;\nuse Symfony\\Component\\Form\\Extension\\Core\\Type\\FormType;\nuse Symfony\\Component\\Form\\Extension\\Core\\Type\\SubmitType;\nuse Symfony\\Component\\Form\\Extension\\Core\\Type\\TextareaType;\nuse Symfony\\Component\\Form\\Extension\\Core\\Type\\TextType;\nuse Symfony\\Component\\Form\\FormBuilderInterface;\nuse Symfony\\Component\\OptionsResolver\\OptionsResolver;\nuse Symfony\\Component\\Validator\\Constraints;\n\nclass HeurekaShopCertificationFormType extends AbstractType\n{\n    \/**\n     * @param \\Symfony\\Component\\Form\\FormBuilderInterface $builder\n     * @param array $options\n     *\/\n    public function buildForm(FormBuilderInterface $builder, array $options)\n    {\n        $builderSettingsGroup = $builder->create('settings', FormType::class, [\n            'inherit_data' => true,\n            'label' => t('Settings'),\n            'is_group_container' => true,\n            'is_group_container_to_render_as_the_last_one' => true,\n        ]);\n\n        $builderSettingsGroup\n            ->add('heurekaApiKey', TextType::class, [\n                'required' => false,\n                'constraints' => [\n                    new Constraints\\Length([\n                        'min' => 32,\n                        'max' => 32,\n                        'exactMessage' => 'Heureka API must be {{ limit }} characters',\n                    ]),\n                ],\n                'label' => t('Code of service Heureka - Verified by Customer'),\n                'icon_title' => t('Enter 32-digit code which will be sent to server') . ' ' . $options['server_name'],\n            ])\n            ->add('heurekaWidgetCode', TextareaType::class, [\n                'required' => false,\n                'label' => t('Heureka Widget code'),\n            ]);\n\n        $builder\n            ->add($builderSettingsGroup)\n            ->add('save', SubmitType::class);\n    }\n\n    \/**\n     * @param \\Symfony\\Component\\OptionsResolver\\OptionsResolver $resolver\n     *\/\n    public function configureOptions(OptionsResolver $resolver)\n    {\n        $resolver\n            ->setRequired('server_name')\n            ->setAllowedTypes('server_name', ['string', 'null'])\n            ->setDefaults([\n            'attr' => ['novalidate' => 'novalidate'],\n            ]);\n    }\n}\n","lang_cluster":"PHP","length":64,"code_uid":"76675cbaab7d45aeb4d9ba7e7a30c4ca"}
{"diff_hunk":"@@ -13,9 +13,11 @@ declare(strict_types=1);\n \n namespace Sonata\\MediaBundle\\Listener\\ODM;\n \n-use Doctrine\\Common\\EventArgs;\n+use Doctrine\\ODM\\MongoDB\\Event\\LifecycleEventArgs as ODMLifecycleEventArgs;\n use Doctrine\\ODM\\MongoDB\\Events;\n+use Doctrine\\Persistence\\Event\\LifecycleEventArgs;\n use Sonata\\MediaBundle\\Listener\\BaseMediaEventSubscriber;\n+use Sonata\\MediaBundle\\Model\\MediaInterface;\n \n final class MediaEventSubscriber extends BaseMediaEventSubscriber\n {","old_code":"<?php\n\ndeclare(strict_types=1);\n\n\/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Listener\\ODM;\n\nuse Doctrine\\Common\\EventArgs;\nuse Doctrine\\ODM\\MongoDB\\Events;\nuse Sonata\\MediaBundle\\Listener\\BaseMediaEventSubscriber;\n\nfinal class MediaEventSubscriber extends BaseMediaEventSubscriber\n{\n    public function getSubscribedEvents()\n    {\n        return [\n            Events::prePersist,\n            Events::preUpdate,\n            Events::preRemove,\n            Events::postUpdate,\n            Events::postRemove,\n            Events::postPersist,\n        ];\n    }\n\n    protected function recomputeSingleEntityChangeSet(EventArgs $args): void\n    {\n        $em = $args->getDocumentManager();\n\n        $em->getUnitOfWork()->recomputeSingleDocumentChangeSet(\n            $em->getClassMetadata(\\get_class($args->getDocument())),\n            $args->getDocument()\n        );\n    }\n\n    protected function getMedia(EventArgs $args)\n    {\n        return $args->getDocument();\n    }\n}\n","lang_cluster":"PHP","length":48,"code_uid":"221704daca324633a0450dfa60c6b9d7"}
{"diff_hunk":"@@ -17,8 +17,6 @@ use Ergonode\\Multimedia\\Application\\Model\\MultimediaUploadModel;\n use Ergonode\\Multimedia\\Application\\Form\\MultimediaUploadForm;\n use Ergonode\\Multimedia\\Domain\\Command\\AddMultimediaCommand;\n use Ergonode\\SharedKernel\\Domain\\Aggregate\\MultimediaId;\n-use Ergonode\\Multimedia\\Domain\\Query\\MultimediaQueryInterface;\n-use Ergonode\\Multimedia\\Infrastructure\\Service\\HashCalculationServiceInterface;\n use Ergonode\\SharedKernel\\Domain\\Bus\\CommandBusInterface;\n use Symfony\\Component\\Form\\FormFactoryInterface;\n use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\IsGranted;","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Ergonode Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types=1);\n\nnamespace Ergonode\\Multimedia\\Application\\Controller\\Api\\Multimedia;\n\nuse Ergonode\\Api\\Application\\Exception\\FormValidationHttpException;\nuse Swagger\\Annotations as SWG;\nuse Symfony\\Component\\HttpFoundation\\Request;\nuse Symfony\\Component\\Routing\\Annotation\\Route;\nuse Ergonode\\Multimedia\\Application\\Model\\MultimediaUploadModel;\nuse Ergonode\\Multimedia\\Application\\Form\\MultimediaUploadForm;\nuse Ergonode\\Multimedia\\Domain\\Command\\AddMultimediaCommand;\nuse Ergonode\\SharedKernel\\Domain\\Aggregate\\MultimediaId;\nuse Ergonode\\Multimedia\\Domain\\Query\\MultimediaQueryInterface;\nuse Ergonode\\Multimedia\\Infrastructure\\Service\\HashCalculationServiceInterface;\nuse Ergonode\\SharedKernel\\Domain\\Bus\\CommandBusInterface;\nuse Symfony\\Component\\Form\\FormFactoryInterface;\nuse Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\IsGranted;\n\n\/**\n * @Route(\n *     name=\"ergonode_multimedia_upload\",\n *     path=\"\/multimedia\/upload\",\n *     methods={\"POST\"},\n * )\n *\/\nclass UploadMultimediaAction\n{\n    private MultimediaQueryInterface $query;\n\n    private HashCalculationServiceInterface $hashService;\n\n    private FormFactoryInterface $formFactory;\n\n    private CommandBusInterface $commandBus;\n\n    public function __construct(\n        MultimediaQueryInterface $query,\n        HashCalculationServiceInterface $hashService,\n        FormFactoryInterface $formFactory,\n        CommandBusInterface $commandBus\n    ) {\n        $this->query = $query;\n        $this->hashService = $hashService;\n        $this->formFactory = $formFactory;\n        $this->commandBus = $commandBus;\n    }\n\n    \/**\n     * @IsGranted(\"ERGONODE_ROLE_MULTIMEDIA_POST\")\n     *\n     * @SWG\\Tag(name=\"Multimedia\")\n     * @SWG\\Parameter(\n     *     name=\"upload\",\n     *     in=\"formData\",\n     *     type=\"file\",\n     *     description=\"The field used to upload multimedia\",\n     * )\n     * @SWG\\Response(\n     *     response=201,\n     *     description=\"Returns multimedia ID\",\n     * )\n     * @SWG\\Response(\n     *     response=400,\n     *     description=\"Validation error\",\n     *     @SWG\\Schema(ref=\"#\/definitions\/validation_error_response\")\n     * )\n     *\n     * @throws \\Exception\n     *\/\n    public function __invoke(Request $request): MultimediaId\n    {\n        $uploadModel = new MultimediaUploadModel();\n\n        $form = $this->formFactory->create(MultimediaUploadForm::class, $uploadModel);\n        $form->handleRequest($request);\n\n        if ($form->isSubmitted() && $form->isValid()) {\n            $hash = $this->hashService->calculateHash($uploadModel->upload);\n            if ($this->query->fileExists($hash)) {\n                $id = $this->query->findIdByHash($hash);\n            } else {\n                $command = new AddMultimediaCommand(MultimediaId::generate(), $uploadModel->upload);\n                $this->commandBus->dispatch($command);\n                $id = $command->getId();\n            }\n        } else {\n            throw new FormValidationHttpException($form);\n        }\n\n        return $id;\n    }\n}\n","lang_cluster":"PHP","length":99,"code_uid":"91f82a538c4a480d82670e9083d874d9"}
{"diff_hunk":"@@ -5,6 +5,7 @@ namespace Shopsys\\FrameworkBundle\\Model\\Product\\Search\\Export;\n use Doctrine\\ORM\\EntityManagerInterface;\n use Doctrine\\ORM\\Query\\Expr\\Join;\n use Doctrine\\ORM\\QueryBuilder;\n+use Shopsys\\FrameworkBundle\\Component\\Paginator\\QueryPaginator;\n use Shopsys\\FrameworkBundle\\Model\\Product\\Product;\n use Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibility;\n ","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\Model\\Product\\Search\\Export;\n\nuse Doctrine\\ORM\\EntityManagerInterface;\nuse Doctrine\\ORM\\Query\\Expr\\Join;\nuse Doctrine\\ORM\\QueryBuilder;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\Product;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibility;\n\n\/**\n * @deprecated Use ProductSearchExportWithFilterRepository instead\n *\/\nclass ProductSearchExportRepository\n{\n    \/**\n     * @var \\Doctrine\\ORM\\EntityManagerInterface\n     *\/\n    protected $em;\n\n    \/**\n     * @param \\Doctrine\\ORM\\EntityManagerInterface $em\n     *\/\n    public function __construct(EntityManagerInterface $em)\n    {\n        $this->em = $em;\n    }\n\n    \/**\n     * @param int $domainId\n     * @param string $locale\n     * @param int $startFrom\n     * @param int $batchSize\n     * @return array\n     *\/\n    public function getProductsData(int $domainId, string $locale, int $startFrom, int $batchSize): array\n    {\n        $queryBuilder = $this->createQueryBuilder($domainId, $locale)\n            ->setFirstResult($startFrom)\n            ->setMaxResults($batchSize);\n\n        $query = $queryBuilder->getQuery();\n        return $query->getArrayResult();\n    }\n\n    \/**\n     * @param int $domainId\n     * @param string $locale\n     * @return \\Doctrine\\ORM\\QueryBuilder\n     *\/\n    protected function createQueryBuilder(int $domainId, string $locale): QueryBuilder\n    {\n        $fields = 'p.id, p.catnum, p.partno, p.ean, t.name, d.description, d.shortDescription';\n        $queryBuilder = $this->em->createQueryBuilder()\n            ->select($fields)\n            ->from(Product::class, 'p')\n                ->where('p.variantType != :variantTypeVariant')\n            ->join(ProductVisibility::class, 'prv', Join::WITH, 'prv.product = p.id')\n                ->andWhere('prv.domainId = :domainId')\n                ->andWhere('prv.visible = TRUE')\n            ->join('p.translations', 't')\n                ->andWhere('t.locale = :locale')\n            ->join('p.domains', 'd')\n                ->andWhere('d.domainId = :domainId')\n            ->groupBy($fields)\n            ->orderBy('p.id');\n\n        $queryBuilder->setParameter('domainId', $domainId)\n            ->setParameter('locale', $locale)\n            ->setParameter('variantTypeVariant', Product::VARIANT_TYPE_VARIANT);\n\n        return $queryBuilder;\n    }\n}\n","lang_cluster":"PHP","length":74,"code_uid":"ac0cd40c64e84b49b227d5ff6743c491"}
{"diff_hunk":"@@ -124,3 +124,45 @@ func GetDBAImage() string {\n func GetBgsyncImage() string {\n \treturn fmt.Sprintf(\"%s:%s\", BgsyncImg, BgsyncTag)\n }\n+\n+\/\/ GetDockerComposeVersion runs docker-compose -v to get the current version\n+func GetDockerComposeVersion() (string, error) {\n+\tif DockerComposeVersion != \"\" {\n+\t\treturn DockerComposeVersion, nil\n+\t}\n+\n+\texecutableName := \"docker-compose\"\n+\n+\tpath, err := exec.LookPath(executableName)\n+\tif err != nil {\n+\t\treturn \"\", fmt.Errorf(\"no docker-compose\")\n+\t}\n+\n+\tout, err := exec.Command(path, \"version\", \"--short\").Output()\n+\tif err != nil {\n+\t\treturn \"\", err\n+\t}\n+\n+\tv := string(out)\n+\tDockerComposeVersion = strings.TrimSpace(v)\n+\treturn DockerComposeVersion, nil\n+}\n+\n+\/\/ GetDockerVersion gets the cached or api-sourced version of docker engine\n+func GetDockerVersion() (string, error) {\n+\tif DockerVersion != \"\" {\n+\t\treturn DockerVersion, nil\n+\t}\n+\tvar client *docker.Client\n+\tvar err error\n+\tif client, err = docker.NewClientFromEnv(); err != nil {\n+\t\treturn \"\", err\n+\t}\n+\n+\tv, err := client.Version()\n+\tif err != nil {\n+\t\treturn \"\", err\n+\t}\n+\tDockerVersion = v.Get(\"Version\")\n+\treturn DockerVersion, nil\n+}","old_code":"package version\n\nimport (\n\t\"fmt\"\n)\n\n\/\/ MariaDBDefaultVersion is the default version we use in the db container\nconst MariaDBDefaultVersion = \"10.2\"\n\n\/\/ VERSION is supplied with the git committish this is built from\nvar VERSION = \"\"\n\n\/\/ IMPORTANT: These versions are overridden by version ldflags specifications VERSION_VARIABLES in the Makefile\n\n\/\/ DdevVersion is the current version of ddev, by default the git committish (should be current git tag)\nvar DdevVersion = \"v0.0.0-overridden-by-make\" \/\/ Note that this is overridden by make\n\n\/\/ SentryDSN is the ddev-specific key for the Sentry service.\n\/\/ It is compiled in using link-time variables\nvar SentryDSN = \"\"\n\n\/\/ DockerVersionConstraint is the current minimum version of docker required for ddev.\n\/\/ See https:\/\/godoc.org\/github.com\/Masterminds\/semver#hdr-Checking_Version_Constraints\n\/\/ for examples defining version constraints.\n\/\/ REMEMBER TO CHANGE docs\/index.md if you touch this!\nvar DockerVersionConstraint = \">= 18.06.0-ce\"\n\n\/\/ DockerComposeVersionConstraint is the current minimum version of docker-compose required for ddev.\n\/\/ REMEMBER TO CHANGE docs\/index.md if you touch this!\nvar DockerComposeVersionConstraint = \">= 1.21.0\"\n\n\/\/ DockerComposeFileFormatVersion is the compose version to be used\nvar DockerComposeFileFormatVersion = \"3.6\"\n\n\/\/ WebImg defines the default web image used for applications.\nvar WebImg = \"drud\/ddev-webserver\"\n\n\/\/ WebTag defines the default web image tag for drud dev\nvar WebTag = \"20181124_pecl_upload_progress\" \/\/ Note that this can be overridden by make\n\n\/\/ DBImg defines the default db image used for applications.\nvar DBImg = \"drud\/ddev-dbserver\"\n\n\/\/ BaseDBTag is the main tag, DBTag is constructed from it\nvar BaseDBTag = \"20181203_mariadb_2_versions\"\n\n\/\/ DBAImg defines the default phpmyadmin image tag used for applications.\nvar DBAImg = \"drud\/phpmyadmin\"\n\n\/\/ DBATag defines the default phpmyadmin image tag used for applications.\nvar DBATag = \"v1.4.0\" \/\/ Note that this can be overridden by make\n\n\/\/ BgsyncImg defines the default bgsync image tag used for applications.\nvar BgsyncImg = \"drud\/ddev-bgsync\"\n\n\/\/ BgsyncTag defines the default phpmyadmin image tag used for applications.\nvar BgsyncTag = \"20181117_bgsync\" \/\/ Note that this can be overridden by make\n\n\/\/ RouterImage defines the image used for the router.\nvar RouterImage = \"drud\/ddev-router\"\n\n\/\/ RouterTag defines the tag used for the router.\nvar RouterTag = \"v1.4.0\" \/\/ Note that this can be overridden by make\n\nvar SSHAuthImage = \"drud\/ddev-ssh-agent\"\n\nvar SSHAuthTag = \"20181122_load_all_keys\"\n\n\/\/ COMMIT is the actual committish, supplied by make\nvar COMMIT = \"COMMIT should be overridden\"\n\n\/\/ BUILDINFO is information with date and context, supplied by make\nvar BUILDINFO = \"BUILDINFO should have new info\"\n\n\/\/ DockerVersion is cached version of docker\nvar DockerVersion = \"\"\n\n\/\/ DockerComposeVersion is filled with the version we find for docker-compose\nvar DockerComposeVersion = \"\"\n\n\/\/ DDevTLD defines the tld to use for DDev site URLs.\nconst DDevTLD = \"ddev.local\"\n\n\/\/ GetVersionInfo returns a map containing the version info defined above.\nfunc GetVersionInfo() map[string]string {\n\tversionInfo := make(map[string]string)\n\n\tversionInfo[\"cli\"] = DdevVersion\n\tversionInfo[\"web\"] = GetWebImage()\n\tversionInfo[\"db\"] = GetDBImage()\n\tversionInfo[\"dba\"] = GetDBAImage()\n\tversionInfo[\"bgsync\"] = BgsyncImg + \":\" + BgsyncTag\n\tversionInfo[\"router\"] = RouterImage + \":\" + RouterTag\n\tversionInfo[\"ddev-ssh-agent\"] = SSHAuthImage + \":\" + SSHAuthTag\n\tversionInfo[\"commit\"] = COMMIT\n\tversionInfo[\"domain\"] = DDevTLD\n\tversionInfo[\"build info\"] = BUILDINFO\n\tversionInfo[\"docker\"] = DockerVersion\n\tversionInfo[\"docker-compose\"] = DockerComposeVersion\n\n\treturn versionInfo\n}\n\n\/\/ GetWebImage returns the correctly formatted web image:tag reference\nfunc GetWebImage() string {\n\treturn fmt.Sprintf(\"%s:%s\", WebImg, WebTag)\n}\n\n\/\/ GetDBImage returns the correctly formatted db image:tag reference\nfunc GetDBImage(mariaDBVersion ...string) string {\n\tversion := MariaDBDefaultVersion\n\tif len(mariaDBVersion) > 0 {\n\t\tversion = mariaDBVersion[0]\n\t}\n\treturn fmt.Sprintf(\"%s:%s\", DBImg, BaseDBTag+\"-\"+version)\n}\n\n\/\/ GetDBAImage returns the correctly formatted dba image:tag reference\nfunc GetDBAImage() string {\n\treturn fmt.Sprintf(\"%s:%s\", DBAImg, DBATag)\n}\n\n\/\/ GetDBAImage returns the correctly formatted dba image:tag reference\nfunc GetBgsyncImage() string {\n\treturn fmt.Sprintf(\"%s:%s\", BgsyncImg, BgsyncTag)\n}\n","lang_cluster":"PHP","length":126,"code_uid":"09036c729d1d4f3693876fcd48556829"}
{"diff_hunk":"@@ -2,10 +2,12 @@\n \n namespace EasyCorp\\Bundle\\EasyAdminBundle\\DependencyInjection;\n \n+use EasyCorp\\Bundle\\EasyAdminBundle\\Cache\\CacheWarmer;\n use EasyCorp\\Bundle\\EasyAdminBundle\\Registry\\CrudControllerRegistry;\n use EasyCorp\\Bundle\\EasyAdminBundle\\Registry\\DashboardControllerRegistry;\n use Symfony\\Component\\DependencyInjection\\Compiler\\CompilerPassInterface;\n use Symfony\\Component\\DependencyInjection\\ContainerBuilder;\n+use Symfony\\Component\\DependencyInjection\\Reference;\n \n \/**\n  * Creates the services of the Dashboard and CRUD controller registries. They can't","old_code":"<?php\n\nnamespace EasyCorp\\Bundle\\EasyAdminBundle\\DependencyInjection;\n\nuse EasyCorp\\Bundle\\EasyAdminBundle\\Registry\\CrudControllerRegistry;\nuse EasyCorp\\Bundle\\EasyAdminBundle\\Registry\\DashboardControllerRegistry;\nuse Symfony\\Component\\DependencyInjection\\Compiler\\CompilerPassInterface;\nuse Symfony\\Component\\DependencyInjection\\ContainerBuilder;\n\n\/**\n * Creates the services of the Dashboard and CRUD controller registries. They can't\n * be defined as normal services because they cause circular dependencies.\n * See https:\/\/github.com\/EasyCorp\/EasyAdminBundle\/issues\/3541\n *\n * @author Javier Eguiluz <javier.eguiluz@gmail.com>\n *\/\nclass CreateControllerRegistriesPass implements CompilerPassInterface\n{\n    public function process(ContainerBuilder $container)\n    {\n        $this->createDashboardControllerRegistryService($container);\n        $this->createCrudControllerRegistryService($container);\n    }\n\n    private function createDashboardControllerRegistryService(ContainerBuilder $container): void\n    {\n        $dashboardControllersFqcn = array_keys($container->findTaggedServiceIds(EasyAdminExtension::TAG_DASHBOARD_CONTROLLER, true));\n\n        $container\n            ->register(DashboardControllerRegistry::class, DashboardControllerRegistry::class)\n            ->setPublic(false)\n            ->setArguments([\n                $container->getParameter('kernel.secret'),\n                $container->getParameter('kernel.cache_dir'),\n                $dashboardControllersFqcn,\n            ]);\n    }\n\n    private function createCrudControllerRegistryService(ContainerBuilder $container): void\n    {\n        $crudControllersFqcn = array_keys($container->findTaggedServiceIds(EasyAdminExtension::TAG_CRUD_CONTROLLER, true));\n\n        $container\n            ->register(CrudControllerRegistry::class, CrudControllerRegistry::class)\n            ->setPublic(false)\n            ->setArguments([\n                $container->getParameter('kernel.secret'),\n                $crudControllersFqcn,\n            ]);\n    }\n}\n","lang_cluster":"PHP","length":51,"code_uid":"7c63d80698684c228411c03050c9a85d"}
{"diff_hunk":"@@ -90,46 +90,6 @@ class ProductsResolver implements ResolverInterface, AliasedInterface\n         return $paginator->auto($argument, count($products));\n     }\n \n-    \/**\n-     * @param \\Shopsys\\FrameworkBundle\\Model\\Category\\Category $category\n-     * @param int $offset\n-     * @param int $limit\n-     * @return \\Shopsys\\FrameworkBundle\\Model\\Product\\Product[]\n-     *\/\n-    protected function getProductsByCategory(Category $category, int $offset, int $limit): array\n-    {\n-        $queryBuilder = $this->productOnCurrentDomainFacade->getAllListableTranslatedAndOrderedQueryBuilderByCategory(\n-            ProductListOrderingConfig::ORDER_BY_PRIORITY,\n-            $category\n-        );\n-\n-        $queryBuilder->setFirstResult($offset)\n-            ->setMaxResults($limit + 2);\n-        $query = $queryBuilder->getQuery();\n-        $query->setHint(Query::HINT_CUSTOM_OUTPUT_WALKER, SortableNullsWalker::class);\n-\n-        return $query->execute();\n-    }\n-\n-    \/**\n-     * @param int $offset\n-     * @param int $limit\n-     * @return \\Shopsys\\FrameworkBundle\\Model\\Product\\Product[]\n-     *\/\n-    protected function getProductsForAll(int $offset, int $limit): array\n-    {\n-        $queryBuilder = $this->productOnCurrentDomainFacade->getAllListableTranslatedAndOrderedQueryBuilder(\n-            ProductListOrderingConfig::ORDER_BY_PRIORITY\n-        );\n-\n-        $queryBuilder->setFirstResult($offset)\n-            ->setMaxResults($limit + 2);\n-        $query = $queryBuilder->getQuery();\n-        $query->setHint(Query::HINT_CUSTOM_OUTPUT_WALKER, SortableNullsWalker::class);\n-\n-        return $query->execute();\n-    }\n-\n     \/**\n      * @param \\Overblog\\GraphQLBundle\\Definition\\Argument $argument\n      *\/","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Shopsys\\FrontendApiBundle\\Model\\Resolver\\Products;\n\nuse Doctrine\\ORM\\Query;\nuse Overblog\\GraphQLBundle\\Definition\\Argument;\nuse Overblog\\GraphQLBundle\\Definition\\Resolver\\AliasedInterface;\nuse Overblog\\GraphQLBundle\\Definition\\Resolver\\ResolverInterface;\nuse Overblog\\GraphQLBundle\\Relay\\Connection\\ConnectionBuilder;\nuse Overblog\\GraphQLBundle\\Relay\\Connection\\Paginator;\nuse Shopsys\\FrameworkBundle\\Component\\Doctrine\\SortableNullsWalker;\nuse Shopsys\\FrameworkBundle\\Model\\Category\\Category;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\Listing\\ProductListOrderingConfig;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\ProductOnCurrentDomainFacade;\n\nclass ProductsResolver implements ResolverInterface, AliasedInterface\n{\n    protected const DEFAULT_FIRST_LIMIT = 10;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductOnCurrentDomainFacade\n     *\/\n    protected $productOnCurrentDomainFacade;\n\n    \/**\n     * @var \\Overblog\\GraphQLBundle\\Relay\\Connection\\ConnectionBuilder\n     *\/\n    protected $connectionBuilder;\n\n    \/**\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductOnCurrentDomainFacade $productOnCurrentDomainFacade\n     *\/\n    public function __construct(\n        ProductOnCurrentDomainFacade $productOnCurrentDomainFacade\n    ) {\n        $this->productOnCurrentDomainFacade = $productOnCurrentDomainFacade;\n        $this->connectionBuilder = new ConnectionBuilder();\n    }\n\n    \/**\n     * @param \\Overblog\\GraphQLBundle\\Definition\\Argument $argument\n     * @return \\Overblog\\GraphQLBundle\\Relay\\Connection\\ConnectionInterface|object\n     *\/\n    public function resolve(Argument $argument)\n    {\n        if ($argument->offsetExists('last')) {\n            $limit = (int)$argument->offsetGet('last');\n            $cursor = $argument->offsetGet('before');\n            $offset = max((int)$this->connectionBuilder->cursorToOffset($cursor) - $limit, 0);\n        } else {\n            $this->setDefaultFirstOffsetIfNecessary($argument);\n            $limit = (int)$argument->offsetGet('first');\n            $cursor = $argument->offsetGet('after');\n            $offset = (int)$this->connectionBuilder->cursorToOffset($cursor);\n        }\n\n        $products = $this->getProductsForAll($offset, $limit);\n        $paginator = new Paginator(function () use ($products) {\n            return $products;\n        });\n\n        return $paginator->auto($argument, count($products));\n    }\n\n    \/**\n     * @param \\Overblog\\GraphQLBundle\\Definition\\Argument $argument\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Category\\Category $category\n     * @return \\Overblog\\GraphQLBundle\\Relay\\Connection\\ConnectionInterface|object\n     *\/\n    public function resolveByCategory(Argument $argument, Category $category)\n    {\n        if ($argument->offsetExists('last')) {\n            $limit = (int)$argument->offsetGet('last');\n            $cursor = $argument->offsetGet('before');\n            $offset = max((int)$this->connectionBuilder->cursorToOffset($cursor) - $limit, 0);\n        } else {\n            $this->setDefaultFirstOffsetIfNecessary($argument);\n            $limit = (int)$argument->offsetGet('first');\n            $cursor = $argument->offsetGet('after');\n            $offset = (int)$this->connectionBuilder->cursorToOffset($cursor);\n        }\n\n        $products = $this->getProductsByCategory($category, $offset, $limit);\n        $paginator = new Paginator(function () use ($products) {\n            return $products;\n        });\n\n        return $paginator->auto($argument, count($products));\n    }\n\n    \/**\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Category\\Category $category\n     * @param int $offset\n     * @param int $limit\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Product\\Product[]\n     *\/\n    protected function getProductsByCategory(Category $category, int $offset, int $limit): array\n    {\n        $queryBuilder = $this->productOnCurrentDomainFacade->getAllListableTranslatedAndOrderedQueryBuilderByCategory(\n            ProductListOrderingConfig::ORDER_BY_PRIORITY,\n            $category\n        );\n\n        $queryBuilder->setFirstResult($offset)\n            ->setMaxResults($limit + 2);\n        $query = $queryBuilder->getQuery();\n        $query->setHint(Query::HINT_CUSTOM_OUTPUT_WALKER, SortableNullsWalker::class);\n\n        return $query->execute();\n    }\n\n    \/**\n     * @param int $offset\n     * @param int $limit\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Product\\Product[]\n     *\/\n    protected function getProductsForAll(int $offset, int $limit): array\n    {\n        $queryBuilder = $this->productOnCurrentDomainFacade->getAllListableTranslatedAndOrderedQueryBuilder(\n            ProductListOrderingConfig::ORDER_BY_PRIORITY\n        );\n\n        $queryBuilder->setFirstResult($offset)\n            ->setMaxResults($limit + 2);\n        $query = $queryBuilder->getQuery();\n        $query->setHint(Query::HINT_CUSTOM_OUTPUT_WALKER, SortableNullsWalker::class);\n\n        return $query->execute();\n    }\n\n    \/**\n     * @param \\Overblog\\GraphQLBundle\\Definition\\Argument $argument\n     *\/\n    protected function setDefaultFirstOffsetIfNecessary(Argument $argument): void\n    {\n        if ($argument->offsetExists('first') === false) {\n            $argument->offsetSet('first', static::DEFAULT_FIRST_LIMIT);\n        }\n    }\n\n    \/**\n     * @return string[]\n     *\/\n    public static function getAliases(): array\n    {\n        return [\n            'resolve' => 'products',\n        ];\n    }\n}\n","lang_cluster":"PHP","length":152,"code_uid":"307e9f857ccc4021b63d2cba90097c31"}
{"diff_hunk":"@@ -24,8 +24,9 @@ class FPN(nn.Module):\n             build the feature pyramid. Default: -1, which means the last level.\n         add_extra_convs (bool): Whether to add conv layers on top of the\n             original feature maps. Default: False.\n-        extra_convs_on_inputs (bool): Whether to apply extra conv on\n-            the original feature from the backbone. Default: False.\n+        extra_convs_on_inputs (bool): Whether to apply extra convs\n+            on the original feature from the backbone. Default: True. If True,\n+            it is equivalent to `extra_convs_source='inputs'`.\n         relu_before_extra_convs (bool): Whether to apply relu before the extra\n             conv. Default: False.\n         no_norm_on_lateral (bool): Whether to apply norm on lateral.","old_code":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule, xavier_init\n\nfrom mmdet.core import auto_fp16\nfrom ..builder import NECKS\n\n\n@NECKS.register_module()\nclass FPN(nn.Module):\n    \"\"\"\n    Feature Pyramid Network.\n\n    This is an implementation of - Feature Pyramid Networks for Object\n    Detection (https:\/\/arxiv.org\/abs\/1612.03144)\n\n    Args:\n        in_channels (List[int]): Number of input channels per scale.\n        out_channels (int): Number of output channels (used at each scale)\n        num_outs (int): Number of output scales.\n        start_level (int): Index of the start input backbone level used to\n            build the feature pyramid. Default: 0.\n        end_level (int): Index of the end input backbone level (exclusive) to\n            build the feature pyramid. Default: -1, which means the last level.\n        add_extra_convs (bool): Whether to add conv layers on top of the\n            original feature maps. Default: False.\n        extra_convs_on_inputs (bool): Whether to apply extra conv on\n            the original feature from the backbone. Default: False.\n        relu_before_extra_convs (bool): Whether to apply relu before the extra\n            conv. Default: False.\n        no_norm_on_lateral (bool): Whether to apply norm on lateral.\n            Default: False.\n        conv_cfg (dict): Config dict for convolution layer. Default: None.\n        norm_cfg (dict): Config dict for normalization layer. Default: None.\n        act_cfg (str): Config dict for activation layer in ConvModule.\n            Default: None.\n\n    Example:\n        >>> import torch\n        >>> in_channels = [2, 3, 5, 7]\n        >>> scales = [340, 170, 84, 43]\n        >>> inputs = [torch.rand(1, c, s, s)\n        ...           for c, s in zip(in_channels, scales)]\n        >>> self = FPN(in_channels, 11, len(in_channels)).eval()\n        >>> outputs = self.forward(inputs)\n        >>> for i in range(len(outputs)):\n        ...     print(f'outputs[{i}].shape = {outputs[i].shape}')\n        outputs[0].shape = torch.Size([1, 11, 340, 340])\n        outputs[1].shape = torch.Size([1, 11, 170, 170])\n        outputs[2].shape = torch.Size([1, 11, 84, 84])\n        outputs[3].shape = torch.Size([1, 11, 43, 43])\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=0,\n                 end_level=-1,\n                 add_extra_convs=False,\n                 extra_convs_on_inputs=True,\n                 relu_before_extra_convs=False,\n                 no_norm_on_lateral=False,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 act_cfg=None):\n        super(FPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.relu_before_extra_convs = relu_before_extra_convs\n        self.no_norm_on_lateral = no_norm_on_lateral\n        self.fp16_enabled = False\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n        self.extra_convs_on_inputs = extra_convs_on_inputs\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n                act_cfg=act_cfg,\n                inplace=False)\n            fpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                act_cfg=act_cfg,\n                inplace=False)\n\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                if i == 0 and self.extra_convs_on_inputs:\n                    in_channels = self.in_channels[self.backbone_end_level - 1]\n                else:\n                    in_channels = out_channels\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                    inplace=False)\n                self.fpn_convs.append(extra_fpn_conv)\n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n    @auto_fp16()\n    def forward(self, inputs):\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            prev_shape = laterals[i - 1].shape[2:]\n            laterals[i - 1] += F.interpolate(\n                laterals[i], size=prev_shape, mode='nearest')\n\n        # build outputs\n        # part 1: from original levels\n        outs = [\n            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n        ]\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.extra_convs_on_inputs:\n                    orig = inputs[self.backbone_end_level - 1]\n                    outs.append(self.fpn_convs[used_backbone_levels](orig))\n                else:\n                    outs.append(self.fpn_convs[used_backbone_levels](outs[-1]))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)\n","lang_cluster":"Python","length":182,"code_uid":"d58ff7add0704b5088f8a4dd53ceb921"}
{"diff_hunk":"@@ -1,4 +1,8 @@\n+import base64\n import pytz\n+import requests\n+import six\n+import time\n from flask import current_app\n import spotipy.oauth2\n ","old_code":"import pytz\nfrom flask import current_app\nimport spotipy.oauth2\n\nfrom listenbrainz.db import spotify as db_spotify\nimport datetime\n\nSPOTIFY_API_RETRIES = 5\n\nSPOTIFY_PERMISSIONS_SCOPE = 'user-read-currently-playing streaming user-read-birthdate user-read-email user-read-private user-read-recently-played'\n\n\nclass Spotify:\n    def __init__(self, user_id, musicbrainz_id, musicbrainz_row_id, user_token, token_expires,\n                 refresh_token, last_updated, active, error_message, latest_listened_at):\n        self.user_id = user_id\n        self.user_token = user_token\n        self.token_expires = token_expires\n        self.refresh_token = refresh_token\n        self.last_updated = last_updated\n        self.active = active\n        self.error_message = error_message\n        self.musicbrainz_id = musicbrainz_id\n        self.latest_listened_at = latest_listened_at\n        self.musicbrainz_row_id = musicbrainz_row_id\n\n    def get_spotipy_client(self):\n        return spotipy.Spotify(auth=self.user_token)\n\n    @property\n    def last_updated_iso(self):\n        if self.last_updated is None:\n            return None\n        return self.last_updated.isoformat() + \"Z\"\n\n    @property\n    def latest_listened_at_iso(self):\n        if self.latest_listened_at is None:\n            return None\n        return self.latest_listened_at.isoformat() + \"Z\"\n\n    @property\n    def token_expired(self):\n        now = datetime.datetime.utcnow()\n        now = now.replace(tzinfo=pytz.UTC)\n        return now >= self.token_expires\n\n    @staticmethod\n    def from_dbrow(row):\n        return Spotify(\n           user_id=row['user_id'],\n           user_token=row['user_token'],\n           token_expires=row['token_expires'],\n           refresh_token=row['refresh_token'],\n           last_updated=row['last_updated'],\n           active=row['active'],\n           error_message=row['error_message'],\n           musicbrainz_id=row['musicbrainz_id'],\n           musicbrainz_row_id=row['musicbrainz_row_id'],\n           latest_listened_at=row['latest_listened_at'],\n        )\n\n    def __str__(self):\n        return \"<Spotify(user:%s): %s>\" % (self.user_id, self.musicbrainz_id)\n\n\ndef refresh_user_token(spotify_user):\n    \"\"\" Refreshes the user token for the given spotify user.\n\n    Args:\n        spotify_user (domain.spotify.Spotify): the user whose token is to be refreshed\n\n    Returns:\n        user (domain.spotify.Spotify): the same user with updated tokens\n    \"\"\"\n    auth = get_spotify_oauth()\n\n    retries = SPOTIFY_API_RETRIES\n    new_token = None\n    while retries > 0:\n        new_token = auth.refresh_access_token(spotify_user.refresh_token)\n        if new_token:\n            break\n        retries -= 1\n    if new_token is None:\n        raise SpotifyAPIError('Could not refresh API Token for Spotify user')\n\n    access_token = new_token['access_token']\n    refresh_token = new_token['refresh_token']\n    expires_at = new_token['expires_at']\n    db_spotify.update_token(spotify_user.user_id, access_token, refresh_token, expires_at)\n    return get_user(spotify_user.user_id)\n\n\ndef get_spotify_oauth():\n    \"\"\" Returns a spotipy OAuth instance that can be used to authenticate with spotify.\n    \"\"\"\n    client_id = current_app.config['SPOTIFY_CLIENT_ID']\n    client_secret = current_app.config['SPOTIFY_CLIENT_SECRET']\n    scope = SPOTIFY_PERMISSIONS_SCOPE\n    redirect_url = current_app.config['SPOTIFY_CALLBACK_URL']\n    return spotipy.oauth2.SpotifyOAuth(client_id, client_secret, redirect_uri=redirect_url, scope=scope)\n\n\ndef get_user(user_id):\n    \"\"\" Returns a Spotify instance corresponding to the specified LB row ID.\n    If the user_id is not present in the spotify table, returns None\n\n    Args:\n        user_id (int): the ListenBrainz row ID of the user\n    \"\"\"\n    row = db_spotify.get_user(user_id)\n    if row:\n        return Spotify.from_dbrow(row)\n    return None\n\n\ndef remove_user(user_id):\n    \"\"\" Delete user entry for user with specified ListenBrainz user ID.\n\n    Args:\n        user_id (int): the ListenBrainz row ID of the user\n    \"\"\"\n    db_spotify.delete_spotify(user_id)\n\n\ndef add_new_user(user_id, spot_access_token):\n    \"\"\"Create a spotify row for a user based on OAuth access tokens\n\n    Args:\n        user_id: A flask auth `current_user.id`\n        spot_access_token: A spotipy access token from SpotifyOAuth.get_access_token\n    \"\"\"\n\n    access_token = spot_access_token['access_token']\n    refresh_token = spot_access_token['refresh_token']\n    expires_at = spot_access_token['expires_at']\n\n    db_spotify.create_spotify(user_id, access_token, refresh_token, expires_at)\n\n\ndef get_active_users_to_process():\n    \"\"\" Returns a list of Spotify user instances that need their Spotify listens imported.\n    \"\"\"\n    return [Spotify.from_dbrow(row) for row in db_spotify.get_active_users_to_process()]\n\n\ndef update_last_updated(user_id, success=True, error_message=None):\n    \"\"\" Update the last_update field for user with specified user ID.\n    Also, set the user as active or inactive depending on whether their listens\n    were imported without error.\n\n    If there was an error, add the error to the db.\n\n    Args:\n        user_id (int): the ListenBrainz row ID of the user\n        success (bool): flag representing whether the last import was successful or not.\n        error_message (str): the user-friendly error message to be displayed.\n    \"\"\"\n    if error_message:\n        db_spotify.add_update_error(user_id, error_message)\n    else:\n        db_spotify.update_last_updated(user_id, success)\n\n\ndef update_latest_listened_at(user_id, timestamp):\n    \"\"\" Update the latest_listened_at field for user with specified ListenBrainz user ID.\n\n    Args:\n        user_id (int): the ListenBrainz row ID of the user\n        timestamp (int): the unix timestamp of the latest listen imported for the user\n    \"\"\"\n    db_spotify.update_latest_listened_at(user_id, timestamp)\n\n\nclass SpotifyImporterException(Exception):\n    pass\n\nclass SpotifyListenBrainzError(Exception):\n    pass\n\nclass SpotifyAPIError(Exception):\n    pass\n","lang_cluster":"Python","length":183,"code_uid":"5c96976a8b364556bc907cce454b40c2"}
{"diff_hunk":"@@ -16,11 +16,18 @@ async def test_commands_exist():\n     await m.load_flow(tflow())\n \n     for binding in km.bindings:\n-        cmd, *args = command.lexer(binding.command)\n+        results = command_manager.parse_partial(binding.command)\n+\n+        cmd = results[0][0].value\n+        args = [a.value for a in results[0][1:]]\n+\n         assert cmd in m.commands.commands\n \n         cmd_obj = m.commands.commands[cmd]\n         try:\n             cmd_obj.prepare_args(args)\n         except Exception as e:\n+\n+            import pdb\n+            pdb.set_trace()\n             raise ValueError(\"Invalid command: {}\".format(binding.command)) from e","old_code":"from mitmproxy.test.tflow import tflow\nfrom mitmproxy.tools.console import defaultkeys\nfrom mitmproxy.tools.console import keymap\nfrom mitmproxy.tools.console import master\nfrom mitmproxy import command\n\nimport pytest\n\n\n@pytest.mark.asyncio\nasync def test_commands_exist():\n    km = keymap.Keymap(None)\n    defaultkeys.map(km)\n    assert km.bindings\n    m = master.ConsoleMaster(None)\n    await m.load_flow(tflow())\n\n    for binding in km.bindings:\n        cmd, *args = command.lexer(binding.command)\n        assert cmd in m.commands.commands\n\n        cmd_obj = m.commands.commands[cmd]\n        try:\n            cmd_obj.prepare_args(args)\n        except Exception as e:\n            raise ValueError(\"Invalid command: {}\".format(binding.command)) from e\n","lang_cluster":"Python","length":26,"code_uid":"fcd715eb657845de838613d8b4c342e2"}
{"diff_hunk":"@@ -7,6 +7,7 @@ from mmdet.core.bbox import distance2bbox\n from mmdet.core.mask.structures import BitmapMasks, PolygonMasks\n from mmdet.core.utils import (center_of_mass, filter_scores_and_topk,\n                               flip_tensor, mask2ndarray, select_single_mlvl)\n+from mmdet.utils import find_latest_checkpoint\n \n \n def dummy_raw_polygon_masks(size):","old_code":"# Copyright (c) OpenMMLab. All rights reserved.\nimport numpy as np\nimport pytest\nimport torch\n\nfrom mmdet.core.bbox import distance2bbox\nfrom mmdet.core.mask.structures import BitmapMasks, PolygonMasks\nfrom mmdet.core.utils import (center_of_mass, filter_scores_and_topk,\n                              flip_tensor, mask2ndarray, select_single_mlvl)\n\n\ndef dummy_raw_polygon_masks(size):\n    \"\"\"\n    Args:\n        size (tuple): expected shape of dummy masks, (N, H, W)\n\n    Return:\n        list[list[ndarray]]: dummy mask\n    \"\"\"\n    num_obj, height, width = size\n    polygons = []\n    for _ in range(num_obj):\n        num_points = np.random.randint(5) * 2 + 6\n        polygons.append([np.random.uniform(0, min(height, width), num_points)])\n    return polygons\n\n\ndef test_mask2ndarray():\n    raw_masks = np.ones((3, 28, 28))\n    bitmap_mask = BitmapMasks(raw_masks, 28, 28)\n    output_mask = mask2ndarray(bitmap_mask)\n    assert np.allclose(raw_masks, output_mask)\n\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    output_mask = mask2ndarray(polygon_masks)\n    assert output_mask.shape == (3, 28, 28)\n\n    raw_masks = np.ones((3, 28, 28))\n    output_mask = mask2ndarray(raw_masks)\n    assert np.allclose(raw_masks, output_mask)\n\n    raw_masks = torch.ones((3, 28, 28))\n    output_mask = mask2ndarray(raw_masks)\n    assert np.allclose(raw_masks, output_mask)\n\n    # test unsupported type\n    raw_masks = []\n    with pytest.raises(TypeError):\n        output_mask = mask2ndarray(raw_masks)\n\n\ndef test_distance2bbox():\n    point = torch.Tensor([[74., 61.], [-29., 106.], [138., 61.], [29., 170.]])\n\n    distance = torch.Tensor([[0., 0, 1., 1.], [1., 2., 10., 6.],\n                             [22., -29., 138., 61.], [54., -29., 170., 61.]])\n    expected_decode_bboxes = torch.Tensor([[74., 61., 75., 62.],\n                                           [0., 104., 0., 112.],\n                                           [100., 90., 100., 120.],\n                                           [0., 120., 100., 120.]])\n    out_bbox = distance2bbox(point, distance, max_shape=(120, 100))\n    assert expected_decode_bboxes.allclose(out_bbox)\n    out = distance2bbox(point, distance, max_shape=torch.Tensor((120, 100)))\n    assert expected_decode_bboxes.allclose(out)\n\n    batch_point = point.unsqueeze(0).repeat(2, 1, 1)\n    batch_distance = distance.unsqueeze(0).repeat(2, 1, 1)\n    batch_out = distance2bbox(\n        batch_point, batch_distance, max_shape=(120, 100))[0]\n    assert out.allclose(batch_out)\n    batch_out = distance2bbox(\n        batch_point, batch_distance, max_shape=[(120, 100), (120, 100)])[0]\n    assert out.allclose(batch_out)\n\n    batch_out = distance2bbox(point, batch_distance, max_shape=(120, 100))[0]\n    assert out.allclose(batch_out)\n\n    # test max_shape is not equal to batch\n    with pytest.raises(AssertionError):\n        distance2bbox(\n            batch_point,\n            batch_distance,\n            max_shape=[(120, 100), (120, 100), (32, 32)])\n\n    rois = torch.zeros((0, 4))\n    deltas = torch.zeros((0, 4))\n    out = distance2bbox(rois, deltas, max_shape=(120, 100))\n    assert rois.shape == out.shape\n\n    rois = torch.zeros((2, 0, 4))\n    deltas = torch.zeros((2, 0, 4))\n    out = distance2bbox(rois, deltas, max_shape=(120, 100))\n    assert rois.shape == out.shape\n\n\n@pytest.mark.parametrize('mask', [\n    torch.ones((28, 28)),\n    torch.zeros((28, 28)),\n    torch.rand(28, 28) > 0.5,\n    torch.tensor([[0, 0, 0, 0], [0, 1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]])\n])\ndef test_center_of_mass(mask):\n    center_h, center_w = center_of_mass(mask)\n    if mask.shape[0] == 4:\n        assert center_h == 1.5\n        assert center_w == 1.5\n    assert isinstance(center_h, torch.Tensor) \\\n           and isinstance(center_w, torch.Tensor)\n    assert 0 <= center_h <= 28 \\\n           and 0 <= center_w <= 28\n\n\ndef test_flip_tensor():\n    img = np.random.random((1, 3, 10, 10))\n    src_tensor = torch.from_numpy(img)\n\n    # test flip_direction parameter error\n    with pytest.raises(AssertionError):\n        flip_tensor(src_tensor, 'flip')\n\n    # test tensor dimension\n    with pytest.raises(AssertionError):\n        flip_tensor(src_tensor[0], 'vertical')\n\n    hfilp_tensor = flip_tensor(src_tensor, 'horizontal')\n    expected_hflip_tensor = torch.from_numpy(img[..., ::-1, :].copy())\n    expected_hflip_tensor.allclose(hfilp_tensor)\n\n    vfilp_tensor = flip_tensor(src_tensor, 'vertical')\n    expected_vflip_tensor = torch.from_numpy(img[..., ::-1].copy())\n    expected_vflip_tensor.allclose(vfilp_tensor)\n\n    diag_filp_tensor = flip_tensor(src_tensor, 'diagonal')\n    expected_diag_filp_tensor = torch.from_numpy(img[..., ::-1, ::-1].copy())\n    expected_diag_filp_tensor.allclose(diag_filp_tensor)\n\n\ndef test_select_single_mlvl():\n    mlvl_tensors = [torch.rand(2, 1, 10, 10)] * 5\n    mlvl_tensor_list = select_single_mlvl(mlvl_tensors, 1)\n    assert len(mlvl_tensor_list) == 5 and mlvl_tensor_list[0].ndim == 3\n\n\ndef test_filter_scores_and_topk():\n    score = torch.tensor([[0.1, 0.3, 0.2], [0.12, 0.7, 0.9], [0.02, 0.8, 0.08],\n                          [0.4, 0.1, 0.08]])\n    bbox_pred = torch.tensor([[0.2, 0.3], [0.4, 0.7], [0.1, 0.1], [0.5, 0.1]])\n    score_thr = 0.15\n    nms_pre = 4\n    # test results type error\n    with pytest.raises(NotImplementedError):\n        filter_scores_and_topk(score, score_thr, nms_pre, (score, ))\n\n    filtered_results = filter_scores_and_topk(\n        score, score_thr, nms_pre, results=dict(bbox_pred=bbox_pred))\n    filtered_score, labels, keep_idxs, results = filtered_results\n    assert filtered_score.allclose(torch.tensor([0.9, 0.8, 0.7, 0.4]))\n    assert labels.allclose(torch.tensor([2, 1, 1, 0]))\n    assert keep_idxs.allclose(torch.tensor([1, 2, 1, 3]))\n    assert results['bbox_pred'].allclose(\n        torch.tensor([[0.4, 0.7], [0.1, 0.1], [0.4, 0.7], [0.5, 0.1]]))\n","lang_cluster":"Python","length":162,"code_uid":"6bd65234ab3d426597c07fd375bfeb65"}
{"diff_hunk":"@@ -69,14 +69,13 @@ class UtilsTestCase(SparkTestCase):\n                         'release_mbid': '1',\n                         'listen_count': 1\n                     }]\n-                     }\n-\n-        self.assertDictEqual(user_utils.get_recordings('table'), dictionary)\n-        self.assertEqual(user_utils.get_recordings('table')['user1'][0]['listen_count'], 2)\n-\n+                      }\n+                    \n+        self.assertDictEqual(user.utils.get_recordings(df), dictionary)\n+        self.assertEqual(user.utils.get_recordings(df)['user1'][0]['listen_count'], 2)\n+    \n     def test_get_releases(self):\n         df = self.create_df()\n-        df.createOrReplaceTempView('table')\n         dictionary =  {\n                     'user1' : [{\n                         'artist_name': 'artist1',","old_code":"import listenbrainz_spark.stats.user.utils as user_utils\nfrom listenbrainz_spark import utils\nfrom listenbrainz_spark.tests import SparkTestCase\n\nfrom pyspark.sql import Row\n\nclass UtilsTestCase(SparkTestCase):\n\n    def create_df(self):\n        df = utils.create_dataframe(Row(user_name='user2', artist_name='artist1', artist_msid='1',artist_mbids='1',\n            track_name='test', recording_msid='1', recording_mbid='1', release_name='test',release_msid='1',\n            release_mbid='1'), schema=None)\n        df1 = utils.create_dataframe(Row(user_name='user1',artist_name='artist1', artist_msid='1',artist_mbids='1',\n            track_name='test', recording_msid='1', recording_mbid='1', release_name='test',release_msid='1',\n             release_mbid='1'), schema=None)\n        df = df.union(df1)\n        df2 = utils.create_dataframe(Row(user_name='user1',artist_name='artist1', artist_msid='1',artist_mbids='1',\n            track_name='test', recording_msid='1', recording_mbid='1', release_name='test',release_msid='1',\n            release_mbid='1'), schema=None)\n        df = df.union(df2)\n        return df\n\n    def test_get_artists(self):\n        df = self.create_df()\n        df.createOrReplaceTempView('table')\n        dictionary = {\n                    'user1': [{\n                        'artist_name': 'artist1',\n                        'artist_msid': '1',\n                        'artist_mbids': '1',\n                        'listen_count': 2\n                    }],\n                        'user2': [{\n                        'artist_name': 'artist1',\n                        'artist_msid': '1',\n                        'artist_mbids': '1',\n                        'listen_count': 1\n                    }]\n                     }\n\n        self.assertDictEqual(user_utils.get_artists('table'), dictionary)\n        self.assertEqual(user_utils.get_artists('table')['user1'][0]['listen_count'], 2)\n\n    def test_get_recordings(self):\n        df = self.create_df()\n        df.createOrReplaceTempView('table')\n        dictionary =  {\n                    'user1' : [{\n                        'track_name': 'test',\n                        'recording_msid': '1',\n                        'recording_mbid': '1',\n                        'artist_name': 'artist1',\n                        'artist_msid': '1',\n                        'artist_mbids': '1',\n                        'release_name': 'test',\n                        'release_msid': '1',\n                        'release_mbid': '1',\n                        'listen_count': 2\n                    }],\n                    'user2' : [{\n                        'track_name': 'test',\n                        'recording_msid': '1',\n                        'recording_mbid': '1',\n                        'artist_name': 'artist1',\n                        'artist_msid': '1',\n                        'artist_mbids': '1',\n                        'release_name': 'test',\n                        'release_msid': '1',\n                        'release_mbid': '1',\n                        'listen_count': 1\n                    }]\n                     }\n\n        self.assertDictEqual(user_utils.get_recordings('table'), dictionary)\n        self.assertEqual(user_utils.get_recordings('table')['user1'][0]['listen_count'], 2)\n\n    def test_get_releases(self):\n        df = self.create_df()\n        df.createOrReplaceTempView('table')\n        dictionary =  {\n                    'user1' : [{\n                        'artist_name': 'artist1',\n                        'artist_msid': '1',\n                        'artist_mbids': '1',\n                        'release_name': 'test',\n                        'release_msid': '1',\n                        'release_mbid': '1',\n                        'listen_count': 2\n                    }],\n                    'user2' : [{\n                        'artist_name': 'artist1',\n                        'artist_msid': '1',\n                        'artist_mbids': '1',\n                        'release_name': 'test',\n                        'release_msid': '1',\n                        'release_mbid': '1',\n                        'listen_count': 1\n                    }]\n                     }\n\n        self.assertDictEqual(user_utils.get_releases('table'), dictionary)\n        self.assertEqual(user_utils.get_releases('table')['user1'][0]['listen_count'], 2)\n","lang_cluster":"Python","length":102,"code_uid":"1bbc4b85b3cb4da7992c8f11bb4bb6bb"}
{"diff_hunk":"@@ -1,11 +1,37 @@\n import glob\n-from mitmproxy import utils, script\n+import json\n+import os\n+from contextlib import contextmanager\n+\n+from mitmproxy import utils, script, contentviews\n from mitmproxy.proxy import config\n-from . import tservers\n+from netlib import tutils as netutils\n+from netlib.http import Headers\n+from . import tservers, tutils\n+\n+example_dir = utils.Data(__name__).path(\"..\/..\/examples\")\n+\n+\n+class DummyContext(object):\n+    \"\"\"Emulate script.ScriptContext() functionality.\"\"\"\n+\n+    def log(self, *args, **kwargs):\n+        pass\n+\n+    def add_contentview(self, view_obj):\n+        pass\n+\n+\n+@contextmanager\n+def example(command):\n+    command = os.path.join(example_dir, command)\n+    ctx = DummyContext()\n+    s = script.Script(command, ctx)\n+    yield s\n+    s.unload()\n \n \n def test_load_scripts():\n-    example_dir = utils.Data(__name__).path(\"..\/..\/examples\")\n     scripts = glob.glob(\"%s\/*.py\" % example_dir)\n \n     tmaster = tservers.TestMaster(config.ProxyConfig())","old_code":"import glob\nfrom mitmproxy import utils, script\nfrom mitmproxy.proxy import config\nfrom . import tservers\n\n\ndef test_load_scripts():\n    example_dir = utils.Data(__name__).path(\"..\/..\/examples\")\n    scripts = glob.glob(\"%s\/*.py\" % example_dir)\n\n    tmaster = tservers.TestMaster(config.ProxyConfig())\n\n    for f in scripts:\n        if \"har_extractor\" in f:\n            continue\n        if \"flowwriter\" in f:\n            f += \" -\"\n        if \"iframe_injector\" in f:\n            f += \" foo\"  # one argument required\n        if \"filt\" in f:\n            f += \" ~a\"\n        if \"modify_response_body\" in f:\n            f += \" foo bar\"  # two arguments required\n        try:\n            s = script.Script(f, script.ScriptContext(tmaster))  # Loads the script file.\n        except Exception as v:\n            if \"ImportError\" not in str(v):\n                raise\n        else:\n            s.unload()\n","lang_cluster":"Python","length":30,"code_uid":"3fc07aa81e2944b6bd3f6242d74b7ccb"}
{"diff_hunk":"@@ -8,6 +8,7 @@ from bzt.modules.gatling import DataLogReader as GatlingLogReader\n from bzt.modules.grinder import DataLogReader as GrinderLogReader\n from bzt.modules.jmeter import JTLReader, XMLJTLReader\n from bzt.modules.pbench import PBenchKPIReader\n+from bzt.utils import dehumanize_time\n \n \n class ExternalResultsLoader(ScenarioExecutor, AggregatorListener):","old_code":"import re\n\nfrom bzt import TaurusInternalException, TaurusConfigError\nfrom bzt.engine import ScenarioExecutor\nfrom bzt.modules.ab import TSVDataReader\nfrom bzt.modules.aggregator import AggregatorListener, ConsolidatingAggregator, DataPoint\nfrom bzt.modules.gatling import DataLogReader as GatlingLogReader\nfrom bzt.modules.grinder import DataLogReader as GrinderLogReader\nfrom bzt.modules.jmeter import JTLReader, XMLJTLReader\nfrom bzt.modules.pbench import PBenchKPIReader\n\n\nclass ExternalResultsLoader(ScenarioExecutor, AggregatorListener):\n    AB_HEADER = \"starttime\\tseconds\\tctime\\tdtime\\tttime\\twait\"\n    PBENCH_FORMAT = re.compile(\"^[0-9]+\\.[0-9]{3}\\t[^\\t]*\\t([0-9]+\\t){9}[0-9]+$\")\n\n    def __init__(self):\n        # TODO: document this executor\n        super(ExternalResultsLoader, self).__init__()\n        self.data_file = None\n        self.errors_file = None\n        self.reader = None\n        self._last_ts = -1\n        self._prev_ts = -1\n\n    def prepare(self):\n        exc = TaurusConfigError(\"Option is required for executor: data-file\")\n        self.data_file = self.execution.get(\"data-file\", exc)\n        self.data_file = self.engine.find_file(self.data_file)\n        self.label = self.data_file\n        self.errors_file = self.execution.get(\"errors-jtl\", None)\n        if self.errors_file:\n            self.errors_file = self.engine.find_file(self.errors_file)\n\n        self.reader = self._get_reader()\n        if isinstance(self.engine.aggregator, ConsolidatingAggregator):\n            self.engine.aggregator.add_underling(self.reader)\n\n        if isinstance(self.engine.aggregator, ConsolidatingAggregator):\n            self.engine.aggregator.add_listener(self)\n\n    def _get_reader(self):\n        with open(self.data_file) as fhd:\n            header = fhd.readline(2048).strip()  # just header chunk of file\n\n        # TODO: detect CSV dialect for JTLs\n\n        if header.startswith(self.AB_HEADER):\n            return TSVDataReader(self.data_file, self.log)\n        elif header.startswith(\"<?xml\"):\n            return XMLJTLReader(self.data_file, self.log)\n        elif self.PBENCH_FORMAT.match(header):\n            return PBenchKPIReader(self.data_file, self.log, self.errors_file)\n        elif header.startswith(\"RUN\\t\") or \"\\tRUN\\t\" in header:\n            return GatlingLogReader(self.data_file, self.log, None)\n        elif \"timestamp\" in header.lower() and \"elapsed\" in header.lower():\n            return JTLReader(self.data_file, self.log, self.errors_file)\n        elif \"worker process\" in header.lower() and header.startswith(\"worker.\"):\n            return GrinderLogReader(self.data_file, self.log)\n        else:\n            self.log.info(\"Header line was: %s\", header)\n            raise TaurusInternalException(\"Unable to detect results format for: %s\" % self.data_file)\n\n    def aggregated_second(self, data):\n        self._last_ts = data[DataPoint.TIMESTAMP]\n\n    def check(self):\n        if self._last_ts > 0 and self._last_ts == self._prev_ts:\n            return True\n        else:\n            self._prev_ts = self._last_ts\n","lang_cluster":"Python","length":71,"code_uid":"824dc46e3b7249ca93f0c58e0ea728d3"}
{"diff_hunk":"@@ -192,3 +192,22 @@ def concurrent(fn):\n         return _concurrent\n     raise NotImplementedError(\n         \"Concurrent decorator not supported for '%s' method.\" % fn.func_name)\n+\n+\n+class ScriptModified(PatternMatchingEventHandler):\n+    \n+    def __init__(self, FlowMaster):\n+        self.FlowMaster = FlowMaster\n+        PatternMatchingEventHandler.__init__(self, ignore_directories=True, patterns=[\"*.py\"])\n+    \n+    def on_modified(self, event=FileModifiedEvent):\n+        self.FlowMaster.reload_scripts()\n+        signals.status_message.send(message=\"script: <{0}> reloaded.\".format(event.src_path))\n+\n+\n+def ObserveScripts(FlowMaster, path):\n+    script_dir = os.path.dirname(path)\n+    event_handler = ScriptModified(FlowMaster)\n+    observer = Observer()\n+    observer.schedule(event_handler, script_dir)\n+    observer.start()","old_code":"from __future__ import absolute_import\nimport os\nimport traceback\nimport threading\nimport shlex\nimport sys\n\n\nclass ScriptError(Exception):\n    pass\n\n\nclass ScriptContext:\n    \"\"\"\n    The script context should be used to interact with the global mitmproxy state from within a\n    script.\n    \"\"\"\n    def __init__(self, master):\n        self._master = master\n\n    def log(self, message, level=\"info\"):\n        \"\"\"\n            Logs an event.\n\n            By default, only events with level \"error\" get displayed. This can be controlled with the \"-v\" switch.\n            How log messages are handled depends on the front-end. mitmdump will print them to stdout,\n            mitmproxy sends output to the eventlog for display (\"e\" keyboard shortcut).\n        \"\"\"\n        self._master.add_event(message, level)\n\n    def kill_flow(self, f):\n        \"\"\"\n            Kills a flow immediately. No further data will be sent to the client or the server.\n        \"\"\"\n        f.kill(self._master)\n\n    def duplicate_flow(self, f):\n        \"\"\"\n            Returns a duplicate of the specified flow. The flow is also\n            injected into the current state, and is ready for editing, replay,\n            etc.\n        \"\"\"\n        self._master.pause_scripts = True\n        f = self._master.duplicate_flow(f)\n        self._master.pause_scripts = False\n        return f\n\n    def replay_request(self, f):\n        \"\"\"\n            Replay the request on the current flow. The response will be added\n            to the flow object.\n        \"\"\"\n        return self._master.replay_request(f, block=True, run_scripthooks=False)\n\n    @property\n    def app_registry(self):\n        return self._master.apps\n\n\nclass Script:\n    \"\"\"\n        Script object representing an inline script.\n    \"\"\"\n\n    def __init__(self, command, master):\n        self.command = command\n        self.args = self.parse_command(command)\n        self.ctx = ScriptContext(master)\n        self.ns = None\n        self.load()\n\n    @classmethod\n    def parse_command(cls, command):\n        if not command or not command.strip():\n            raise ScriptError(\"Empty script command.\")\n        if os.name == \"nt\":  # Windows: escape all backslashes in the path.\n            backslashes = shlex.split(command, posix=False)[0].count(\"\\\\\")\n            command = command.replace(\"\\\\\", \"\\\\\\\\\", backslashes)\n        args = shlex.split(command)\n        args[0] = os.path.expanduser(args[0])\n        if not os.path.exists(args[0]):\n            raise ScriptError(\n                (\"Script file not found: %s.\\r\\n\"\n                 \"If your script path contains spaces, \"\n                 \"make sure to wrap it in additional quotes, e.g. -s \\\"'.\/foo bar\/baz.py' --args\\\".\") %\n                args[0])\n        elif os.path.isdir(args[0]):\n            raise ScriptError(\"Not a file: %s\" % args[0])\n        return args\n\n    def load(self):\n        \"\"\"\n            Loads an inline script.\n\n            Returns:\n                The return value of self.run(\"start\", ...)\n\n            Raises:\n                ScriptError on failure\n        \"\"\"\n        if self.ns is not None:\n            self.unload()\n        script_dir = os.path.dirname(os.path.abspath(self.args[0]))\n        ns = {'__file__': os.path.abspath(self.args[0])}\n        sys.path.append(script_dir)\n        try:\n            execfile(self.args[0], ns, ns)\n        except Exception as e:\n            # Python 3: use exception chaining, https:\/\/www.python.org\/dev\/peps\/pep-3134\/\n            raise ScriptError(traceback.format_exc(e))\n        sys.path.pop()\n        self.ns = ns\n        return self.run(\"start\", self.args)\n\n    def unload(self):\n        ret = self.run(\"done\")\n        self.ns = None\n        return ret\n\n    def run(self, name, *args, **kwargs):\n        \"\"\"\n            Runs an inline script hook.\n\n            Returns:\n                The return value of the method.\n                None, if the script does not provide the method.\n\n            Raises:\n                ScriptError if there was an exception.\n        \"\"\"\n        f = self.ns.get(name)\n        if f:\n            try:\n                return f(self.ctx, *args, **kwargs)\n            except Exception as e:\n                raise ScriptError(traceback.format_exc(e))\n        else:\n            return None\n\n\nclass ReplyProxy(object):\n    def __init__(self, original_reply, script_thread):\n        self.original_reply = original_reply\n        self.script_thread = script_thread\n        self._ignore_call = True\n        self.lock = threading.Lock()\n\n    def __call__(self, *args, **kwargs):\n        with self.lock:\n            if self._ignore_call:\n                self.script_thread.start()\n                self._ignore_call = False\n                return\n        self.original_reply(*args, **kwargs)\n\n    def __getattr__(self, k):\n        return getattr(self.original_reply, k)\n\n\ndef _handle_concurrent_reply(fn, o, *args, **kwargs):\n    # Make first call to o.reply a no op and start the script thread.\n    # We must not start the script thread before, as this may lead to a nasty race condition\n    # where the script thread replies a different response before the normal reply, which then gets swallowed.\n\n    def run():\n        fn(*args, **kwargs)\n        # If the script did not call .reply(), we have to do it now.\n        reply_proxy()\n\n    script_thread = ScriptThread(target=run)\n\n    reply_proxy = ReplyProxy(o.reply, script_thread)\n    o.reply = reply_proxy\n\n\nclass ScriptThread(threading.Thread):\n    name = \"ScriptThread\"\n\n\ndef concurrent(fn):\n    if fn.func_name in (\n            \"request\",\n            \"response\",\n            \"error\",\n            \"clientconnect\",\n            \"serverconnect\",\n            \"clientdisconnect\",\n            \"next_layer\"):\n        def _concurrent(ctx, obj):\n            _handle_concurrent_reply(fn, obj, ctx, obj)\n\n        return _concurrent\n    raise NotImplementedError(\n        \"Concurrent decorator not supported for '%s' method.\" % fn.func_name)\n","lang_cluster":"Python","length":194,"code_uid":"7ce86e5d9e0841f0a1b3e6c4ef18f067"}
{"diff_hunk":"@@ -19,15 +19,10 @@\n #  DEALINGS IN THE SOFTWARE.\n \"\"\"Login Command Module.\"\"\"\n \n-import fcntl\n import os\n-import signal\n-import struct\n-import sys\n-import termios\n+from subprocess import run\n \n import click\n-import pexpect\n \n from molecule import logger, scenarios, util\n from molecule.command import base","old_code":"#  Copyright (c) 2015-2018 Cisco Systems, Inc.\n#\n#  Permission is hereby granted, free of charge, to any person obtaining a copy\n#  of this software and associated documentation files (the \"Software\"), to\n#  deal in the Software without restriction, including without limitation the\n#  rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or\n#  sell copies of the Software, and to permit persons to whom the Software is\n#  furnished to do so, subject to the following conditions:\n#\n#  The above copyright notice and this permission notice shall be included in\n#  all copies or substantial portions of the Software.\n#\n#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n#  DEALINGS IN THE SOFTWARE.\n\"\"\"Login Command Module.\"\"\"\n\nimport fcntl\nimport os\nimport signal\nimport struct\nimport sys\nimport termios\n\nimport click\nimport pexpect\n\nfrom molecule import logger, scenarios, util\nfrom molecule.command import base\n\nLOG = logger.get_logger(__name__)\n\n\nclass Login(base.Base):\n    \"\"\"\n    Login Command Class.\n\n    .. program:: molecule login\n\n    .. option:: molecule login\n\n        Target the default scenario.\n\n    .. program:: molecule login --scenario-name foo\n\n    .. option:: molecule login --scenario-name foo\n\n        Targeting a specific scenario.\n\n    .. program:: molecule login --host hostname\n\n    .. option:: molecule login --host hostname\n\n        Targeting a specific running host.\n\n    .. program:: molecule login --host hostname --scenario-name foo\n\n    .. option:: molecule login --host hostname --scenario-name foo\n\n        Targeting a specific running host and scenario.\n\n    .. program:: molecule --debug login\n\n    .. option:: molecule --debug login\n\n        Executing with `debug`.\n\n    .. program:: molecule --base-config base.yml login\n\n    .. option:: molecule --base-config base.yml login\n\n        Executing with a `base-config`.\n\n    .. program:: molecule --env-file foo.yml login\n\n    .. option:: molecule --env-file foo.yml login\n\n        Load an env file to read variables from when rendering\n        molecule.yml.\n    \"\"\"\n\n    def __init__(self, c):\n        \"\"\"Construct Login.\"\"\"\n        super(Login, self).__init__(c)\n        self._pt = None\n\n    def execute(self):\n        \"\"\"\n        Execute the actions necessary to perform a `molecule login` and \\\n        returns None.\n\n        :return: None\n        \"\"\"\n        c = self._config\n        if (not c.state.created) and c.driver.managed:\n            msg = \"Instances not created.  Please create instances first.\"\n            util.sysexit_with_message(msg)\n\n        hosts = [d[\"name\"] for d in self._config.platforms.instances]\n        hostname = self._get_hostname(hosts)\n        self._get_login(hostname)\n\n    def _get_hostname(self, hosts):\n        hostname = self._config.command_args.get(\"host\")\n        if hostname is None:\n            if len(hosts) == 1:\n                hostname = hosts[0]\n            else:\n                msg = (\n                    \"There are {} running hosts. Please specify \"\n                    \"which with --host.\\n\\n\"\n                    \"Available hosts:\\n{}\".format(len(hosts), \"\\n\".join(sorted(hosts)))\n                )\n                util.sysexit_with_message(msg)\n        match = [x for x in hosts if x.startswith(hostname)]\n        if len(match) == 0:\n            msg = (\n                \"There are no hosts that match '{}'.  You \"\n                \"can only login to valid hosts.\"\n            ).format(hostname)\n            util.sysexit_with_message(msg)\n        elif len(match) != 1:\n            # If there are multiple matches, but one of them is an exact string\n            # match, assume this is the one they're looking for and use it.\n            if hostname in match:\n                match = [hostname]\n            else:\n                msg = (\n                    \"There are {} hosts that match '{}'. You \"\n                    \"can only login to one at a time.\\n\\n\"\n                    \"Available hosts:\\n{}\".format(\n                        len(match), hostname, \"\\n\".join(sorted(hosts))\n                    )\n                )\n                util.sysexit_with_message(msg)\n\n        return match[0]\n\n    def _get_login(self, hostname):  # pragma: no cover\n        lines, columns = os.popen(\"stty size\", \"r\").read().split()\n        login_options = self._config.driver.login_options(hostname)\n        login_options[\"columns\"] = columns\n        login_options[\"lines\"] = lines\n        login_cmd = self._config.driver.login_cmd_template.format(**login_options)\n\n        dimensions = (int(lines), int(columns))\n        cmd = \"\/usr\/bin\/env {}\".format(login_cmd)\n        self._pt = pexpect.spawn(cmd, dimensions=dimensions)\n        signal.signal(signal.SIGWINCH, self._sigwinch_passthrough)\n        self._pt.interact()\n\n    def _sigwinch_passthrough(self, sig, data):  # pragma: no cover\n        tiocgwinsz = 1074295912  # assume\n        if \"TIOCGWINSZ\" in dir(termios):\n            tiocgwinsz = termios.TIOCGWINSZ\n        s = struct.pack(\"HHHH\", 0, 0, 0, 0)\n        a = struct.unpack(\"HHHH\", fcntl.ioctl(sys.stdout.fileno(), tiocgwinsz, s))\n        self._pt.setwinsize(a[0], a[1])\n\n\n@base.click_command_ex()\n@click.pass_context\n@click.option(\"--host\", \"-h\", help=\"Host to access.\")\n@click.option(\n    \"--scenario-name\",\n    \"-s\",\n    default=base.MOLECULE_DEFAULT_SCENARIO_NAME,\n    help=\"Name of the scenario to target. ({})\".format(\n        base.MOLECULE_DEFAULT_SCENARIO_NAME\n    ),\n)\ndef login(ctx, host, scenario_name):  # pragma: no cover\n    \"\"\"Log in to one instance.\"\"\"\n    args = ctx.obj.get(\"args\")\n    subcommand = base._get_subcommand(__name__)\n    command_args = {\"subcommand\": subcommand, \"host\": host}\n\n    s = scenarios.Scenarios(base.get_configs(args, command_args), scenario_name)\n    for scenario in s.all:\n        base.execute_subcommand(scenario.config, subcommand)\n","lang_cluster":"Python","length":184,"code_uid":"d713128ae15346ad881c9e2d85d8bdab"}
{"diff_hunk":"@@ -1,11 +1,12 @@\n # -*- coding: UTF-8 -*-\n #A part of NonVisual Desktop Access (NVDA)\n-#Copyright (C) 2016 NV Access Limited\n+#Copyright (C) 2016-2018 NV Access Limited\n #This file is covered by the GNU General Public License.\n #See the file COPYING for more details.\n \n import wx\n from wx.lib.mixins import listctrl as listmix\n+import winUser\n \n class AutoWidthColumnListCtrl(wx.ListCtrl, listmix.ListCtrlAutoWidthMixin):\n \t\"\"\"","old_code":"# -*- coding: UTF-8 -*-\r\n#A part of NonVisual Desktop Access (NVDA)\r\n#Copyright (C) 2016 NV Access Limited\r\n#This file is covered by the GNU General Public License.\r\n#See the file COPYING for more details.\r\n\r\nimport wx\r\nfrom wx.lib.mixins import listctrl as listmix\r\n\r\nclass AutoWidthColumnListCtrl(wx.ListCtrl, listmix.ListCtrlAutoWidthMixin):\r\n\t\"\"\"\r\n\tA list control that allows you to specify a column to resize to take up the remaining width of a wx.ListCtrl\r\n\t\"\"\"\r\n\tdef __init__(self, parent, id=wx.ID_ANY, autoSizeColumnIndex=\"LAST\", pos=wx.DefaultPosition, size=wx.DefaultSize, style=0):\r\n\t\t\"\"\" initialiser\r\n\t\t\tTakes the same parameter as a wx.ListCtrl with the following additions:\r\n\t\t\tautoSizeColumnIndex - defaults to \"LAST\" which results in the last column being resized. Pass the index of \r\n\t\t\tthe column to be resized.\r\n\t\t\"\"\"\r\n\t\twx.ListCtrl.__init__(self, parent, id, pos, size, style)\r\n\t\tlistmix.ListCtrlAutoWidthMixin.__init__(self)\r\n\t\tself.setResizeColumn(autoSizeColumnIndex)\r\n\r\nclass SelectOnFocusSpinCtrl(wx.SpinCtrl):\r\n\t\"\"\"\r\n\tA spin control that automatically selects the value when the control gains focus.\r\n\tThis makes editing the values quicker.\r\n\t\"\"\"\r\n\tdef __init__(self, parent, id=wx.ID_ANY, value=wx.EmptyString, pos=wx.DefaultPosition, size=wx.DefaultSize, style=wx.SP_ARROW_KEYS|wx.ALIGN_RIGHT, min=0, max=100, initial=0, name=wx.SpinCtrlNameStr):\r\n\t\t\"\"\" initialiser - Takes the same parameters as a wx.SpinCtrl.\r\n\t\t\"\"\"\r\n\t\twx.SpinCtrl.__init__(self, parent, id, value, pos, size, style, min, max, initial, name)\r\n\t\tself.Bind(wx.EVT_SET_FOCUS, self.OnSetFocus)\r\n\r\n\tdef OnSetFocus(self, evt):\r\n\t\tnumChars = len(str(self.GetValue()))\r\n\t\tself.SetSelection(0, numChars)\r\n\t\tevt.Skip()\r\n\r\n","lang_cluster":"Python","length":39,"code_uid":"56d8e24ccc49419da71dbb1b07bec14c"}
{"diff_hunk":"@@ -113,4 +113,11 @@ class QuadMeshPlot(RasterPlot):\n         if self.invert_axes:\n             y, x = 'x', 'y'\n             zdata = zdata.T\n+\n+        plot_opts = element.opts.get('plot', 'plotly')\n+        nodata = plot_opts.kwargs.get('nodata')\n+        if nodata is not None and (zdata.dtype.kind  == 'i'):\n+            zdata = zdata.astype(np.float64)\n+            zdata[zdata == nodata] = np.NaN\n+\n         return [{x: xc, y: yc, 'z': zdata}]","old_code":"from __future__ import absolute_import, division, unicode_literals\n\nimport numpy as np\nimport param\n\nfrom ...core.options import SkipRendering\nfrom ...element import Image, Raster\nfrom ..mixins import HeatMapMixin\nfrom .element import ColorbarPlot\n\n\nclass RasterPlot(ColorbarPlot):\n\n    padding = param.ClassSelector(default=0, class_=(int, float, tuple))\n\n    style_opts = ['visible', 'cmap', 'alpha']\n\n    @classmethod\n    def trace_kwargs(cls, is_geo=False, **kwargs):\n        return {'type': 'heatmap'}\n\n    def graph_options(self, element, ranges, style, **kwargs):\n        opts = super(RasterPlot, self).graph_options(element, ranges, style, **kwargs)\n        copts = self.get_color_opts(element.vdims[0], element, ranges, style)\n        opts['zmin'] = copts.pop('cmin')\n        opts['zmax'] = copts.pop('cmax')\n        opts['zauto'] = copts.pop('cauto')\n        return dict(opts, **copts)\n\n    def get_data(self, element, ranges, style, **kwargs):\n        if isinstance(element, Image):\n            l, b, r, t = element.bounds.lbrt()\n        else:\n            l, b, r, t = element.extents\n        array = element.dimension_values(2, flat=False)\n        if type(element) is Raster:\n            array=array.T[::-1,...]\n        ny, nx = array.shape\n        dx, dy = float(r-l)\/nx, float(t-b)\/ny\n        x0, y0 = l+dx\/2., b+dy\/2.\n        if self.invert_axes:\n            x0, y0, dx, dy = y0, x0, dy, dx\n            array = array.T\n        return [dict(x0=x0, y0=y0, dx=dx, dy=dy, z=array)]\n\n\nclass HeatMapPlot(HeatMapMixin, RasterPlot):\n\n    def init_layout(self, key, element, ranges, **kwargs):\n        layout = super(HeatMapPlot, self).init_layout(key, element, ranges)\n        gridded = element.gridded\n        xdim, ydim = gridded.dimensions()[:2]\n\n        if self.invert_axes:\n            xaxis, yaxis = ('yaxis', 'xaxis')\n        else:\n            xaxis, yaxis = ('xaxis', 'yaxis')\n\n        shape = gridded.interface.shape(gridded, gridded=True)\n\n        xtype = gridded.interface.dtype(gridded, xdim)\n        if xtype.kind in 'SUO':\n            layout[xaxis]['tickvals'] = np.arange(shape[1])\n            layout[xaxis]['ticktext'] = gridded.dimension_values(0, expanded=False)\n\n        ytype = gridded.interface.dtype(gridded, ydim)\n        if ytype.kind in 'SUO':\n            layout[yaxis]['tickvals'] = np.arange(shape[0])\n            layout[yaxis]['ticktext'] = gridded.dimension_values(1, expanded=False)\n        return layout\n\n    def get_data(self, element, ranges, style, **kwargs):\n        if not element._unique:\n            self.param.warning('HeatMap element index is not unique,  ensure you '\n                               'aggregate the data before displaying it, e.g. '\n                               'using heatmap.aggregate(function=np.mean). '\n                               'Duplicate index values have been dropped.')\n\n        gridded = element.gridded\n        xdim, ydim = gridded.dimensions()[:2]\n        data = gridded.dimension_values(2, flat=False)\n\n        xtype = gridded.interface.dtype(gridded, xdim)\n        if xtype.kind in 'SUO':\n            xvals = np.arange(data.shape[1]+1)-0.5\n        else:\n            xvals = gridded.interface.coords(gridded, xdim, edges=True, ordered=True)\n\n        ytype = gridded.interface.dtype(gridded, ydim)\n        if ytype.kind in 'SUO':\n            yvals = np.arange(data.shape[0]+1)-0.5\n        else:\n            yvals = gridded.interface.coords(gridded, ydim, edges=True, ordered=True)\n\n        if self.invert_axes:\n            xvals, yvals = yvals, xvals\n            data = data.T\n\n        return [dict(x=xvals, y=yvals, z=data)]\n\n\nclass QuadMeshPlot(RasterPlot):\n\n    def get_data(self, element, ranges, style, **kwargs):\n        x, y, z = element.dimensions()[:3]\n        irregular = element.interface.irregular(element, x)\n        if irregular:\n            raise SkipRendering(\"Plotly QuadMeshPlot only supports rectilinear meshes\")\n        xc, yc = (element.interface.coords(element, x, edges=True, ordered=True),\n                  element.interface.coords(element, y, edges=True, ordered=True))\n        zdata = element.dimension_values(z, flat=False)\n        x, y = ('x', 'y')\n        if self.invert_axes:\n            y, x = 'x', 'y'\n            zdata = zdata.T\n        return [{x: xc, y: yc, 'z': zdata}]\n","lang_cluster":"Python","length":116,"code_uid":"e76696916faa4c808bd018abaf5fb9ce"}
{"diff_hunk":"@@ -91,7 +91,6 @@ def init_error_handlers(app):\n         return error.render_error()\n \n \n-\n class InvalidAPIUsage(Exception):\n     \"\"\" General error class for the API_compat to render errors in multiple formats \"\"\"\n     def __init__(self, api_error, status_code=500, output_format=\"xml\"):","old_code":"from flask import render_template, make_response\nfrom yattag import Doc\nimport yattag\nimport ujson\nimport collections\n\nLastFMError = collections.namedtuple('LastFMError', ['code', 'message'])\n\n# List of errors compatible with LastFM messages for API_compat.\nclass CompatError(object):\n    DOES_NOT_EXIST           = LastFMError(code = 1, message = \"This error does not exist\")\n    INVALID_SERVICE          = LastFMError(code = 2, message = \"Invalid service -This service does not exist\")\n    INVALID_METHOD           = LastFMError(code = 3, message = \"Invalid Method - No method with that name in this package\")\n    INVALID_TOKEN            = LastFMError(code = 4, message = \"Invalid Token - Invalid authentication token supplied\")\n    INVALID_FORMAT           = LastFMError(code = 5, message = \"Invalid format - This service doesn't exist in that format\")\n    INVALID_PARAMETERS       = LastFMError(code = 6, message = \"Invalid parameters - \" \\\n                                                               \"Your request is missing a required parameter\")\n    INVALID_RESOURCE         = LastFMError(code = 7, message = \"Invalid resource specified\")\n    OP_FAILED                = LastFMError(code = 8, message = \"Operation failed - Most likely the backend service failed. \" \\\n                                                               \"Please try again.\")\n    INVALID_SESSION_KEY      = LastFMError(code = 9, message = \"Invalid session key - Please re-authenticate\")\n    INVALID_API_KEY          = LastFMError(code = 10, message = \"Invalid API key - You must be granted a valid key by last.fm\")\n    SERVICE_OFFLINE          = LastFMError(code = 11, message = \"Service Offline - This service is temporarily offline. \" \\\n                                                                \"Try again later.\")\n    SUBSCRIBERS_ONLY         = LastFMError(code = 12, message = \"Subscribers Only - This station is only available to \" \\\n                                                                \"paid last.fm subscribers\")\n    INVALID_METHOD_SIGNATURE = LastFMError(code = 13, message = \"Invalid method signature supplied\")\n    UNAUTHORIZED_TOKEN       = LastFMError(code = 14, message = \"Unauthorized Token - This token has not been authorized\")\n    TOKEN_EXPIRED            = LastFMError(code = 15, message = \"This token has expired\")\n    SERVICE_UNAVAILABLE      = LastFMError(code = 16, message = \"The service is temporarily unavailable, please try again.\")\n    NEED_LOGIN               = LastFMError(code = 17, message = \"Login: User requires to be logged in\")\n    TRIAL_EXPIRED            = LastFMError(code = 18, message = \"Trial Expired - This user has no free radio plays left. \" \\\n                                                                \"Subscription required.\")\n    DOES_NOT_EXIST_19        = LastFMError(code = 19, message = \"This error does not exist\")\n    NOT_ENOUGH_CONTENT       = LastFMError(code = 20, message = \"Not Enough Content - There is not enough content to \" \\\n                                                                \"play this station\")\n    NOT_ENOUGH_MEMBERS       = LastFMError(code = 21, message = \"Not Enough Members - This group does not have enough members \" \\\n                                                                \"for radio\")\n    NOT_ENOUGH_FANS          = LastFMError(code = 22, message = \"Not Enough Fans - This artist does not have enough fans \" \\\n                                                                \"for radio\")\n    NOT_ENOUGH_NEIGHBOURS    = LastFMError(code = 23, message = \"Not Enough Neighbours - There are not enough neighbours \" \\\n                                                                \"for radio\")\n    NO_PEAK_RADIO            = LastFMError(code = 24, message = \"No Peak Radio - This user is not allowed to listen to \" \\\n                                                                \"radio during peak usage\")\n    RADIO_NOT_FOUND          = LastFMError(code = 25, message = \"Radio Not Found - Radio station not found\")\n    API_KEY_SUSPENDED        = LastFMError(code = 26, message = \"API Key Suspended - This application is not allowed to make \"\n                                                                \"requests to the web services\")\n    DEPRECATED               = LastFMError(code = 27, message = \"Deprecated - This type of request is no longer supported\")\n    RATE_LIMIT_EXCEEDED      = LastFMError(code = 29, message = \"Rate Limit Exceded - Your IP has made too many requests in \" \\\n                                                                \"exceeding our API guidelines\")\n\n\ndef init_error_handlers(app):\n\n    def error_wrapper(template, error, code):\n        resp = make_response(render_template(template, error=error))\n        resp.headers['Access-Control-Allow-Origin'] = '*'\n        return resp, code\n\n    @app.errorhandler(400)\n    def bad_request(error):\n        return error_wrapper('errors\/400.html', error, 400)\n\n    @app.errorhandler(401)\n    def unauthorized(error):\n        return error_wrapper('errors\/401.html', error, 401)\n\n    @app.errorhandler(403)\n    def forbidden(error):\n        return error_wrapper('errors\/403.html', error, 403)\n\n    @app.errorhandler(404)\n    def not_found(error):\n        return error_wrapper('errors\/404.html', error, 404)\n\n    @app.errorhandler(413)\n    def file_size_too_large(error):\n        return error_wrapper('errors\/413.html', error, 413)\n\n    @app.errorhandler(500)\n    def internal_server_error(error):\n        return error_wrapper('errors\/500.html', error, 500)\n\n    @app.errorhandler(503)\n    def service_unavailable(error):\n        return error_wrapper('errors\/503.html', error, 503)\n\n    # Handle error of API_compat\n    @app.errorhandler(InvalidAPIUsage)\n    def handle_api_compat_error(error):\n        return error.render_error()\n\n\n\nclass InvalidAPIUsage(Exception):\n    \"\"\" General error class for the API_compat to render errors in multiple formats \"\"\"\n    def __init__(self, api_error, status_code=500, output_format=\"xml\"):\n        Exception.__init__(self)\n        self.api_error = api_error\n        self.status_code = status_code\n        self.output_format = output_format\n\n    def render_error(self):\n        return {\n            \"json\": self.to_json,\n            \"xml\": self.to_xml\n        }.get(self.output_format, self.to_xml)()\n\n    def to_json(self):\n        return ujson.dumps({\n            \"error\": self.api_error.code,\n            \"message\": self.api_error.message\n        }, indent=4)\n\n    def to_xml(self):\n        doc, tag, text = Doc().tagtext()\n        with tag('lfm', status=\"failed\"):\n            with tag('error', code=self.api_error.code):\n                text(self.api_error.message)\n        return '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n' + yattag.indent(doc.getvalue())\n","lang_cluster":"Python","length":120,"code_uid":"e9be3ce5b1224d4989b0c26acac747f8"}
{"diff_hunk":"@@ -67,6 +67,6 @@ t = TLSClientAutomaton(client_hello=ch,\n                        resumption_master_secret=args.res_master,\n                        session_ticket_file_in=args.session_ticket_file_in,\n                        session_ticket_file_out=args.session_ticket_file_out,\n-                      )\n+                       debug=args.debug)\n t.run()\n ","old_code":"#!\/usr\/bin\/env python\n\n## This file is part of Scapy\n## This program is published under a GPLv2 license\n\n\"\"\"\nBasic TLS client. A ciphersuite may be commanded via a first argument.\nDefault protocol version is TLS 1.3.\n\"\"\"\n\nimport os\nimport sys\n\nbasedir = os.path.abspath(os.path.join(os.path.dirname(__file__),\"..\/..\/\"))\nsys.path=[basedir]+sys.path\n\nfrom scapy.layers.tls.automaton_cli import TLSClientAutomaton\nfrom scapy.layers.tls.basefields import _tls_version_options\nfrom scapy.layers.tls.handshake import TLSClientHello, TLS13ClientHello\n\nfrom argparse import ArgumentParser\n\npsk = None\nparser = ArgumentParser(description='Simple TLS Client')\nparser.add_argument(\"--psk\",\n                    help=\"External PSK for symmetric authentication (for TLS 1.3)\")  # noqa: E501\nparser.add_argument(\"--no_pfs\", action=\"store_true\",\n                    help=\"Disable (EC)DHE exchange with PFS\")\nparser.add_argument(\"--ciphersuite\", help=\"Ciphersuite preference\")\nparser.add_argument(\"--version\", help=\"TLS Version\", default=\"tls13\")\nparser.add_argument(\"--ticket_in\", dest='session_ticket_file_in',\n                    help=\"File to read a ticket from (for TLS 1.3)\")\nparser.add_argument(\"--ticket_out\", dest='session_ticket_file_out',\n                    help=\"File to write a ticket to (for TLS 1.3)\")\nparser.add_argument(\"--res_master\",\n                    help=\"Resumption master secret (for TLS 1.3)\")\n\nargs = parser.parse_args()\n\n# By default, PFS is set\nif args.no_pfs:\n    psk_mode = \"psk_ke\"\nelse:\n    psk_mode = \"psk_dhe_ke\"\n\nv = _tls_version_options.get(args.version, None)\nif not v:\n    sys.exit(\"Unrecognized TLS version option.\")\n\n\n\nif args.ciphersuite:\n    ciphers = int(args.ciphersuite, 16)\n    if ciphers not in list(range(0x1301, 0x1306)):\n        ch = TLSClientHello(ciphers=ciphers)\n    else:\n        ch = TLS13ClientHello(ciphers=ciphers)\nelse:\n    ch = None\n\nt = TLSClientAutomaton(client_hello=ch,\n                       version=args.version,\n                       mycert=basedir+\"\/test\/tls\/pki\/cli_cert.pem\",\n                       mykey=basedir+\"\/test\/tls\/pki\/cli_key.pem\",\n                       psk=args.psk,\n                       psk_mode=psk_mode,\n                       resumption_master_secret=args.res_master,\n                       session_ticket_file_in=args.session_ticket_file_in,\n                       session_ticket_file_out=args.session_ticket_file_out,\n                      )\nt.run()\n\n","lang_cluster":"Python","length":72,"code_uid":"125d486ed34f4872b752510bee89b5b0"}
{"diff_hunk":"@@ -24,7 +24,11 @@ from databricks.koalas.dask.compatibility import string_types\n \n class Metadata(object):\n     \"\"\"\n-    Manages column names and index information\n+    Manages column names and index information.\n+\n+    :ivar _column_fields: list of the Spark field names to be seen as columns in Koalas DataFrame.\n+    :ivar _index_info: list of pair holding the Spark field names for indexes,\n+                       and the index name to be seen in Koalas DataFrame.\n     \"\"\"\n \n     def __init__(self, column_fields, index_info=None):","old_code":"#\n# Copyright (C) 2019 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\nA metadata to manage indexes.\n\"\"\"\nimport pandas as pd\n\nfrom databricks.koalas.dask.compatibility import string_types\n\n\nclass Metadata(object):\n    \"\"\"\n    Manages column names and index information\n    \"\"\"\n\n    def __init__(self, column_fields, index_info=None):\n        \"\"\" Create a new metadata to manage column fields and index fields and names.\n\n        :param column_fields: list of string\n                              Field names to appear as columns.\n        :param index_info: list of string pair\n                           Each pair holds the index field name which exists in Spark fields,\n                           and the index name.\n        \"\"\"\n        assert all(isinstance(col, string_types) for col in column_fields)\n        assert index_info is None \\\n            or all(isinstance(index_field, string_types)\n                   and (index_name is None or isinstance(index_name, string_types))\n                   for index_field, index_name in index_info)\n        self._column_fields = column_fields\n        self._index_info = index_info or []\n\n    @property\n    def column_fields(self):\n        return self._column_fields\n\n    @property\n    def index_info(self):\n        return self._index_info\n\n    @property\n    def index_fields(self):\n        return [index_field for index_field, _ in self._index_info]\n\n    @property\n    def index_names(self):\n        return [name for _, name in self._index_info]\n\n    @property\n    def all_fields(self):\n        index_fields = self.index_fields\n        return index_fields + [field for field in self._column_fields\n                               if field not in index_fields]\n\n    def copy(self, column_fields=None, index_info=None):\n        if column_fields is None:\n            column_fields = self._column_fields\n        if index_info is None:\n            index_info = self._index_info\n        return Metadata(column_fields=column_fields.copy(), index_info=index_info.copy())\n\n    @staticmethod\n    def from_pandas(pdf):\n        column_fields = [str(col) for col in pdf.columns]\n        index = pdf.index\n        if isinstance(index, pd.MultiIndex):\n            if index.names is None:\n                index_info = [('__index_level_{}__'.format(i), None)\n                              for i in range(len(index.levels))]\n            else:\n                index_info = [('__index_level_{}__'.format(i) if name is None else name, name)\n                              for i, name in enumerate(index.names)]\n        else:\n            index_info = [(index.name\n                          if index.name is not None else '__index_level_0__', index.name)]\n\n        return Metadata(column_fields=column_fields, index_info=index_info)\n","lang_cluster":"Python","length":91,"code_uid":"8b000a1b3af04635ab7318fb01ba4929"}
{"diff_hunk":"@@ -63,13 +63,13 @@ def test_no_cache():\n                    names=[\"A\", \"B\", \"C\"])\n     assert dt0.names == (\"A\", \"B\", \"C\")\n     assert dt0.ltypes == (dt.ltype.real, dt.ltype.str, dt.ltype.int)\n-    assert dt0.stypes == (dt.float64, dt.str32, dt.int8)\n+    assert dt0.stypes == (dt.float64, dt.str32, dt.int32)\n     assert dt0.colindex(\"B\") == 1\n     frame_integrity_check(dt0)\n     dt0.key = \"C\"\n     assert dt0.names == (\"C\", \"A\", \"B\")\n     assert dt0.ltypes == (dt.ltype.int, dt.ltype.real, dt.ltype.str)\n-    assert dt0.stypes == (dt.int8, dt.float64, dt.str32)\n+    assert dt0.stypes == (dt.int32, dt.float64, dt.str32)\n     assert dt0.colindex(\"B\") == 2\n     frame_integrity_check(dt0)\n ","old_code":"#!\/usr\/bin\/env python\n# -*- coding: utf-8 -*-\n#-------------------------------------------------------------------------------\n# Copyright 2018 H2O.ai\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the \"Software\"),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and\/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#-------------------------------------------------------------------------------\nimport datatable as dt\nimport pytest\nimport random\nfrom datatable.internal import frame_integrity_check\nfrom tests import isview\n\n\ndef test_keys_simple():\n    dt0 = dt.Frame([[\"Joe\", \"Mary\", \"Leslie\", \"Adam\", \"Alice\"],\n                    [1, 5, 15, 12, 8],\n                    [3.6, 9.78, 2.01, -4.23, 5.3819]],\n                   names=[\"name\", \"sex\", \"avg\"])\n    assert dt0.key == tuple()\n    dt0.key = \"name\"\n    frame_integrity_check(dt0)\n    assert dt0.key == (\"name\",)\n    assert dt0.shape == (5, 3)\n    assert dt0.names == (\"name\", \"sex\", \"avg\")\n    assert dt0.to_list() == [[\"Adam\", \"Alice\", \"Joe\", \"Leslie\", \"Mary\"],\n                             [12, 8, 1, 15, 5],\n                             [-4.23, 5.3819, 3.6, 2.01, 9.78]]\n    dt0.key = \"sex\"\n    frame_integrity_check(dt0)\n    assert not isview(dt0)\n    assert dt0.key == (\"sex\",)\n    assert dt0.shape == (5, 3)\n    assert dt0.names == (\"sex\", \"name\", \"avg\")\n    assert dt0.to_list() == [[1, 5, 8, 12, 15],\n                             [\"Joe\", \"Mary\", \"Alice\", \"Adam\", \"Leslie\"],\n                             [3.6, 9.78, 5.3819, -4.23, 2.01]]\n    dt0.key = None\n    assert dt0.key == tuple()\n\n\n\ndef test_no_cache():\n    # Check that assigning a key properly disposes of potentially cached\n    # types \/ names of the Frame\n    dt0 = dt.Frame([[1.1] * 4, list(\"ABCD\"), [3, 5, 2, 1]],\n                   names=[\"A\", \"B\", \"C\"])\n    assert dt0.names == (\"A\", \"B\", \"C\")\n    assert dt0.ltypes == (dt.ltype.real, dt.ltype.str, dt.ltype.int)\n    assert dt0.stypes == (dt.float64, dt.str32, dt.int8)\n    assert dt0.colindex(\"B\") == 1\n    frame_integrity_check(dt0)\n    dt0.key = \"C\"\n    assert dt0.names == (\"C\", \"A\", \"B\")\n    assert dt0.ltypes == (dt.ltype.int, dt.ltype.real, dt.ltype.str)\n    assert dt0.stypes == (dt.int8, dt.float64, dt.str32)\n    assert dt0.colindex(\"B\") == 2\n    frame_integrity_check(dt0)\n\n\n\ndef test_multi_key():\n    dt0 = dt.Frame(D=range(6), A=[3, 7, 5, 2, 2, 3], B=[1, 2, 2, 3, 4, 4])\n    dt0.key = [\"A\", \"B\"]\n    frame_integrity_check(dt0)\n    assert dt0.key == (\"A\", \"B\")\n    assert dt0.names == (\"A\", \"B\", \"D\")\n    assert dt0.to_list() == [[2, 2, 3, 3, 5, 7],\n                             [3, 4, 1, 4, 2, 2],\n                             [3, 4, 0, 5, 2, 1]]\n\n\n\ndef test_key_invalid1():\n    dt0 = dt.Frame(A=range(5), B=[3] * 5)\n    with pytest.raises(TypeError) as e:\n        dt0.key = 0\n    assert (\"Key should be a column name, or a list\/tuple of column names\"\n            in str(e.value))\n    with pytest.raises(TypeError) as e:\n        dt0.key = [\"A\", None]\n    assert (\"Key should be a list\/tuple of column names, instead element 1 \"\n            \"was a <class 'NoneType'>\" == str(e.value))\n\n\n\ndef test_key_invalid2():\n    dt0 = dt.Frame([[\"Joe\", \"Mary\", \"Leslie\", \"Adam\", \"Alice\"],\n                    [7, 9, 2, 2, 7],\n                    [3, 4, 5, 3, 4]],\n                   names=[\"name\", \"A\", \"B\"])\n    with pytest.raises(ValueError) as e:\n        dt0.key = \"A\"\n    assert (\"the values are not unique\" in str(e.value))\n\n    dt0.key = [\"A\", \"B\"]\n    assert dt0.key == (\"A\", \"B\")\n    assert dt0.names == (\"A\", \"B\", \"name\")\n    assert dt0.to_list() == [[2, 2, 7, 7, 9],\n                             [3, 5, 3, 4, 4],\n                             [\"Adam\", \"Leslie\", \"Joe\", \"Alice\", \"Mary\"]]\n\n    with pytest.raises(ValueError) as e:\n        dt0.key = \"B\"\n    assert (\"the values are not unique\" in str(e.value))\n    # Check that the columns don't get reordered during a bad key assignment\n    assert dt0.key == (\"A\", \"B\")\n    assert dt0.names == (\"A\", \"B\", \"name\")\n\n\n\ndef test_key_duplicate():\n    dt0 = dt.Frame(A=range(5))\n    with pytest.raises(ValueError) as e:\n        dt0.key = (\"A\", \"A\")\n    assert (\"Column `A` is specified multiple times within the key\" ==\n            str(e.value))\n\n\ndef test_set_empty_key():\n    dt0 = dt.Frame([range(5), [None] * 5], names=[\"A\", \"B\"])\n    dt0.key = []\n    assert dt0.key == tuple()\n    dt0.key = \"A\"\n    assert dt0.key == (\"A\",)\n    dt0.key = []\n    frame_integrity_check(dt0)\n    assert dt0.key == tuple()\n    assert dt0.names == (\"A\", \"B\")\n\n\ndef test_key_save(tempfile):\n    dt0 = dt.Frame(D=range(6), A=[3, 7, 5, 2, 2, 3], B=[1, 2, 2, 3, 4, 4])\n    dt0.key = [\"A\", \"B\"]\n    frame_integrity_check(dt0)\n    dt0.to_jay(tempfile)\n    dt1 = dt.open(tempfile)\n    assert dt1.key == (\"A\", \"B\")\n    frame_integrity_check(dt1)\n\n\ndef test_key_after_group():\n    n = 1000\n    DT = dt.Frame(A=[random.choice(\"abcd\") for _ in range(n)])\n    tmp = DT[:, dt.count(), dt.by(0)]\n    frame_integrity_check(tmp)\n    tmp.key = \"A\"\n    assert tmp.to_list()[0] == [\"a\", \"b\", \"c\", \"d\"]\n    assert sum(tmp.to_list()[1]) == n\n\n","lang_cluster":"Python","length":166,"code_uid":"d121133a74ec410ca8698e32809dc5cb"}
{"diff_hunk":"@@ -117,6 +117,7 @@ class MaskTestMixin(object):\n             segm_result = [[] for _ in range(self.mask_head.num_classes - 1)]\n         else:\n             aug_masks = []\n+            mask_roi_extractor = self.mask_roi_extractor\n             for x, img_meta in zip(feats, img_metas):\n                 img_shape = img_meta[0]['img_shape']\n                 scale_factor = img_meta[0]['scale_factor']","old_code":"from mmdet.core import (bbox2roi, bbox_mapping, merge_aug_proposals,\n                        merge_aug_bboxes, merge_aug_masks, multiclass_nms)\n\n\nclass RPNTestMixin(object):\n\n    def simple_test_rpn(self, x, img_meta, rpn_test_cfg):\n        rpn_outs = self.rpn_head(x)\n        proposal_inputs = rpn_outs + (img_meta, rpn_test_cfg)\n        proposal_list = self.rpn_head.get_proposals(*proposal_inputs)\n        return proposal_list\n\n    def aug_test_rpn(self, feats, img_metas, rpn_test_cfg):\n        imgs_per_gpu = len(img_metas[0])\n        aug_proposals = [[] for _ in range(imgs_per_gpu)]\n        for x, img_meta in zip(feats, img_metas):\n            proposal_list = self.simple_test_rpn(x, img_meta, rpn_test_cfg)\n            for i, proposals in enumerate(proposal_list):\n                aug_proposals[i].append(proposals)\n        # after merging, proposals will be rescaled to the original image size\n        merged_proposals = [\n            merge_aug_proposals(proposals, img_meta, rpn_test_cfg)\n            for proposals, img_meta in zip(aug_proposals, img_metas)\n        ]\n        return merged_proposals\n\n\nclass BBoxTestMixin(object):\n\n    def simple_test_bboxes(self,\n                           x,\n                           img_meta,\n                           proposals,\n                           rcnn_test_cfg,\n                           rescale=False):\n        \"\"\"Test only det bboxes without augmentation.\"\"\"\n        rois = bbox2roi(proposals)\n        roi_feats = self.bbox_roi_extractor(\n            x[:len(self.bbox_roi_extractor.featmap_strides)], rois)\n        cls_score, bbox_pred = self.bbox_head(roi_feats)\n        img_shape = img_meta[0]['img_shape']\n        scale_factor = img_meta[0]['scale_factor']\n        det_bboxes, det_labels = self.bbox_head.get_det_bboxes(\n            rois,\n            cls_score,\n            bbox_pred,\n            img_shape,\n            scale_factor,\n            rescale=rescale,\n            nms_cfg=rcnn_test_cfg)\n        return det_bboxes, det_labels\n\n    def aug_test_bboxes(self, feats, img_metas, proposal_list, rcnn_test_cfg):\n        aug_bboxes = []\n        aug_scores = []\n        for x, img_meta in zip(feats, img_metas):\n            # only one image in the batch\n            img_shape = img_meta[0]['img_shape']\n            scale_factor = img_meta[0]['scale_factor']\n            flip = img_meta[0]['flip']\n            # TODO more flexible\n            proposals = bbox_mapping(proposal_list[0][:, :4], img_shape,\n                                     scale_factor, flip)\n            rois = bbox2roi([proposals])\n            # recompute feature maps to save GPU memory\n            roi_feats = self.bbox_roi_extractor(\n                x[:len(self.bbox_roi_extractor.featmap_strides)], rois)\n            cls_score, bbox_pred = self.bbox_head(roi_feats)\n            bboxes, scores = self.bbox_head.get_det_bboxes(\n                rois,\n                cls_score,\n                bbox_pred,\n                img_shape,\n                scale_factor,\n                rescale=False,\n                nms_cfg=None)\n            aug_bboxes.append(bboxes)\n            aug_scores.append(scores)\n        # after merging, bboxes will be rescaled to the original image size\n        merged_bboxes, merged_scores = merge_aug_bboxes(\n            aug_bboxes, aug_scores, img_metas, self.test_cfg.rcnn)\n        det_bboxes, det_labels = multiclass_nms(\n            merged_bboxes, merged_scores, self.test_cfg.rcnn.score_thr,\n            self.test_cfg.rcnn.nms_thr, self.test_cfg.rcnn.max_per_img)\n        return det_bboxes, det_labels\n\n\nclass MaskTestMixin(object):\n\n    def simple_test_mask(self,\n                         x,\n                         img_meta,\n                         det_bboxes,\n                         det_labels,\n                         rescale=False):\n        # image shape of the first image in the batch (only one)\n        ori_shape = img_meta[0]['ori_shape']\n        scale_factor = img_meta[0]['scale_factor']\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes - 1)]\n        else:\n            # if det_bboxes is rescaled to the original image size, we need to\n            # rescale it back to the testing scale to obtain RoIs.\n            _bboxes = (det_bboxes[:, :4] * scale_factor\n                       if rescale else det_bboxes)\n            mask_rois = bbox2roi([_bboxes])\n            mask_feats = self.mask_roi_extractor(\n                x[:len(self.mask_roi_extractor.featmap_strides)], mask_rois)\n            mask_pred = self.mask_head(mask_feats)\n            segm_result = self.mask_head.get_seg_masks(\n                mask_pred, _bboxes, det_labels, self.test_cfg.rcnn, ori_shape,\n                scale_factor, rescale)\n        return segm_result\n\n    def aug_test_mask(self, feats, img_metas, det_bboxes, det_labels):\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes - 1)]\n        else:\n            aug_masks = []\n            for x, img_meta in zip(feats, img_metas):\n                img_shape = img_meta[0]['img_shape']\n                scale_factor = img_meta[0]['scale_factor']\n                flip = img_meta[0]['flip']\n                _bboxes = bbox_mapping(det_bboxes[:, :4], img_shape,\n                                       scale_factor, flip)\n                mask_rois = bbox2roi([_bboxes])\n                mask_feats = self.mask_roi_extractor(\n                    x[:len(self.mask_roi_extractor.featmap_strides)],\n                    mask_rois)\n                mask_pred = self.mask_head(mask_feats)\n                # convert to numpy array to save memory\n                aug_masks.append(mask_pred.sigmoid().cpu().numpy())\n            merged_masks = merge_aug_masks(aug_masks, img_metas,\n                                           self.test_cfg.rcnn)\n\n            ori_shape = img_metas[0][0]['ori_shape']\n            segm_result = self.mask_head.get_seg_masks(\n                merged_masks,\n                det_bboxes,\n                det_labels,\n                self.test_cfg.rcnn,\n                ori_shape,\n                scale_factor=1.0,\n                rescale=False)\n        return segm_result\n","lang_cluster":"Python","length":145,"code_uid":"86e7df8b70064b1fbdec34c02a4114da"}
{"diff_hunk":"@@ -83,6 +83,12 @@ class GenericBaseModel(CloudFormationModel):\n         \"\"\"Return template configurations used to create the final API requests (implemented by subclasses).\"\"\"\n         pass\n \n+    # TODO: rework to normal instance method when resources aren't mutated in different place anymore\n+    @staticmethod\n+    def add_defaults(resource, stack_name: str):\n+        \"\"\"Set any defaults required, including auto-generating names. Must be called before deploying the resource\"\"\"\n+        pass\n+\n     # ----------------------\n     # GENERIC BASE METHODS\n     # ----------------------","old_code":"import logging\n\n# TODO: remove\nfrom moto.cloudformation.exceptions import UnformattedGetAttTemplateException\nfrom moto.core.models import CloudFormationModel\n\nfrom localstack.utils.aws import aws_stack\nfrom localstack.utils.common import camel_to_snake_case\n\nLOG = logging.getLogger(__name__)\n\n# dict key used to store the deployment state of a resource\nKEY_RESOURCE_STATE = \"_state_\"\n\n# ref attribute definitions\nREF_ATTRS = [\"PhysicalResourceId\", \"Ref\"]\nREF_ID_ATTRS = REF_ATTRS + [\"Id\"]\nREF_ARN_ATTRS = [\"Ref\", \"Arn\"]\n\n\nclass DependencyNotYetSatisfied(Exception):\n    \"\"\"Exception indicating that a resource dependency is not (yet) deployed\/available.\"\"\"\n\n    def __init__(self, resource_ids, message=None):\n        message = message or \"Unresolved dependencies: %s\" % resource_ids\n        super(DependencyNotYetSatisfied, self).__init__(message)\n        resource_ids = resource_ids if isinstance(resource_ids, list) else [resource_ids]\n        self.resource_ids = resource_ids\n\n\nclass GenericBaseModel(CloudFormationModel):\n    \"\"\"Abstract base class representing a resource model class in LocalStack.\n    This class keeps references to a combination of (1) the CF resource\n    properties (as defined in the template), and (2) the current deployment\n    state of a resource.\n\n    Concrete subclasses will implement convenience methods to manage resources,\n    e.g., fetching the latest deployment state, getting the resource name, etc.\n    \"\"\"\n\n    def __init__(self, resource_json, region_name=None, **params):\n        self.region_name = region_name or aws_stack.get_region()\n        self.resource_json = resource_json\n        self.resource_type = resource_json[\"Type\"]\n        # Properties, as defined in the resource template\n        self.properties = resource_json[\"Properties\"] = resource_json.get(\"Properties\") or {}\n        # State, as determined from the deployed resource; use a special dict key here to keep\n        # track of state changes within resource_json (this way we encapsulate all state details\n        # in `resource_json` and the changes will survive creation of multiple instances of this class)\n        self.state = resource_json[KEY_RESOURCE_STATE] = resource_json.get(KEY_RESOURCE_STATE) or {}\n\n    # ----------------------\n    # ABSTRACT BASE METHODS\n    # ----------------------\n\n    def get_resource_name(self):\n        \"\"\"Return the name of this resource, based on its properties (to be overwritten by subclasses)\"\"\"\n        return None\n\n    def get_physical_resource_id(self, attribute=None, **kwargs):\n        \"\"\"Determine the physical resource ID (Ref) of this resource (to be overwritten by subclasses)\"\"\"\n        return None\n\n    # TODO: change the signature to pass in a Stack instance (instead of stack_name and resources)\n    def fetch_state(self, stack_name, resources):\n        \"\"\"Fetch the latest deployment state of this resource, or return None if not currently deployed.\"\"\"\n        return None\n\n    # TODO: change the signature to pass in a Stack instance (instead of stack_name and resources)\n    def update_resource(self, new_resource, stack_name, resources):\n        \"\"\"Update the deployment of this resource, using the updated properties (implemented by subclasses).\"\"\"\n        # TODO: evaluate if we can add a generic implementation here, using \"update\" parameters from\n        # get_deploy_templates() responses, and based on checking whether resource attributes have changed\n        pass\n\n    @classmethod\n    def cloudformation_type(cls):\n        \"\"\"Return the CloudFormation resource type name, e.g., \"AWS::S3::Bucket\" (implemented by subclasses).\"\"\"\n        return super(GenericBaseModel, cls).cloudformation_type()\n\n    @staticmethod\n    def get_deploy_templates():\n        \"\"\"Return template configurations used to create the final API requests (implemented by subclasses).\"\"\"\n        pass\n\n    # ----------------------\n    # GENERIC BASE METHODS\n    # ----------------------\n\n    def get_cfn_attribute(self, attribute_name):\n        \"\"\"Retrieve the given CF attribute for this resource (inherited from moto's CloudFormationModel)\"\"\"\n        if attribute_name in REF_ARN_ATTRS and hasattr(self, \"arn\"):\n            return self.arn\n        if attribute_name in REF_ATTRS:\n            result = self.get_physical_resource_id(attribute=attribute_name)\n            if result:\n                return result\n        props = self.props\n        if attribute_name in props:\n            return props.get(attribute_name)\n\n        raise UnformattedGetAttTemplateException()\n\n    # ---------------------\n    # GENERIC UTIL METHODS\n    # ---------------------\n\n    def fetch_and_update_state(self, *args, **kwargs):\n        from localstack.utils.cloudformation import template_deployer\n\n        try:\n            state = self.fetch_state(*args, **kwargs)\n            self.update_state(state)\n            return state\n        except Exception as e:\n            if not template_deployer.check_not_found_exception(\n                e, self.resource_type, self.properties\n            ):\n                LOG.debug(\"Unable to fetch state for resource %s: %s\" % (self, e))\n\n    def fetch_state_if_missing(self, *args, **kwargs):\n        if not self.state:\n            self.fetch_and_update_state(*args, **kwargs)\n        return self.state\n\n    def set_resource_state(self, state):\n        \"\"\"Set the deployment state of this resource.\"\"\"\n        self.state = state or {}\n\n    def update_state(self, details):\n        \"\"\"Update the deployment state of this resource (existing attributes will be overwritten).\"\"\"\n        details = details or {}\n        self.state.update(details)\n        return self.props\n\n    @property\n    def physical_resource_id(self):\n        \"\"\"Return the (cached) physical resource ID.\"\"\"\n        return self.resource_json.get(\"PhysicalResourceId\")\n\n    @property\n    def logical_resource_id(self):\n        \"\"\"Return the logical resource ID.\"\"\"\n        return self.resource_json.get(\"LogicalResourceId\")\n\n    @property\n    def props(self):\n        \"\"\"Return a copy of (1) the resource properties (from the template), combined with\n        (2) the current deployment state properties of the resource.\"\"\"\n        result = dict(self.properties)\n        result.update(self.state or {})\n        return result\n\n    @property\n    def resource_id(self):\n        \"\"\"Return the logical resource ID of this resource (i.e., the ref. name within the stack's resources).\"\"\"\n        return self.resource_json[\"LogicalResourceId\"]\n\n    @classmethod\n    def update_from_cloudformation_json(\n        cls, original_resource, new_resource_name, cloudformation_json, region_name\n    ):\n        props = cloudformation_json.get(\"Properties\", {})\n        for key, val in props.items():\n            snake_key = camel_to_snake_case(key)\n            lower_key = key.lower()\n            for candidate in [key, lower_key, snake_key]:\n                if hasattr(original_resource, candidate) or candidate == snake_key:\n                    setattr(original_resource, candidate, val)\n                    break\n        return original_resource\n\n    @classmethod\n    def create_from_cloudformation_json(cls, resource_name, resource_json, region_name):\n        return cls(\n            resource_name=resource_name,\n            resource_json=resource_json,\n            region_name=region_name,\n        )\n\n    @classmethod\n    def resolve_refs_recursively(cls, stack_name, value, resources):\n        # TODO: restructure code to avoid circular import here\n        from localstack.utils.cloudformation.template_deployer import resolve_refs_recursively\n\n        return resolve_refs_recursively(stack_name, value, resources)\n","lang_cluster":"Python","length":186,"code_uid":"4d572a01734a4c9f993750d03bccd9aa"}
{"diff_hunk":"@@ -41,6 +41,18 @@ class WorkflowValidator:\n         \"\"\"\n         self.config = config\n         self.physical_key = physical_key\n+        self.loaded_schemas_by_id = {}\n+        self.loaded_schemas = {}\n+\n+    @staticmethod\n+    def get_config_data_version_str(data: dict) -> str:\n+        version_obj = data['version']\n+        assert isinstance(version_obj, (str, dict))\n+        return version_obj if isinstance(version_obj, str) else version_obj['base']\n+\n+    @classmethod\n+    def is_supported_config_data_version(cls, version: ConfigDataVersion):\n+        return cls.CONFIG_DATA_VERSION >= version\n \n     @classmethod\n     def load(cls, pk: util.PhysicalKey):","old_code":"import functools\nimport json\n\nimport botocore.exceptions\nimport jsonschema\nimport pkg_resources\nimport yaml\n\nfrom quilt3.data_transfer import get_bytes_and_effective_pk\n\nfrom .. import util\nfrom ..backends import PackageRegistry\n\n\n@functools.lru_cache(maxsize=None)\ndef _get_conf_validator():\n    schema = json.loads(pkg_resources.resource_string(__name__, 'config-1.schema.json'))\n    return jsonschema.Draft7Validator(schema).validate\n\n\nSUPPORTED_META_SCHEMAS = {\n    'http:\/\/json-schema.org\/draft-07\/schema#': jsonschema.Draft7Validator,\n}\n\n\ndef _schema_load_object_hook(o):\n    if '$ref' in o:\n        raise util.QuiltException(\"Currently we don't support $ref in schema.\")\n    return o\n\n\n_load_schema_json = json.JSONDecoder(object_hook=_schema_load_object_hook).decode\n\n\nclass WorkflowValidator:\n    def __init__(self, config: dict, physical_key: util.PhysicalKey):\n        \"\"\"\n        Args:\n            config: validated workflow config or `None` if there is no config\n            physical_key: from where config was loaded\n        \"\"\"\n        self.config = config\n        self.physical_key = physical_key\n\n    @classmethod\n    def load(cls, pk: util.PhysicalKey):\n        data = None\n        try:\n            data, pk = get_bytes_and_effective_pk(pk)\n        except FileNotFoundError:\n            pass\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] != 'NoSuchKey':\n                raise util.QuiltException(f\"Couldn't load workflows config. {e}.\")\n        if data is None:\n            return\n\n        try:\n            # TODO: raise if objects contain duplicate properties\n            config = yaml.safe_load(data.decode())\n        except yaml.YAMLError as e:\n            raise util.QuiltException(\"Couldn't parse workflows config as YAML.\") from e\n        conf_validator = _get_conf_validator()\n        try:\n            conf_validator(config)\n        except jsonschema.ValidationError as e:\n            raise util.QuiltException(f'Workflows config failed validation: {e.message}.') from e\n\n        return cls(config, pk)\n\n    def validate(self, workflow, meta, message):\n        if workflow is ...:\n            workflow = self.config.get('default_workflow')\n\n        result = {\n            'id': workflow,\n            'config': str(self.physical_key),\n        }\n        if workflow is None:\n            if self.config.get('is_workflow_required', True):\n                raise util.QuiltException('Workflow required, but none specified.')\n            return result\n\n        workflows_data = self.config['workflows']\n        if workflow not in workflows_data:\n            raise util.QuiltException(f'There is no {workflow!r} workflow in config.')\n        workflow_data = workflows_data[workflow]\n        metadata_schema_id = workflow_data.get('metadata_schema')\n        if metadata_schema_id:\n            schemas = self.config.get('schemas', {})\n            if metadata_schema_id not in schemas:\n                raise util.QuiltException(f'There is no {metadata_schema_id!r} in schemas.')\n            schema_url = schemas[metadata_schema_id]['url']\n            try:\n                schema_pk = util.PhysicalKey.from_url(schema_url)\n            except util.URLParseError as e:\n                raise util.QuiltException(f\"Couldn't parse URL {schema_url!r}. {e}.\")\n            if schema_pk.is_local() and not self.physical_key.is_local():\n                raise util.QuiltException(f\"Local schema {str(schema_pk)!r} can't be used on the remote registry.\")\n\n            handled_exception = (OSError if schema_pk.is_local() else botocore.exceptions.ClientError)\n            try:\n                schema_data, schema_pk_to_store = get_bytes_and_effective_pk(schema_pk)\n            except handled_exception as e:\n                raise util.QuiltException(f\"Couldn't load schema at {schema_pk}. {e}.\")\n            try:\n                schema = _load_schema_json(schema_data.decode())\n            except json.JSONDecodeError as e:\n                raise util.QuiltException(f\"Couldn't parse {schema_pk} as JSON. {e}.\")\n\n            validator_cls = jsonschema.Draft7Validator\n            if isinstance(schema, dict) and '$schema' in schema:\n                meta_schema = schema['$schema']\n                if not isinstance(meta_schema, str):\n                    raise util.QuiltException('$schema must be a string.')\n                validator_cls = SUPPORTED_META_SCHEMAS.get(meta_schema)\n                if validator_cls is None:\n                    raise util.QuiltException(f\"Unsupported meta-schema: {meta_schema}.\")\n            try:\n                jsonschema.validate(meta, schema, cls=validator_cls)\n            except jsonschema.ValidationError as e:\n                raise util.QuiltException(f\"Metadata failed validation: {e.message}.\")\n            result['schemas'] = {metadata_schema_id: str(schema_pk_to_store)}\n        if workflow_data.get('is_message_required', False) and not message:\n            raise util.QuiltException('Commit message is required by workflow, but none was provided.')\n\n        return result\n\n\ndef validate(*, registry: PackageRegistry, workflow, meta, message):\n    # workflow is ... => no workflow provided by user;\n    # workflow is None => don't use any workflow.\n    if not (workflow in (None, ...) or isinstance(workflow, str)):\n        raise TypeError\n\n    workflow_validator = registry.get_workflow_validator()\n    if workflow_validator is None:\n        if workflow is ...:\n            return\n        raise util.QuiltException(f'{workflow!r} workflow is specified, but no workflows config exist.')\n\n    return workflow_validator.validate(workflow, meta, message)\n","lang_cluster":"Python","length":142,"code_uid":"81955205d869419b98f9523e17356c0b"}
{"diff_hunk":"@@ -22,12 +22,7 @@\n from invenio.ext.assets import Bundle\n \n js = Bundle(\n-    \"vendors\/jquery\/dist\/jquery.js\",\n-    \"vendors\/bootstrap\/dist\/js\/bootstrap.js\",\n     \"vendors\/jquery-ui\/jquery-ui.js\",\n-    \"vendors\/jquery.jeditable\/index.js\",\n-    \"vendors\/jquery.hotkeys\/jquery.hotkeys.js\",\n-    \"vendors\/json2\/json2.js\",\n     \"js\/editor\/refextract.js\",\n     \"js\/editor\/display.js\",\n     \"js\/editor\/engine.js\",","old_code":"# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2015 CERN.\n#\n# Invenio is free software; you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"Editor bundles.\"\"\"\n\nfrom invenio.ext.assets import Bundle\n\njs = Bundle(\n    \"vendors\/jquery\/dist\/jquery.js\",\n    \"vendors\/bootstrap\/dist\/js\/bootstrap.js\",\n    \"vendors\/jquery-ui\/jquery-ui.js\",\n    \"vendors\/jquery.jeditable\/index.js\",\n    \"vendors\/jquery.hotkeys\/jquery.hotkeys.js\",\n    \"vendors\/json2\/json2.js\",\n    \"js\/editor\/refextract.js\",\n    \"js\/editor\/display.js\",\n    \"js\/editor\/engine.js\",\n    \"js\/editor\/keys.js\",\n    \"js\/editor\/menu.js\",\n    \"js\/editor\/holdingpen.js\",\n    \"js\/editor\/marcxml.js\",\n    \"js\/editor\/clipboard.js\",\n    output=\"editor.js\",\n    weight=51,\n)\n\nstyles = Bundle(\n    \"css\/editor\/base.css\",\n    \"vendors\/jquery-ui\/themes\/redmond\/jquery-ui.css\",\n    output=\"editor.css\",\n    filters=\"cleancss\",\n    weight=51\n)\n","lang_cluster":"Python","length":49,"code_uid":"367c087fc22041f59db3ef49bbd35ab9"}
{"diff_hunk":"@@ -60,24 +60,23 @@ def test_generate_thumbnail(\n         body=input_file.read_bytes(),\n         status=200\n     )\n-\n     # Create the lambda request event\n-    event = _make_event({\"url\": url, \"size\": thumb_size})\n-\n+    event = _make_event({\"url\": url, **params})\n     # Get the response\n     response = lambda_handler(event, None)\n-\n     # Assert the request was handled with no errors\n     assert response[\"statusCode\"] == 200\n-\n     # Parse the body \/ the returned thumbnail\n     body = json.loads(read_body(response))\n-\n-    # Assert basic metadata was fill properly\n-    assert body[\"info\"][\"original_size\"] == expected_original_size\n+    # Assert basic metadata was filled properly\n+    if expected_original_size:  # PDFs don't have an expected size\n+        assert body[\"info\"][\"original_size\"] == expected_original_size\n     assert body[\"info\"][\"thumbnail_size\"] == expected_thumb_size\n-\n     # Assert the produced image is the same as the expected\n-    actual = AICSImage(base64.b64decode(body['thumbnail'])).reader.data\n-    expected = AICSImage(data_dir \/ expected_thumb).reader.data\n+    if params.get('input') == 'pdf':\n+        actual = Image.open(BytesIO(base64.b64decode(body['thumbnail'])))\n+        expected = Image.open(data_dir \/ expected_thumb)\n+    else:\n+        actual = AICSImage(base64.b64decode(body['thumbnail'])).reader.data\n+        expected = AICSImage(data_dir \/ expected_thumb).reader.data\n     assert np.array_equal(actual, expected)","old_code":"import base64\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\nimport responses\nfrom aicsimageio import AICSImage\n\nfrom t4_lambda_shared.utils import read_body\n\nfrom ..index import lambda_handler\n\n\n@pytest.fixture\ndef data_dir():\n    return Path(__file__).parent \/ 'data'\n\n\ndef _make_event(query, headers=None):\n    return {\n        'httpMethod': 'POST',\n        'path': '\/foo',\n        'pathParameters': {},\n        'queryStringParameters': query or None,\n        'headers': headers or None,\n        'body': None,\n        'isBase64Encoded': False,\n    }\n\n\n@responses.activate\n@pytest.mark.parametrize(\"input_file, thumb_size, expected_thumb, expected_original_size, expected_thumb_size\", [\n    (\"penguin.jpg\", \"w256h256\", \"penguin-256.jpg\", [1526, 1290, 3], [217, 256]),\n    (\"cell.tiff\", \"w640h480\", \"cell-480.png\", [15, 1, 158, 100], [514, 480]),\n    (\"cell.png\", \"w64h64\", \"cell-64.png\", [168, 104, 3], [39, 64]),\n    (\"sat_greyscale.tiff\", \"w640h480\", \"sat_greyscale-480.png\", [512, 512], [480, 480]),\n    (\"generated.ome.tiff\", \"w256h256\", \"generated-256.png\", [6, 36, 76, 68], [224, 167]),\n    (\"sat_rgb.tiff\", \"w256h256\", \"sat_rgb-256.png\", [256, 256, 4], [256, 256]),\n    (\"single_cell.ome.tiff\", \"w256h256\", \"single_cell.png\", [6, 40, 152, 126], [256, 205]),\n    # Test for statusCode error\n    pytest.param(\"cell.png\", \"w1h1\", None, None, None, marks=pytest.mark.raises(exception=AssertionError))\n])\ndef test_generate_thumbnail(\n    data_dir,\n    input_file,\n    thumb_size,\n    expected_thumb,\n    expected_original_size,\n    expected_thumb_size\n):\n    # Resolve the input file path\n    input_file = data_dir \/ input_file\n\n    # Mock the request\n    url = f\"https:\/\/example.com\/{input_file}\"\n    responses.add(\n        responses.GET,\n        url=url,\n        body=input_file.read_bytes(),\n        status=200\n    )\n\n    # Create the lambda request event\n    event = _make_event({\"url\": url, \"size\": thumb_size})\n\n    # Get the response\n    response = lambda_handler(event, None)\n\n    # Assert the request was handled with no errors\n    assert response[\"statusCode\"] == 200\n\n    # Parse the body \/ the returned thumbnail\n    body = json.loads(read_body(response))\n\n    # Assert basic metadata was fill properly\n    assert body[\"info\"][\"original_size\"] == expected_original_size\n    assert body[\"info\"][\"thumbnail_size\"] == expected_thumb_size\n\n    # Assert the produced image is the same as the expected\n    actual = AICSImage(base64.b64decode(body['thumbnail'])).reader.data\n    expected = AICSImage(data_dir \/ expected_thumb).reader.data\n    assert np.array_equal(actual, expected)\n","lang_cluster":"Python","length":83,"code_uid":"2682a827c9cd41ff82b7647dc8a3cc25"}
{"diff_hunk":"@@ -95,17 +95,29 @@ class Chooser(urwid.WidgetWrap, layoutwidget.LayoutWidget):\n         self.master = master\n         self.choices = choices\n         self.callback = callback\n+        shortcutwidth = 3\n         choicewidth = max([len(i) for i in choices])\n-        self.width = max(choicewidth, len(title)) + 5\n+        self.width = max(shortcutwidth+choicewidth, len(title)) + 5\n+        self.possible_shortcuts = \"123456789abcdefghijklmnopqrstuvwxyz\"\n+        self.shortcuts = self.get_shortcuts(choices)\n+\n+        shortcuts_walker = urwid.SimpleListWalker([\n+            urwid.Text([(\"key\", s), \")\"], align=\"left\") for s in self.shortcuts\n+        ])\n+        shortcuts_listbox = urwid.ListBox(shortcuts_walker)\n+        shortcuts_listbox._selectable = False  # We don't want to focus on it\n         self.walker = ChooserListWalker(choices, current)\n+        content = urwid.Columns([(shortcutwidth, shortcuts_listbox),\n+                                 (choicewidth+3, urwid.ListBox(self.walker))],\n+                                focus_column=1)\n         super().__init__(\n             urwid.AttrWrap(\n                 urwid.LineBox(\n                     urwid.BoxAdapter(\n-                        urwid.ListBox(self.walker),\n+                        content,\n                         len(choices)\n                     ),\n-                    title= title\n+                    title=title\n                 ),\n                 \"background\"\n             )","old_code":"import math\n\nimport urwid\n\nfrom mitmproxy.tools.console import signals\nfrom mitmproxy.tools.console import grideditor\nfrom mitmproxy.tools.console import layoutwidget\nfrom mitmproxy.tools.console import keymap\n\n\nclass SimpleOverlay(urwid.Overlay, layoutwidget.LayoutWidget):\n\n    def __init__(self, master, widget, parent, width, valign=\"middle\"):\n        self.widget = widget\n        self.master = master\n        super().__init__(\n            widget,\n            parent,\n            align=\"center\",\n            width=width,\n            valign=valign,\n            height=\"pack\"\n        )\n\n    @property\n    def keyctx(self):\n        return getattr(self.widget, \"keyctx\")\n\n    def key_responder(self):\n        return self.widget.key_responder()\n\n    def focus_changed(self):\n        return self.widget.focus_changed()\n\n    def view_changed(self):\n        return self.widget.view_changed()\n\n    def layout_popping(self):\n        return self.widget.layout_popping()\n\n\nclass Choice(urwid.WidgetWrap):\n    def __init__(self, txt, focus, current):\n        if current:\n            s = \"option_active_selected\" if focus else \"option_active\"\n        else:\n            s = \"option_selected\" if focus else \"text\"\n        return super().__init__(\n            urwid.AttrWrap(\n                urwid.Padding(urwid.Text(txt)),\n                s,\n            )\n        )\n\n    def selectable(self):\n        return True\n\n    def keypress(self, size, key):\n        return key\n\n\nclass ChooserListWalker(urwid.ListWalker):\n    def __init__(self, choices, current):\n        self.index = 0\n        self.choices = choices\n        self.current = current\n\n    def _get(self, idx, focus):\n        c = self.choices[idx]\n        return Choice(c, focus, c == self.current)\n\n    def set_focus(self, index):\n        self.index = index\n\n    def get_focus(self):\n        return self._get(self.index, True), self.index\n\n    def get_next(self, pos):\n        if pos >= len(self.choices) - 1:\n            return None, None\n        pos = pos + 1\n        return self._get(pos, False), pos\n\n    def get_prev(self, pos):\n        pos = pos - 1\n        if pos < 0:\n            return None, None\n        return self._get(pos, False), pos\n\n\nclass Chooser(urwid.WidgetWrap, layoutwidget.LayoutWidget):\n    keyctx = \"chooser\"\n\n    def __init__(self, master, title, choices, current, callback):\n        self.master = master\n        self.choices = choices\n        self.callback = callback\n        choicewidth = max([len(i) for i in choices])\n        self.width = max(choicewidth, len(title)) + 5\n        self.walker = ChooserListWalker(choices, current)\n        super().__init__(\n            urwid.AttrWrap(\n                urwid.LineBox(\n                    urwid.BoxAdapter(\n                        urwid.ListBox(self.walker),\n                        len(choices)\n                    ),\n                    title= title\n                ),\n                \"background\"\n            )\n        )\n\n    def selectable(self):\n        return True\n\n    def keypress(self, size, key):\n        key = self.master.keymap.handle_only(\"chooser\", key)\n        if key == \"m_select\":\n            self.callback(self.choices[self.walker.index])\n            signals.pop_view_state.send(self)\n            return\n        elif key == \"esc\":\n            signals.pop_view_state.send(self)\n            return\n\n        binding = self.master.keymap.get(\"global\", key)\n        # This is extremely awkward. We need a better way to match nav keys only.\n        if binding and binding.command.startswith(\"console.nav\"):\n            self.master.keymap.handle(\"global\", key)\n        elif key in keymap.navkeys:\n            return super().keypress(size, key)\n\n\nclass OptionsOverlay(urwid.WidgetWrap, layoutwidget.LayoutWidget):\n    keyctx = \"grideditor\"\n\n    def __init__(self, master, name, vals, vspace):\n        \"\"\"\n            vspace: how much vertical space to keep clear\n        \"\"\"\n        cols, rows = master.ui.get_cols_rows()\n        self.ge = grideditor.OptionsEditor(master, name, vals)\n        super().__init__(\n            urwid.AttrWrap(\n                urwid.LineBox(\n                    urwid.BoxAdapter(self.ge, rows - vspace),\n                    title=name\n                ),\n                \"background\"\n            )\n        )\n        self.width = math.ceil(cols * 0.8)\n\n    def key_responder(self):\n        return self.ge.key_responder()\n\n    def layout_popping(self):\n        return self.ge.layout_popping()\n\n\nclass DataViewerOverlay(urwid.WidgetWrap, layoutwidget.LayoutWidget):\n    keyctx = \"grideditor\"\n\n    def __init__(self, master, vals):\n        \"\"\"\n            vspace: how much vertical space to keep clear\n        \"\"\"\n        cols, rows = master.ui.get_cols_rows()\n        self.ge = grideditor.DataViewer(master, vals)\n        super().__init__(\n            urwid.AttrWrap(\n                urwid.LineBox(\n                    urwid.BoxAdapter(self.ge, rows - 5),\n                    title=\"Data viewer\"\n                ),\n                \"background\"\n            )\n        )\n        self.width = math.ceil(cols * 0.8)\n\n    def key_responder(self):\n        return self.ge.key_responder()\n\n    def layout_popping(self):\n        return self.ge.layout_popping()\n","lang_cluster":"Python","length":186,"code_uid":"99f8ea633ff74a9c9395d36d43d9c426"}
{"diff_hunk":"@@ -56,3 +56,19 @@ class TestModel:\n         model_roles = (cloudsql_connection.execute(query).fetchone())\n         assert model_roles\n         assert model_roles[0] > 0\n+\n+\n+    @pytest.mark.client\n+    @pytest.mark.e2e\n+    @pytest.mark.model\n+    def test_model_use(self, forseti_cli: ForsetiCli, forseti_model_readonly):\n+        # Arrange\n+        model_name, handle, _ = forseti_model_readonly\n+\n+        # Act\n+        forseti_cli.model_use(model_name)\n+        result = forseti_cli.config_show()\n+\n+        # Assert\n+        assert handle\n+        assert re.search(fr'{handle}', str(result.stdout))","old_code":"# Copyright 2020 The Forseti Security Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Model end-to-end tests\"\"\"\n\nimport pytest\nimport re\nfrom endtoend_tests.helpers.forseti_cli import ForsetiCli\nfrom sqlalchemy.sql import text\n\n\nclass TestModel:\n    \"\"\"Model tests\n\n    Execute the basic model functionality such as: create, get, use, etc.\n    \"\"\"\n\n    @pytest.mark.client\n    @pytest.mark.e2e\n    @pytest.mark.model\n    def test_model_use(self, forseti_cli: ForsetiCli, forseti_model_readonly):\n        # Arrange\n        model_name, handle, _ = forseti_model_readonly\n\n        # Act\n        forseti_cli.model_use(model_name)\n        result = forseti_cli.config_show()\n\n        # Assert\n        assert handle\n        assert re.search(fr'{handle}', str(result.stdout))\n\n    @pytest.mark.e2e\n    @pytest.mark.model\n    @pytest.mark.server\n    def test_model_roles(self, cloudsql_connection, forseti_model_readonly):\n        # Arrange\/Act\n        model_name, handle, _ = forseti_model_readonly\n\n        # Assert\n        table_name = f'forseti_security.{handle}_roles'\n        query = text('SELECT '\n                     'COUNT(*) '\n                     f'FROM {table_name}')\n        model_roles = (cloudsql_connection.execute(query).fetchone())\n        assert model_roles\n        assert model_roles[0] > 0\n","lang_cluster":"Python","length":58,"code_uid":"7ced1bcac87643379c37662105d6dcb3"}
{"diff_hunk":"@@ -35,9 +35,9 @@ LOGGER = log_util.get_logger(__name__)\n class ViolationDao(dao.Dao):\n     \"\"\"Data access object (DAO) for rule violations.\"\"\"\n \n-    violation_attribute_list = ['resource_type', 'resource_id', 'rule_name',\n-                                'rule_index', 'violation_type',\n-                                'violation_data']\n+    violation_attribute_list = [\n+        'violation_hash', 'resource_type', 'resource_id', 'rule_name',\n+        'rule_index', 'violation_type', 'violation_data', 'created_at_datetime']\n     frozen_violation_attribute_list = frozenset(violation_attribute_list)\n     Violation = namedtuple('Violation', frozen_violation_attribute_list)\n ","old_code":"# Copyright 2017 The Forseti Security Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Provides the data access object (DAO) for Organizations.\"\"\"\n\nimport json\n\nfrom collections import defaultdict\nfrom collections import namedtuple\n\nimport MySQLdb\n\nfrom google.cloud.security.common.data_access import dao\nfrom google.cloud.security.common.data_access import errors as db_errors\nfrom google.cloud.security.common.data_access import violation_map as vm\nfrom google.cloud.security.common.data_access.sql_queries import load_data\nfrom google.cloud.security.common.data_access.sql_queries import select_data\nfrom google.cloud.security.common.util import log_util\n\n\nLOGGER = log_util.get_logger(__name__)\n\n\nclass ViolationDao(dao.Dao):\n    \"\"\"Data access object (DAO) for rule violations.\"\"\"\n\n    violation_attribute_list = ['resource_type', 'resource_id', 'rule_name',\n                                'rule_index', 'violation_type',\n                                'violation_data']\n    frozen_violation_attribute_list = frozenset(violation_attribute_list)\n    Violation = namedtuple('Violation', frozen_violation_attribute_list)\n\n    def insert_violations(self, violations,\n                          snapshot_timestamp=None):\n        \"\"\"Import violations into database.\n\n        Args:\n            violations (iterator): An iterator of RuleViolations.\n            snapshot_timestamp (str): The snapshot timestamp to associate\n                these violations with.\n\n        Returns:\n            tuple: A tuple of (int, list) containing the count of inserted\n                rows and a list of violations that encountered an error during\n                insert.\n\n        Raise:\n            MySQLError: is raised when the snapshot table can not be created.\n        \"\"\"\n\n        resource_name = 'violations'\n\n        try:\n            # Make sure to have a reasonable timestamp to use.\n            if not snapshot_timestamp:\n                snapshot_timestamp = self.get_latest_snapshot_timestamp(\n                    ('PARTIAL_SUCCESS', 'SUCCESS'))\n\n            # Create the violations snapshot table.\n            snapshot_table = self.create_snapshot_table(\n                resource_name, snapshot_timestamp)\n        # TODO: Remove this exception handling by moving the check for\n        # violations table outside of the scanners.\n        except MySQLdb.OperationalError, e:\n            if e[0] == 1050:\n                LOGGER.debug('Violations table already exists: %s', e)\n                snapshot_table = self._create_snapshot_table_name(\n                    resource_name, snapshot_timestamp)\n            else:\n                raise db_errors.MySQLError(resource_name, e)\n        except MySQLdb.Error, e:\n            raise db_errors.MySQLError(resource_name, e)\n\n        inserted_rows = 0\n        violation_errors = []\n        for violation in violations:\n            violation = self.Violation(\n                resource_type=violation['resource_type'],\n                resource_id=violation['resource_id'],\n                rule_name=violation['rule_name'],\n                rule_index=violation['rule_index'],\n                violation_type=violation['violation_type'],\n                violation_data=violation['violation_data'])\n            for formatted_violation in _format_violation(violation,\n                                                         resource_name):\n                try:\n                    self.execute_sql_with_commit(\n                        resource_name,\n                        load_data.INSERT_VIOLATION.format(snapshot_table),\n                        formatted_violation)\n                    inserted_rows += 1\n                except MySQLdb.Error, e:\n                    LOGGER.error('Unable to insert violation %s due to %s',\n                                 formatted_violation, e)\n                    violation_errors.append(formatted_violation)\n\n        return (inserted_rows, violation_errors)\n\n    def get_all_violations(self, timestamp, violation_type=None):\n        \"\"\"Get all the violations.\n\n        Args:\n            timestamp (str): The timestamp of the snapshot.\n            violation_type (str): The violation type.\n\n        Returns:\n            list: A list of dict of the violations data.\n        \"\"\"\n        if not violation_type:\n            resource_name = 'all_violations'\n            query = select_data.SELECT_ALL_VIOLATIONS\n            params = ()\n        else:\n            resource_name = violation_type\n            query = select_data.SELECT_VIOLATIONS_BY_TYPE\n            params = (violation_type,)\n\n        violations_sql = query.format(timestamp)\n        rows = self.execute_sql_with_fetch(\n            resource_name, violations_sql, params)\n        return rows\n\n\ndef _format_violation(violation, resource_name):\n    \"\"\"Violation formating stub that uses a map to call the formating\n    function for the resource.\n\n    Args:\n        violation (iterator): An iterator of RuleViolations.\n        resource_name (str): String that defines a resource.\n\n    Returns:\n        tuple: A tuple of formatted violation.\n    \"\"\"\n    formatted_output = vm.VIOLATION_MAP[resource_name](violation)\n    return formatted_output\n\n\ndef map_by_resource(violation_rows):\n    \"\"\"Create a map of violation types to violations of that resource.\n\n    Args:\n        violation_rows (list): A list of dict of violation data.\n\n    Returns:\n        dict: A dict of violation types mapped to the list of corresponding\n            violation types, i.e. { resource => [violation_data...] }.\n    \"\"\"\n    v_by_type = defaultdict(list)\n\n    for v_data in violation_rows:\n        try:\n            v_data['violation_data'] = json.loads(v_data['violation_data'])\n        except ValueError:\n            LOGGER.warn('Invalid violation data, unable to parse json for %s',\n                        v_data['violation_data'])\n\n        v_resource = vm.VIOLATION_RESOURCES.get(v_data['violation_type'])\n        if v_resource:\n            v_by_type[v_resource].append(v_data)\n\n    return dict(v_by_type)\n","lang_cluster":"Python","length":173,"code_uid":"a4b7a7e4b4514007b000b9e881292bf2"}
{"diff_hunk":"@@ -101,6 +101,7 @@ def venv_python(*args, output=False):\n def test_toolchain():\n     \"\"\"Test if imports work properly.\"\"\"\n     utils.print_title(\"Checking toolchain\")\n+\n     packages = ['sip', 'PyQt5.QtCore', 'PyQt5.QtWebKit', 'qutebrowser.app']\n     if g_args.dev:\n         packages += get_dev_packages(short=True)","old_code":"#!\/usr\/bin\/env python3\n# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2014-2015 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n\"\"\"Initialize a virtualenv suitable to be used for qutebrowser.\"\"\"\n\nimport os\nimport re\nimport sys\nimport glob\nimport os.path\nimport shutil\nimport argparse\nimport subprocess\nimport distutils.sysconfig  # pylint: disable=import-error\n# see https:\/\/bitbucket.org\/logilab\/pylint\/issue\/73\/\n\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), os.pardir))\nfrom scripts import utils\n\n\ng_path = None\ng_args = None\n\n\ndef parse_args():\n    \"\"\"Parse the commandline arguments.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--force', help=\"Force creating a new virtualenv.\",\n                        action='store_true')\n    parser.add_argument('--dev', help=\"Set up an environment suitable for \"\n                                      \"developing qutebrowser.\",\n                        action='store_true')\n    parser.add_argument('path', help=\"Path to the virtualenv folder\",\n                        default='.venv', nargs='?')\n    return parser.parse_args()\n\n\ndef check_exists():\n    \"\"\"Check if the virtualenv already exists.\"\"\"\n    if os.path.exists(g_path):\n        if g_args.force:\n            print(\"Deleting old virtualenv at {}\".format(g_path))\n            shutil.rmtree(g_path)\n        else:\n            print(\"virtualenv at {} does already exist!\".format(g_path),\n                  file=sys.stderr)\n            sys.exit(1)\n\n\ndef get_dev_packages(short=False):\n    \"\"\"Get a list of packages to install.\n\n    Args:\n        short: Remove the version specification.\n    \"\"\"\n    packages = ['colorlog', 'flake8', 'astroid==1.2.1', 'pylint==1.3.1',\n                'pep257', 'colorama', 'beautifulsoup4']\n    if short:\n        packages = [re.split(r'[<>=]', p)[0] for p in packages]\n    return packages\n\n\ndef install_dev_packages():\n    \"\"\"Install the packages needed for development.\"\"\"\n    for pkg in get_dev_packages():\n        utils.print_subtitle(\"Installing {}\".format(pkg))\n        if os.name == 'nt':\n            venv_python('-m', 'pip', 'install', '--no-clean', pkg)\n        else:\n            venv_python('-m', 'pip', 'install', pkg)\n\n\ndef venv_python(*args, output=False):\n    \"\"\"Call the virtualenv's python with the given arguments.\"\"\"\n    subdir = 'Scripts' if os.name == 'nt' else 'bin'\n    executable = os.path.join(g_path, subdir, os.path.basename(sys.executable))\n    if output:\n        return subprocess.check_output([executable] + list(args),\n                                       universal_newlines=True)\n    else:\n        subprocess.check_call([executable] + list(args))\n\n\ndef test_toolchain():\n    \"\"\"Test if imports work properly.\"\"\"\n    utils.print_title(\"Checking toolchain\")\n    packages = ['sip', 'PyQt5.QtCore', 'PyQt5.QtWebKit', 'qutebrowser.app']\n    if g_args.dev:\n        packages += get_dev_packages(short=True)\n    for pkg in packages:\n        if pkg == 'beautifulsoup4':\n            pkg = 'bs4'\n        print(\"Importing {}\".format(pkg))\n        venv_python('-c', 'import {}'.format(pkg))\n\n\ndef link_pyqt():\n    \"\"\"Symlink the systemwide PyQt\/sip into the virtualenv.\"\"\"\n    if os.name == 'nt':\n        return\n    utils.print_title(\"Softlinking PyQt5\")\n    sys_path = distutils.sysconfig.get_python_lib()\n    venv_path = venv_python(\n        '-c', 'from distutils.sysconfig import get_python_lib\\n'\n              'print(get_python_lib())', output=True).rstrip()\n    globbed_sip = (glob.glob(os.path.join(sys_path, 'sip*.so')) +\n                   glob.glob(os.path.join(sys_path, 'sip*.pyd')))\n    if not globbed_sip:\n        print(\"Did not find sip in {}!\".format(sys_path), file=sys.stderr)\n        sys.exit(1)\n    files = [\n        'PyQt5',\n    ]\n    files += [os.path.basename(e) for e in globbed_sip]\n    for fn in files:\n        source = os.path.join(sys_path, fn)\n        link_name = os.path.join(venv_path, fn)\n        if not os.path.exists(source):\n            raise FileNotFoundError(source)\n        print('{} -> {}'.format(source, link_name))\n        os.symlink(source, link_name)\n\n\ndef create_venv():\n    \"\"\"Create a new virtualenv.\"\"\"\n    utils.print_title(\"Creating virtualenv\")\n    if os.name == 'nt':\n        sys_site = ['--system-site-packages']\n    else:\n        sys_site = []\n    subprocess.check_call(['virtualenv'] + sys_site +\n                          ['-p', sys.executable, g_path])\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    global g_path, g_args\n    g_args = parse_args()\n    if not g_args.path:\n        print(\"Refusing to run with empty path!\", file=sys.stderr)\n        sys.exit(1)\n    g_path = os.path.abspath(g_args.path)\n    check_exists()\n    create_venv()\n    utils.print_title(\"Calling setup.py\")\n    venv_python('setup.py', 'develop')\n    if g_args.dev:\n        utils.print_title(\"Installing developer packages\")\n        install_dev_packages()\n    link_pyqt()\n    test_toolchain()\n\n\nif __name__ == '__main__':\n    main()\n","lang_cluster":"Python","length":172,"code_uid":"06ec340b15784f288c386ec7d0a45947"}
{"diff_hunk":"@@ -63,12 +63,12 @@ def build_extensions_nupic():\n   os.chdir(buildScriptsDir)\n \n   # Generate build files with CMake\n-  return_code = subprocess.call(\"cmake \" + sourceDir + ' ' + cmakeOptions, shell=True)\n+  return_code = subprocess.call('cmake ' + sourceDir + ' ' + cmakeOptions, shell=True)\n   if (return_code != 0):\n     sys.exit(\"Unable to generate build scripts!\")\n \n   # Build library with Make\n-  return_code = subprocess.call(\"make \" + makeOptions, shell=True)\n+  return_code = subprocess.call('make ' + makeOptions, shell=True)\n   if (return_code != 0):\n     sys.exit(\"Unable to build the library!\")\n ","old_code":"import sys\nimport os\nimport subprocess\nfrom setuptools import setup\n\n\"\"\"\nThis file only will call CMake process to generate scripts, build, and then install the NuPIC binaries.\nANY EXTRA code related to build process MUST be put into CMake file.\n\"\"\"\n\nrepositoryDir = os.getcwd()\n\n\n# Read command line options looking for extra options for CMake and Make\n# For example, an user could type:\n#   python setup.py install make_options='-j3'\n# which will add '-j3' option to Make commandline\ncmakeOptions = \"\"\nmakeOptions = \"\"\nsetupOptions = \"\"\nmustBuildExtensions = False\nfor arg in sys.argv:\n  if (\"cmake_options\" in arg) or (\"make_options\" in arg):\n    (option, _, rhs) = arg.partition(\"=\")\n    if option[0] == \"--cmake_options\":\n      cmakeOptions = rhs\n    if option[0] == \"--make_options\":\n      makeOptions = rhs\n  elif (not \"setup.py\" in arg):\n    if (\"build\" in arg) or (\"install\" in arg):\n      mustBuildExtensions = True\n    setupOptions += arg + \" \"\n\n\n# Get properties of the project like version, notes, etc\nproperties = {}\nexecfile(os.path.join(repositoryDir, \"nupic\", \"__init__.py\"), {}, properties)\n\n\ndef findPackages(repositoryDir):\n  \"\"\"\n  Traverse nupic directory and create packages for each subdir containing a\n  __init__.py file\n  \"\"\"\n  packages = []\n  for root, dirs, files in os.walk(repositoryDir + '\/nupic'):\n    if '__init__.py' in files:\n      subdir = root.replace(repositoryDir + '\/', '')\n      packages.append(subdir.replace('\/', '.'))\n  return packages\n\n\ndef build_extensions_nupic():\n  \"\"\"\n  CMake-specific build operations\n  \"\"\"\n\n  # Prepare directories to the CMake process\n  sourceDir = repositoryDir\n  buildScriptsDir = repositoryDir + '\/build\/scripts'\n  if not os.path.exists(buildScriptsDir):\n    os.makedirs(buildScriptsDir)\n  os.chdir(buildScriptsDir)\n\n  # Generate build files with CMake\n  return_code = subprocess.call(\"cmake \" + sourceDir + ' ' + cmakeOptions, shell=True)\n  if (return_code != 0):\n    sys.exit(\"Unable to generate build scripts!\")\n\n  # Build library with Make\n  return_code = subprocess.call(\"make \" + makeOptions, shell=True)\n  if (return_code != 0):\n    sys.exit(\"Unable to build the library!\")\n\n\ndef setup_nupic():\n  \"\"\"\n  Package setup operations\n  \"\"\"\n  \n  # Setup library\n  os.chdir(repositoryDir)\n  setup(\n    name = 'nupic',\n    version = properties[\"__version__\"],\n    packages = findPackages(repositoryDir),\n    package_data = {\n      'nupic': ['README.md', 'LICENSE.txt', '*.so', '*.dll', '*.dylib'],\n      'nupic.bindings': ['_*.so', '_*.dll'],\n      'nupic.data': ['*.json'],\n      'nupic.frameworks.opf.exp_generator': ['*.json', '*.tpl'],\n      'nupic.frameworks.opf.jsonschema': ['*.json'],\n      'nupic.support.resources.images': ['*.png', '*.gif', '*.ico', '*.graffle'],\n      'nupic.swarming.jsonschema': ['*.json']},\n    description = 'Numenta Platform for Intelligent Computing',\n    author='Numenta',\n    author_email='help@numenta.org',\n    url='https:\/\/github.com\/numenta\/nupic',\n    classifiers=[\n      'Programming Language :: Python',\n      'Programming Language :: Python :: 2',\n      'License :: OSI Approved :: GNU General Public License (GPL)',\n      'Operating System :: OS Independent',\n      'Development Status :: 5 - Production\/Stable',\n      'Environment :: Console',\n      'Intended Audience :: Science\/Research',\n      'Topic :: Scientific\/Engineering :: Artificial Intelligence'\n    ],\n    long_description = \"\"\"\\\nNuPIC is a library that provides the building blocks for online prediction systems. The library contains the Cortical Learning Algorithm (CLA), but also the Online Prediction Framework (OPF) that allows clients to build prediction systems out of encoders, models, and metrics.\n\nFor more information, see numenta.org or the NuPIC wiki (https:\/\/github.com\/numenta\/nupic\/wiki).\n\"\"\"\n  )\n\n\n# Build and setup NuPIC\nif mustBuildExtensions:\n  build_extensions_nupic()\nsetup_nupic()\n","lang_cluster":"Python","length":120,"code_uid":"f55288659ec4454b84b64976b84aa2bc"}
{"diff_hunk":"@@ -50,10 +50,10 @@ def scope_logged_job2():\n \n     # end_get_logger\n     @graph\n-    def thing():\n+    def thing_two():\n         ambitious_op()\n \n-    return thing\n+    return thing_two\n \n \n @repository","old_code":"from dagster import repository\n\n\ndef scope_logged_job():\n\n    from dagster import graph\n\n    # start_python_logger\n\n    import logging\n    from dagster import op\n\n    @op\n    def ambitious_op():\n        my_logger = logging.getLogger(\"my_logger\")\n        try:\n            x = 1 \/ 0\n            return x\n        except ZeroDivisionError:\n            my_logger.error(\"Couldn't divide by zero!\")\n\n        return None\n\n    # end_python_logger\n    @graph\n    def thing():\n        ambitious_op()\n\n    return thing\n\n\ndef scope_logged_job2():\n\n    from dagster import graph\n\n    # start_get_logger\n\n    from dagster import get_dagster_logger, op\n\n    @op\n    def ambitious_op():\n        my_logger = get_dagster_logger()\n        try:\n            x = 1 \/ 0\n            return x\n        except ZeroDivisionError:\n            my_logger.error(\"Couldn't divide by zero!\")\n\n        return None\n\n    # end_get_logger\n    @graph\n    def thing():\n        ambitious_op()\n\n    return thing\n\n\n@repository\ndef python_logging_repo():\n    return [scope_logged_job(), scope_logged_job2()]\n","lang_cluster":"Python","length":61,"code_uid":"54e114204be74016bb070882acba7efb"}
{"diff_hunk":"@@ -15,7 +15,7 @@ from astroid import nodes\n \n from pylint.checkers import BaseTokenChecker\n from pylint.checkers.utils import check_messages\n-from pylint.interfaces import IAstroidChecker, ITokenChecker\n+from pylint.interfaces import HIGH, IAstroidChecker, ITokenChecker\n \n \n class ElseifUsedChecker(BaseTokenChecker):","old_code":"# Copyright (c) 2015 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2016-2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2016 Glenn Matthews <glmatthe@cisco.com>\n# Copyright (c) 2018 Ville Skytt\u00e4 <ville.skytta@iki.fi>\n# Copyright (c) 2019-2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2020 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2020 Anthony Sottile <asottile@umich.edu>\n# Copyright (c) 2021 Dani\u00ebl van Noord <13665637+DanielNoord@users.noreply.github.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n\n# Licensed under the GPL: https:\/\/www.gnu.org\/licenses\/old-licenses\/gpl-2.0.html\n# For details: https:\/\/github.com\/PyCQA\/pylint\/blob\/main\/LICENSE\n\nfrom astroid import nodes\n\nfrom pylint.checkers import BaseTokenChecker\nfrom pylint.checkers.utils import check_messages\nfrom pylint.interfaces import IAstroidChecker, ITokenChecker\n\n\nclass ElseifUsedChecker(BaseTokenChecker):\n    \"\"\"Checks for use of \"else if\" when an \"elif\" could be used\"\"\"\n\n    __implements__ = (ITokenChecker, IAstroidChecker)\n    name = \"else_if_used\"\n    msgs = {\n        \"R5501\": (\n            'Consider using \"elif\" instead of \"else if\"',\n            \"else-if-used\",\n            \"Used when an else statement is immediately followed by \"\n            \"an if statement and does not contain statements that \"\n            \"would be unrelated to it.\",\n        )\n    }\n\n    def __init__(self, linter=None):\n        super().__init__(linter)\n        self._init()\n\n    def _init(self):\n        self._elifs = []\n        self._if_counter = 0\n\n    def process_tokens(self, tokens):\n        # Process tokens and look for 'if' or 'elif'\n        for _, token, _, _, _ in tokens:\n            if token == \"elif\":\n                self._elifs.append(True)\n            elif token == \"if\":\n                self._elifs.append(False)\n\n    def leave_module(self, _: nodes.Module) -> None:\n        self._init()\n\n    def visit_ifexp(self, node: nodes.IfExp) -> None:\n        if isinstance(node.parent, nodes.FormattedValue):\n            return\n        self._if_counter += 1\n\n    def visit_comprehension(self, node: nodes.Comprehension) -> None:\n        self._if_counter += len(node.ifs)\n\n    @check_messages(\"else-if-used\")\n    def visit_if(self, node: nodes.If) -> None:\n        if isinstance(node.parent, nodes.If):\n            orelse = node.parent.orelse\n            # current if node must directly follow an \"else\"\n            if orelse and orelse == [node]:\n                if not self._elifs[self._if_counter]:\n                    self.add_message(\"else-if-used\", node=node)\n        self._if_counter += 1\n\n\ndef register(linter):\n    \"\"\"Required method to auto register this checker.\n\n    :param linter: Main interface object for Pylint plugins\n    :type linter: Pylint object\n    \"\"\"\n    linter.register_checker(ElseifUsedChecker(linter))\n","lang_cluster":"Python","length":80,"code_uid":"2c2a2bdac1044cb59fb3ffb36a7aadf6"}
{"diff_hunk":"@@ -106,7 +106,7 @@ bind_layers(SNAP, DTP, code=0x2004, OUI=0xc)\n \n \n def negotiate_trunk(iface=conf.iface, mymac=str(RandMAC())):\n-    print \"Trying to negotiate a trunk on interface %s\" % iface\n+    print(\"Trying to negotiate a trunk on interface %s\" % iface)\n     p = Dot3(src=mymac, dst=\"01:00:0c:cc:cc:cc\")\/LLC()\/SNAP()\/DTP(tlvlist=[DTPDomain(),DTPStatus(),DTPType(),DTPNeighbor(neighbor=mymac)])\n     sendp(p)\n ","old_code":"#!\/usr\/bin\/env python\n\n# scapy.contrib.description = DTP\n# scapy.contrib.status = loads\n\n\"\"\"\n    DTP Scapy Extension\n    ~~~~~~~~~~~~~~~~~~~\n\n    :version: 2008-12-22\n    :author: Jochen Bartl <lobo@c3a.de>\n\n    :Thanks:\n\n    - TLV code derived from the CDP implementation of scapy. (Thanks to Nicolas Bareil and Arnaud Ebalard)\n        http:\/\/trac.secdev.org\/scapy\/ticket\/18\n\"\"\"\n\nfrom scapy.packet import *\nfrom scapy.fields import *\nfrom scapy.layers.l2 import SNAP,Dot3,LLC\nfrom scapy.sendrecv import sendp\n\nclass DtpGenericTlv(Packet):\n    name = \"DTP Generic TLV\"\n    fields_desc = [ XShortField(\"type\", 0x0001),\n            FieldLenField(\"length\", None, length_of=lambda pkt:pkt.value + 4),\n            StrLenField(\"value\", \"\", length_from=lambda pkt:pkt.length - 4)\n            ]\n\n    def guess_payload_class(self, p):\n        return conf.padding_layer\n\nclass RepeatedTlvListField(PacketListField):\n    def __init__(self, name, default, cls):\n        PacketField.__init__(self, name, default, cls)\n\n    def getfield(self, pkt, s):\n        lst = []\n        remain = s\n        while len(remain) > 0:\n            p = self.m2i(pkt,remain)\n            if conf.padding_layer in p:\n                pad = p[conf.padding_layer]\n                remain = pad.load\n                del(pad.underlayer.payload)\n            else:\n                remain = \"\"\n            lst.append(p)\n        return remain,lst\n\n    def addfield(self, pkt, s, val):\n        return s+reduce(str.__add__, map(str, val),\"\")\n\n_DTP_TLV_CLS = {\n                    0x0001 : \"DTPDomain\",\n                    0x0002 : \"DTPStatus\",\n                    0x0003 : \"DTPType\",\n                    0x0004 : \"DTPNeighbor\"\n                   }\n\nclass DTPDomain(DtpGenericTlv):\n    name = \"DTP Domain\"\n    fields_desc = [ ShortField(\"type\", 1),\n            FieldLenField(\"length\", None, \"domain\", adjust=lambda pkt,x:x + 4),\n            StrLenField(\"domain\", b\"\\x00\", length_from=lambda pkt:pkt.length - 4)\n            ]\n\nclass DTPStatus(DtpGenericTlv):\n    name = \"DTP Status\"\n    fields_desc = [ ShortField(\"type\", 2),\n            FieldLenField(\"length\", None, \"status\", adjust=lambda pkt,x:x + 4),\n            StrLenField(\"status\", b\"\\x03\", length_from=lambda pkt:pkt.length - 4)\n            ]\n\nclass DTPType(DtpGenericTlv):\n    name = \"DTP Type\"\n    fields_desc = [ ShortField(\"type\", 3),\n            FieldLenField(\"length\", None, \"dtptype\", adjust=lambda pkt,x:x + 4),\n            StrLenField(\"dtptype\", b\"\\xa5\", length_from=lambda pkt:pkt.length - 4)\n            ]\n\nclass DTPNeighbor(DtpGenericTlv):\n    name = \"DTP Neighbor\"\n    fields_desc = [ ShortField(\"type\", 4),\n            #FieldLenField(\"length\", None, \"neighbor\", adjust=lambda pkt,x:x + 4),\n            ShortField(\"len\", 10),\n            MACField(\"neighbor\", None)\n            ]\n\ndef _DTPGuessPayloadClass(p, **kargs):\n    cls = conf.raw_layer\n    if len(p) >= 2:\n        t = struct.unpack(\"!H\", p[:2])[0]\n        clsname = _DTP_TLV_CLS.get(t, \"DtpGenericTlv\")\n        cls = globals()[clsname]\n    return cls(p, **kargs)\n\nclass DTP(Packet):\n    name = \"DTP\"\n    fields_desc = [ ByteField(\"ver\", 1),\n                    RepeatedTlvListField(\"tlvlist\", [], _DTPGuessPayloadClass)\n                ]\n\nbind_layers(SNAP, DTP, code=0x2004, OUI=0xc)\n\n\ndef negotiate_trunk(iface=conf.iface, mymac=str(RandMAC())):\n    print \"Trying to negotiate a trunk on interface %s\" % iface\n    p = Dot3(src=mymac, dst=\"01:00:0c:cc:cc:cc\")\/LLC()\/SNAP()\/DTP(tlvlist=[DTPDomain(),DTPStatus(),DTPType(),DTPNeighbor(neighbor=mymac)])\n    sendp(p)\n\nif __name__ == \"__main__\":\n    from scapy.main import interact\n    interact(mydict=globals(), mybanner=\"DTP\")\n","lang_cluster":"Python","length":115,"code_uid":"dda90ced69bd448bbbbc5bb3c0fad8a2"}
{"diff_hunk":"@@ -114,27 +114,43 @@ def det2json(dataset, results):\n                 data['score'] = float(bboxes[i][4])\n                 data['category_id'] = dataset.cat_ids[label]\n                 json_results.append(data)\n-    return json_results\n+    return dict(bbox=json_results)\n \n \n def segm2json(dataset, results):\n-    json_results = []\n+    bbox_json_results = []\n+    segm_json_results = []\n     for idx in range(len(dataset)):\n         img_id = dataset.img_ids[idx]\n-        det, seg = results[idx]\n+        det, (seg, mask_scores) = results[idx]\n         for label in range(len(det)):\n+            # bbox results\n             bboxes = det[label]\n-            segms = seg[label]\n             for i in range(bboxes.shape[0]):\n                 data = dict()\n                 data['image_id'] = img_id\n                 data['bbox'] = xyxy2xywh(bboxes[i])\n                 data['score'] = float(bboxes[i][4])\n                 data['category_id'] = dataset.cat_ids[label]\n+                bbox_json_results.append(data)\n+\n+            # segm results\n+            segms = seg[label]\n+            if mask_scores is not None:\n+                mask_score = mask_scores[label]\n+            for i in range(bboxes.shape[0]):\n+                data = dict()\n+                data['image_id'] = img_id\n+                data['bbox'] = xyxy2xywh(bboxes[i])\n+                if mask_scores is not None:\n+                    data['score'] = float(mask_score[i])\n+                else:\n+                    data['score'] = float(bboxes[i][4])\n+                data['category_id'] = dataset.cat_ids[label]\n                 segms[i]['counts'] = segms[i]['counts'].decode()\n                 data['segmentation'] = segms[i]\n-                json_results.append(data)\n-    return json_results\n+                segm_json_results.append(data)\n+    return dict(bbox=bbox_json_results, segm=segm_json_results)\n \n \n def results2json(dataset, results, out_file):","old_code":"import mmcv\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nfrom .recall import eval_recalls\n\n\ndef coco_eval(result_file, result_types, coco, max_dets=(100, 300, 1000)):\n    for res_type in result_types:\n        assert res_type in [\n            'proposal', 'proposal_fast', 'bbox', 'segm', 'keypoints'\n        ]\n\n    if mmcv.is_str(coco):\n        coco = COCO(coco)\n    assert isinstance(coco, COCO)\n\n    if result_types == ['proposal_fast']:\n        ar = fast_eval_recall(result_file, coco, np.array(max_dets))\n        for i, num in enumerate(max_dets):\n            print('AR@{}\\t= {:.4f}'.format(num, ar[i]))\n        return\n\n    assert result_file.endswith('.json')\n    coco_dets = coco.loadRes(result_file)\n\n    img_ids = coco.getImgIds()\n    for res_type in result_types:\n        iou_type = 'bbox' if res_type == 'proposal' else res_type\n        cocoEval = COCOeval(coco, coco_dets, iou_type)\n        cocoEval.params.imgIds = img_ids\n        if res_type == 'proposal':\n            cocoEval.params.useCats = 0\n            cocoEval.params.maxDets = list(max_dets)\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()\n\n\ndef fast_eval_recall(results,\n                     coco,\n                     max_dets,\n                     iou_thrs=np.arange(0.5, 0.96, 0.05)):\n    if mmcv.is_str(results):\n        assert results.endswith('.pkl')\n        results = mmcv.load(results)\n    elif not isinstance(results, list):\n        raise TypeError(\n            'results must be a list of numpy arrays or a filename, not {}'.\n            format(type(results)))\n\n    gt_bboxes = []\n    img_ids = coco.getImgIds()\n    for i in range(len(img_ids)):\n        ann_ids = coco.getAnnIds(imgIds=img_ids[i])\n        ann_info = coco.loadAnns(ann_ids)\n        if len(ann_info) == 0:\n            gt_bboxes.append(np.zeros((0, 4)))\n            continue\n        bboxes = []\n        for ann in ann_info:\n            if ann.get('ignore', False) or ann['iscrowd']:\n                continue\n            x1, y1, w, h = ann['bbox']\n            bboxes.append([x1, y1, x1 + w - 1, y1 + h - 1])\n        bboxes = np.array(bboxes, dtype=np.float32)\n        if bboxes.shape[0] == 0:\n            bboxes = np.zeros((0, 4))\n        gt_bboxes.append(bboxes)\n\n    recalls = eval_recalls(\n        gt_bboxes, results, max_dets, iou_thrs, print_summary=False)\n    ar = recalls.mean(axis=1)\n    return ar\n\n\ndef xyxy2xywh(bbox):\n    _bbox = bbox.tolist()\n    return [\n        _bbox[0],\n        _bbox[1],\n        _bbox[2] - _bbox[0] + 1,\n        _bbox[3] - _bbox[1] + 1,\n    ]\n\n\ndef proposal2json(dataset, results):\n    json_results = []\n    for idx in range(len(dataset)):\n        img_id = dataset.img_ids[idx]\n        bboxes = results[idx]\n        for i in range(bboxes.shape[0]):\n            data = dict()\n            data['image_id'] = img_id\n            data['bbox'] = xyxy2xywh(bboxes[i])\n            data['score'] = float(bboxes[i][4])\n            data['category_id'] = 1\n            json_results.append(data)\n    return json_results\n\n\ndef det2json(dataset, results):\n    json_results = []\n    for idx in range(len(dataset)):\n        img_id = dataset.img_ids[idx]\n        result = results[idx]\n        for label in range(len(result)):\n            bboxes = result[label]\n            for i in range(bboxes.shape[0]):\n                data = dict()\n                data['image_id'] = img_id\n                data['bbox'] = xyxy2xywh(bboxes[i])\n                data['score'] = float(bboxes[i][4])\n                data['category_id'] = dataset.cat_ids[label]\n                json_results.append(data)\n    return json_results\n\n\ndef segm2json(dataset, results):\n    json_results = []\n    for idx in range(len(dataset)):\n        img_id = dataset.img_ids[idx]\n        det, seg = results[idx]\n        for label in range(len(det)):\n            bboxes = det[label]\n            segms = seg[label]\n            for i in range(bboxes.shape[0]):\n                data = dict()\n                data['image_id'] = img_id\n                data['bbox'] = xyxy2xywh(bboxes[i])\n                data['score'] = float(bboxes[i][4])\n                data['category_id'] = dataset.cat_ids[label]\n                segms[i]['counts'] = segms[i]['counts'].decode()\n                data['segmentation'] = segms[i]\n                json_results.append(data)\n    return json_results\n\n\ndef results2json(dataset, results, out_file):\n    if isinstance(results[0], list):\n        json_results = det2json(dataset, results)\n    elif isinstance(results[0], tuple):\n        json_results = segm2json(dataset, results)\n    elif isinstance(results[0], np.ndarray):\n        json_results = proposal2json(dataset, results)\n    else:\n        raise TypeError('invalid type of results')\n    mmcv.dump(json_results, out_file)\n","lang_cluster":"Python","length":149,"code_uid":"37f99f01a2b24450be97a5777d4ad757"}
{"diff_hunk":"@@ -6,6 +6,7 @@ from kaitaistruct import KaitaiStream\n from mitmproxy.contrib.kaitaistruct import png\n from mitmproxy.contrib.kaitaistruct import gif\n from mitmproxy.contrib.kaitaistruct import jpeg\n+from mitmproxy.contrib.kaitaistruct import ico\n \n Metadata = typing.List[typing.Tuple[str, str]]\n ","old_code":"import io\nimport typing\n\nfrom kaitaistruct import KaitaiStream\n\nfrom mitmproxy.contrib.kaitaistruct import png\nfrom mitmproxy.contrib.kaitaistruct import gif\nfrom mitmproxy.contrib.kaitaistruct import jpeg\n\nMetadata = typing.List[typing.Tuple[str, str]]\n\n\ndef parse_png(data: bytes) -> Metadata:\n    img = png.Png(KaitaiStream(io.BytesIO(data)))\n    parts = [\n        ('Format', 'Portable network graphics'),\n        ('Size', \"{0} x {1} px\".format(img.ihdr.width, img.ihdr.height))\n    ]\n    for chunk in img.chunks:\n        if chunk.type == 'gAMA':\n            parts.append(('gamma', str(chunk.body.gamma_int \/ 100000)))\n        elif chunk.type == 'pHYs':\n            aspectx = chunk.body.pixels_per_unit_x\n            aspecty = chunk.body.pixels_per_unit_y\n            parts.append(('aspect', \"{0} x {1}\".format(aspectx, aspecty)))\n        elif chunk.type == 'tEXt':\n            parts.append((chunk.body.keyword, chunk.body.text))\n        elif chunk.type == 'iTXt':\n            parts.append((chunk.body.keyword, chunk.body.text))\n        elif chunk.type == 'zTXt':\n            parts.append((chunk.body.keyword, chunk.body.text_datastream.decode('iso8859-1')))\n    return parts\n\n\ndef parse_gif(data: bytes) -> Metadata:\n    img = gif.Gif(KaitaiStream(io.BytesIO(data)))\n    descriptor = img.logical_screen_descriptor\n    parts = [\n        ('Format', 'Compuserve GIF'),\n        ('Version', \"GIF{}\".format(img.hdr.version)),\n        ('Size', \"{} x {} px\".format(descriptor.screen_width, descriptor.screen_height)),\n        ('background', str(descriptor.bg_color_index))\n    ]\n    ext_blocks = []\n    for block in img.blocks:\n        if block.block_type.name == 'extension':\n            ext_blocks.append(block)\n    comment_blocks = []\n    for block in ext_blocks:\n        if block.body.label._name_ == 'comment':\n            comment_blocks.append(block)\n    for block in comment_blocks:\n        entries = block.body.body.entries\n        for entry in entries:\n            comment = entry.bytes\n            if comment is not b'':\n                parts.append(('comment', str(comment)))\n    return parts\n\n\ndef parse_jpeg(data: bytes) -> Metadata:\n    img = jpeg.Jpeg(KaitaiStream(io.BytesIO(data)))\n    parts = [\n        ('Format', 'JPEG (ISO 10918)')\n    ]\n    for segment in img.segments:\n        if segment.marker._name_ == 'sof0':\n            parts.append(('Size', \"{0} x {1} px\".format(segment.data.image_width, segment.data.image_height)))\n        if segment.marker._name_ == 'app0':\n            parts.append(('jfif_version', \"({0}, {1})\".format(segment.data.version_major, segment.data.version_minor)))\n            parts.append(('jfif_density', \"({0}, {1})\".format(segment.data.density_x, segment.data.density_y)))\n            parts.append(('jfif_unit', str(segment.data.density_units._value_)))\n        if segment.marker._name_ == 'com':\n            parts.append(('comment', str(segment.data)))\n        if segment.marker._name_ == 'app1':\n            if hasattr(segment.data, 'body'):\n                for field in segment.data.body.data.body.ifd0.fields:\n                    if field.data is not None:\n                        parts.append((field.tag._name_, field.data.decode('UTF-8').strip('\\x00')))\n    return parts\n","lang_cluster":"Python","length":80,"code_uid":"20871f79000f4211a3a793ae462c7e84"}
{"diff_hunk":"@@ -104,6 +104,22 @@ class TemporalMemoryPerformanceTest(unittest.TestCase):\n   # Helper functions\n   # ==============================\n \n+  def _generateSequence(self):\n+    sequence = []    \n+    with open (_INPUT_FILE_PATH) as fin:\n+      reader = csv.reader(fin)\n+      headers = reader.next()\n+      types = reader.next()\n+      reader.next()\n+      for _ in xrange(NUM_PATTERNS):\n+        record = reader.next()\n+        value = float(record[1])\n+        encodedValue = self.scalarEncoder.encode(value)\n+        activeBits = set(encodedValue.nonzero()[0])\n+        sequence.append(activeBits)\n+    return sequence\n+\n+\n   def _feedAll(self, sequence, learn=True, num=1):\n     repeatedSequence = sequence * num\n     times = []","old_code":"#!\/usr\/bin\/env python\n# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2014, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU Affero Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU Affero Public License for more details.\n#\n# You should have received a copy of the GNU Affero Public License\n# along with this program.  If not, see http:\/\/www.gnu.org\/licenses.\n#\n# http:\/\/numenta.org\/licenses\/\n# ----------------------------------------------------------------------\n\nimport time\nimport unittest\n\nimport numpy\n\nfrom nupic.data.generators.pattern_machine import PatternMachine\nfrom nupic.data.generators.sequence_machine import SequenceMachine\nfrom nupic.research.temporal_memory import TemporalMemory as TemporalMemoryPy\nfrom nupic.bindings.algorithms import TemporalMemory as TemporalMemoryCPP\nfrom nupic.research.TP import TP\nfrom nupic.research.TP10X2 import TP10X2\n\n\n\n# ==============================\n# Tests\n# ==============================\n\nclass TemporalMemoryPerformanceTest(unittest.TestCase):\n\n  def setUp(self):\n    self.tmPy = TemporalMemoryPy(columnDimensions=[2048],\n                                 cellsPerColumn=32,\n                                 initialPermanence=0.5,\n                                 connectedPermanence=0.8,\n                                 minThreshold=10,\n                                 maxNewSynapseCount=12,\n                                 permanenceIncrement=0.1,\n                                 permanenceDecrement=0.05,\n                                 activationThreshold=15)\n\n    self.tmCPP = TemporalMemoryCPP(columnDimensions=[2048],\n                                   cellsPerColumn=32,\n                                   initialPermanence=0.5,\n                                   connectedPermanence=0.8,\n                                   minThreshold=10,\n                                   maxNewSynapseCount=12,\n                                   permanenceIncrement=0.1,\n                                   permanenceDecrement=0.05,\n                                   activationThreshold=15)\n\n    self.tp = TP(numberOfCols=2048,\n                 cellsPerColumn=32,\n                 initialPerm=0.5,\n                 connectedPerm=0.8,\n                 minThreshold=10,\n                 newSynapseCount=12,\n                 permanenceInc=0.1,\n                 permanenceDec=0.05,\n                 activationThreshold=15,\n                 globalDecay=0, burnIn=1,\n                 checkSynapseConsistency=False,\n                 pamLength=1)\n\n    self.tp10x2 = TP10X2(numberOfCols=2048,\n                         cellsPerColumn=32,\n                         initialPerm=0.5,\n                         connectedPerm=0.8,\n                         minThreshold=10,\n                         newSynapseCount=12,\n                         permanenceInc=0.1,\n                         permanenceDec=0.05,\n                         activationThreshold=15,\n                         globalDecay=0, burnIn=1,\n                         checkSynapseConsistency=False,\n                         pamLength=1)\n\n    self.patternMachine = PatternMachine(2048, 40, num=100)\n    self.sequenceMachine = SequenceMachine(self.patternMachine)\n\n\n  def testSingleSequence(self):\n    print \"Test: Single sequence\"\n    sequence = self.sequenceMachine.generateFromNumbers(range(50))\n    times = self._feedAll(sequence)\n\n    self.assertTrue(times[1] < times[0])\n    self.assertTrue(times[3] < times[2])\n\n\n  # ==============================\n  # Helper functions\n  # ==============================\n\n  def _feedAll(self, sequence, learn=True, num=1):\n    repeatedSequence = sequence * num\n    times = []\n\n    def tmComputeFn(pattern, instance):\n      instance.compute(pattern, learn)\n\n    def tpComputeFn(pattern, instance):\n      array = self._patternToNumpyArray(pattern)\n      instance.compute(array, enableLearn=learn, computeInfOutput=True)\n\n    elapsed = self._feedOne(repeatedSequence, self.tmPy, tmComputeFn)\n    times.append(elapsed)\n    print \"TM (py):\\t{0}s\".format(elapsed)\n\n    elapsed = self._feedOne(repeatedSequence, self.tmCPP, tmComputeFn)\n    times.append(elapsed)\n    print \"TM (C++):\\t{0}s\".format(elapsed)\n\n    elapsed = self._feedOne(repeatedSequence, self.tp, tpComputeFn)\n    times.append(elapsed)\n    print \"TP:\\t\\t{0}s\".format(elapsed)\n\n    elapsed = self._feedOne(repeatedSequence, self.tp10x2, tpComputeFn)\n    times.append(elapsed)\n    print \"TP10X2:\\t\\t{0}s\".format(elapsed)\n\n    return times\n\n\n  @staticmethod\n  def _feedOne(sequence, instance, computeFn):\n    start = time.clock()\n\n    for pattern in sequence:\n      if pattern == None:\n        instance.reset()\n      else:\n        computeFn(pattern, instance)\n\n    elapsed = time.clock() - start\n\n    return elapsed\n\n\n  @staticmethod\n  def _patternToNumpyArray(pattern):\n    array = numpy.zeros(2048, dtype='int32')\n    array[list(pattern)] = 1\n\n    return array\n\n\n\n# ==============================\n# Main\n# ==============================\n\nif __name__ == \"__main__\":\n  unittest.main()\n","lang_cluster":"Python","length":166,"code_uid":"807ea5bfe049449a8c8137aea3dbec2f"}
{"diff_hunk":"@@ -1,9 +1,13 @@\n import colander\n \n-from kinto.core import resource\n+from pyramid import httpexceptions\n+from pyramid.security import NO_PERMISSION_REQUIRED\n+\n+from kinto.core import resource, Service\n from kinto.core.utils import instance_uri\n-from kinto.core.storage import Filter\n+from kinto.core.storage import Filter, Sort\n from kinto.core.resource.viewset import ViewSet\n+from kinto.core.utils import COMPARISON\n \n \n class HistorySchema(resource.ResourceSchema):","old_code":"import colander\n\nfrom kinto.core import resource\nfrom kinto.core.utils import instance_uri\nfrom kinto.core.storage import Filter\nfrom kinto.core.resource.viewset import ViewSet\n\n\nclass HistorySchema(resource.ResourceSchema):\n    user_id = colander.SchemaNode(colander.String())\n    uri = colander.SchemaNode(colander.String())\n    action = colander.SchemaNode(colander.String())\n    date = colander.SchemaNode(colander.String())\n    resource_name = colander.SchemaNode(colander.String())\n    bucket_id = colander.SchemaNode(colander.String(), missing=colander.drop)\n    collection_id = colander.SchemaNode(colander.String(), missing=colander.drop)\n    group_id = colander.SchemaNode(colander.String(), missing=colander.drop)\n    record_id = colander.SchemaNode(colander.String(), missing=colander.drop)\n    target = colander.SchemaNode(colander.Mapping())\n\n    class Options:\n        preserve_unknown = False\n\n\n# Add custom OpenAPI tags\/operation ids\ncollection_get_arguments = getattr(ViewSet, \"collection_get_arguments\", {})\ncollection_delete_arguments = getattr(ViewSet, \"collection_delete_arguments\", {})\n\nget_history_arguments = {'tags': ['History'], 'operation_id': 'get_history',\n                         **collection_get_arguments}\ndelete_history_arguments = {'tags': ['History'], 'operation_id': 'delete_history',\n                            **collection_delete_arguments}\n\n\n@resource.register(name='history',\n                   collection_path='\/buckets\/{{bucket_id}}\/history',\n                   record_path=None,\n                   collection_methods=('GET', 'DELETE'),\n                   default_arguments={'tags': ['History']},\n                   collection_get_arguments=get_history_arguments,\n                   collection_delete_arguments=delete_history_arguments)\nclass History(resource.ShareableResource):\n\n    schema = HistorySchema\n\n    def get_parent_id(self, request):\n        self.bucket_id = request.matchdict['bucket_id']\n        return instance_uri(request, 'bucket', id=self.bucket_id)\n\n    def _extract_filters(self):\n        filters = super()._extract_filters()\n        filters_str_id = []\n        for filt in filters:\n            if filt.field in ('record_id', 'collection_id', 'bucket_id'):\n                if isinstance(filt.value, int):\n                    filt = Filter(filt.field, str(filt.value), filt.operator)\n            filters_str_id.append(filt)\n\n        return filters_str_id\n","lang_cluster":"Python","length":59,"code_uid":"4c79f2349dd741e0b3ceeb774dd818bb"}
{"diff_hunk":"@@ -125,5 +125,21 @@ class ServerTestRun(unittest.TestCase):\n         self.assertEqual(work['task_id'], 'A')\n \n \n+class URLLibServerTestRun(ServerTestRun):\n+\n+    @mock.patch.object(luigi.rpc, 'HAS_REQUESTS', False)\n+    def start_server(self, *args, **kwargs):\n+        super(URLLibServerTestRun, self).start_server(*args, **kwargs)\n+\n+\n+@attr('unix')\n+class UNIXServerTestRun(unittest.TestCase):\n+    server_client_class = UNIXServerClient\n+\n+    def tearDown(self):\n+        super(self, ServerTestRun).tearDown()\n+        shutil.rmtree(self.server_client.tempdir)\n+\n+\n if __name__ == '__main__':\n     unittest.main()","old_code":"# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport multiprocessing\nimport random\nimport signal\nimport time\nimport tempfile\n\nfrom helpers import unittest, with_config, skipOnTravis\nimport luigi.rpc\nimport luigi.server\nfrom luigi.scheduler import CentralPlannerScheduler\n\nfrom tornado.testing import AsyncHTTPTestCase\n\n\nclass ServerTestBase(AsyncHTTPTestCase):\n\n    def get_app(self):\n        return luigi.server.app(CentralPlannerScheduler())\n\n    def setUp(self):\n        super(ServerTestBase, self).setUp()\n\n        self._old_fetch = luigi.rpc.RemoteScheduler._fetch\n\n        def _fetch(obj, url, body, *args, **kwargs):\n            response = self.fetch(url, body=body, method='POST')\n            if response.code >= 400:\n                raise luigi.rpc.RPCError(\n                    'Errror when connecting to remote scheduler'\n                )\n            return response.body.decode('utf-8')\n\n        luigi.rpc.RemoteScheduler._fetch = _fetch\n\n    def tearDown(self):\n        super(ServerTestBase, self).tearDown()\n        luigi.rpc.RemoteScheduler._fetch = self._old_fetch\n\n\nclass ServerTest(ServerTestBase):\n\n    def test_visualizer(self):\n        page = self.fetch('\/').body\n        self.assertTrue(page.find(b'<title>') != -1)\n\n    def _test_404(self, path):\n        response = self.fetch(path)\n\n        self.assertEqual(response.code, 404)\n\n    def test_404(self):\n        self._test_404('\/foo')\n\n    def test_api_404(self):\n        self._test_404('\/api\/foo')\n\n\nclass ServerTestRun(unittest.TestCase):\n    \"\"\"Test to start and stop the server in a more \"standard\" way\n    \"\"\"\n\n    def run_server(self):\n        luigi.server.run(api_port=self._api_port, address='127.0.0.1')\n\n    def start_server(self):\n        self._api_port = random.randint(1024, 9999)\n        self._process = multiprocessing.Process(target=self.run_server)\n        self._process.start()\n        time.sleep(0.1)  # wait for server to start\n        self.sch = luigi.rpc.RemoteScheduler(host='localhost', port=self._api_port)\n        self.sch._wait = lambda: None\n\n    def stop_server(self):\n        self._process.terminate()\n        self._process.join(1)\n        if self._process.is_alive():\n            os.kill(self._process.pid, signal.SIGKILL)\n\n    def setUp(self):\n        state_path = tempfile.mktemp(suffix=self.id())\n        luigi.configuration.get_config().set('scheduler', 'state_path', state_path)\n        self.start_server()\n\n    def tearDown(self):\n        self.stop_server()\n\n    def test_ping(self):\n        self.sch.ping(worker='xyz')\n\n    def test_raw_ping(self):\n        self.sch._request('\/api\/ping', {'worker': 'xyz'})\n\n    def test_raw_ping_extended(self):\n        self.sch._request('\/api\/ping', {'worker': 'xyz', 'foo': 'bar'})\n\n    def test_404(self):\n        with self.assertRaises(luigi.rpc.RPCError):\n            self.sch._request('\/api\/fdsfds', {'dummy': 1})\n\n    @skipOnTravis('https:\/\/travis-ci.org\/spotify\/luigi\/jobs\/72953884')\n    def test_save_state(self):\n        self.sch.add_task('X', 'B', deps=('A',))\n        self.sch.add_task('X', 'A')\n        self.assertEqual(self.sch.get_work('X')['task_id'], 'A')\n        self.stop_server()\n        self.start_server()\n        work = self.sch.get_work('X')['running_tasks'][0]\n        self.assertEqual(work['task_id'], 'A')\n\n\nif __name__ == '__main__':\n    unittest.main()\n","lang_cluster":"Python","length":129,"code_uid":"106d35b785b642699fe852528e71154f"}
{"diff_hunk":"@@ -74,22 +74,29 @@ class BibSortWasher(object):\n         \"\"\"\n         if not val:\n             return ''\n-        val_tokens = str(val).split(\" \", 1) #split in leading_word, phrase_without_leading_word\n-        if len(val_tokens) == 2 and val_tokens[0].lower() in LEADING_ARTICLES:\n-            return val_tokens[1].strip().lower()\n-        return val.lower()\n+        val = decode_to_unicode(val).lower().encode('UTF-8')\n+        val_tokens = val.split(\" \", 1) #split in leading_word, phrase_without_leading_word\n+        if len(val_tokens) == 2 and val_tokens[0].strip() in LEADING_ARTICLES:\n+            return val_tokens[1].strip()\n+        return val.strip()\n \n     def _sort_case_insensitive_strip_accents(self, val):\n         \"\"\"Remove accents and convert to lower case\"\"\"\n         if not val:\n             return ''\n-        return strip_accents(str(val).lower())\n+        return translate_to_ascii(val).pop().lower()\n+\n+    def _sort_nosymbols_case_insensitive_strip_accents(self, val):\n+        \"\"\"Remove accents, remove symbols, and convert to lower case\"\"\"\n+        if not val:\n+            return ''\n+        return ''.join(_RE_NOSYMBOLS.findall(translate_to_ascii(val).pop().lower()))\n \n     def _sort_case_insensitive(self, val):\n         \"\"\"Conversion to lower case\"\"\"\n         if not val:\n             return ''\n-        return str(val).lower()\n+        return decode_to_unicode(val).lower().encode('UTF-8')\n \n     def _sort_dates(self, val):\n         \"\"\"","old_code":"## -*- mode: python; coding: utf-8; -*-\n##\n## This file is part of Invenio.\n## Copyright (C) 2010, 2011, 2012 CERN.\n##\n## Invenio is free software; you can redistribute it and\/or\n## modify it under the terms of the GNU General Public License as\n## published by the Free Software Foundation; either version 2 of the\n## License, or (at your option) any later version.\n##\n## Invenio is distributed in the hope that it will be useful, but\n## WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n## General Public License for more details.\n##\n## You should have received a copy of the GNU General Public License\n## along with Invenio; if not, write to the Free Software Foundation, Inc.,\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"Applies a transformation function to a value\"\"\"\n\nfrom time import strptime\nfrom invenio.dateutils import strftime\nfrom invenio.textutils import strip_accents\n\nLEADING_ARTICLES = ['the', 'a', 'an', 'at', 'on', 'of']\n\n\nclass InvenioBibSortWasherNotImplementedError(Exception):\n    \"\"\"Exception raised when a washer method\n    defined in the bibsort config file is not implemented\"\"\"\n    pass\n\n\nclass BibSortWasher(object):\n    \"\"\"Implements all the washer methods\"\"\"\n\n    def __init__(self, washer):\n        self.washer = washer\n        fnc_name = '_' + washer\n        try:\n            self.washer_fnc = self.__getattribute__(fnc_name)\n        except AttributeError, err:\n            raise InvenioBibSortWasherNotImplementedError(err)\n\n    def get_washer(self):\n        \"\"\"Returns the washer name\"\"\"\n        return self.washer\n\n    def get_transformed_value(self, val):\n        \"\"\"Returns the value\"\"\"\n        return self.washer_fnc(val)\n\n    def _sort_alphanumerically_remove_leading_articles_strip_accents(self, val):\n        \"\"\"\n        Convert:\n        'The title' => 'title'\n        'A title' => 'title'\n        'Title' => 'title'\n        \"\"\"\n        if not val:\n            return ''\n        val_tokens = str(val).split(\" \", 1) #split in leading_word, phrase_without_leading_word\n        if len(val_tokens) == 2 and val_tokens[0].lower() in LEADING_ARTICLES:\n            return strip_accents(val_tokens[1].strip().lower())\n        return strip_accents(val.lower())\n\n    def _sort_alphanumerically_remove_leading_articles(self, val):\n        \"\"\"\n        Convert:\n        'The title' => 'title'\n        'A title' => 'title'\n        'Title' => 'title'\n        \"\"\"\n        if not val:\n            return ''\n        val_tokens = str(val).split(\" \", 1) #split in leading_word, phrase_without_leading_word\n        if len(val_tokens) == 2 and val_tokens[0].lower() in LEADING_ARTICLES:\n            return val_tokens[1].strip().lower()\n        return val.lower()\n\n    def _sort_case_insensitive_strip_accents(self, val):\n        \"\"\"Remove accents and convert to lower case\"\"\"\n        if not val:\n            return ''\n        return strip_accents(str(val).lower())\n\n    def _sort_case_insensitive(self, val):\n        \"\"\"Conversion to lower case\"\"\"\n        if not val:\n            return ''\n        return str(val).lower()\n\n    def _sort_dates(self, val):\n        \"\"\"\n        Convert:\n        '8 nov 2010' => '2010-11-08'\n        'nov 2010' => '2010-11-01'\n        '2010' => '2010-01-01'\n        \"\"\"\n        datetext_format = \"%Y-%m-%d\"\n        try:\n            datestruct = strptime(val, datetext_format)\n        except ValueError:\n            try:\n                datestruct = strptime(val, \"%d %b %Y\")\n            except ValueError:\n                try:\n                    datestruct = strptime(val, \"%b %Y\")\n                except ValueError:\n                    try:\n                        datestruct = strptime(val, \"%Y\")\n                    except ValueError:\n                        return val\n        return strftime(datetext_format, datestruct)\n\n    def _sort_numerically(self, val):\n        \"\"\"\n        Convert:\n        1245 => float(1245)\n        \"\"\"\n        try:\n            return float(val)\n        except ValueError:\n            return 0\n\n\ndef get_all_available_washers():\n    \"\"\"\n    Returns all the available washer functions without the leading '_'\n    \"\"\"\n    method_list = dir(BibSortWasher)\n    return [method[1:] for method in method_list if method.startswith('_') and method.find('__') < 0]\n","lang_cluster":"Python","length":133,"code_uid":"918ad097e1d9469687c4ae378d4fb60c"}
{"diff_hunk":"@@ -11,6 +11,7 @@ from localstack.services.cloudformation.service_models import (\n )\n from localstack.utils import common\n from localstack.utils.aws import aws_stack\n+from localstack.utils.common import short_uid\n \n \n class EventConnection(GenericBaseModel):","old_code":"import json\n\nfrom localstack.services.cloudformation.deployment_utils import (\n    PLACEHOLDER_RESOURCE_NAME,\n    select_parameters,\n)\nfrom localstack.services.cloudformation.service_models import (\n    REF_ATTRS,\n    REF_ID_ATTRS,\n    GenericBaseModel,\n)\nfrom localstack.utils import common\nfrom localstack.utils.aws import aws_stack\n\n\nclass EventConnection(GenericBaseModel):\n    @staticmethod\n    def cloudformation_type():\n        return \"AWS::Events::Connection\"\n\n    def fetch_state(self, stack_name, resources):\n        client = aws_stack.connect_to_service(\"events\")\n        conn_name = self.resolve_refs_recursively(stack_name, self.props.get(\"Name\"), resources)\n        return client.describe_connection(Name=conn_name)\n\n    def get_cfn_attribute(self, attribute_name):\n        props = self.props\n        if attribute_name in REF_ID_ATTRS:\n            return props.get(\"Name\")\n        if attribute_name == \"Arn\":\n            return props.get(\"ConnectionArn\")\n        # TODO: handle \"SecretArn\" attribute\n        return super(EventConnection, self).get_cfn_attribute(attribute_name)\n\n    @classmethod\n    def get_deploy_templates(cls):\n        return {\n            \"create\": {\"function\": \"create_connection\"},\n            \"delete\": {\"function\": \"delete_connection\", \"parameters\": {\"Name\": \"Name\"}},\n        }\n\n\nclass EventBus(GenericBaseModel):\n    @staticmethod\n    def cloudformation_type():\n        return \"AWS::Events::EventBus\"\n\n    def fetch_state(self, stack_name, resources):\n        event_bus_name = self.resolve_refs_recursively(\n            stack_name, self.props.get(\"Name\"), resources\n        )\n        client = aws_stack.connect_to_service(\"events\")\n        return client.describe_event_bus(Name=event_bus_name)\n\n    def get_cfn_attribute(self, attribute_name):\n        props = self.props\n        if attribute_name in REF_ATTRS + [\"Name\"]:\n            return props.get(\"Name\")\n        if attribute_name == \"Arn\":\n            return props.get(\"Arn\")\n        return super(EventBus, self).get_cfn_attribute(attribute_name)\n\n    @classmethod\n    def get_deploy_templates(cls):\n        return {\n            \"create\": {\"function\": \"create_event_bus\", \"parameters\": {\"Name\": \"Name\"}},\n            \"delete\": {\"function\": \"delete_event_bus\", \"parameters\": {\"Name\": \"Name\"}},\n        }\n\n\nclass EventsRule(GenericBaseModel):\n    @staticmethod\n    def cloudformation_type():\n        return \"AWS::Events::Rule\"\n\n    def get_cfn_attribute(self, attribute_name):\n        if attribute_name == \"Arn\":\n            return self.params.get(\"Arn\") or aws_stack.events_rule_arn(self.params.get(\"Name\"))\n        return super(EventsRule, self).get_cfn_attribute(attribute_name)\n\n    def get_physical_resource_id(self, attribute=None, **kwargs):\n        return self.props.get(\"Name\")\n\n    def fetch_state(self, stack_name, resources):\n        rule_name = self.resolve_refs_recursively(stack_name, self.props.get(\"Name\"), resources)\n        result = aws_stack.connect_to_service(\"events\").describe_rule(Name=rule_name) or {}\n        return result if result.get(\"Name\") else None\n\n    @classmethod\n    def get_deploy_templates(cls):\n        def events_put_rule_params(params, **kwargs):\n            attrs = [\n                \"ScheduleExpression\",\n                \"EventPattern\",\n                \"State\",\n                \"Description\",\n                \"Name\",\n                \"EventBusName\",\n            ]\n            result = select_parameters(*attrs)(params, **kwargs)\n            result[\"Name\"] = result.get(\"Name\") or PLACEHOLDER_RESOURCE_NAME\n\n            def wrap_in_lists(o, **kwargs):\n                if isinstance(o, dict):\n                    for k, v in o.items():\n                        if not isinstance(v, (dict, list)):\n                            o[k] = [v]\n                return o\n\n            pattern = result.get(\"EventPattern\")\n            if isinstance(pattern, dict):\n                wrapped = common.recurse_object(pattern, wrap_in_lists)\n                result[\"EventPattern\"] = json.dumps(wrapped)\n            return result\n\n        return {\n            \"create\": [\n                {\"function\": \"put_rule\", \"parameters\": events_put_rule_params},\n                {\n                    \"function\": \"put_targets\",\n                    \"parameters\": {\n                        \"Rule\": PLACEHOLDER_RESOURCE_NAME,\n                        \"EventBusName\": \"EventBusName\",\n                        \"Targets\": \"Targets\",\n                    },\n                },\n            ],\n            \"delete\": {\n                \"function\": \"delete_rule\",\n                \"parameters\": {\"Name\": \"PhysicalResourceId\"},\n            },\n        }\n","lang_cluster":"Python","length":132,"code_uid":"ed4442e9d82e4bb3a9fa0ddbdab2cc7e"}
{"diff_hunk":"@@ -44,6 +44,15 @@ class DetectionBlock(nn.Module):\n         self.conv2 = ConvModule(\n             out_channels, double_out_channels, 3, padding=1, **cfg)\n         self.conv3 = ConvModule(double_out_channels, out_channels, 1, **cfg)\n+\n+        if self.spp_on:\n+            self.poolers = [\n+                nn.MaxPool2d(size, 1, padding=(size - 1) \/\/ 2)\n+                for size in spp_scales\n+            ]\n+            self.conv_spp = ConvModule(out_channels * (len(spp_scales) + 1),\n+                                       out_channels, 1, **cfg)\n+\n         self.conv4 = ConvModule(\n             out_channels, double_out_channels, 3, padding=1, **cfg)\n         self.conv5 = ConvModule(double_out_channels, out_channels, 1, **cfg)","old_code":"# Copyright (c) 2019 Western Digital Corporation or its affiliates.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule\n\nfrom ..builder import NECKS\n\n\nclass DetectionBlock(nn.Module):\n    \"\"\"Detection block in YOLO neck.\n\n    Let out_channels = n, the DetectionBlock contains:\n    Six ConvLayers, 1 Conv2D Layer and 1 YoloLayer.\n    The first 6 ConvLayers are formed the following way:\n        1x1xn, 3x3x2n, 1x1xn, 3x3x2n, 1x1xn, 3x3x2n.\n    The Conv2D layer is 1x1x255.\n    Some block will have branch after the fifth ConvLayer.\n    The input channel is arbitrary (in_channels)\n\n    Args:\n        in_channels (int): The number of input channels.\n        out_channels (int): The number of output channels.\n        conv_cfg (dict): Config dict for convolution layer. Default: None.\n        norm_cfg (dict): Dictionary to construct and config norm layer.\n            Default: dict(type='BN', requires_grad=True)\n        act_cfg (dict): Config dict for activation layer.\n            Default: dict(type='LeakyReLU', negative_slope=0.1).\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 conv_cfg=None,\n                 norm_cfg=dict(type='BN', requires_grad=True),\n                 act_cfg=dict(type='LeakyReLU', negative_slope=0.1)):\n        super(DetectionBlock, self).__init__()\n        double_out_channels = out_channels * 2\n\n        # shortcut\n        cfg = dict(conv_cfg=conv_cfg, norm_cfg=norm_cfg, act_cfg=act_cfg)\n        self.conv1 = ConvModule(in_channels, out_channels, 1, **cfg)\n        self.conv2 = ConvModule(\n            out_channels, double_out_channels, 3, padding=1, **cfg)\n        self.conv3 = ConvModule(double_out_channels, out_channels, 1, **cfg)\n        self.conv4 = ConvModule(\n            out_channels, double_out_channels, 3, padding=1, **cfg)\n        self.conv5 = ConvModule(double_out_channels, out_channels, 1, **cfg)\n\n    def forward(self, x):\n        tmp = self.conv1(x)\n        tmp = self.conv2(tmp)\n        tmp = self.conv3(tmp)\n        tmp = self.conv4(tmp)\n        out = self.conv5(tmp)\n        return out\n\n\n@NECKS.register_module()\nclass YOLOV3Neck(nn.Module):\n    \"\"\"The neck of YOLOV3.\n\n    It can be treated as a simplified version of FPN. It\n    will take the result from Darknet backbone and do some upsampling and\n    concatenation. It will finally output the detection result.\n\n    Note:\n        The input feats should be from top to bottom.\n            i.e., from high-lvl to low-lvl\n        But YOLOV3Neck will process them in reversed order.\n            i.e., from bottom (high-lvl) to top (low-lvl)\n\n    Args:\n        num_scales (int): The number of scales \/ stages.\n        in_channels (int): The number of input channels.\n        out_channels (int): The number of output channels.\n        conv_cfg (dict): Config dict for convolution layer. Default: None.\n        norm_cfg (dict): Dictionary to construct and config norm layer.\n            Default: dict(type='BN', requires_grad=True)\n        act_cfg (dict): Config dict for activation layer.\n            Default: dict(type='LeakyReLU', negative_slope=0.1).\n    \"\"\"\n\n    def __init__(self,\n                 num_scales,\n                 in_channels,\n                 out_channels,\n                 conv_cfg=None,\n                 norm_cfg=dict(type='BN', requires_grad=True),\n                 act_cfg=dict(type='LeakyReLU', negative_slope=0.1)):\n        super(YOLOV3Neck, self).__init__()\n        assert (num_scales == len(in_channels) == len(out_channels))\n        self.num_scales = num_scales\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        # shortcut\n        cfg = dict(conv_cfg=conv_cfg, norm_cfg=norm_cfg, act_cfg=act_cfg)\n\n        # To support arbitrary scales, the code looks awful, but it works.\n        # Better solution is welcomed.\n        self.detect1 = DetectionBlock(in_channels[0], out_channels[0], **cfg)\n        for i in range(1, self.num_scales):\n            in_c, out_c = self.in_channels[i], self.out_channels[i]\n            self.add_module(f'conv{i}', ConvModule(in_c, out_c, 1, **cfg))\n            # in_c + out_c : High-lvl feats will be cat with low-lvl feats\n            self.add_module(f'detect{i+1}',\n                            DetectionBlock(in_c + out_c, out_c, **cfg))\n\n    def forward(self, feats):\n        assert len(feats) == self.num_scales\n\n        # processed from bottom (high-lvl) to top (low-lvl)\n        outs = []\n        out = self.detect1(feats[-1])\n        outs.append(out)\n\n        for i, x in enumerate(reversed(feats[:-1])):\n            conv = getattr(self, f'conv{i+1}')\n            tmp = conv(out)\n\n            # Cat with low-lvl feats\n            tmp = F.interpolate(tmp, scale_factor=2)\n            tmp = torch.cat((tmp, x), 1)\n\n            detect = getattr(self, f'detect{i+2}')\n            out = detect(tmp)\n            outs.append(out)\n\n        return tuple(outs)\n\n    def init_weights(self):\n        \"\"\"Initialize the weights of module.\"\"\"\n        # init is done in ConvModule\n        pass\n","lang_cluster":"Python","length":136,"code_uid":"781a2952df0941959a42733abda5e4cf"}
{"diff_hunk":"@@ -71,8 +71,10 @@ def _file_lines(path):\n \n \n DOT_FILES = [\"packages_No_Name.dot\", \"classes_No_Name.dot\"]\n+COLORIZED_DOT_FILES = [\"packages_colorized.dot\", \"classes_colorized.dot\"]\n VCG_FILES = [\"packages_No_Name.vcg\", \"classes_No_Name.vcg\"]\n PUML_FILES = [\"packages_No_Name.puml\", \"classes_No_Name.puml\"]\n+COLORIZED_PUML_FILES = [\"packages_colorized.puml\", \"classes_colorized.puml\"]\n \n \n @pytest.fixture()","old_code":"# Copyright (c) 2008, 2010, 2013 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2014-2018, 2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2014 Arun Persaud <arun@nubati.net>\n# Copyright (c) 2015 Ionel Cristian Maries <contact@ionelmc.ro>\n# Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2018 Ville Skytt\u00e4 <ville.skytta@iki.fi>\n# Copyright (c) 2019-2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2019 Ashley Whetter <ashley@awhetter.co.uk>\n# Copyright (c) 2020 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2020 Anthony Sottile <asottile@umich.edu>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Mark Byrne <31762852+mbyrnepr2@users.noreply.github.com>\n# Copyright (c) 2021 Andreas Finkler <andi.finkler@gmail.com>\n\n# Licensed under the GPL: https:\/\/www.gnu.org\/licenses\/old-licenses\/gpl-2.0.html\n# For details: https:\/\/github.com\/PyCQA\/pylint\/blob\/main\/LICENSE\n\n\"\"\"\nUnit test for ``DiagramWriter``\n\"\"\"\n\n\nimport codecs\nimport os\nfrom difflib import unified_diff\n\nimport pytest\n\nfrom pylint.pyreverse.diadefslib import DefaultDiadefGenerator, DiadefsHandler\nfrom pylint.pyreverse.inspector import Linker\nfrom pylint.pyreverse.writer import DiagramWriter\n\n_DEFAULTS = {\n    \"all_ancestors\": None,\n    \"show_associated\": None,\n    \"module_names\": None,\n    \"output_format\": \"dot\",\n    \"diadefs_file\": None,\n    \"quiet\": 0,\n    \"show_ancestors\": None,\n    \"classes\": (),\n    \"all_associated\": None,\n    \"mode\": \"PUB_ONLY\",\n    \"show_builtin\": False,\n    \"only_classnames\": False,\n    \"output_directory\": \"\",\n}\n\n\nclass Config:\n    \"\"\"config object for tests\"\"\"\n\n    def __init__(self):\n        for attr, value in _DEFAULTS.items():\n            setattr(self, attr, value)\n\n\ndef _file_lines(path):\n    # we don't care about the actual encoding, but python3 forces us to pick one\n    with codecs.open(path, encoding=\"latin1\") as stream:\n        lines = [\n            line.strip()\n            for line in stream.readlines()\n            if (\n                line.find(\"squeleton generated by \") == -1\n                and not line.startswith('__revision__ = \"$Id:')\n            )\n        ]\n    return [line for line in lines if line]\n\n\nDOT_FILES = [\"packages_No_Name.dot\", \"classes_No_Name.dot\"]\nVCG_FILES = [\"packages_No_Name.vcg\", \"classes_No_Name.vcg\"]\nPUML_FILES = [\"packages_No_Name.puml\", \"classes_No_Name.puml\"]\n\n\n@pytest.fixture()\ndef setup_dot(default_config, get_project):\n    writer = DiagramWriter(default_config)\n    project = get_project(os.path.join(os.path.dirname(__file__), \"..\", \"data\"))\n    yield from _setup(project, default_config, writer)\n\n\n@pytest.fixture()\ndef setup_vcg(vcg_config, get_project):\n    writer = DiagramWriter(vcg_config)\n    project = get_project(os.path.join(os.path.dirname(__file__), \"..\", \"data\"))\n    yield from _setup(project, vcg_config, writer)\n\n\n@pytest.fixture()\ndef setup_puml(puml_config, get_project):\n    writer = DiagramWriter(puml_config)\n    project = get_project(os.path.join(os.path.dirname(__file__), \"..\", \"data\"))\n    yield from _setup(project, puml_config, writer)\n\n\ndef _setup(project, config, writer):\n    linker = Linker(project)\n    handler = DiadefsHandler(config)\n    dd = DefaultDiadefGenerator(linker, handler).visit(project)\n    for diagram in dd:\n        diagram.extract_relationships()\n    writer.write(dd)\n    yield\n    for fname in DOT_FILES + VCG_FILES + PUML_FILES:\n        try:\n            os.remove(fname)\n        except FileNotFoundError:\n            continue\n\n\n@pytest.mark.usefixtures(\"setup_dot\")\n@pytest.mark.parametrize(\"generated_file\", DOT_FILES)\ndef test_dot_files(generated_file):\n    _assert_files_are_equal(generated_file)\n\n\n@pytest.mark.usefixtures(\"setup_vcg\")\n@pytest.mark.parametrize(\"generated_file\", VCG_FILES)\ndef test_vcg_files(generated_file):\n    _assert_files_are_equal(generated_file)\n\n\n@pytest.mark.usefixtures(\"setup_puml\")\n@pytest.mark.parametrize(\"generated_file\", PUML_FILES)\ndef test_puml_files(generated_file):\n    _assert_files_are_equal(generated_file)\n\n\ndef _assert_files_are_equal(generated_file):\n    expected_file = os.path.join(os.path.dirname(__file__), \"data\", generated_file)\n    generated = _file_lines(generated_file)\n    expected = _file_lines(expected_file)\n    generated = \"\\n\".join(generated)\n    expected = \"\\n\".join(expected)\n    files = f\"\\n *** expected : {expected_file}, generated : {generated_file} \\n\"\n    diff = \"\\n\".join(\n        line for line in unified_diff(expected.splitlines(), generated.splitlines())\n    )\n    assert expected == generated, f\"{files}{diff}\"\n","lang_cluster":"Python","length":142,"code_uid":"604082edf9864aa19ab35b78861729a5"}
{"diff_hunk":"@@ -29,4 +29,45 @@ def pre_process_resource_data(resource_data_iterator,\n         DataFrame: DataFrame table with all the resource_data.\n     \"\"\"\n \n-    return None\n+    df = pd.DataFrame(resource_data_json)\n+\n+    existing_columns = df.columns\n+\n+    new_columns = list(set(existing_columns).intersection(selected_features))\n+\n+    df = df[new_columns]\n+\n+    return df\n+\n+\n+if __name__ == '__main__':\n+    import json\n+    from os import sys, path\n+    import sys\n+\n+\n+    sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))\n+    print sys.path\n+\n+    from resources.firewall_rule import FirewallRule\n+\n+\n+    with open('..\/sample_datasets\/dataset_firewall.json') as firewall_dataset:\n+        firewall_rules = json.load(firewall_dataset)\n+        firewall_rules_data = [i.get('data') for i in firewall_rules]\n+\n+        flattened_firewall_rules = FirewallRule.flatten_firewall_rules(firewall_rules_data)\n+        flattened_firewall_rules_dict = [i.to_dict() for i in flattened_firewall_rules]\n+\n+        df_filtered = pre_process_firewall_data(\n+            flattened_firewall_rules_dict,\n+            ['priority',\n+             'ip_addr',\n+             'ip_bits',\n+             'identifier',\n+             'action',\n+             'ip_protocol',\n+             'ports',\n+             'direction',\n+             'disabled'])\n+        print df_filtered","old_code":"# Copyright 2017 The Forseti Security Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\ndef pre_process_resource_data(resource_data_iterator,\n                              resource_type,\n                              selected_features):\n    \"\"\"Pre process resource data.\n\n    Args:\n        resource_data_iterator (list): An iterator that will iterate through\n            all the resource data. Resource data is in JSON string format.\n        resource_type (ResourceType): The type of the resource data.\n        selected_features (list): A list of selected features, if the\n            list is empty, we will include all the features.\n\n    Returns:\n        DataFrame: DataFrame table with all the resource_data.\n    \"\"\"\n\n    return None\n","lang_cluster":"Python","length":32,"code_uid":"25a28fadf1bd49eb9e57ba3dbe3e631f"}
{"diff_hunk":"@@ -80,6 +80,13 @@ class EvalHook(Hook):\n         for name, val in eval_res.items():\n             runner.log_buffer.output[name] = val\n         runner.log_buffer.ready = True\n+        if self.save_best is not None:\n+            if self.key_indicator == 'auto':\n+                # infer from eval_results\n+                self._init_rule(self.rule, list(eval_res.keys())[0])\n+            return eval_res[self.key_indicator]\n+        else:\n+            return None\n \n \n class DistEvalHook(EvalHook):","old_code":"import os.path as osp\nimport warnings\n\nfrom mmcv.runner import Hook\nfrom torch.utils.data import DataLoader\n\n\nclass EvalHook(Hook):\n    \"\"\"Evaluation hook.\n\n    Notes:\n        If new arguments are added for EvalHook, tools\/test.py may be\n    effected.\n\n    Attributes:\n        dataloader (DataLoader): A PyTorch dataloader.\n        start (int, optional): Evaluation starting epoch. It enables evaluation\n            before the training starts if ``start`` <= the resuming epoch.\n            If None, whether to evaluate is merely decided by ``interval``.\n            Default: None.\n        interval (int): Evaluation interval (by epochs). Default: 1.\n        **eval_kwargs: Evaluation arguments fed into the evaluate function of\n            the dataset.\n    \"\"\"\n\n    def __init__(self, dataloader, start=None, interval=1, **eval_kwargs):\n        if not isinstance(dataloader, DataLoader):\n            raise TypeError('dataloader must be a pytorch DataLoader, but got'\n                            f' {type(dataloader)}')\n        if not interval > 0:\n            raise ValueError(f'interval must be positive, but got {interval}')\n        if start is not None and start < 0:\n            warnings.warn(\n                f'The evaluation start epoch {start} is smaller than 0, '\n                f'use 0 instead', UserWarning)\n            start = 0\n        self.dataloader = dataloader\n        self.interval = interval\n        self.start = start\n        self.eval_kwargs = eval_kwargs\n        self.initial_epoch_flag = True\n\n    def before_train_epoch(self, runner):\n        \"\"\"Evaluate the model only at the start of training.\"\"\"\n        if not self.initial_epoch_flag:\n            return\n        if self.start is not None and runner.epoch >= self.start:\n            self.after_train_epoch(runner)\n        self.initial_epoch_flag = False\n\n    def evaluation_flag(self, runner):\n        \"\"\"Judge whether to perform_evaluation after this epoch.\n\n        Returns:\n            bool: The flag indicating whether to perform evaluation.\n        \"\"\"\n        if self.start is None:\n            if not self.every_n_epochs(runner, self.interval):\n                # No evaluation during the interval epochs.\n                return False\n        elif (runner.epoch + 1) < self.start:\n            # No evaluation if start is larger than the current epoch.\n            return False\n        else:\n            # Evaluation only at epochs 3, 5, 7... if start==3 and interval==2\n            if (runner.epoch + 1 - self.start) % self.interval:\n                return False\n        return True\n\n    def after_train_epoch(self, runner):\n        if not self.evaluation_flag(runner):\n            return\n        from mmdet.apis import single_gpu_test\n        results = single_gpu_test(runner.model, self.dataloader, show=False)\n        self.evaluate(runner, results)\n\n    def evaluate(self, runner, results):\n        eval_res = self.dataloader.dataset.evaluate(\n            results, logger=runner.logger, **self.eval_kwargs)\n        for name, val in eval_res.items():\n            runner.log_buffer.output[name] = val\n        runner.log_buffer.ready = True\n\n\nclass DistEvalHook(EvalHook):\n    \"\"\"Distributed evaluation hook.\n\n    Notes:\n        If new arguments are added, tools\/test.py may be effected.\n\n    Attributes:\n        dataloader (DataLoader): A PyTorch dataloader.\n        start (int, optional): Evaluation starting epoch. It enables evaluation\n            before the training starts if ``start`` <= the resuming epoch.\n            If None, whether to evaluate is merely decided by ``interval``.\n            Default: None.\n        interval (int): Evaluation interval (by epochs). Default: 1.\n        tmpdir (str | None): Temporary directory to save the results of all\n            processes. Default: None.\n        gpu_collect (bool): Whether to use gpu or cpu to collect results.\n            Default: False.\n        **eval_kwargs: Evaluation arguments fed into the evaluate function of\n            the dataset.\n    \"\"\"\n\n    def __init__(self,\n                 dataloader,\n                 start=None,\n                 interval=1,\n                 tmpdir=None,\n                 gpu_collect=False,\n                 **eval_kwargs):\n        super().__init__(\n            dataloader, start=start, interval=interval, **eval_kwargs)\n        self.tmpdir = tmpdir\n        self.gpu_collect = gpu_collect\n\n    def after_train_epoch(self, runner):\n        if not self.evaluation_flag(runner):\n            return\n        from mmdet.apis import multi_gpu_test\n        tmpdir = self.tmpdir\n        if tmpdir is None:\n            tmpdir = osp.join(runner.work_dir, '.eval_hook')\n        results = multi_gpu_test(\n            runner.model,\n            self.dataloader,\n            tmpdir=tmpdir,\n            gpu_collect=self.gpu_collect)\n        if runner.rank == 0:\n            print('\\n')\n            self.evaluate(runner, results)\n","lang_cluster":"Python","length":132,"code_uid":"04c1139a23c1418b949c2816c7bfff74"}
{"diff_hunk":"@@ -17,6 +17,7 @@\n \n from selenium.webdriver.chromium.options import ChromiumOptions\n from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n+from typing import Optional, NoReturn\n \n \n class Options(ChromiumOptions):","old_code":"# Licensed to the Software Freedom Conservancy (SFC) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The SFC licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom selenium.webdriver.chromium.options import ChromiumOptions\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n\n\nclass Options(ChromiumOptions):\n\n    @property\n    def default_capabilities(self) -> dict:\n        return DesiredCapabilities.CHROME.copy()\n\n    def enable_mobile(self, android_package=\"com.android.chrome\", android_activity=None, device_serial=None):\n        super().enable_mobile(android_package, android_activity, device_serial)\n","lang_cluster":"Python","length":29,"code_uid":"9db26ffe033c4061a67c7c0c77d6c837"}
{"diff_hunk":"@@ -71,6 +71,9 @@ public class GraphqlHandlerTest {\n \n     SessionMap sessions = new LocalSessionMap(tracer, events);\n     distributor = new LocalDistributor(tracer, events, clientFactory, sessions, registrationSecret);\n+    stereotype = new ImmutableCapabilities(\"browserName\", \"cheese\");\n+    caps = new ImmutableCapabilities(\"browserName\", \"cheese\");\n+    payload = NewSessionPayload.create(caps);\n   }\n \n   @Test","old_code":"\/\/ Licensed to the Software Freedom Conservancy (SFC) under one\n\/\/ or more contributor license agreements.  See the NOTICE file\n\/\/ distributed with this work for additional information\n\/\/ regarding copyright ownership.  The SFC licenses this file\n\/\/ to you under the Apache License, Version 2.0 (the\n\/\/ \"License\"); you may not use this file except in compliance\n\/\/ with the License.  You may obtain a copy of the License at\n\/\/\n\/\/   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing,\n\/\/ software distributed under the License is distributed on an\n\/\/ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\/\/ KIND, either express or implied.  See the License for the\n\/\/ specific language governing permissions and limitations\n\/\/ under the License.\n\npackage org.openqa.selenium.grid.graphql;\n\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.openqa.selenium.Capabilities;\nimport org.openqa.selenium.ImmutableCapabilities;\nimport org.openqa.selenium.events.EventBus;\nimport org.openqa.selenium.events.local.GuavaEventBus;\nimport org.openqa.selenium.grid.data.CreateSessionRequest;\nimport org.openqa.selenium.grid.distributor.Distributor;\nimport org.openqa.selenium.grid.distributor.local.LocalDistributor;\nimport org.openqa.selenium.grid.node.ActiveSession;\nimport org.openqa.selenium.grid.node.Node;\nimport org.openqa.selenium.grid.node.SessionFactory;\nimport org.openqa.selenium.grid.node.local.LocalNode;\nimport org.openqa.selenium.grid.security.Secret;\nimport org.openqa.selenium.grid.sessionmap.SessionMap;\nimport org.openqa.selenium.grid.sessionmap.local.LocalSessionMap;\nimport org.openqa.selenium.json.Json;\nimport org.openqa.selenium.remote.http.Contents;\nimport org.openqa.selenium.remote.http.HttpClient;\nimport org.openqa.selenium.remote.http.HttpHandler;\nimport org.openqa.selenium.remote.http.HttpRequest;\nimport org.openqa.selenium.remote.http.HttpResponse;\nimport org.openqa.selenium.remote.tracing.DefaultTestTracer;\nimport org.openqa.selenium.remote.tracing.Tracer;\n\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.openqa.selenium.json.Json.MAP_TYPE;\nimport static org.openqa.selenium.remote.http.HttpMethod.GET;\n\npublic class GraphqlHandlerTest {\n\n  private final Secret registrationSecret = new Secret(\"stilton\");\n  private final URI publicUri = new URI(\"http:\/\/example.com\/grid-o-matic\");\n  private Distributor distributor;\n  private Tracer tracer;\n  private EventBus events;\n\n  public GraphqlHandlerTest() throws URISyntaxException {\n  }\n\n  @Before\n  public void setupGrid() {\n    tracer = DefaultTestTracer.createTracer();\n    events = new GuavaEventBus();\n    HttpClient.Factory clientFactory = HttpClient.Factory.createDefault();\n\n    SessionMap sessions = new LocalSessionMap(tracer, events);\n    distributor = new LocalDistributor(tracer, events, clientFactory, sessions, registrationSecret);\n  }\n\n  @Test\n  public void shouldBeAbleToGetGridUri() {\n    GraphqlHandler handler = new GraphqlHandler(distributor, publicUri);\n\n    Map<String, Object> topLevel = executeQuery(handler, \"{ grid { uri } }\");\n\n    assertThat(topLevel).isEqualTo(Map.of(\"data\", Map.of(\"grid\", Map.of(\"uri\", publicUri.toString()))));\n  }\n\n  @Test\n  public void shouldReturnAnEmptyListForNodesIfNoneAreRegistered() {\n    GraphqlHandler handler = new GraphqlHandler(distributor, publicUri);\n\n    Map<String, Object> topLevel = executeQuery(handler, \"{ grid { nodes { uri } } }\");\n\n    assertThat(topLevel)\n      .describedAs(topLevel.toString())\n      .isEqualTo(Map.of(\"data\", Map.of(\"grid\", Map.of(\"nodes\", List.of()))));\n  }\n\n  @Test\n  public void shouldBeAbleToGetUrlsOfAllNodes() throws URISyntaxException {\n    Capabilities stereotype = new ImmutableCapabilities(\"cheese\", \"stilton\");\n    String nodeUri = \"http:\/\/localhost:5556\";\n    Node node = LocalNode.builder(tracer, events, new URI(nodeUri), publicUri, registrationSecret)\n      .add(stereotype, new SessionFactory() {\n        @Override\n        public Optional<ActiveSession> apply(CreateSessionRequest createSessionRequest) {\n          return Optional.empty();\n        }\n\n        @Override\n        public boolean test(Capabilities capabilities) {\n          return false;\n        }\n      })\n      .build();\n    distributor.add(node);\n\n    GraphqlHandler handler = new GraphqlHandler(distributor, publicUri);\n    Map<String, Object> topLevel = executeQuery(handler, \"{ grid { nodes { uri } } }\");\n\n    assertThat(topLevel)\n      .describedAs(topLevel.toString())\n      .isEqualTo(Map.of(\"data\", Map.of(\"grid\", Map.of(\"nodes\", List.of(Map.of(\"uri\", nodeUri))))));\n  }\n\n  private Map<String, Object> executeQuery(HttpHandler handler, String query) {\n    HttpResponse res = handler.execute(\n      new HttpRequest(GET, \"\/graphql\")\n        .setContent(Contents.asJson(Map.of(\"query\", query))));\n\n    return new Json().toType(Contents.string(res), MAP_TYPE);\n  }\n}\n","lang_cluster":"Python","length":130,"code_uid":"aef092f5fa5641afb443b56c387f95e6"}
{"diff_hunk":"@@ -24,13 +24,15 @@ Unit test for ``DiagramWriter``\n import codecs\n import os\n from difflib import unified_diff\n+from typing import Callable, Iterator, List\n from unittest.mock import Mock\n \n import pytest\n \n from pylint.pyreverse.diadefslib import DefaultDiadefGenerator, DiadefsHandler\n-from pylint.pyreverse.inspector import Linker\n+from pylint.pyreverse.inspector import Linker, Project\n from pylint.pyreverse.writer import DiagramWriter\n+from pylint.testutils.pyreverse import PyreverseConfig\n \n _DEFAULTS = {\n     \"all_ancestors\": None,","old_code":"# Copyright (c) 2008, 2010, 2013 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2014-2018, 2020 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014 Google, Inc.\n# Copyright (c) 2014 Arun Persaud <arun@nubati.net>\n# Copyright (c) 2015 Ionel Cristian Maries <contact@ionelmc.ro>\n# Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n# Copyright (c) 2018 Ville Skytt\u00e4 <ville.skytta@iki.fi>\n# Copyright (c) 2019-2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2019 Ashley Whetter <ashley@awhetter.co.uk>\n# Copyright (c) 2020 hippo91 <guillaume.peillex@gmail.com>\n# Copyright (c) 2020 Anthony Sottile <asottile@umich.edu>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Mark Byrne <31762852+mbyrnepr2@users.noreply.github.com>\n# Copyright (c) 2021 Andreas Finkler <andi.finkler@gmail.com>\n\n# Licensed under the GPL: https:\/\/www.gnu.org\/licenses\/old-licenses\/gpl-2.0.html\n# For details: https:\/\/github.com\/PyCQA\/pylint\/blob\/main\/LICENSE\n\n\"\"\"\nUnit test for ``DiagramWriter``\n\"\"\"\n\n\nimport codecs\nimport os\nfrom difflib import unified_diff\nfrom unittest.mock import Mock\n\nimport pytest\n\nfrom pylint.pyreverse.diadefslib import DefaultDiadefGenerator, DiadefsHandler\nfrom pylint.pyreverse.inspector import Linker\nfrom pylint.pyreverse.writer import DiagramWriter\n\n_DEFAULTS = {\n    \"all_ancestors\": None,\n    \"show_associated\": None,\n    \"module_names\": None,\n    \"output_format\": \"dot\",\n    \"diadefs_file\": None,\n    \"quiet\": 0,\n    \"show_ancestors\": None,\n    \"classes\": (),\n    \"all_associated\": None,\n    \"mode\": \"PUB_ONLY\",\n    \"show_builtin\": False,\n    \"only_classnames\": False,\n    \"output_directory\": \"\",\n}\n\n\nclass Config:\n    \"\"\"config object for tests\"\"\"\n\n    def __init__(self):\n        for attr, value in _DEFAULTS.items():\n            setattr(self, attr, value)\n\n\ndef _file_lines(path):\n    # we don't care about the actual encoding, but python3 forces us to pick one\n    with codecs.open(path, encoding=\"latin1\") as stream:\n        lines = [\n            line.strip()\n            for line in stream.readlines()\n            if (\n                line.find(\"squeleton generated by \") == -1\n                and not line.startswith('__revision__ = \"$Id:')\n            )\n        ]\n    return [line for line in lines if line]\n\n\nDOT_FILES = [\"packages_No_Name.dot\", \"classes_No_Name.dot\"]\nCOLORIZED_DOT_FILES = [\"packages_colorized.dot\", \"classes_colorized.dot\"]\nVCG_FILES = [\"packages_No_Name.vcg\", \"classes_No_Name.vcg\"]\nPUML_FILES = [\"packages_No_Name.puml\", \"classes_No_Name.puml\"]\nCOLORIZED_PUML_FILES = [\"packages_colorized.puml\", \"classes_colorized.puml\"]\n\n\n@pytest.fixture()\ndef setup_dot(default_config, get_project):\n    writer = DiagramWriter(default_config)\n    project = get_project(os.path.join(os.path.dirname(__file__), \"..\", \"data\"))\n    yield from _setup(project, default_config, writer)\n\n\n@pytest.fixture()\ndef setup_colorized_dot(colorized_dot_config, get_project):\n    writer = DiagramWriter(colorized_dot_config)\n    project = get_project(\n        os.path.join(os.path.dirname(__file__), \"..\", \"data\"), name=\"colorized\"\n    )\n    yield from _setup(project, colorized_dot_config, writer)\n\n\n@pytest.fixture()\ndef setup_vcg(vcg_config, get_project):\n    writer = DiagramWriter(vcg_config)\n    project = get_project(os.path.join(os.path.dirname(__file__), \"..\", \"data\"))\n    yield from _setup(project, vcg_config, writer)\n\n\n@pytest.fixture()\ndef setup_puml(puml_config, get_project):\n    writer = DiagramWriter(puml_config)\n    project = get_project(os.path.join(os.path.dirname(__file__), \"..\", \"data\"))\n    yield from _setup(project, puml_config, writer)\n\n\n@pytest.fixture()\ndef setup_colorized_puml(colorized_puml_config, get_project):\n    writer = DiagramWriter(colorized_puml_config)\n    project = get_project(\n        os.path.join(os.path.dirname(__file__), \"..\", \"data\"), name=\"colorized\"\n    )\n    yield from _setup(project, colorized_puml_config, writer)\n\n\ndef _setup(project, config, writer):\n    linker = Linker(project)\n    handler = DiadefsHandler(config)\n    dd = DefaultDiadefGenerator(linker, handler).visit(project)\n    for diagram in dd:\n        diagram.extract_relationships()\n    writer.write(dd)\n    yield\n    for fname in (\n        DOT_FILES + COLORIZED_DOT_FILES + VCG_FILES + PUML_FILES + COLORIZED_PUML_FILES\n    ):\n        try:\n            os.remove(fname)\n        except FileNotFoundError:\n            continue\n\n\n@pytest.mark.usefixtures(\"setup_dot\")\n@pytest.mark.parametrize(\"generated_file\", DOT_FILES)\ndef test_dot_files(generated_file):\n    _assert_files_are_equal(generated_file)\n\n\n@pytest.mark.usefixtures(\"setup_colorized_dot\")\n@pytest.mark.parametrize(\"generated_file\", COLORIZED_DOT_FILES)\ndef test_colorized_dot_files(generated_file):\n    _assert_files_are_equal(generated_file)\n\n\n@pytest.mark.usefixtures(\"setup_vcg\")\n@pytest.mark.parametrize(\"generated_file\", VCG_FILES)\ndef test_vcg_files(generated_file):\n    _assert_files_are_equal(generated_file)\n\n\n@pytest.mark.usefixtures(\"setup_puml\")\n@pytest.mark.parametrize(\"generated_file\", PUML_FILES)\ndef test_puml_files(generated_file):\n    _assert_files_are_equal(generated_file)\n\n\n@pytest.mark.usefixtures(\"setup_colorized_puml\")\n@pytest.mark.parametrize(\"generated_file\", COLORIZED_PUML_FILES)\ndef test_colorized_puml_files(generated_file):\n    _assert_files_are_equal(generated_file)\n\n\ndef _assert_files_are_equal(generated_file):\n    expected_file = os.path.join(os.path.dirname(__file__), \"data\", generated_file)\n    generated = _file_lines(generated_file)\n    expected = _file_lines(expected_file)\n    generated = \"\\n\".join(generated)\n    expected = \"\\n\".join(expected)\n    files = f\"\\n *** expected : {expected_file}, generated : {generated_file} \\n\"\n    diff = \"\\n\".join(\n        line for line in unified_diff(expected.splitlines(), generated.splitlines())\n    )\n    assert expected == generated, f\"{files}{diff}\"\n\n\ndef test_color_for_stdlib_module(default_config):\n    writer = DiagramWriter(default_config)\n    obj = Mock()\n    obj.node = Mock()\n    obj.node.qname.return_value = \"collections\"\n    assert writer.get_shape_color(obj) == \"grey\"\n","lang_cluster":"Python","length":185,"code_uid":"49359558938e4ebebada9e4936406e45"}
{"diff_hunk":"@@ -11,9 +11,9 @@ def get_listening_challenge(source, course):\n     return [\n         {\n             \"type\": \"listeningExercise\",\n-            \"answer\": source.in_target_language[0],\n-            \"meaning\": source.in_source_language[0],\n-            \"audio\": audio_id(course.target_language, source.in_target_language[0]),\n+            \"answer\": _remove_special_characters_for_display(source.in_target_language[0]),\n+            \"meaning\": _remove_special_characters_for_display(source.in_source_language[0]),\n+            \"audio\": audio_id(course.target_language, _remove_special_characters_for_display(source.in_target_language[0])),\n             \"id\": get_dumb_opaque_id(\"Word\", source, \"listeningExercise\"),\n             \"priority\": 1,\n             \"group\": get_dumb_opaque_id(\"Group\", source),","old_code":"import editdistance  # type: ignore\n\nfrom librelingo_utils import get_dumb_opaque_id, audio_id, clean_word, iterate_phrases\nfrom .dictionary import _define_words_in_sentence\n\n\ndef get_listening_challenge(source, course):\n    if not course.settings.audio_files_enabled:\n        return []\n\n    return [\n        {\n            \"type\": \"listeningExercise\",\n            \"answer\": source.in_target_language[0],\n            \"meaning\": source.in_source_language[0],\n            \"audio\": audio_id(course.target_language, source.in_target_language[0]),\n            \"id\": get_dumb_opaque_id(\"Word\", source, \"listeningExercise\"),\n            \"priority\": 1,\n            \"group\": get_dumb_opaque_id(\"Group\", source),\n        }\n    ]\n\n\ndef get_short_input_challenge(source, course):\n    return [\n        {\n            \"type\": \"shortInput\",\n            \"pictures\": [pic + \".jpg\" for pic in source.pictures]\n            if source.pictures\n            else None,\n            \"formInTargetLanguage\": source.in_target_language,\n            \"phrase\": _define_words_in_sentence(\n                course, source.in_source_language[0], reverse=False\n            ),\n            \"id\": get_dumb_opaque_id(\"Word\", source, \"shortInput\"),\n            \"priority\": 1,\n            \"group\": get_dumb_opaque_id(\"Group\", source),\n        }\n    ]\n\n\ndef get_cards_challenge(word, _):\n    return [\n        {\n            \"type\": \"cards\",\n            \"pictures\": [pic + \".jpg\" for pic in word.pictures]\n            if word.pictures\n            else None,\n            \"formInTargetLanguage\": word.in_target_language[0],\n            \"meaningInSourceLanguage\": word.in_source_language[0],\n            \"id\": get_dumb_opaque_id(\"Word\", word, \"cards\"),\n            \"priority\": 0,\n            \"group\": get_dumb_opaque_id(\"Group\", word),\n        }\n    ]\n\n\ndef get_options_challenge(phrase, _):\n    return [\n        {\n            \"type\": \"options\",\n            \"formInTargetLanguage\": phrase.in_target_language[0],\n            \"meaningInSourceLanguage\": phrase.in_source_language[0],\n            \"id\": get_dumb_opaque_id(\"Options\", phrase, \"options\"),\n            \"priority\": 0,\n            \"group\": get_dumb_opaque_id(\"Group\", phrase),\n        }\n    ]\n\n\ndef get_chips_from_string(phrase):\n    return list(map(clean_word, phrase.split()))\n\n\ndef get_chips_from_phrase(get_input_texts, phrase, course):\n    extra_chips = []\n    solution_chips = get_chips_from_string(get_input_texts(phrase)[0])\n\n    for phrase in iterate_phrases(course):\n        for variant in get_input_texts(phrase):\n            for chip in get_chips_from_string(variant):\n                if chip not in solution_chips:\n                    extra_chips.append(chip)\n\n    chips_already_added = set()\n    deduplicated_chips = []\n\n    for chip in extra_chips:\n        if chip.lower() not in chips_already_added:\n            deduplicated_chips.append(chip)\n            chips_already_added.add(chip.lower())\n\n    extra_chips = sorted(\n        deduplicated_chips,\n        key=lambda chip: sum(\n            editdistance.eval(other_chip, chip) for other_chip in solution_chips\n        ),\n    )\n\n    return solution_chips + extra_chips[0 : max(len(solution_chips) - 1, 2)]\n\n\ndef get_solutions_from_phrase(get_input_texts, phrase):\n    return [get_chips_from_string(x) for x in get_input_texts(phrase)]\n\n\ndef create_chips_challenge_generator(reverse):\n    def get_input_texts(phrase):\n        return phrase.in_source_language if reverse else phrase.in_target_language\n\n    def get_phrase_texts(phrase):\n        return phrase.in_target_language if reverse else phrase.in_source_language\n\n    def get_input_text(phrase):\n        return get_input_texts(phrase)[0]\n\n    def get_phrase_text(phrase):\n        return get_phrase_texts(phrase)[0]\n\n    def is_long_enough_to_have_chips(phrase):\n        if len(phrase.in_source_language[0].split()) < 2:\n            return False\n        if len(phrase.in_target_language[0].split()) < 2:\n            return False\n        return True\n\n    def get_chips_challenge(phrase, course):\n        if not is_long_enough_to_have_chips(phrase):\n            return []\n\n        return [\n            {\n                \"type\": \"chips\",\n                \"translatesToSourceLanguage\": reverse,\n                \"phrase\": _define_words_in_sentence(\n                    course, get_phrase_text(phrase), reverse\n                ),\n                \"chips\": get_chips_from_phrase(get_input_texts, phrase, course),\n                \"solutions\": get_solutions_from_phrase(get_input_texts, phrase),\n                \"formattedSolution\": get_input_text(phrase),\n                \"id\": get_dumb_opaque_id(\n                    \"Chips\", phrase, \"reverse chips\" if reverse else \"chips\"\n                ),\n                \"priority\": 2,\n                \"group\": get_dumb_opaque_id(\"Group\", phrase),\n            }\n        ]\n\n    return get_chips_challenge\n\n\nget_chips_challenge = create_chips_challenge_generator(False)\nget_reverse_chips_challenge = create_chips_challenge_generator(True)\n","lang_cluster":"Python","length":153,"code_uid":"ae248d1cee4746a4ae1e971cff698c84"}
{"diff_hunk":"@@ -9,7 +9,6 @@ class FlushViewTest(BaseWebTest, unittest.TestCase):\n \n     collection_url = '\/buckets\/beers\/collections\/barley\/records'\n \n-    @authorize(authz_class='kinto.tests.support.AllowAuthorizationPolicy')\n     def setUp(self):\n         super(FlushViewTest, self).setUp()\n ","old_code":"from cliquet.tests.support import authorize\n\nfrom .support import (BaseWebTest, unittest, get_user_headers,\n                      MINIMALIST_BUCKET, MINIMALIST_COLLECTION,\n                      MINIMALIST_RECORD)\n\n\nclass FlushViewTest(BaseWebTest, unittest.TestCase):\n\n    collection_url = '\/buckets\/beers\/collections\/barley\/records'\n\n    @authorize(authz_class='kinto.tests.support.AllowAuthorizationPolicy')\n    def setUp(self):\n        super(FlushViewTest, self).setUp()\n\n        bucket = MINIMALIST_BUCKET.copy()\n\n        self.alice_headers = self.headers.copy()\n        self.alice_headers.update(**get_user_headers('alice'))\n        alice_principal = ('basicauth:8df4b22019cc89d0bb679bc51373a9da56a'\n                           '7ae9978c52fbe684510c3d257c855')\n        bucket['permissions'] = {'write': [alice_principal]}\n\n        # Create shared bucket.\n        self.app.put_json('\/buckets\/beers', bucket,\n                          headers=self.headers)\n        self.app.put_json('\/buckets\/beers\/collections\/barley',\n                          MINIMALIST_COLLECTION,\n                          headers=self.headers)\n\n        # Records for alice and bob.\n        self.app.post_json(self.collection_url,\n                           MINIMALIST_RECORD,\n                           headers=self.headers,\n                           status=201)\n        self.app.post_json(self.collection_url,\n                           MINIMALIST_RECORD,\n                           headers=self.alice_headers,\n                           status=201)\n\n    def get_app_settings(self, extra=None):\n        if extra is None:\n            extra = {}\n        extra.setdefault('kinto.flush_endpoint_enabled', True)\n        settings = super(FlushViewTest, self).get_app_settings(extra)\n        return settings\n\n    def test_returns_404_if_not_enabled_in_configuration(self):\n        extra = {'kinto.flush_endpoint_enabled': False}\n        app = self._get_test_app(settings=extra)\n        app.post('\/__flush__', headers=self.headers, status=404)\n\n    @authorize(authz_class='kinto.tests.support.AllowAuthorizationPolicy')\n    def test_removes_every_records_of_everykind(self):\n        self.app.get(self.collection_url, headers=self.headers)\n        self.app.get(self.collection_url, headers=self.alice_headers)\n\n        self.app.post('\/__flush__', headers=self.headers, status=202)\n\n        self.app.get(self.collection_url, headers=self.headers, status=403)\n        self.app.get(self.collection_url,\n                     headers=self.alice_headers,\n                     status=403)\n","lang_cluster":"Python","length":63,"code_uid":"c78699dc3d9c46c6a5acb252eefdb76c"}
{"diff_hunk":"@@ -79,6 +79,7 @@ def build_assets_job(\n \n     op_defs = build_op_deps(assets, source_assets_by_key.keys())\n     root_manager = build_root_manager(source_assets_by_key)\n+    partitions_def, tags_for_partition_fn = build_partitions_info()\n \n     return GraphDefinition(\n         name=name,","old_code":"from typing import AbstractSet, Any, Dict, List, Mapping, Optional, Sequence, Tuple, Union, cast\n\nfrom dagster import check\nfrom dagster.core.definitions.config import ConfigMapping\nfrom dagster.core.definitions.decorators.op import op\nfrom dagster.core.definitions.dependency import (\n    DependencyDefinition,\n    IDependencyDefinition,\n    NodeInvocation,\n)\nfrom dagster.core.definitions.events import AssetKey\nfrom dagster.core.definitions.executor_definition import ExecutorDefinition\nfrom dagster.core.definitions.graph_definition import GraphDefinition\nfrom dagster.core.definitions.job_definition import JobDefinition\nfrom dagster.core.definitions.op_definition import OpDefinition\nfrom dagster.core.definitions.output import Out, OutputDefinition\nfrom dagster.core.definitions.partition import PartitionedConfig\nfrom dagster.core.definitions.resource_definition import ResourceDefinition\nfrom dagster.core.errors import DagsterInvalidDefinitionError\nfrom dagster.core.execution.context.input import InputContext, build_input_context\nfrom dagster.core.execution.context.output import build_output_context\nfrom dagster.core.storage.root_input_manager import RootInputManagerDefinition, root_input_manager\nfrom dagster.utils.backcompat import experimental\nfrom dagster.utils.merger import merge_dicts\n\nfrom .asset import AssetsDefinition\nfrom .foreign_asset import ForeignAsset\n\n\n@experimental\ndef build_assets_job(\n    name: str,\n    assets: List[AssetsDefinition],\n    source_assets: Optional[Sequence[Union[ForeignAsset, AssetsDefinition]]] = None,\n    resource_defs: Optional[Dict[str, ResourceDefinition]] = None,\n    description: Optional[str] = None,\n    config: Union[ConfigMapping, Dict[str, Any], PartitionedConfig] = None,\n    tags: Optional[Dict[str, Any]] = None,\n    executor_def: Optional[ExecutorDefinition] = None,\n) -> JobDefinition:\n    \"\"\"Builds a job that materializes the given assets.\n\n    The dependencies between the ops in the job are determined by the asset dependencies defined\n    in the metadata on the provided asset nodes.\n\n    Args:\n        name (str): The name of the job.\n        assets (List[AssetsDefinition]): A list of assets or\n            multi-assets - usually constructed using the :py:func:`@asset` or :py:func:`@multi_asset`\n            decorator.\n        source_assets (Optional[Sequence[Union[ForeignAsset, AssetsDefinition]]]): A list of\n            assets that are not materialized by this job, but that assets in this job depend on.\n        resource_defs (Optional[Dict[str, ResourceDefinition]]): Resource defs to be included in\n            this job.\n        description (Optional[str]): A description of the job.\n\n    Examples:\n\n        .. code-block:: python\n\n            @asset\n            def asset1():\n                return 5\n\n            @asset\n            def asset2(asset1):\n                return my_upstream_asset + 1\n\n            my_assets_job = build_assets_job(\"my_assets_job\", assets=[asset1, asset2])\n\n    Returns:\n        JobDefinition: A job that materializes the given assets.\n    \"\"\"\n    check.str_param(name, \"name\")\n    check.list_param(assets, \"assets\", of_type=AssetsDefinition)\n    check.opt_list_param(source_assets, \"source_assets\", of_type=(ForeignAsset, AssetsDefinition))\n    check.opt_str_param(description, \"description\")\n    source_assets_by_key = build_source_assets_by_key(source_assets)\n\n    op_defs = build_op_deps(assets, source_assets_by_key.keys())\n    root_manager = build_root_manager(source_assets_by_key)\n\n    return GraphDefinition(\n        name=name,\n        node_defs=[asset.op for asset in assets],\n        dependencies=op_defs,\n        description=description,\n        input_mappings=None,\n        output_mappings=None,\n        config=None,\n    ).to_job(\n        resource_defs=merge_dicts(resource_defs or {}, {\"root_manager\": root_manager}),\n        config=config,\n        tags=tags,\n        executor_def=executor_def,\n    )\n\n\ndef build_source_assets_by_key(\n    source_assets: Optional[Sequence[Union[ForeignAsset, AssetsDefinition]]]\n) -> Mapping[AssetKey, Union[ForeignAsset, OutputDefinition]]:\n    source_assets_by_key: Dict[AssetKey, Union[ForeignAsset, OutputDefinition]] = {}\n    for asset_source in source_assets or []:\n        if isinstance(asset_source, ForeignAsset):\n            source_assets_by_key[asset_source.key] = asset_source\n        elif isinstance(asset_source, AssetsDefinition):\n            for asset_key, output_def in asset_source.output_defs_by_asset_key.items():\n                if asset_key:\n                    source_assets_by_key[asset_key] = output_def\n\n    return source_assets_by_key\n\n\ndef build_op_deps(\n    multi_asset_defs: List[AssetsDefinition], source_paths: AbstractSet[AssetKey]\n) -> Dict[Union[str, NodeInvocation], Dict[str, IDependencyDefinition]]:\n    op_outputs_by_asset: Dict[AssetKey, Tuple[OpDefinition, str]] = {}\n    for multi_asset_def in multi_asset_defs:\n        for asset_key, output_def in multi_asset_def.output_defs_by_asset_key.items():\n            if asset_key in op_outputs_by_asset:\n                raise DagsterInvalidDefinitionError(\n                    f\"The same asset key was included for two definitions: '{asset_key.to_string()}'\"\n                )\n\n            op_outputs_by_asset[asset_key] = (multi_asset_def.op, output_def.name)\n\n    op_deps: Dict[Union[str, NodeInvocation], Dict[str, IDependencyDefinition]] = {}\n    for multi_asset_def in multi_asset_defs:\n        op_name = multi_asset_def.op.name\n        op_deps[op_name] = {}\n        for asset_key, input_def in multi_asset_def.input_defs_by_asset_key.items():\n            if asset_key in op_outputs_by_asset:\n                op_def, output_name = op_outputs_by_asset[asset_key]\n                op_deps[op_name][input_def.name] = DependencyDefinition(op_def.name, output_name)\n            elif asset_key not in source_paths and not input_def.dagster_type.is_nothing:\n                raise DagsterInvalidDefinitionError(\n                    f\"Input asset '{asset_key.to_string()}' for asset '{op_name}' is not \"\n                    \"produced by any of the provided asset ops and is not one of the provided \"\n                    \"sources\"\n                )\n\n    return op_deps\n\n\ndef build_root_manager(\n    source_assets_by_key: Mapping[AssetKey, Union[ForeignAsset, OutputDefinition]]\n) -> RootInputManagerDefinition:\n    source_asset_io_manager_keys = {\n        source_asset.io_manager_key for source_asset in source_assets_by_key.values()\n    }\n\n    @root_input_manager(required_resource_keys=source_asset_io_manager_keys)\n    def _root_manager(input_context: InputContext) -> Any:\n        source_asset_key = cast(AssetKey, input_context.asset_key)\n        source_asset = source_assets_by_key[source_asset_key]\n\n        @op(out={source_asset_key.path[-1]: Out(asset_key=source_asset_key)})\n        def _op():\n            pass\n\n        output_context = build_output_context(\n            name=source_asset_key.path[-1],\n            step_key=\"none\",\n            solid_def=_op,\n            metadata=merge_dicts(\n                source_asset.metadata or {}, {\"logical_asset_key\": source_asset_key}\n            ),\n        )\n        input_context_with_upstream = build_input_context(\n            name=input_context.name,\n            metadata=input_context.metadata,\n            config=input_context.config,\n            dagster_type=input_context.dagster_type,\n            upstream_output=output_context,\n            op_def=input_context.op_def,\n        )\n\n        io_manager = getattr(cast(Any, input_context.resources), source_asset.io_manager_key)\n        return io_manager.load_input(input_context_with_upstream)\n\n    return _root_manager\n","lang_cluster":"Python","length":181,"code_uid":"1d104193b77e440bbd2e34652aac7eba"}
{"diff_hunk":"@@ -93,19 +93,24 @@ def get_artist(user_name):\n         return '', 204\n \n     count = min(count, MAX_ITEMS_PER_GET)\n-    total_artist_count = stats['artist'][_range]['count']\n+    try:\n+        total_artist_count = stats['artist'][stats_range]['count']\n+    except KeyError:\n+        return '', 204\n \n     count = count + offset\n-    artist_list = stats['artist']['all_time']['artists'][offset:count]\n+    artist_list = stats['artist'][stats_range]['artists'][offset:count]\n \n     return jsonify({'payload': {\n-        'user_id': user_name,\n+        \"user_id\": user_name,\n         \"artists\": artist_list,\n         \"count\": len(artist_list),\n         \"total_artist_count\": total_artist_count,\n         \"offset\": offset,\n-        \"range\": _range,\n-        'last_updated': int(stats['last_updated'].timestamp())\n+        \"range\": stats_range,\n+        \"from\": int(stats['artist'][stats_range]['from']),\n+        \"to\": int(stats['artist'][stats_range]['to']),\n+        \"last_updated\": int(stats['last_updated'].timestamp())\n     }})\n \n ","old_code":"from datetime import datetime\n\nfrom flask import Blueprint, current_app, jsonify, request\n\nimport listenbrainz.db.stats as db_stats\nimport listenbrainz.db.user as db_user\nfrom listenbrainz.webserver.decorators import crossdomain\nfrom listenbrainz.webserver.errors import (APIBadRequest,\n                                           APIInternalServerError, APINotFound,\n                                           APIServiceUnavailable,\n                                           APIUnauthorized)\nfrom listenbrainz.webserver.rate_limiter import ratelimit\nfrom listenbrainz.webserver.views.api_tools import DEFAULT_ITEMS_PER_GET, MAX_ITEMS_PER_GET\n\nstats_api_bp = Blueprint('stats_api_v1', __name__)\n\n\n@stats_api_bp.route(\"\/user\/<user_name>\/artists\")\n@crossdomain()\n@ratelimit()\ndef get_artist(user_name):\n    \"\"\"\n    Get top artists for user ``user_name``.\n\n\n    An sample response from the endpoint may look like::\n\n        {\n            \"payload\": {\n                \"artists\": [\n                    {\n                       \"artist_mbids\": [\"93e6118e-7fa8-49f6-9e02-699a1ebce105\"],\n                       \"artist_msid\": \"d340853d-7408-4a0d-89c2-6ff13e568815\",\n                       \"artist_name\": \"The Local train\",\n                       \"listen_count\": 385\n                    },\n                    {\n                       \"artist_mbids\": [\"ae9ed5e2-4caf-4b3d-9cb3-2ad626b91714\"],\n                       \"artist_msid\": \"ba64b195-01dd-4613-9534-bb87dc44cffb\",\n                       \"artist_name\": \"Lenka\",\n                       \"listen_count\": 333\n                    },\n                    {\n                       \"artist_mbids\": [\"cc197bad-dc9c-440d-a5b5-d52ba2e14234\"],\n                       \"artist_msid\": \"6599e41e-390c-4855-a2ac-68ee798538b4\",\n                       \"artist_name\": \"Coldplay\",\n                       \"listen_count\": 321\n                    }\n                ],\n                \"count\": 3,\n                \"total_artist_count\": 175,\n                \"range\": \"all_time\",\n                \"last_updated\": 1588494361,\n                \"user_id\": \"John Doe\"\n            }\n        }\n\n    .. note::\n        - This endpoint is currently in beta\n        - ``artist_mbids`` and ``artist_msid`` are optional fields and may not be present in all the responses\n        - As of now we are only calculating ``all_time`` statistics for artist.\n          However, we plan to add other time intervals in the future.\n\n    :param count: Optional, number of artists to return, Default: :data:`~webserver.views.api.DEFAULT_ITEMS_PER_GET`\n        Max: :data:`~webserver.views.api.MAX_ITEMS_PER_GET`\n    :type count: ``int``\n    :param offset: Optional, number of artists to skip from the beginning, for pagination.\n        Ex. An offset of 5 means the top 5 artists will be skipped, defaults to 0\n    :type offset: ``int``\n    :param range: Optional, time interval for which statistics should be collected, defaults to ``all_time``,\n        we currently only support all time but more intervals will be added.\n    :type range: ``str``\n    :statuscode 200: Successful query, you have data!\n    :statuscode 204: Statistics for the user haven't been calculated, empty response will be returned\n    :statuscode 400: Bad request, check ``response['error']`` for more details\n    :statuscode 404: User not found\n    :resheader Content-Type: *application\/json*\n    \"\"\"\n\n    _range = request.args.get('range', default='all_time')\n    if _range != 'all_time':\n        raise APIBadRequest(\"We currently only support the `all_time` range.\")\n\n    offset = _get_non_negative_param('offset', default=0)\n    count = _get_non_negative_param('count', default=DEFAULT_ITEMS_PER_GET)\n\n    user = db_user.get_by_mb_id(user_name)\n    if user is None:\n        raise APINotFound(\"Cannot find user: %s\" % user_name)\n\n    stats = db_stats.get_user_artists(user['id'])\n    if stats is None:\n        return '', 204\n\n    count = min(count, MAX_ITEMS_PER_GET)\n    total_artist_count = stats['artist'][_range]['count']\n\n    count = count + offset\n    artist_list = stats['artist']['all_time']['artists'][offset:count]\n\n    return jsonify({'payload': {\n        'user_id': user_name,\n        \"artists\": artist_list,\n        \"count\": len(artist_list),\n        \"total_artist_count\": total_artist_count,\n        \"offset\": offset,\n        \"range\": _range,\n        'last_updated': int(stats['last_updated'].timestamp())\n    }})\n\n\ndef _get_non_negative_param(param, default=None):\n    \"\"\" Gets the value of a request parameter, validating that it is non-negative\n\n    Args:\n        param (str): the parameter to get\n        default: the value to return if the parameter doesn't exist in the request\n    \"\"\"\n    value = request.args.get(param, default)\n    if value is not None:\n        try:\n            value = int(value)\n        except ValueError:\n            raise APIBadRequest(\"'{}' should be a non-negative integer\".format(param))\n\n        if value < 0:\n            raise APIBadRequest(\"'{}' should be a non-negative integer\".format(param))\n    return value\n","lang_cluster":"Python","length":128,"code_uid":"bdc7c2ff709b4bfebcb74e92dfd37aa5"}
{"diff_hunk":"@@ -40,6 +40,12 @@ def setup_app(app):\n         context = {}\n         for func in getattr(g, '_template_context_processor', []):\n             context.update(func())\n+\n+        # used by ``template_args`` decorator.\n+        endpoint = current_app.view_functions.get(request.endpoint)\n+        for func in getattr(endpoint, '_invenio_template_args', []):\n+            context.update(func())\n+\n         reset_template_context()\n         return context\n ","old_code":"# -*- coding: utf-8 -*-\n##\n## This file is part of Invenio.\n## Copyright (C) 2012, 2013, 2014 CERN.\n##\n## Invenio is free software; you can redistribute it and\/or\n## modify it under the terms of the GNU General Public License as\n## published by the Free Software Foundation; either version 2 of the\n## License, or (at your option) any later version.\n##\n## Invenio is distributed in the hope that it will be useful, but\n## WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n## General Public License for more details.\n##\n## You should have received a copy of the GNU General Public License\n## along with Invenio; if not, write to the Free Software Foundation, Inc.,\n## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"Additional decorator for extending template context with new objects.\"\"\"\n\nfrom flask import g\n\n\ndef register_template_context_processor(f):\n    \"\"\"Register globally the context processor.\"\"\"\n    g._template_context_processor.append(f)\n\n\ndef setup_app(app):\n    \"\"\"Initialize template context processor extension.\"\"\"\n    @app.before_request\n    def reset_template_context():\n        \"\"\"Reset custom template context buffer.\"\"\"\n        g._template_context_processor = []\n\n    @app.context_processor\n    def inject_template_context():\n        \"\"\"Update `Jinja2` context by dynamic context processors.\"\"\"\n        context = {}\n        for func in getattr(g, '_template_context_processor', []):\n            context.update(func())\n        reset_template_context()\n        return context\n\n    return app\n","lang_cluster":"Python","length":46,"code_uid":"68693b45d22f480daaf25425073e59e9"}
{"diff_hunk":"@@ -63,7 +63,7 @@ class BitmapArrayEncoderTest(unittest.TestCase):\n \n   def testEncodeArray(self):\n     \"\"\"Send bitmap as array of indicies\"\"\"\n-    e = self._encoder(self.n, self.w, self.name)\n+    e = self._encoder(self.n, self.w, name=self.name)\n     bitmap = [2,7,15,18,23]\n     out = e.encode(bitmap)\n     assert out.sum() == len(bitmap)*self.w","old_code":"#!\/usr\/bin\/env python\n# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have purchased from\n# Numenta, Inc. a separate commercial license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see http:\/\/www.gnu.org\/licenses.\n#\n# http:\/\/numenta.org\/licenses\/\n# ----------------------------------------------------------------------\n\n\"\"\"Unit tests for BitmapArray Encoder.\"\"\"\n\nCL_VERBOSITY = 0\n\nimport cPickle as pickle\nimport unittest2 as unittest\n\nimport numpy\n\nfrom nupic.encoders.bitmaparray import BitmapArrayEncoder\n\n\n\nclass BitmapArrayEncoderTest(unittest.TestCase):\n  \"\"\"Unit tests for BitmapArrayEncoder class.\"\"\"\n\n\n  def setUp(self):\n    self.n = 25\n    self.w = 1\n    self.name = \"foo\"\n    self._encoder = BitmapArrayEncoder\n\n\n  def testInitialization(self):\n    e = self._encoder(self.n, self.w, self.name)\n    self.assertEqual(type(e), self._encoder)\n\n\n  def testEncodeString(self):\n    \"\"\"Send array as csv string.\"\"\"\n    e = self._encoder(self.n, self.w, self.name)\n    bitmap = \"2,7,15,18,23\"\n    out = e.encode(bitmap)\n    assert out.sum() == len(bitmap.split(','))*self.w\n\n    x = e.decode(out)\n    assert isinstance(x[0], dict)\n    assert self.name in x[0]\n\n\n  def testEncodeArray(self):\n    \"\"\"Send bitmap as array of indicies\"\"\"\n    e = self._encoder(self.n, self.w, self.name)\n    bitmap = [2,7,15,18,23]\n    out = e.encode(bitmap)\n    assert out.sum() == len(bitmap)*self.w\n\n    x = e.decode(out)\n    assert isinstance(x[0], dict)\n    assert self.name in x[0]\n\n\n  def testClosenessScores(self):\n    \"\"\"Compare two bitmaps for closeness\"\"\"\n    e = self._encoder(self.n, self.w, self.name)\n\n    \"\"\"Identical => 1\"\"\"\n    bitmap1 = [2,7,15,18,23]\n    bitmap2 = [2,7,15,18,23]\n    out1 = e.encode(bitmap1)\n    out2 = e.encode(bitmap2)\n    c = e.closenessScores(out1, out2)\n    assert c[0] == 1.0\n\n    \"\"\"No overlap => 0\"\"\"\n    bitmap1 = [2,7,15,18,23]\n    bitmap2 = [3,9,14,19,24]\n    out1 = e.encode(bitmap1)\n    out2 = e.encode(bitmap2)\n    c = e.closenessScores(out1, out2)\n    assert c[0] == 0.0\n\n    \"\"\"Similar => 4 of 5 match\"\"\"\n    bitmap1 = [2,7,15,18,23]\n    bitmap2 = [2,7,17,18,23]\n    out1 = e.encode(bitmap1)\n    out2 = e.encode(bitmap2)\n    c = e.closenessScores(out1, out2)\n    assert c[0] == 0.8\n\n    \"\"\"Little => 1 of 5 match\"\"\"\n    bitmap1 = [2,7,15,18,23]\n    bitmap2 = [3,7,17,19,24]\n    out1 = e.encode(bitmap1)\n    out2 = e.encode(bitmap2)\n    c = e.closenessScores(out1, out2)\n    assert c[0] == 0.2\n\n    \"\"\"Extra active bit => off by 1 of 5\"\"\"\n    bitmap1 = [2,7,15,18,23]\n    bitmap2 = [2,7,11,15,18,23]\n    out1 = e.encode(bitmap1)\n    out2 = e.encode(bitmap2)\n    c = e.closenessScores(out1, out2)\n    assert c[0] == 0.8\n\n    \"\"\"Missing active bit => off by 1 of 5\"\"\"\n    bitmap1 = [2,7,15,18,23]\n    bitmap2 = [2,7,18,23]\n    out1 = e.encode(bitmap1)\n    out2 = e.encode(bitmap2)\n    c = e.closenessScores(out1, out2)\n    assert c[0] == 0.8\n\n\n  def testRobustness(self):\n    \"\"\"Encode bitmaps with robustness (w) set\"\"\"\n    self.w = 3\n    self.n = self.n * self.w\n    self.testEncodeString()\n    self.testEncodeArray()\n    self.testClosenessScores()\n\n\n\nif __name__ == '__main__':\n  unittest.main()\n","lang_cluster":"Python","length":140,"code_uid":"cc95fc69c5bc416faad66bb22a0b387a"}
{"diff_hunk":"@@ -111,11 +111,11 @@ class GrpcInventory(inventory_pb2_grpc.InventoryServicer):\n             object: Inventory API object that is requested.\n         \"\"\"\n \n-        inventory_index = self.inventory.Get(request.id)\n+        inventory_index = self.inventory.get(request.id)\n         return inventory_pb2.GetReply(\n             inventory=inventory_pb_from_object(inventory_index))\n \n-    def Delete(self, request, _):\n+    def delete(self, request, _):\n         \"\"\"Deletes existing inventory.\n \n         Returns:","old_code":"# Copyright 2017 The Forseti Security Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\" Inventory gRPC service. \"\"\"\n\nimport google.protobuf.timestamp_pb2 as timestamp\n\nfrom google.cloud.forseti.services.inventory import inventory_pb2\nfrom google.cloud.forseti.services.inventory import inventory_pb2_grpc\nfrom google.cloud.forseti.services.inventory import inventory\nfrom google.cloud.forseti.services.utils import autoclose_stream\n\n# TODO: The next editor must remove this disable and correct issues.\n# pylint: disable=missing-type-doc,missing-return-type-doc,missing-return-doc\n# pylint: disable=missing-param-doc,no-member\n\n\ndef inventory_pb_from_object(inventory_index):\n    \"\"\"Convert internal inventory data structure to protobuf.\"\"\"\n\n    return inventory_pb2.InventoryIndex(\n        id=inventory_index.id,\n        start_time=timestamp.Timestamp().FromDatetime(\n            inventory_index.start_time),\n        complete_time=timestamp.Timestamp().FromDatetime(\n            inventory_index.complete_time),\n        schema_version=inventory_index.schema_version,\n        count_objects=inventory_index.counter,\n        status=inventory_index.status,\n        warnings=inventory_index.warnings,\n        errors=inventory_index.errors)\n\n\nclass GrpcInventory(inventory_pb2_grpc.InventoryServicer):\n    \"\"\"Inventory gRPC handler.\"\"\"\n\n    def __init__(self, inventory_api):\n        super(GrpcInventory, self).__init__()\n        self.inventory = inventory_api\n\n    def Ping(self, request, _):\n        \"\"\"Ping implemented to check service availability.\n\n        Args:\n            request (object): gRPC request object.\n            _ (object): Unused.\n\n        Returns:\n            object: PingReply containing echo of data.\n        \"\"\"\n\n        return inventory_pb2.PingReply(data=request.data)\n\n    @autoclose_stream\n    def Create(self, request, _):\n        \"\"\"Creates a new inventory.\n\n        Args:\n            request (object): gRPC request object.\n            _ (object): Unused.\n\n        Yields:\n            object: Inventory progress updates.\n        \"\"\"\n\n        for progress in self.inventory.Create(request.background,\n                                              request.model_name):\n            yield inventory_pb2.Progress(\n                id=progress.inventory_id,\n                final_message=progress.final_message,\n                step=progress.step,\n                warnings=progress.warnings,\n                errors=progress.errors,\n                last_warning=repr(progress.last_warning),\n                last_error=repr(progress.last_error))\n\n    @autoclose_stream\n    def List(self, request, _):\n        \"\"\"Lists existing inventory.\n\n        Args:\n            request (object): gRPC request object.\n            _ (object): Unused.\n\n        Yields:\n            object: Each Inventory API object.\n        \"\"\"\n\n        for inventory_index in self.inventory.List():\n            yield inventory_pb_from_object(inventory_index)\n\n    def Get(self, request, _):\n        \"\"\"Gets existing inventory.\n\n        Args:\n            request (object): gRPC request object.\n            _ (object): Unused.\n\n        Returns:\n            object: Inventory API object that is requested.\n        \"\"\"\n\n        inventory_index = self.inventory.Get(request.id)\n        return inventory_pb2.GetReply(\n            inventory=inventory_pb_from_object(inventory_index))\n\n    def Delete(self, request, _):\n        \"\"\"Deletes existing inventory.\n\n        Returns:\n            request (object): gRPC request object.\n            _ (object): Unused\n\n        Returns:\n            object: Inventory API object that is deleted.\n        \"\"\"\n\n        inventory_index = self.inventory.Delete(request.id)\n        return inventory_pb2.DeleteReply(\n            inventory=inventory_pb_from_object(inventory_index))\n\n\nclass GrpcInventoryFactory(object):\n    \"\"\"Factory class for Inventory service gRPC interface\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n\n    def create_and_register_service(self, server):\n        \"\"\"Creates an inventory service and registers it in the server.\n\n        Args:\n            server (object): Server to register service to.\n\n        Returns:\n            object: The instantiated gRPC service for inventory.\n        \"\"\"\n\n        service = GrpcInventory(\n            inventory_api=inventory.Inventory(\n                self.config))\n        inventory_pb2_grpc.add_InventoryServicer_to_server(service, server)\n        return service\n","lang_cluster":"Python","length":154,"code_uid":"0bb939df76a44e9bab4d75548a83450a"}
{"diff_hunk":"@@ -1,6 +1,8 @@\n+import uuid\n from ...core import Store, HoloMap\n from ..renderer import Renderer, MIME_TYPES\n from .widgets import BokehScrubberWidget, BokehSelectionWidget\n+from .util import models_to_json\n \n import param\n from param.parameterized import bothmethod","old_code":"from ...core import Store, HoloMap\nfrom ..renderer import Renderer, MIME_TYPES\nfrom .widgets import BokehScrubberWidget, BokehSelectionWidget\n\nimport param\nfrom param.parameterized import bothmethod\n\nfrom bokeh.embed import notebook_div\nfrom bokeh.io import load_notebook, Document\nfrom bokeh.resources import CDN\n\ntry:\n    from bokeh.protocol import serialize_json\n    old_bokeh = True\nexcept ImportError:\n    from bokeh._json_encoder import serialize_json\n    old_bokeh = False\n\nclass BokehRenderer(Renderer):\n\n    backend = param.String(default='bokeh', doc=\"The backend name.\")\n\n    fig = param.ObjectSelector(default='auto', objects=['html', 'json', 'auto'], doc=\"\"\"\n        Output render format for static figures. If None, no figure\n        rendering will occur. \"\"\")\n\n    # Defines the valid output formats for each mode.\n    mode_formats = {'fig': {'default': ['html', 'json', 'auto']},\n                    'holomap': {'default': ['widgets', 'scrubber', 'auto', None]}}\n\n    widgets = {'scrubber': BokehScrubberWidget,\n               'widgets': BokehSelectionWidget}\n\n    js_dependencies = Renderer.js_dependencies + CDN.js_files\n\n    css_dependencies = Renderer.css_dependencies + CDN.css_files\n\n    _loaded = False\n\n    def __call__(self, obj, fmt=None):\n        \"\"\"\n        Render the supplied HoloViews component using the appropriate\n        backend. The output is not a file format but a suitable,\n        in-memory byte stream together with any suitable metadata.\n        \"\"\"\n        plot, fmt =  self._validate(obj, fmt)\n        info = {'file-ext': fmt, 'mime_type': MIME_TYPES[fmt]}\n\n        if isinstance(plot, tuple(self.widgets.values())):\n            return plot(), info\n        elif fmt == 'html':\n            html = self.figure_data(plot)\n            html = '<center>%s<\/center>' % html\n            return html, info\n        elif fmt == 'json':\n            plotobjects = [h for handles in plot.traverse(lambda x: x.current_handles)\n                           for h in handles]\n            data = dict(data=[])\n            ids = []\n            if not old_bokeh:\n                data['root'] = plot.state._id\n            json_data = []\n            for plotobj in plotobjects:\n                if plotobj.ref['id'] in ids:\n                    continue\n                else:\n                    ids.append(plotobj.ref['id'])\n                if old_bokeh:\n                    json = plotobj.vm_serialize(changed_only=True)\n                else:\n                    json = plotobj.to_json(False)\n                json_data.append({'id': plotobj.ref['id'],\n                                  'type': plotobj.ref['type'],\n                                  'data': json})\n            data['data'] = json_data\n            return serialize_json(data), info\n\n\n    def figure_data(self, plot, fmt='html', **kwargs):\n        if not old_bokeh:\n            doc = Document()\n            doc.add_root(plot.state)\n            plot.set_root(plot.state)\n            plot.set_document(doc)\n        return notebook_div(plot.state)\n\n\n    @classmethod\n    def plot_options(cls, obj, percent_size):\n        \"\"\"\n        Given a holoviews object and a percentage size, apply heuristics\n        to compute a suitable figure size. For instance, scaling layouts\n        and grids linearly can result in unwieldy figure sizes when there\n        are a large number of elements. As ad hoc heuristics are used,\n        this functionality is kept separate from the plotting classes\n        themselves.\n\n        Used by the IPython Notebook display hooks and the save\n        utility. Note that this can be overridden explicitly per object\n        using the fig_size and size plot options.\n        \"\"\"\n        factor = percent_size \/ 100.0\n        obj = obj.last if isinstance(obj, HoloMap) else obj\n        plot = Store.registry[cls.backend].get(type(obj), None)\n        options = Store.lookup_options(cls.backend, obj, 'plot').options\n        if not hasattr(plot, 'width') or not hasattr(plot, 'height'):\n            from .plot import BokehPlot\n            plot = BokehPlot\n        width = options.get('width', plot.width) * factor\n        height = options.get('height', plot.height) * factor\n        return dict(options, **{'width':int(width), 'height': int(height)})\n\n\n    @bothmethod\n    def get_size(self_or_cls, plot):\n        \"\"\"\n        Return the display size associated with a plot before\n        rendering to any particular format. Used to generate\n        appropriate HTML display.\n\n        Returns a tuple of (width, height) in pixels.\n        \"\"\"\n        return (plot.state.height, plot.state.height)\n\n    @classmethod\n    def load_nb(cls):\n        \"\"\"\n        Loads the bokeh notebook resources.\n        \"\"\"\n        load_notebook(hide_banner=True)\n","lang_cluster":"Python","length":130,"code_uid":"19839cefc52c40dd8c00e64eb49057d8"}
{"diff_hunk":"@@ -11,7 +11,7 @@\n #\n # It's strongly recommended that you check this file into your version control system.\n \n-ActiveRecord::Schema.define(version: 20140505150000) do\n+ActiveRecord::Schema.define(version: 20140516154809) do\n \n   create_table \"approval_groups\", force: true do |t|\n     t.string   \"name\"","old_code":"# encoding: UTF-8\n# This file is auto-generated from the current state of the database. Instead\n# of editing this file, please use the migrations feature of Active Record to\n# incrementally modify your database, and then regenerate this schema definition.\n#\n# Note that this schema.rb definition is the authoritative source for your\n# database schema. If you need to create the application database on another\n# system, you should be using db:schema:load, not running all the migrations\n# from scratch. The latter is a flawed and unsustainable approach (the more migrations\n# you'll amass, the slower it'll run and the greater likelihood for issues).\n#\n# It's strongly recommended that you check this file into your version control system.\n\nActiveRecord::Schema.define(version: 20140505150000) do\n\n  create_table \"approval_groups\", force: true do |t|\n    t.string   \"name\"\n    t.datetime \"created_at\"\n    t.datetime \"updated_at\"\n    t.integer  \"cart_id\"\n  end\n\n  create_table \"approver_comments\", force: true do |t|\n    t.text     \"comment_text\"\n    t.integer  \"approver_id\"\n    t.datetime \"created_at\"\n    t.datetime \"updated_at\"\n  end\n\n  create_table \"approvers\", force: true do |t|\n    t.string   \"email_address\"\n    t.integer  \"approval_group_id\"\n    t.datetime \"created_at\"\n    t.datetime \"updated_at\"\n    t.string   \"status\"\n  end\n\n  create_table \"cart_items\", force: true do |t|\n    t.string   \"vendor\"\n    t.text     \"description\"\n    t.string   \"url\"\n    t.text     \"notes\"\n    t.integer  \"quantity\"\n    t.text     \"details\"\n    t.string   \"part_number\"\n    t.float    \"price\"\n    t.integer  \"cart_id\"\n    t.datetime \"created_at\"\n    t.datetime \"updated_at\"\n  end\n\n  create_table \"carts\", force: true do |t|\n    t.string   \"name\"\n    t.string   \"status\"\n    t.datetime \"created_at\"\n    t.datetime \"updated_at\"\n    t.integer  \"external_id\"\n  end\n\n  create_table \"comments\", force: true do |t|\n    t.text     \"comment_text\"\n    t.integer  \"cart_id\"\n    t.datetime \"created_at\"\n    t.datetime \"updated_at\"\n  end\n\n  create_table \"requesters\", force: true do |t|\n    t.string   \"email_address\"\n    t.datetime \"created_at\"\n    t.datetime \"updated_at\"\n    t.integer  \"approval_group_id\"\n  end\n\nend\n","lang_cluster":"Ruby","length":74,"code_uid":"640c762687344e89b600ebd5c71f53a2"}
{"diff_hunk":"@@ -87,6 +87,10 @@ module Travis\n               end\n             end\n \n+            def cmd(cmd, *args)\n+              script.cmd(\"rvm #{USE_RUBY} do #{cmd}\", *args)\n+            end\n+\n             def options\n               config.flat_map { |k,v| option(k,v) }.compact.join(\" \")\n             end","old_code":"module Travis\n  module Build\n    class Script\n      module Addons\n        class Deploy\n          VERSIONED_RUNTIMES = [:jdk, :node, :perl, :php, :python, :ruby, :scala, :node]\n          attr_accessor :script, :config\n\n          def initialize(script, config)\n            @silent = false\n            @script = script\n            @config = config.respond_to?(:to_hash) ? config.to_hash : {}\n          end\n\n          def after_success\n            script.if(want) { run }\n          end\n\n          private\n            def want\n              on          = config.delete(:on) || config.delete(:true) || {}\n              on          = { branch: on.to_str } if on.respond_to? :to_str\n              on[:ruby] ||= on[:rvm] if on.include? :rvm\n              conditions  = [ want_push(on), want_repo(on), want_branch(on), want_runtime(on), want_condition(on), want_tags(on) ]\n              conditions.flatten.compact.map { |c| \"(#{c})\" }.join(\" && \")\n            end\n\n            def want_push(on)\n              '$TRAVIS_PULL_REQUEST = false'\n            end\n\n            def want_repo(on)\n              \"$TRAVIS_REPO_SLUG = \\\"#{on[:repo]}\\\"\" if on[:repo]\n            end\n\n            def want_branch(on)\n              return if on[:all_branches]\n              branches  = Array(on[:branch] || default_branches)\n              branches.map { |b| \"$TRAVIS_BRANCH = #{b}\" }.join(' || ')\n            end\n\n            def want_tags(on)\n              '$(git describe --exact-match)' if on[:tags]\n            end\n\n            def want_condition(on)\n              on[:condition]\n            end\n\n            def want_runtime(on)\n              VERSIONED_RUNTIMES.map do |runtime|\n                next unless on.include? runtime\n                \"$TRAVIS_#{runtime.to_s.upcase}_VERSION = \\\"#{on[runtime]}\\\"\"\n              end\n            end\n\n            def run\n              script.fold('dpl.0') { install }\n              script.cmd(\"dpl #{options} --fold || (#{die})\", echo: false, assert: false)\n            end\n\n            def install(edge = config[:edge])\n              return script.cmd(\"gem install dpl\", echo: false, assert: true) unless edge\n              script.cmd(\"git clone https:\/\/github.com\/rkh\/dpl.git\")\n              script.cmd(\"cd dpl\")\n              script.cmd(\"gem build dpl.gemspec\")\n              install(false)\n              script.cmd(\"cd ..\")\n            end\n\n            def die\n              'echo \"failed to deploy\"; travis_terminate 2'\n            end\n\n            def default_branches\n              default_branches = config.values.grep(Hash).map(&:keys).flatten(1).uniq.compact\n              default_branches.any? ? default_branches : 'master'\n            end\n\n            def option(key, value)\n              case value\n              when Array      then value.map { |v| option(key, v) }\n              when Hash       then option(key, value[script.data.branch.to_sym])\n              when true       then \"--#{key}\"\n              when nil, false then nil\n              else \"--%s=%p\" % [key, value]\n              end\n            end\n\n            def options\n              config.flat_map { |k,v| option(k,v) }.compact.join(\" \")\n            end\n        end\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":97,"code_uid":"2d52f91232d6486aa2c8ad87f01f0bff"}
{"diff_hunk":"@@ -62,7 +62,7 @@\n           <div class=\"panel panel-default\">\n             <div class=\"panel-body\">\n               <h4 style=\"margin-top:0px\" style=\"display: none\" data-topic=\"users\"><%= _('New users') %><\/h4>\n-              <h4 style=\"margin-top:0px\" style=\"display: none\" data-topic=\"plans\"><%= _('Plans') %><\/h4>\n+              <h4 style=\"margin-top:0px\" style=\"display: none\" data-topic=\"plans\"><%= _('New plans') %><\/h4>\n               <strong data-range style=\"font-size: 36px;\"><\/strong>\n             <\/div>\n           <\/div>","old_code":"<div class=\"row\">\n  <div class=\"col-md-12\">\n    <h1>Usage statistics<\/h1>\n  <\/div>\n<\/div>\n<div class=\"row\">\n  <div class=\"col-md-12\">\n    <h3><%= _('Run your own filter') %><\/h3>\n    <% if current_user.api_token.present? %>\n      <form class=\"usage_index\">\n        <%= hidden_field_tag('api_token', current_user.api_token) %>\n        <div class=\"row\">\n          <div class=\"col-md-3\">\n            <div class=\"form-group\">\n              <%= label_tag('topic', _('Topic')) %>\n              <%= select_tag('topic', options_for_select(\n                [\n                  [_('Users'), 'users', { 'data-url': users_joined_api_v0_statistics_path }],\n                  [_('Plans'), 'plans', { 'data-url': created_plans_api_v0_statistics_path }]\n                ]), class: 'form-control') %>\n            <\/div>\n          <\/div>\n          <div class=\"col-md-2\">\n            <div class=\"form-group\">\n              <%= label_tag('start_date', _('Start date')) %>\n              <%= date_field_tag('start_date', nil, class: 'form-control') %>\n            <\/div>\n          <\/div>\n          <div class=\"col-md-2\">\n            <div class=\"form-group\">\n              <%= label_tag('end_date', _('End date')) %>\n              <%= date_field_tag('end_date', nil, class: 'form-control') %>\n            <\/div>\n          <\/div>\n          <% if current_user.can_super_admin? %>\n            <div class=\"col-md-3\">\n              <div class=\"form-group\">\n                <%= label_tag(:org_id, _('Organisation')) %>\n                <%= select_tag(:org_id, options_from_collection_for_select(orgs, :id, :name, current_user.org_id), class: 'form-control') %>\n              <\/div>\n            <\/div>\n          <% else %>\n            <%= hidden_field_tag(:org_id, current_user.org_id) %>\n          <% end %>\n          <div class=\"col-md-2\">\n            <%= submit_tag(_('Go'), class: 'btn btn-default') %>\n          <\/div>\n        <\/div>\n      <\/form>\n    <% else %>\n      <p class=\"bg-warning\">\n        <%= _('You don\\'t have access to use the API. An api token is needed to generate usage statistics.') %>\n      <\/p>\n    <% end %>\n  <\/div>\n<\/div>\n<div class=\"row\">\n  <div class=\"col-md-12\">\n    <div data-topics style=\"display: none\">\n      <div class=\"row\">\n        <div class=\"col-md-3\">\n          <div class=\"panel panel-default\">\n            <div class=\"panel-body\">\n              <h4 style=\"margin-top:0px\" style=\"display: none\" data-topic=\"users\"><%= _('New users') %><\/h4>\n              <h4 style=\"margin-top:0px\" style=\"display: none\" data-topic=\"plans\"><%= _('Plans') %><\/h4>\n              <strong data-range style=\"font-size: 36px;\"><\/strong>\n            <\/div>\n          <\/div>\n        <\/div>\n        <div class=\"col-md-3\">\n          <div class=\"panel panel-default\">\n            <div class=\"panel-body\">\n              <h4 style=\"margin-top:0px\" style=\"display: none\" data-topic=\"users\"><%= _('Total users') %><\/h4>\n              <h4 style=\"margin-top:0px\" style=\"display: none\" data-topic=\"plans\"><%= _('Total plans') %><\/h4>\n              <strong data-totals style=\"font-size: 36px;\"><\/strong>\n            <\/div>\n          <\/div>\n        <\/div>\n      <\/div>\n    <\/div>\n    <hr \/>\n  <\/div>\n<\/div>\n<div class=\"row\">\n  <div class=\"col-md-6\">\n    <div class=\"pull-left\">\n      <h4><%= _('No. users joined during last year') %><\/h4>\n    <\/div>\n    <div class=\"pull-right\">\n      <button type=\"button\" class=\"btn btn-default\" data-url=\"<%= users_joined_api_v0_statistics_path(format: :csv) %>\">\n        <%= _('Download') %> <i class=\"fa fa-download\" aria-hidden=\"true\"><\/i>\n      <\/button>\n    <\/div>\n    <div class=\"clearfix\"><\/div>\n    <p class=\"alert alert-info\" style=\"display: none;\"><%= _('There is no data available for users joined yet.') %><\/p>\n    <canvas id=\"yearly_users\"><\/canvas>\n  <\/div>\n  <div class=\"col-md-6\">\n    <div class=\"pull-left\">\n      <h4><%= _('No. plans during last year') %><\/h4>\n    <\/div>\n    <div class=\"pull-right\">\n      <button type=\"button\" class=\"btn btn-default\" data-url=\"<%= created_plans_api_v0_statistics_path(format: :csv) %>\">\n        <%= _('Download') %> <i class=\"fa fa-download\" aria-hidden=\"true\"><\/i>\n      <\/button>\n    <\/div>\n    <div class=\"clearfix\"><\/div>\n    <p class=\"alert alert-info\" style=\"display: none;\"><%= _('There is no data available for plans yet.') %><\/p>\n    <canvas id=\"yearly_plans\"><\/canvas>\n  <\/div>\n<\/div>","lang_cluster":"Ruby","length":111,"code_uid":"a9ee74744acd44618e1819c670f64861"}
{"diff_hunk":"@@ -141,10 +141,6 @@ module Bolt\n       inventory_target.password\n     end\n \n-    def options\n-      inventory_target.options\n-    end\n-\n     def plugin_hooks\n       inventory_target.plugin_hooks\n     end","old_code":"# frozen_string_literal: true\n\nrequire 'bolt\/error'\nrequire 'bolt\/util'\n\nmodule Bolt\n  class Target\n    attr_accessor :inventory\n\n    # Target.new from a data hash\n    def self.from_hash(hash, inventory)\n      target = inventory.create_target_from_hash(hash)\n      new(target.name, inventory)\n    end\n\n    # Target.new from a plan initialized with a hash\n    def self.from_asserted_hash(hash)\n      inventory = Puppet.lookup(:bolt_inventory)\n      from_hash(hash, inventory)\n    end\n\n    # TODO: Disallow any positional argument other than URI.\n    # Target.new from a plan with just a uri. Puppet requires the arguments to\n    # this method to match (by name) the attributes defined on the datatype.\n    # rubocop:disable Lint\/UnusedMethodArgument\n    def self.from_asserted_args(uri = nil,\n                                name = nil,\n                                safe_name = nil,\n                                target_alias = nil,\n                                config = nil,\n                                facts = nil,\n                                vars = nil,\n                                features = nil,\n                                plugin_hooks = nil)\n      from_asserted_hash('uri' => uri)\n    end\n    # rubocop:enable Lint\/UnusedMethodArgument\n\n    def initialize(name, inventory = nil)\n      @name = name\n      @inventory = inventory\n    end\n\n    # features returns an array to be compatible with plans\n    def features\n      @inventory.features(self).to_a\n    end\n\n    # Use feature_set internally to access set\n    def feature_set\n      @inventory.features(self)\n    end\n\n    def vars\n      @inventory.vars(self)\n    end\n\n    def facts\n      @inventory.facts(self)\n    end\n\n    def to_s\n      safe_name\n    end\n\n    def config\n      inventory_target.config\n    end\n\n    def safe_name\n      inventory_target.safe_name\n    end\n\n    def target_alias\n      inventory_target.target_alias\n    end\n\n    def to_h\n      options.to_h.merge(\n        'name' => name,\n        'uri' => uri,\n        'protocol' => protocol,\n        'user' => user,\n        'password' => password,\n        'host' => host,\n        'port' => port\n      )\n    end\n\n    def detail\n      {\n        'name' => name,\n        'uri' => uri,\n        'alias' => target_alias,\n        'config' => {\n          'transport' => transport,\n          transport => options.to_h\n        },\n        'vars' => vars,\n        'features' => features,\n        'facts' => facts,\n        'plugin_hooks' => plugin_hooks\n      }\n    end\n\n    def inventory_target\n      @inventory.targets[@name]\n    end\n\n    def host\n      inventory_target.host\n    end\n\n    attr_reader :name\n\n    def uri\n      inventory_target.uri\n    end\n\n    def remote?\n      protocol == 'remote'\n    end\n\n    def port\n      inventory_target.port\n    end\n\n    def transport\n      inventory_target.transport\n    end\n\n    def protocol\n      inventory_target.protocol || inventory_target.transport\n    end\n\n    def user\n      inventory_target.user\n    end\n\n    def password\n      inventory_target.password\n    end\n\n    def options\n      inventory_target.options\n    end\n\n    def plugin_hooks\n      inventory_target.plugin_hooks\n    end\n\n    def eql?(other)\n      self.class.equal?(other.class) && @name == other.name\n    end\n    alias == eql?\n  end\nend\n","lang_cluster":"Ruby","length":157,"code_uid":"1a45b4dd64c446328c5c55dd32186821"}
{"diff_hunk":"@@ -9,7 +9,8 @@ module Travis\n     class Script\n       class Nix < Script\n         DEFAULTS = {\n-          nix: '2.0.4'\n+          nix: '2.0.4',\n+          channels: {}\n         }\n \n         def export","old_code":"# Maintained by\n#  - Domen Ko\u017ear        @domenkozar   <domen@dev.si>\n#  - Rok Garbas         @garbas       <rok@garbas.si>\n#  - Matthew Bauer      @matthewbauer <mjbauer95@gmail.com>\n#  - Graham Christensen @grahamc      <graham@grahamc.com>\n\nmodule Travis\n  module Build\n    class Script\n      class Nix < Script\n        DEFAULTS = {\n          nix: '2.0.4'\n        }\n\n        def export\n          super\n\n          # prevent curl from polluting logs but still show errors\n          sh.export 'NIX_CURL_FLAGS', '-sS'\n        end\n\n        def configure\n          super\n\n          sh.cmd \"echo '-s' >> ~\/.curlrc\"\n          sh.cmd \"echo '-S' >> ~\/.curlrc\"\n          sh.cmd \"echo '--retry 3' >> ~\/.curlrc\"\n\n          # Nix needs to be able to exec on \/tmp on Linux\n          # This will emit an error in the container but\n          # it's still needed for \"trusty\" Linux.\n          if config[:os] == 'linux'\n            sh.cmd \"sudo mount -o remount,exec \/run\"\n            sh.cmd \"sudo mount -o remount,exec \/run\/user\"\n            sh.cmd \"sudo mkdir -p -m 0755 \/nix\/\"\n            sh.cmd \"sudo chown $USER \/nix\/\"\n            # Set nix config dir and make config Hydra compatible\n            sh.cmd \"echo 'build-max-jobs = 4' | sudo tee \/etc\/nix\/nix.conf > \/dev\/null\"\n          end\n        end\n\n        def setup\n          super\n\n          version = config[:nix]\n\n          sh.fold 'nix.install' do\n            sh.cmd \"wget --retry-connrefused --waitretry=1 -O \/tmp\/nix-install https:\/\/nixos.org\/releases\/nix\/nix-#{version}\/install\"\n            sh.cmd \"yes | sh \/tmp\/nix-install\"\n\n            if config[:os] == 'linux'\n              # single-user install (linux)\n              sh.cmd 'source ${TRAVIS_HOME}\/.nix-profile\/etc\/profile.d\/nix.sh'\n            else\n              # multi-user install (macos)\n              sh.cmd 'source \/nix\/var\/nix\/profiles\/default\/etc\/profile.d\/nix-daemon.sh'\n            end\n          end\n        end\n\n        def announce\n          super\n\n          sh.echo 'Nix support for Travis CI is community maintained.', ansi: :green\n          sh.echo 'Please open any issues at https:\/\/github.com\/travis-ci\/travis-ci\/issues\/new and cc @domenkozar @garbas @matthewbauer @grahamc', ansi: :green\n\n          sh.cmd \"nix-env --version\"\n          sh.cmd \"nix-instantiate --eval -E 'with import <nixpkgs> {}; lib.version or lib.nixpkgsVersion'\"\n        end\n\n        def script\n          sh.cmd 'nix-build'\n        end\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":77,"code_uid":"a7f3fe39622349c48510f30b3d12d759"}
{"diff_hunk":"@@ -21,12 +21,7 @@ module Bolt\n \n       def initialize\n         super\n-\n-        if Bolt::Util.windows?\n-          raise NotImplementedError, \"The local transport is not yet implemented on Windows\"\n-        else\n-          @conn = Shell.new\n-        end\n+        @conn = Shell.new\n       end\n \n       def in_tmpdir(base)","old_code":"# frozen_string_literal: true\n\nrequire 'json'\nrequire 'fileutils'\nrequire 'tmpdir'\nrequire 'bolt\/transport\/base'\nrequire 'bolt\/util'\n\nmodule Bolt\n  module Transport\n    class Local < Base\n      def self.options\n        %w[tmpdir]\n      end\n\n      def provided_features\n        ['shell']\n      end\n\n      def self.validate(_options); end\n\n      def initialize\n        super\n\n        if Bolt::Util.windows?\n          raise NotImplementedError, \"The local transport is not yet implemented on Windows\"\n        else\n          @conn = Shell.new\n        end\n      end\n\n      def in_tmpdir(base)\n        args = base ? [nil, base] : []\n        Dir.mktmpdir(*args) do |dir|\n          yield dir\n        end\n      rescue StandardError => e\n        raise Bolt::Node::FileError.new(\"Could not make tempdir: #{e.message}\", 'TEMPDIR_ERROR')\n      end\n      private :in_tmpdir\n\n      def copy_file(source, destination)\n        FileUtils.cp_r(source, destination, remove_destination: true)\n      rescue StandardError => e\n        raise Bolt::Node::FileError.new(e.message, 'WRITE_ERROR')\n      end\n\n      def with_tmpscript(script, base)\n        in_tmpdir(base) do |dir|\n          dest = File.join(dir, File.basename(script))\n          copy_file(script, dest)\n          File.chmod(0o750, dest)\n          yield dest, dir\n        end\n      end\n      private :with_tmpscript\n\n      def upload(target, source, destination, _options = {})\n        copy_file(source, destination)\n        Bolt::Result.for_upload(target, source, destination)\n      end\n\n      def run_command(target, command, _options = {})\n        in_tmpdir(target.options['tmpdir']) do |dir|\n          output = @conn.execute(command, dir: dir)\n          Bolt::Result.for_command(target, output.stdout.string, output.stderr.string, output.exit_code)\n        end\n      end\n\n      def run_script(target, script, arguments, _options = {})\n        with_tmpscript(File.absolute_path(script), target.options['tmpdir']) do |file, dir|\n          logger.debug \"Running '#{file}' with #{arguments}\"\n\n          # unpack any Sensitive data AFTER we log\n          arguments = unwrap_sensitive_args(arguments)\n          if arguments.empty?\n            # We will always provide separated arguments, so work-around Open3's handling of a single\n            # argument as the entire command string for script paths containing spaces.\n            arguments = ['']\n          end\n          output = @conn.execute(file, *arguments, dir: dir)\n          Bolt::Result.for_command(target, output.stdout.string, output.stderr.string, output.exit_code)\n        end\n      end\n\n      def run_task(target, task, arguments, _options = {})\n        implementation = select_implementation(target, task)\n        executable = implementation['path']\n        input_method = implementation['input_method']\n        extra_files = implementation['files']\n\n        in_tmpdir(target.options['tmpdir']) do |dir|\n          if extra_files.empty?\n            script = File.join(dir, File.basename(executable))\n          else\n            arguments['_installdir'] = dir\n            script_dest = File.join(dir, task.tasks_dir)\n            FileUtils.mkdir_p([script_dest] + extra_files.map { |file| File.join(dir, File.dirname(file['name'])) })\n\n            script = File.join(script_dest, File.basename(executable))\n            extra_files.each do |file|\n              dest = File.join(dir, file['name'])\n              copy_file(file['path'], dest)\n              File.chmod(0o750, dest)\n            end\n          end\n\n          copy_file(executable, script)\n          File.chmod(0o750, script)\n\n          # unpack any Sensitive data, write it to a separate variable because\n          # we log 'arguments' below\n          unwrapped_arguments = unwrap_sensitive_args(arguments)\n          stdin = STDIN_METHODS.include?(input_method) ? JSON.dump(unwrapped_arguments) : nil\n          env = ENVIRONMENT_METHODS.include?(input_method) ? envify_params(unwrapped_arguments) : nil\n\n          # log the arguments with sensitive data redacted, do NOT log unwrapped_arguments\n          logger.debug(\"Running '#{script}' with #{arguments}\")\n\n          output = @conn.execute(script, stdin: stdin, env: env, dir: dir)\n          Bolt::Result.for_task(target, output.stdout.string, output.stderr.string, output.exit_code)\n        end\n      end\n\n      def connected?(_targets)\n        true\n      end\n    end\n  end\nend\n\nrequire 'bolt\/transport\/local\/shell'\n","lang_cluster":"Ruby","length":132,"code_uid":"0dafda5df7284b818f0834ea3d47bcc3"}
{"diff_hunk":"@@ -25,7 +25,7 @@ DEFAULT_TIMEOUT = 30\n DEFAULT_PORT = 0\n DEFAULT_HOST = None\n DEFAULT_LOG_LEVEL = None\n-DEFAULT_LOG_FILE = None\n+DEFAULT_SERVICE_LOG_PATH = None\n \n \n class WebDriver(RemoteWebDriver):","old_code":"# Licensed to the Software Freedom Conservancy (SFC) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The SFC licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport warnings\n\nfrom selenium.webdriver.common import utils\nfrom selenium.webdriver.remote.webdriver import WebDriver as RemoteWebDriver\nfrom .service import Service\nfrom .options import Options\n\nDEFAULT_TIMEOUT = 30\nDEFAULT_PORT = 0\nDEFAULT_HOST = None\nDEFAULT_LOG_LEVEL = None\nDEFAULT_LOG_FILE = None\n\n\nclass WebDriver(RemoteWebDriver):\n    \"\"\" Controls the IEServerDriver and allows you to drive Internet Explorer \"\"\"\n\n    def __init__(self, executable_path='IEDriverServer.exe', capabilities=None,\n                 port=DEFAULT_PORT, timeout=DEFAULT_TIMEOUT, host=DEFAULT_HOST,\n                 log_level=DEFAULT_LOG_LEVEL, log_file=DEFAULT_LOG_FILE, options=None,\n                 ie_options=None, desired_capabilities=None):\n        \"\"\"\n        Creates a new instance of the chrome driver.\n\n        Starts the service and then creates new instance of chrome driver.\n\n        :Args:\n         - executable_path - path to the executable. If the default is used it assumes the executable is in the $PATH\n         - capabilities: capabilities Dictionary object\n         - port - port you would like the service to run, if left as 0, a free port will be found.\n         - log_level - log level you would like the service to run.\n         - log_file - log file you would like the service to log to.\n         - options: IE Options instance, providing additional IE options\n         - desired_capabilities: alias of capabilities; this will make the signature consistent with RemoteWebDriver.\n        \"\"\"\n        if ie_options:\n            warnings.warn('use options instead of ie_options', DeprecationWarning)\n            options = ie_options\n        self.port = port\n        if self.port == 0:\n            self.port = utils.free_port()\n        self.host = host\n        self.log_level = log_level\n        self.log_file = log_file\n\n        # If both capabilities and desired capabilities are set, ignore desired capabilities.\n        if capabilities is None and desired_capabilities:\n            capabilities = desired_capabilities\n\n        if options is None:\n            if capabilities is None:\n                capabilities = self.create_options().to_capabilities()\n        else:\n            if capabilities is None:\n                capabilities = options.to_capabilities()\n            else:\n                # desired_capabilities stays as passed in\n                capabilities.update(options.to_capabilities())\n\n        self.iedriver = Service(\n            executable_path,\n            port=self.port,\n            host=self.host,\n            log_level=self.log_level,\n            log_file=self.log_file)\n\n        self.iedriver.start()\n\n        RemoteWebDriver.__init__(\n            self,\n            command_executor='http:\/\/localhost:%d' % self.port,\n            desired_capabilities=capabilities)\n        self._is_remote = False\n\n    def quit(self):\n        RemoteWebDriver.quit(self)\n        self.iedriver.stop()\n\n    def create_options(self):\n        return Options()\n","lang_cluster":"Ruby","length":96,"code_uid":"caf1c510f78d45c5bb88d30359181d22"}
{"diff_hunk":"@@ -117,11 +117,11 @@ module Travis\n \n           def set_jl_pkg\n             # Regular expression from: julia:base\/pkg\/entry.jl\n-            urlregex = 'r\"(?:^|[\/\\\\\\\\])(\\w+?)(?:\\.jl)?(?:\\.git)?$\"'\n-            jlcode = \"println(match(#{urlregex}, readchomp(STDIN)).captures[1])\"\n             shurl = \"git remote -v | head -n 1 | cut -f 2 | cut -f 1 -d ' '\"\n-            sh.export 'JL_PKG', \"$(#{shurl} | julia -e '#{jlcode}')\",\n-              echo: false\n+            m = \/(?:^|[\\\/\\\\\\\\])(\\w+?)(?:\\.jl)?(?:\\.git)?$\/.match(shurl)\n+            if m != nil\n+              sh.export 'JL_PKG', m[1], echo: false\n+            end\n           end\n       end\n     end","old_code":"# vim:set ts=2 sw=2 sts=2 autoindent:\n\n# Community maintainers:\n#\n#   Tony Kelman       <tony kelman net, @tkelman>\n#   Pontus Stenetorp  <pontus stenetorp se, @ninjin>\n#   Elliot Saba       <staticfloat gmail com, @staticfloat>\n#   Simon Byrne       <simonbyrne gmail.com, @simonbyrne>\n#\nmodule Travis\n  module Build\n    class Script\n      class Julia < Script\n        DEFAULTS = {\n          julia: 'release',\n        }\n\n        def export\n          super\n\n          sh.export 'TRAVIS_JULIA_VERSION', config[:julia].to_s.shellescape,\n            echo: false\n          sh.export 'JULIA_PROJECT', \"@.\"\n        end\n\n        def setup\n          super\n\n          sh.echo 'Julia for Travis-CI is not officially supported, ' \\\n            'but is community maintained.', ansi: :green\n          sh.echo 'Please file any issues using the following link',\n            ansi: :green\n          sh.echo '  https:\/\/github.com\/travis-ci\/travis-ci\/issues' \\\n            '\/new?labels=julia', ansi: :green\n          sh.echo 'and mention \\`@travis-ci\/julia-maintainers\\`' \\\n            'in the issue', ansi: :green\n\n          sh.fold 'Julia-install' do\n            sh.echo 'Installing Julia', ansi: :yellow\n            sh.cmd 'CURL_USER_AGENT=\"Travis-CI $(curl --version | head -n 1)\"'\n            case config[:os]\n            when 'linux'\n              sh.cmd 'mkdir -p ~\/julia'\n              sh.cmd %Q{curl -A \"$CURL_USER_AGENT\" -s -L --retry 7 '#{julia_url}' } \\\n                       '| tar -C ~\/julia -x -z --strip-components=1 -f -'\n            when 'osx'\n              sh.cmd %Q{curl -A \"$CURL_USER_AGENT\" -s -L --retry 7 -o julia.dmg '#{julia_url}'}\n              sh.cmd 'mkdir juliamnt'\n              sh.cmd 'hdiutil mount -readonly -mountpoint juliamnt julia.dmg'\n              sh.cmd 'cp -a juliamnt\/*.app\/Contents\/Resources\/julia ~\/'\n            else\n              sh.failure \"Operating system not supported: #{config[:os]}\"\n            end\n            sh.cmd 'export PATH=\"${PATH}:${HOME}\/julia\/bin\"'\n          end\n        end\n\n        def announce\n          super\n\n          sh.cmd \"julia -e 'versioninfo()'\"\n          sh.echo ''\n        end\n\n        def script\n          sh.echo 'Executing the default test script', ansi: :green\n          set_jl_pkg\n          sh.echo 'Package name determined from repository url to be ${JL_PKG}',\n            ansi: :green\n          # Check if the repository is a Julia package.\n          sh.if \"-f src\/${JL_PKG}.jl\" do\n            sh.if '-a .git\/shallow' do\n              sh.cmd 'git fetch --unshallow'\n            end\n            sh.cmd 'julia --color=yes -e \"if VERSION < v\\\"0.7.0-DEV.5183\\\" || (!isfile(\\\"Project.toml\\\") && !isfile(\\\"JuliaProject.toml\\\")); Pkg.clone(pwd()); end\"'\n            sh.cmd 'julia --color=yes -e \"Pkg.build(\\\"${JL_PKG}\\\")\"'\n            sh.if '-f test\/runtests.jl' do\n              sh.cmd 'julia --check-bounds=yes --color=yes ' \\\n                '-e \"Pkg.test(\\\"${JL_PKG}\\\", coverage=true)\"'\n            end\n          end\n          sh.else do\n            sh.echo '\\`src\/${JL_PKG}.jl\\` not found, repository is not a '\\\n              'valid Julia package so the default test script is empty',\n              ansi: :yellow\n          end\n        end\n\n        private\n\n          def julia_url\n            case config[:os]\n            when 'linux'\n              osarch = 'linux\/x64'\n              ext = 'linux-x86_64.tar.gz'\n              nightlyext = 'linux64.tar.gz'\n            when 'osx'\n              osarch = 'mac\/x64'\n              ext = 'mac64.dmg'\n              nightlyext = ext\n            end\n            case julia_version = Array(config[:julia]).first.to_s\n            when 'release'\n              # CHANGEME on new minor releases (once or twice a year)\n              url = \"julialang-s3.julialang.org\/bin\/#{osarch}\/0.6\/julia-0.6-latest-#{ext}\"\n            when 'nightly'\n              url = \"julialangnightlies-s3.julialang.org\/bin\/#{osarch}\/julia-latest-#{nightlyext}\"\n            when \/^(\\d+\\.\\d+)\\.\\d+$\/\n              url = \"julialang-s3.julialang.org\/bin\/#{osarch}\/#{$1}\/julia-#{julia_version}-#{ext}\"\n            when \/^(\\d+\\.\\d+)$\/\n              url = \"julialang-s3.julialang.org\/bin\/#{osarch}\/#{$1}\/julia-#{$1}-latest-#{ext}\"\n            else\n              sh.failure \"Unknown Julia version: #{julia_version}\"\n            end\n            \"https:\/\/#{url}\"\n          end\n\n          def set_jl_pkg\n            # Regular expression from: julia:base\/pkg\/entry.jl\n            urlregex = 'r\"(?:^|[\/\\\\\\\\])(\\w+?)(?:\\.jl)?(?:\\.git)?$\"'\n            jlcode = \"println(match(#{urlregex}, readchomp(STDIN)).captures[1])\"\n            shurl = \"git remote -v | head -n 1 | cut -f 2 | cut -f 1 -d ' '\"\n            sh.export 'JL_PKG', \"$(#{shurl} | julia -e '#{jlcode}')\",\n              echo: false\n          end\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":129,"code_uid":"3ce8a0b7a06c44228b3eb90347738b32"}
{"diff_hunk":"@@ -1,5 +1,7 @@\n # frozen_string_literal: true\n \n+require 'addressable'\n+\n module Bolt\n   class ApplyTarget\n     ATTRIBUTES = %i[uri name target_alias config vars facts features","old_code":"# frozen_string_literal: true\n\nmodule Bolt\n  class ApplyTarget\n    ATTRIBUTES = %i[uri name target_alias config vars facts features\n                    plugin_hooks safe_name].freeze\n    COMPUTED = %i[host password port protocol user].freeze\n\n    attr_reader(*ATTRIBUTES)\n    attr_accessor(*COMPUTED)\n\n    # rubocop:disable Lint\/UnusedMethodArgument\n    # Target.new from a plan initialized with a hash\n    def self.from_asserted_hash(hash)\n      raise Bolt::Error.new(\"Target objects cannot be instantiated inside apply blocks\", 'bolt\/apply-error')\n    end\n\n    # Target.new from a plan with just a uri.\n    def self.from_asserted_args(uri = nil,\n                                name = nil,\n                                safe_name = nil,\n                                target_alias = nil,\n                                config = nil,\n                                facts = nil,\n                                vars = nil,\n                                features = nil,\n                                plugin_hooks = nil)\n      raise Bolt::Error.new(\"Target objects cannot be instantiated inside apply blocks\", 'bolt\/apply-error')\n    end\n    # rubocop:enable Lint\/UnusedMethodArgument\n\n    def self._pcore_init_from_hash\n      raise \"ApplyTarget shouldn't be instantiated from a pcore_init class method. How did this get called?\"\n    end\n\n    def _pcore_init_from_hash(init_hash)\n      inventory = Puppet.lookup(:bolt_inventory)\n      initialize(init_hash, inventory.config_hash)\n      inventory.create_apply_target(self)\n      self\n    end\n\n    def initialize(target_hash, config)\n      ATTRIBUTES.each do |attr|\n        instance_variable_set(\"@#{attr}\", target_hash[attr.to_s])\n      end\n\n      # Merge the config hash with inventory config\n      config = Bolt::Util.deep_merge(config, @config || {})\n      transport = config['transport'] || 'ssh'\n      t_conf = config['transports'][transport] || {}\n      uri_obj = parse_uri(uri)\n      @host = uri_obj.hostname || t_conf['host']\n      @password = Addressable::URI.unencode_component(uri_obj.password) || t_conf['password']\n      @port = uri_obj.port || t_conf['port']\n      @protocol = uri_obj.scheme || transport\n      @user = Addressable::URI.unencode_component(uri_obj.user) || t_conf['user']\n    end\n\n    def to_s\n      @safe_name\n    end\n\n    def parse_uri(string)\n      require 'addressable\/uri'\n      if string.nil?\n        Addressable::URI.new\n        # Forbid empty uri\n      elsif string.empty?\n        raise Bolt::ParseError, \"Could not parse target URI: URI is empty string\"\n      elsif string =~ %r{^[^:]+:\/\/}\n        Addressable::URI.parse(string)\n      else\n        # Initialize with an empty scheme to ensure we parse the hostname correctly\n        Addressable::URI.parse(\"\/\/#{string}\")\n      end\n    rescue Addressable::URI::InvalidURIError => e\n      raise Bolt::ParseError, \"Could not parse target URI: #{e.message}\"\n    end\n\n    def hash\n      @name.hash\n    end\n  end\nend\n","lang_cluster":"Ruby","length":85,"code_uid":"ee4919caf592423bab0ad15f5e378545"}
{"diff_hunk":"@@ -5,7 +5,7 @@ class Users::OmniauthCallbacksController < Devise::OmniauthCallbacksController\n   ##\n   # Dynamically build a handler for each omniauth provider\n   # -------------------------------------------------------------\n-  IdentifierScheme.where(active: true).each do |scheme|\n+  IdentifierScheme.authenticatable.each do |scheme|\n     define_method(scheme.name.downcase) do\n       handle_omniauth(scheme)\n     end","old_code":"# frozen_string_literal: true\n\nclass Users::OmniauthCallbacksController < Devise::OmniauthCallbacksController\n\n  ##\n  # Dynamically build a handler for each omniauth provider\n  # -------------------------------------------------------------\n  IdentifierScheme.where(active: true).each do |scheme|\n    define_method(scheme.name.downcase) do\n      handle_omniauth(scheme)\n    end\n  end\n\n  # Processes callbacks from an omniauth provider and directs the user to\n  # the appropriate page:\n  #   Not logged in and uid had no match ---> Sign Up page\n  #   Not logged in and uid had a match ---> Sign In and go to Home Page\n  #   Signed in and uid had no match --> Save the uid and go to the Profile Page\n  #   Signed in and uid had a match --> Go to the Home Page\n  #\n  # scheme - The IdentifierScheme for the provider\n  #\n  def handle_omniauth(scheme)\n    if request.env[\"omniauth.auth\"].nil?\n      user = User.from_omniauth(request.env)\n    else\n      user = User.from_omniauth(request.env[\"omniauth.auth\"])\n    end\n\n    # If the user isn't logged in\n    if current_user.nil?\n      # If the uid didn't have a match in the system send them to register\n      if user.nil?\n        session[\"devise.#{scheme.name.downcase}_data\"] = request.env[\"omniauth.auth\"]\n        redirect_to new_user_registration_url\n\n      # Otherwise sign them in\n      else\n        # Until ORCID becomes supported as a login method\n        if scheme.name == \"shibboleth\"\n          if is_navigational_format?\n            set_flash_message(:notice, :success, kind: scheme.description)\n          end\n          sign_in_and_redirect user, event: :authentication\n        else\n          flash[:notice] = _(\"Successfully signed in\")\n          redirect_to new_user_registration_url\n        end\n      end\n\n    # The user is already logged in and just registering the uid with us\n    else\n      # If the user could not be found by that uid then attach it to their record\n      if user.nil?\n        if UserIdentifier.create(identifier_scheme: scheme,\n                                 identifier: request.env[\"omniauth.auth\"].uid,\n                                 user: current_user)\n          # rubocop:disable Metrics\/LineLength\n          flash[:notice] = _(\"Your account has been successfully linked to %{scheme}.\") % {\n            scheme: scheme.description\n          }\n          # rubocop:enable Metrics\/LineLength\n        else\n          flash[:alert] = _(\"Unable to link your account to %{scheme}.\") % {\n            scheme: scheme.description\n          }\n        end\n\n      else\n        # If a user was found but does NOT match the current user then the identifier has\n        # already been attached to another account (likely the user has 2 accounts)\n        identifier = UserIdentifier.where(\n          identifier: request.env[\"omniauth.auth\"].uid\n        ).first\n        if identifier.user.id != current_user.id\n          # rubocop:disable Metrics\/LineLength\n          flash[:alert] = _(\"The current #{scheme.description} iD has been already linked to a user with email #{identifier.user.email}\")\n          # rubocop:enable Metrics\/LineLength\n        end\n\n        # Otherwise, the identifier was found and it matches the one already associated\n        # with the current user so nothing else needs to be done\n      end\n\n      # Redirect to the User Profile page\n      redirect_to edit_user_registration_path\n    end\n  end\n\n  def failure\n    redirect_to root_path\n  end\n\nend\n","lang_cluster":"Ruby","length":94,"code_uid":"723d2cdb86d9413cbf7186a7a74b6399"}
{"diff_hunk":"@@ -1,5 +1,5 @@\n module ApplicationHelper\n-  \n+\n   def resource_name\n     :user\n   end","old_code":"module ApplicationHelper\n  \n  def resource_name\n    :user\n  end\n\n  # ---------------------------------------------------------------------------\n  def resource\n    @resource ||= User.new\n  end\n\n  # ---------------------------------------------------------------------------\n  def devise_mapping\n    @devise_mapping ||= Devise.mappings[:user]\n  end\n  \n  # ---------------------------------------------------------------------------\n  def hash_to_js_json_variable(obj_name, hash)\n    \"<script type=\\\"text\/javascript\\\">var #{obj_name} = #{hash.to_json};<\/script>\".html_safe\n  end\n\n  # Determines whether or not the URL path passed matches with the full path (including params) of the last URL requested.\n  # see http:\/\/api.rubyonrails.org\/classes\/ActionDispatch\/Request.html#method-i-fullpath for details\n  # ---------------------------------------------------------------------------\n  def isActivePage(path, exact_match = false)\n    if exact_match\n      return request.fullpath == path\n    else\n      return request.fullpath.include?(path)\n    end\n  end\n\n  def is_integer?(string)\n    return string.present? && string.match(\/^(\\d)+$\/)\n  end\n\n  def fingerprinted_asset(name)\n    Rails.env.production? ? \"#{name}-#{ASSET_FINGERPRINT}\" : name\n  end\n\n  def title(page_title)\n    content_for(:title) { page_title }\n  end\nend\n","lang_cluster":"Ruby","length":44,"code_uid":"bc4bf9454ba1487387275c1d0f7b7b9c"}
{"diff_hunk":"@@ -47,6 +47,9 @@ module Beaker\n       @machines.each_key do |type|\n         @hypervisors[type] = Beaker::Hypervisor.create(type, @machines[type], @options)\n         @hosts << @machines[type]\n+        @machines[type].each do |host|\n+          log_provisioning host, true\n+        end\n       end\n       @hosts = @hosts.flatten\n       @hosts","old_code":"[ 'hypervisor' ].each do |lib|\n  require \"beaker\/#{lib}\"\nend\n\nmodule Beaker\n  #Object that holds all the provisioned and non-provisioned virtual machines.\n  #Controls provisioning, configuration, validation and cleanup of those virtual machines.\n  class NetworkManager\n\n    #Determine if a given host should be provisioned.\n    #Provision if:\n    # - only if we are running with ---provision\n    # - only if we have a hypervisor\n    # - only if either the specific hosts has no specification or has 'provision' in its config\n    # - always if it is a vagrant box (vagrant boxes are always provisioned as they always need ssh key hacking)\n    def provision? options, host\n      command_line_says = options[:provision]\n      host_says = host['hypervisor'] && (host.has_key?('provision') ? host['provision'] : true)\n      (command_line_says && host_says) or (host['hypervisor'] =~\/vagrant\/)\n    end\n\n    def initialize(options, logger)\n      @logger = logger\n      @options = options\n      @hosts = []\n      @machines = {}\n      @hypervisors = nil\n    end\n\n    #Provision all virtual machines.  Provision machines according to their set hypervisor, if no hypervisor\n    #is selected assume that the described hosts are already up and reachable and do no provisioning.\n    def provision\n      if @hypervisors\n        cleanup\n      end\n      @hypervisors = {}\n      #sort hosts by their hypervisor, use hypervisor 'none' if no hypervisor is specified\n      @options['HOSTS'].each_key do |name|\n        host = @options['HOSTS'][name]\n        hypervisor = host['hypervisor']\n        hypervisor = provision?(@options, host) ? host['hypervisor'] : 'none'\n        @logger.debug \"Hypervisor for #{name} is #{hypervisor}\"\n        @machines[hypervisor] = [] unless @machines[hypervisor]\n        @machines[hypervisor] << Beaker::Host.create(name, @options)\n      end\n\n      @machines.each_key do |type|\n        @hypervisors[type] = Beaker::Hypervisor.create(type, @machines[type], @options)\n        @hosts << @machines[type]\n      end\n      @hosts = @hosts.flatten\n      @hosts\n    end\n\n    #Validate all provisioned machines, ensure that required packages are installed - if they are missing\n    #attempt to add them.\n    #@raise [Exception] Raise an exception if virtual machines fail to be validated\n    def validate\n      if @hypervisors\n        @hypervisors.each_key do |type|\n          @hypervisors[type].validate\n        end\n      end\n    end\n\n    #Configure all provisioned machines, adding any packages or settings required for SUTs\n    #@raise [Exception] Raise an exception if virtual machines fail to be configured\n    def configure\n      if @hypervisors\n        @hypervisors.each_key do |type|\n          @hypervisors[type].configure\n        end\n      end\n    end\n\n    # configure proxy on all provioned machines\n    #@raise [Exception] Raise an exception if virtual machines fail to be configured\n    def proxy_package_manager\n      if @hypervisors\n        @hypervisors.each_key do |type|\n          @hypervisors[type].proxy_package_manager\n        end\n      end\n    end\n\n    #Shut down network connections and revert all provisioned virtual machines\n    def cleanup\n      #shut down connections\n      @hosts.each {|host| host.close }\n\n      if @hypervisors\n        @hypervisors.each_key do |type|\n          @hypervisors[type].cleanup\n        end\n      end\n      @hypervisors = nil\n    end\n\n  end\nend\n","lang_cluster":"Ruby","length":100,"code_uid":"45ca0cf15ff24b9ab3c9868c0677deac"}
{"diff_hunk":"@@ -17,6 +17,7 @@\n package org.openqa.selenium.edgehtml;\n \n import static org.openqa.selenium.remote.CapabilityType.BROWSER_NAME;\n+import static org.openqa.selenium.remote.CapabilityType.PLATFORM_NAME;\n \n import com.google.auto.service.AutoService;\n ","old_code":"\/\/ Licensed to the Software Freedom Conservancy (SFC) under one\n\/\/ or more contributor license agreements.  See the NOTICE file\n\/\/ distributed with this work for additional information\n\/\/ regarding copyright ownership.  The SFC licenses this file\n\/\/ to you under the Apache License, Version 2.0 (the\n\/\/ \"License\"); you may not use this file except in compliance\n\/\/ with the License.  You may obtain a copy of the License at\n\/\/\n\/\/   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing,\n\/\/ software distributed under the License is distributed on an\n\/\/ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\/\/ KIND, either express or implied.  See the License for the\n\/\/ specific language governing permissions and limitations\n\/\/ under the License.\npackage org.openqa.selenium.edgehtml;\n\nimport static org.openqa.selenium.remote.CapabilityType.BROWSER_NAME;\n\nimport com.google.auto.service.AutoService;\n\nimport org.openqa.selenium.Capabilities;\nimport org.openqa.selenium.ImmutableCapabilities;\nimport org.openqa.selenium.SessionNotCreatedException;\nimport org.openqa.selenium.WebDriver;\nimport org.openqa.selenium.WebDriverException;\nimport org.openqa.selenium.WebDriverInfo;\nimport org.openqa.selenium.remote.BrowserType;\n\nimport java.util.Objects;\nimport java.util.Optional;\n\n@AutoService(WebDriverInfo.class)\npublic class EdgeHtmlDriverInfo implements WebDriverInfo {\n\n  @Override\n  public String getDisplayName() {\n    return \"EdgeHTML\";\n  }\n\n  @Override\n  public Capabilities getCanonicalCapabilities() {\n    return new ImmutableCapabilities(BROWSER_NAME, BrowserType.EDGE, EdgeHtmlOptions.USE_CHROMIUM, false);\n  }\n\n  @Override\n  public boolean isSupporting(Capabilities capabilities) {\n    return (BrowserType.EDGE.equals(capabilities.getBrowserName())\n            || capabilities.getCapability(\"ms:edgeOptions\") != null\n            || capabilities.getCapability(\"edgeOptions\") != null)\n           &&\n           Objects.equals(capabilities.getCapability(EdgeHtmlOptions.USE_CHROMIUM), false);\n  }\n\n  @Override\n  public boolean isAvailable() {\n    try {\n      EdgeHtmlDriverService.createDefaultService();\n      return true;\n    } catch (IllegalStateException | WebDriverException e) {\n      return false;\n    }\n  }\n\n  @Override\n  public int getMaximumSimultaneousSessions() {\n    return 1;\n  }\n\n  @Override\n  public Optional<WebDriver> createDriver(Capabilities capabilities)\n      throws SessionNotCreatedException {\n    if (!isAvailable()) {\n      return Optional.empty();\n    }\n\n    return Optional.of(new EdgeHtmlDriver(new EdgeHtmlOptions().merge(capabilities)));\n  }\n}\n","lang_cluster":"Ruby","length":80,"code_uid":"075c2d84adbf4b67a62845ae7ebc320e"}
{"diff_hunk":"@@ -154,10 +154,9 @@ module Bolt\n         result_promises = targets.zip(futures).flat_map do |target, future|\n           @executor.queue_execute([target]) do |transport, batch|\n             @executor.with_node_logging(\"Applying manifest block\", batch) do\n-              arguments = params.clone\n-              arguments['catalog'] = future.value\n+              arguments = { 'catalog' => future.value, '_noop' => options['_noop'] }\n               raise future.reason if future.rejected?\n-              result = transport.batch_task(batch, catalog_apply_task, arguments, {}, &notify)\n+              result = transport.batch_task(batch, catalog_apply_task, arguments, options, &notify)\n               provide_puppet_missing_errors(result)\n             end\n           end","old_code":"# frozen_string_literal: true\n\nrequire 'json'\nrequire 'logging'\nrequire 'open3'\nrequire 'concurrent'\nrequire 'bolt\/util\/puppet_log_level'\n\nmodule Bolt\n  Task = Struct.new(:name, :implementations, :input_method)\n\n  class Applicator\n    def initialize(inventory, executor, modulepath, pdb_client, hiera_config, max_compiles)\n      @inventory = inventory\n      @executor = executor\n      @modulepath = modulepath\n      @pdb_client = pdb_client\n      @hiera_config = hiera_config ? validate_hiera_config(hiera_config) : nil\n\n      @pool = Concurrent::ThreadPoolExecutor.new(max_threads: max_compiles)\n      @logger = Logging.logger[self]\n    end\n\n    private def libexec\n      @libexec ||= File.join(Gem::Specification.find_by_name('bolt').gem_dir, 'libexec')\n    end\n\n    def catalog_apply_task\n      @catalog_apply_task ||= begin\n        path = File.join(libexec, 'apply_catalog.rb')\n        impl = { 'name' => 'apply_catalog.rb', 'path' => path, 'requirements' => [], 'supports_noop' => true }\n        Task.new('apply_catalog', [impl], 'stdin')\n      end\n    end\n\n    def compile(target, ast, plan_vars)\n      trusted = Puppet::Context::TrustedInformation.new('local', target.host, {})\n\n      catalog_input = {\n        code_ast: ast,\n        modulepath: @modulepath,\n        pdb_config: @pdb_client.config.to_hash,\n        hiera_config: @hiera_config,\n        target: {\n          name: target.host,\n          facts: @inventory.facts(target),\n          variables: @inventory.vars(target).merge(plan_vars),\n          trusted: trusted.to_h\n        }\n      }\n\n      bolt_catalog_exe = File.join(libexec, 'bolt_catalog')\n\n      old_path = ENV['PATH']\n      ENV['PATH'] = \"#{RbConfig::CONFIG['bindir']}#{File::PATH_SEPARATOR}#{old_path}\"\n      out, err, stat = Open3.capture3('ruby', bolt_catalog_exe, 'compile', stdin_data: catalog_input.to_json)\n      ENV['PATH'] = old_path\n\n      # stderr may contain formatted logs from Puppet's logger or other errors.\n      # Print them in order, but handle them separately. Anything not a formatted log is assumed\n      # to be an error message.\n      logs = err.lines.map do |l|\n        begin\n          JSON.parse(l)\n        rescue StandardError\n          l\n        end\n      end\n      logs.each do |log|\n        if log.is_a?(String)\n          @logger.error(log.chomp)\n        else\n          log.map { |k, v| [k.to_sym, v] }.each do |level, msg|\n            bolt_level = Bolt::Util::PuppetLogLevel::MAPPING[level]\n            @logger.send(bolt_level, \"#{target.uri}: #{msg.chomp}\")\n          end\n        end\n      end\n\n      raise(ApplyError, target.uri) unless stat.success?\n      JSON.parse(out)\n    end\n\n    def validate_hiera_config(hiera_config)\n      if File.exist?(File.path(hiera_config))\n        data = File.open(File.path(hiera_config), \"r:UTF-8\") { |f| YAML.safe_load(f.read) }\n        unless data['version'] == 5\n          raise Bolt::ParseError, \"Hiera v5 is required, found v#{data['version'] || 3} in #{hiera_config}\"\n        end\n        hiera_config\n      end\n    end\n\n    def provide_puppet_missing_errors(result)\n      error_hash = result.error_hash\n      exit_code = error_hash['details']['exit_code'] if error_hash && error_hash['details']\n      # If we get exit code 126 or 127 back, it means the shebang command wasn't found; Puppet isn't present\n      if [126, 127].include?(exit_code)\n        Result.new(result.target, error:\n          {\n            'msg' => \"Puppet is not installed on the target, please install it to enable 'apply'\",\n            'kind' => 'bolt\/apply-error'\n          })\n      elsif exit_code == 1 && error_hash['msg'] =~ \/Could not find executable 'ruby.exe'\/\n        # Windows does not have Ruby present\n        Result.new(result.target, error:\n          {\n            'msg' => \"Puppet is not installed on the target in $env:ProgramFiles, please install it to enable 'apply'\",\n            'kind' => 'bolt\/apply-error'\n          })\n      elsif exit_code == 1 && error_hash['msg'] =~ \/cannot load such file -- puppet \\(LoadError\\)\/\n        # Windows uses a Ruby that doesn't have Puppet installed\n        # TODO: fix so we don't find other Rubies, or point to a known issues URL for more info\n        Result.new(result.target, error:\n          {\n            'msg' => 'Found a Ruby without Puppet present, please install Puppet ' \\\n                     \"or remove Ruby from $env:Path to enable 'apply'\",\n            'kind' => 'bolt\/apply-error'\n          })\n      else\n        result\n      end\n    end\n\n    def apply(args, apply_body, scope)\n      raise(ArgumentError, 'apply requires a TargetSpec') if args.empty?\n      type0 = Puppet.lookup(:pal_script_compiler).type('TargetSpec')\n      Puppet::Pal.assert_type(type0, args[0], 'apply targets')\n\n      params = {}\n      if args.count > 1\n        type1 = Puppet.lookup(:pal_script_compiler).type('Hash[String, Data]')\n        Puppet::Pal.assert_type(type1, args[1], 'apply options')\n        params = args[1]\n      end\n\n      # collect plan vars and merge them over target vars\n      plan_vars = scope.to_hash\n      %w[trusted server_facts facts].each { |k| plan_vars.delete(k) }\n\n      targets = @inventory.get_targets(args[0])\n      ast = Puppet::Pops::Serialization::ToDataConverter.convert(apply_body, rich_data: true, symbol_to_string: true)\n      notify = proc { |_| nil }\n\n      @executor.log_action('apply catalog', targets) do\n        futures = targets.map do |target|\n          Concurrent::Future.execute(executor: @pool) do\n            @executor.with_node_logging(\"Compiling manifest block\", [target]) do\n              compile(target, ast, plan_vars)\n            end\n          end\n        end\n\n        result_promises = targets.zip(futures).flat_map do |target, future|\n          @executor.queue_execute([target]) do |transport, batch|\n            @executor.with_node_logging(\"Applying manifest block\", batch) do\n              arguments = params.clone\n              arguments['catalog'] = future.value\n              raise future.reason if future.rejected?\n              result = transport.batch_task(batch, catalog_apply_task, arguments, {}, &notify)\n              provide_puppet_missing_errors(result)\n            end\n          end\n        end\n\n        @executor.await_results(result_promises)\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":170,"code_uid":"b28c5960474e488fa3b83724792185d4"}
{"diff_hunk":"@@ -10,7 +10,8 @@\n <div class=\"row\">\n   <div class=\"col-md-12\">\n     <!-- render navigation tabs for the template-->\n-    <%= render partial: \"\/org_admin\/templates\/navigation\", locals: local_assigns.merge({ modifiable: modifiable }) %>\n+    <%= render partial: \"\/org_admin\/templates\/navigation\",\n+               locals: local_assigns.merge({ modifiable: modifiable }) %>\n     <div class=\"tab-content\">\n       <div role=\"tabpanel\" class=\"tab-pane active\">\n         <div class=\"panel panel-default\">","old_code":"<% title \"#{template.title}\" %>\n<% modifiable = template.latest? && !template.customization_of.present? && template.id.present? && (template.org_id = current_user.org.id) %>\n<div class=\"row\">\n  <div class=\"col-md-12\">\n    <h1><%= template.title %><\/h1>\n    <% referrer = template.customization_of.present? ? customisable_org_admin_templates_path : organisational_org_admin_templates_path %>\n    <%= link_to _('View all templates'), referrer, class: 'btn btn-default pull-right' %>\n  <\/div>\n<\/div>\n<div class=\"row\">\n  <div class=\"col-md-12\">\n    <!-- render navigation tabs for the template-->\n    <%= render partial: \"\/org_admin\/templates\/navigation\", locals: local_assigns.merge({ modifiable: modifiable }) %>\n    <div class=\"tab-content\">\n      <div role=\"tabpanel\" class=\"tab-pane active\">\n        <div class=\"panel panel-default\">\n          <div class=\"panel-body\">\n            <% # locals: { phase, template, edit, current_section } %>\n            <div class=\"pull-left\">\n              <h2><%= _('Phase details')%><\/h2>\n            <\/div>\n            <div class=\"pull-right\">\n              <%= link_to(_('Preview'), preview_org_admin_template_phase_path(template, phase), { class: 'btn btn-default phase_preview_link', role: 'button' }) %>\n            <\/div>\n            <div class=\"clearfix\"><\/div>\n            <div class=\"row\">\n              <div class=\"col-md-12\">\n                <%= render partial: partial_path, locals: local_assigns.merge({ modifiable: modifiable }) %>\n              <\/div>\n            <\/div>\n            <div class=\"row\">\n              <div class=\"col-md-12\">\n                <h2><%= _('Sections') %><\/h2>\n\n                <div class=\"row\">\n\n                  <div class=\"col-sm-6\">\n                    <% if phase.sections.many? %>\n                      <div id=\"sections-accordion-controls\">\n                        <div class=\"accordion-controls\"\n                             data-parent=\"sections_accordion\">\n                          <a href=\"#\" data-toggle-direction=\"show\">\n                            <%= _('expand all') %>\n                          <\/a>\n                          <span>|<\/span>\n                          <a href=\"#\" data-toggle-direction=\"hide\">\n                            <%= _('collapse all') %>\n                          <\/a>\n                        <\/div>\n                      <\/div>\n                    <% end %>\n                  <\/div>\n\n                  <div class=\"col-sm-6\">\n                    <div class='text-right text-muted'>\n                      <i class=\"fa fa-info-circle small\"><\/i>\n                        <%= _(\"Drag arrows to rearrange customized sections.\") %>\n                        <% unless phase.sections.all?(&:modifiable?) %>\n                          <%= _(\"You may place them before or after the main template sections.\") %>\n                        <% end %>\n                    <\/div>\n                    <div class=\"clear\">\n                  <\/div>\n\n              <\/div>\n            <\/div>\n            <%= render partial: 'org_admin\/sections\/index',\n                       locals: local_assigns.merge(modifiable: modifiable) %>\n          <\/div>\n        <\/div>\n      <\/div>\n    <\/div>\n  <\/div>\n<\/div>\n","lang_cluster":"Ruby","length":74,"code_uid":"a71c926b43734ee992d81f05da49b8b2"}
{"diff_hunk":"@@ -36,9 +36,7 @@ class Trail < ActiveRecord::Base\n   end\n \n   def self.completed_for(user)\n-    all.\n-      map { |trail| TrailWithProgress.new(trail, user: user) }.\n-      select(&:complete?)\n+    TrailWithProgressQuery.new(all, user: user).select(&:complete?)\n   end\n \n   def to_s","old_code":"class Trail < ActiveRecord::Base\n  extend FriendlyId\n\n  include PgSearch\n  multisearchable against: [:name, :description], if: :published?\n\n  validates :name, :description, :topic, presence: true\n\n  belongs_to :topic\n  has_many :repositories, dependent: :destroy\n  has_many :statuses, as: :completeable, dependent: :destroy\n  has_many :users, through: :statuses\n  has_many \\\n    :steps,\n    -> { order \"steps.position ASC\" },\n    dependent: :destroy,\n    inverse_of: :trail\n  has_many :exercises,\n    through: :steps,\n    source: :completeable,\n    source_type: \"Exercise\"\n  has_many :videos, through: :steps, source: :completeable, source_type: \"Video\"\n\n  friendly_id :name, use: [:slugged, :finders]\n\n  def self.accessible_without_subscription?\n    false\n  end\n\n  def self.published\n    where(published: true)\n  end\n\n  def self.by_topic\n    includes(:topic).order(\"topics.name\")\n  end\n\n  def self.completed_for(user)\n    all.\n      map { |trail| TrailWithProgress.new(trail, user: user) }.\n      select(&:complete?)\n  end\n\n  def to_s\n    name\n  end\n\n  # Override setters so it preserves the order\n  def step_ids=(new_step_ids)\n    super\n    new_step_ids = new_step_ids.reject(&:blank?).map(&:to_i)\n\n    new_step_ids.each_with_index do |step_id, index|\n      steps.where(id: step_id).update_all(position: index + 1)\n    end\n  end\n\n  def completeables\n    steps.includes(:completeable).map(&:completeable)\n  end\n\n  def steps_remaining_for(user)\n    CompleteableWithProgressQuery.\n      new(user: user, completeables: completeables).\n      count { |completeable| completeable.state != Status::COMPLETE }\n  end\n\n  def update_state_for(user)\n    TrailWithProgress.new(self, user: user).update_status\n  end\n\n  def self.most_recent_published\n    order(created_at: :desc).published\n  end\n\n  def teachers\n    Teacher.joins(:video).merge(videos).to_a.uniq(&:user_id)\n  end\n\n  def topic_name\n    topic.name\n  end\n\n  def first_completeable\n    steps.order(:position).first.completeable\n  end\n\n  def trial_video\n    videos.where(accessible_without_subscription: true).first.wrapped\n  end\nend\n","lang_cluster":"Ruby","length":91,"code_uid":"f2329d25e3004854bd3c996e81be0db9"}
{"diff_hunk":"@@ -55,7 +55,7 @@ module Bolt\n \n       # Write the Puppetfile.\n       @outputter.print_message \"Writing Puppetfile at #{puppetfile_path}\"\n-      puppetfile.write(puppetfile_path)\n+      puppetfile.write(puppetfile_path, moduledir)\n \n       # Install the modules.\n       install_puppetfile(puppetfile_path, moduledir)","old_code":"# frozen_string_literal: true\n\nrequire 'bolt\/error'\nrequire 'bolt\/logger'\n\nmodule Bolt\n  class ModuleInstaller\n    def initialize(outputter, pal)\n      @outputter = outputter\n      @pal       = pal\n      @logger    = Bolt::Logger.logger(self)\n    end\n\n    # Adds a single module to the project.\n    #\n    def add(name, modules, puppetfile_path, moduledir, config_path)\n      require 'bolt\/puppetfile'\n\n      # If the project configuration file already includes this module,\n      # exit early.\n      puppetfile  = Bolt::Puppetfile.new(modules)\n      new_module  = Bolt::Puppetfile::Module.from_hash('name' => name)\n\n      if puppetfile.modules.include?(new_module)\n        @outputter.print_message \"Project configuration file #{config_path} already \"\\\n                                 \"includes module #{new_module}. Nothing to do.\"\n        return true\n      end\n\n      # If the Puppetfile exists, make sure it's managed by Bolt.\n      if puppetfile_path.exist?\n        assert_managed_puppetfile(puppetfile, puppetfile_path)\n      end\n\n      # Create a Puppetfile object that includes the new module and its\n      # dependencies. We error early here so we don't add the new module to the\n      # project config or modify the Puppetfile.\n      puppetfile = add_new_module_to_puppetfile(new_module, modules, puppetfile_path)\n\n      # Add the module to the project configuration.\n      @outputter.print_message \"Updating project configuration file at #{config_path}\"\n\n      data = Bolt::Util.read_yaml_hash(config_path, 'project')\n      data['modules'] ||= []\n      data['modules'] <<  { 'name' => new_module.title }\n\n      begin\n        File.write(config_path, data.to_yaml)\n      rescue SystemCallError => e\n        raise Bolt::FileError.new(\n          \"Unable to update project configuration file: #{e.message}\",\n          config\n        )\n      end\n\n      # Write the Puppetfile.\n      @outputter.print_message \"Writing Puppetfile at #{puppetfile_path}\"\n      puppetfile.write(puppetfile_path)\n\n      # Install the modules.\n      install_puppetfile(puppetfile_path, moduledir)\n    end\n\n    # Creates a new Puppetfile that includes the new module and its dependencies.\n    #\n    private def add_new_module_to_puppetfile(new_module, modules, path)\n      @outputter.print_message \"Resolving module dependencies, this may take a moment\"\n\n      # If there is an existing Puppetfile, add the new module and attempt\n      # to resolve. This will not update the versions of any installed modules.\n      if path.exist?\n        puppetfile = Bolt::Puppetfile.parse(path)\n        puppetfile.add_modules(new_module)\n\n        begin\n          puppetfile.resolve\n          return puppetfile\n        rescue Bolt::Error\n          @logger.debug \"Unable to find a version of #{new_module} compatible \"\\\n                        \"with installed modules. Attempting to re-resolve modules \"\\\n                        \"from project configuration; some versions of installed \"\\\n                        \"modules may change.\"\n        end\n      end\n\n      # If there is not an existing Puppetfile, or resolving with pinned\n      # modules fails, resolve all of the module declarations with the new\n      # module.\n      puppetfile = Bolt::Puppetfile.new(modules)\n      puppetfile.add_modules(new_module)\n      puppetfile.resolve\n      puppetfile\n    end\n\n    # Installs a project's module dependencies.\n    #\n    def install(modules, path, moduledir, force: false, resolve: true)\n      require 'bolt\/puppetfile'\n\n      puppetfile = Bolt::Puppetfile.new(modules)\n\n      # If the Puppetfile exists, check if it includes specs for each declared\n      # module, erroring if there are any missing. Otherwise, resolve the\n      # module dependencies and write a new Puppetfile. Users can forcibly\n      # overwrite an existing Puppetfile with the '--force' option, or opt to\n      # install the Puppetfile as-is with --no-resolve.\n      #\n      # This is just if resolve is not false (nil should default to true)\n      if resolve != false\n        if path.exist? && !force\n          assert_managed_puppetfile(puppetfile, path)\n        else\n          @outputter.print_message \"Resolving module dependencies, this may take a moment\"\n          puppetfile.resolve\n\n          @outputter.print_message \"Writing Puppetfile at #{path}\"\n          puppetfile.write(path)\n        end\n      end\n\n      # Install the modules.\n      install_puppetfile(path, moduledir)\n    end\n\n    # Installs the Puppetfile and generates types.\n    #\n    def install_puppetfile(path, moduledir, config = {})\n      require 'bolt\/puppetfile\/installer'\n\n      @outputter.print_message \"Syncing modules from #{path} to #{moduledir}\"\n      ok = Bolt::Puppetfile::Installer.new(config).install(path, moduledir)\n\n      # Automatically generate types after installing modules\n      @pal.generate_types\n\n      @outputter.print_puppetfile_result(ok, path, moduledir)\n\n      ok\n    end\n\n    # Asserts that an existing Puppetfile is managed by Bolt.\n    #\n    private def assert_managed_puppetfile(puppetfile, path)\n      existing_puppetfile = Bolt::Puppetfile.parse(path)\n\n      unless existing_puppetfile.modules.superset? puppetfile.modules\n        missing_modules = puppetfile.modules - existing_puppetfile.modules\n\n        message = <<~MESSAGE.chomp\n          Puppetfile #{path} is missing specifications for the following\n          module declarations:\n\n          #{missing_modules.map(&:to_hash).to_yaml.lines.drop(1).join.chomp}\n          \n          This may not be a Puppetfile managed by Bolt. To forcibly overwrite the\n          Puppetfile, run 'bolt module install --force'.\n        MESSAGE\n\n        raise Bolt::Error.new(message, 'bolt\/missing-module-specs')\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":163,"code_uid":"231a35f959f248ca8bace5d62cfaf9db"}
{"diff_hunk":"@@ -4,8 +4,9 @@ require 'shellwords'\n module RSpec\n   module Core\n     # Responsible for utilizing externally provided configuration options,\n-    # whether via the command line, `.rspec`, `~\/.rspec`, `.rspec-local`\n-    # or a custom options file.\n+    # whether via the command line, `.rspec`, `~\/.rspec`,\n+    # `$XDG_CONFIG_HOME\/rspec\/options`, `.rspec-local` or a custom options\n+    # file.\n     class ConfigurationOptions\n       # @param args [Array<String>] command line arguments\n       def initialize(args)","old_code":"require 'erb'\nrequire 'shellwords'\n\nmodule RSpec\n  module Core\n    # Responsible for utilizing externally provided configuration options,\n    # whether via the command line, `.rspec`, `~\/.rspec`, `.rspec-local`\n    # or a custom options file.\n    class ConfigurationOptions\n      # @param args [Array<String>] command line arguments\n      def initialize(args)\n        @args = args.dup\n        organize_options\n      end\n\n      # Updates the provided {Configuration} instance based on the provided\n      # external configuration options.\n      #\n      # @param config [Configuration] the configuration instance to update\n      def configure(config)\n        process_options_into config\n        configure_filter_manager config.filter_manager\n        load_formatters_into config\n      end\n\n      # @api private\n      # Updates the provided {FilterManager} based on the filter options.\n      # @param filter_manager [FilterManager] instance to update\n      def configure_filter_manager(filter_manager)\n        @filter_manager_options.each do |command, value|\n          filter_manager.__send__ command, value\n        end\n      end\n\n      # @return [Hash] the final merged options, drawn from all external sources\n      attr_reader :options\n\n      # @return [Array<String>] the original command-line arguments\n      attr_reader :args\n\n    private\n\n      def organize_options\n        @filter_manager_options = []\n\n        @options = (file_options << command_line_options << env_options).each do |opts|\n          @filter_manager_options << [:include, opts.delete(:inclusion_filter)] if opts.key?(:inclusion_filter)\n          @filter_manager_options << [:exclude, opts.delete(:exclusion_filter)] if opts.key?(:exclusion_filter)\n        end\n\n        @options = @options.inject(:libs => [], :requires => []) do |hash, opts|\n          hash.merge(opts) do |key, oldval, newval|\n            [:libs, :requires].include?(key) ? oldval + newval : newval\n          end\n        end\n      end\n\n      UNFORCED_OPTIONS = Set.new([\n        :requires, :profile, :drb, :libs, :files_or_directories_to_run,\n        :full_description, :full_backtrace, :tty\n      ])\n\n      UNPROCESSABLE_OPTIONS = Set.new([:formatters])\n\n      def force?(key)\n        !UNFORCED_OPTIONS.include?(key)\n      end\n\n      def order(keys)\n        OPTIONS_ORDER.reverse_each do |key|\n          keys.unshift(key) if keys.delete(key)\n        end\n        keys\n      end\n\n      OPTIONS_ORDER = [\n        # It's important to set this before anything that might issue a\n        # deprecation (or otherwise access the reporter).\n        :deprecation_stream,\n\n        # load paths depend on nothing, but must be set before `requires`\n        # to support load-path-relative requires.\n        :libs,\n\n        # `files_or_directories_to_run` uses `default_path` so it must be\n        # set before it.\n        :default_path, :only_failures,\n\n        # These must be set before `requires` to support checking\n        # `config.files_to_run` from within `spec_helper.rb` when a\n        # `-rspec_helper` option is used.\n        :files_or_directories_to_run, :pattern, :exclude_pattern,\n\n        # Necessary so that the `--seed` option is applied before requires,\n        # in case required files do something with the provided seed.\n        # (such as seed global randomization with it).\n        :order,\n\n        # In general, we want to require the specified files as early as\n        # possible. The `--require` option is specifically intended to allow\n        # early requires. For later requires, they can just put the require in\n        # their spec files, but `--require` provides a unique opportunity for\n        # users to instruct RSpec to load an extension file early for maximum\n        # flexibility.\n        :requires\n      ]\n\n      def process_options_into(config)\n        opts = options.reject { |k, _| UNPROCESSABLE_OPTIONS.include? k }\n\n        order(opts.keys).each do |key|\n          force?(key) ? config.force(key => opts[key]) : config.__send__(\"#{key}=\", opts[key])\n        end\n      end\n\n      def load_formatters_into(config)\n        options[:formatters].each { |pair| config.add_formatter(*pair) } if options[:formatters]\n      end\n\n      def file_options\n        custom_options_file ? [custom_options] : [global_options, project_options, local_options]\n      end\n\n      def env_options\n        return {} unless ENV['SPEC_OPTS']\n\n        parse_args_ignoring_files_or_dirs_to_run(\n          Shellwords.split(ENV[\"SPEC_OPTS\"]),\n          \"ENV['SPEC_OPTS']\"\n        )\n      end\n\n      def command_line_options\n        @command_line_options ||= Parser.parse(@args)\n      end\n\n      def custom_options\n        options_from(custom_options_file)\n      end\n\n      def local_options\n        @local_options ||= options_from(local_options_file)\n      end\n\n      def project_options\n        @project_options ||= options_from(project_options_file)\n      end\n\n      def global_options\n        @global_options ||= options_from(global_options_file)\n      end\n\n      def options_from(path)\n        args = args_from_options_file(path)\n        parse_args_ignoring_files_or_dirs_to_run(args, path)\n      end\n\n      def parse_args_ignoring_files_or_dirs_to_run(args, source)\n        options = Parser.parse(args, source)\n        options.delete(:files_or_directories_to_run)\n        options\n      end\n\n      def args_from_options_file(path)\n        return [] unless path && File.exist?(path)\n        config_string = options_file_as_erb_string(path)\n        FlatMap.flat_map(config_string.split(\/\\n+\/), &:shellsplit)\n      end\n\n      def options_file_as_erb_string(path)\n        if RUBY_VERSION >= '2.6'\n          ERB.new(File.read(path), :trim_mode => '-').result(binding)\n        else\n          ERB.new(File.read(path), nil, '-').result(binding)\n        end\n      end\n\n      def custom_options_file\n        command_line_options[:custom_options_file]\n      end\n\n      def project_options_file\n        \".\/.rspec\"\n      end\n\n      def local_options_file\n        \".\/.rspec-local\"\n      end\n\n      def global_options_file\n        File.join(File.expand_path(\"~\"), \".rspec\")\n      rescue ArgumentError\n        # :nocov:\n        RSpec.warning \"Unable to find ~\/.rspec because the HOME environment variable is not set\"\n        nil\n        # :nocov:\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":200,"code_uid":"7da9bb1389454d5181e70a6a36c0eb44"}
{"diff_hunk":"@@ -9,6 +9,10 @@ class PurchasesController < ApplicationController\n       @purchase.defaults_from_user(current_user)\n       @active_card = retrieve_active_card\n       km.record(\"Checkout\", { \"Product Name\" => @purchaseable.name, \"Order Total\" => @purchase.price })\n+\n+      if @purchase.subscription? && signed_out?\n+        deny_access(t('shared.subscriptions.user_required'))\n+      end\n     end\n   end\n ","old_code":"class PurchasesController < ApplicationController\n  def new\n    @purchaseable = purchaseable\n\n    if current_user_has_active_subscription?\n      render 'for_subscribers'\n    else\n      @purchase = @purchaseable.purchases.build(variant: params[:variant])\n      @purchase.defaults_from_user(current_user)\n      @active_card = retrieve_active_card\n      km.record(\"Checkout\", { \"Product Name\" => @purchaseable.name, \"Order Total\" => @purchase.price })\n    end\n  end\n\n  def create\n    @purchaseable = purchaseable\n    @purchase = @purchaseable.purchases.build(params[:purchase])\n    @purchase.user = current_user\n\n    if use_coupon?\n      @purchase.coupon = Coupon.active.find_by_id(params[:coupon_id])\n    end\n\n    if use_existing_card?\n      @purchase.stripe_customer = current_user.stripe_customer\n    end\n\n    if @purchase.save\n      create_subscription if @purchaseable.subscription?\n\n      km_http_client.record(@purchase.email, \"Submit Payment\", { \"Product Name\" => @purchaseable.name, \"Order Total\" => @purchase.price })\n      track_purchase_if_paid(@purchase)\n\n      redirect_to success_url\n    else\n      @active_card = retrieve_active_card\n      render :new\n    end\n  end\n\n  def show\n    @purchase = Purchase.find_by_lookup(params[:id])\n    @purchaseable = @purchase.purchaseable\n    unless @purchase.paid?\n      redirect_to @purchaseable\n    end\n    km.record(\"View Receipt\", { \"Product Name\" => @purchaseable.name })\n  end\n\n  def watch\n    @purchase = Purchase.find_by_lookup(params[:id])\n    unless @purchase.paid?\n      redirect_to root_path and return\n    end\n    @purchaseable = @purchase.purchaseable\n\n    if @purchaseable.videos.one?\n      redirect_to [@purchase, @purchaseable.videos.first]\n    else\n      redirect_to [@purchase, :videos]\n    end\n  end\n\n  def paypal\n    @purchase = Purchase.find_by_lookup(params[:id])\n    @purchase.complete_paypal_payment!(params[:token], params[:PayerID])\n    track_purchase_if_paid(@purchase)\n    redirect_to @purchase\n  end\n\n  private\n\n  def create_subscription\n    current_user.create_subscription\n  end\n\n  def track_purchase_if_paid(purchase)\n    if purchase.paid?\n      km_http_client.record(purchase.email, \"Purchased\", { \"Product Name\" => purchase.purchaseable.name, \"Order Total\" => @purchase.price })\n    end\n  end\n\n  def purchaseable\n    if params[:product_id]\n      Product.find(params[:product_id])\n    elsif params[:section_id]\n      Section.find(params[:section_id])\n    end\n  end\n\n  def use_existing_card?\n    params[:use_existing_card] == 'on'\n  end\n\n  def use_coupon?\n    params[:coupon_id].present?\n  end\n\n  def retrieve_active_card\n    if current_user && current_user.stripe_customer\n      Stripe::Customer.retrieve(current_user.stripe_customer)['active_card']\n    end\n  end\n\n  def current_purchase\n    @current_purchase ||= Purchase.find_by_lookup(params[:id])\n  end\n\n  def current_purchaseable\n    @current_purchaseable ||= purchaseable\n  end\n\n  def success_url\n    if @purchase.paypal?\n      @purchase.paypal_url\n    elsif @purchase.subscription?\n      products_path\n    else\n      purchase_path @purchase\n    end\n  end\nend\n","lang_cluster":"Ruby","length":122,"code_uid":"f853dfcbb0b64c6c9486597cfebb190c"}
{"diff_hunk":"@@ -4,6 +4,7 @@ describe Repository do\n   it_behaves_like 'a class inheriting from Product'\n \n   it { should have_many(:collaborations).dependent(:destroy) }\n+  it { should belong_to(:product) }\n \n   it { should validate_presence_of(:github_team) }\n   it { should validate_presence_of(:github_url) }","old_code":"require \"rails_helper\"\n\ndescribe Repository do\n  it_behaves_like 'a class inheriting from Product'\n\n  it { should have_many(:collaborations).dependent(:destroy) }\n\n  it { should validate_presence_of(:github_team) }\n  it { should validate_presence_of(:github_url) }\n\n  describe \"#included_in_plan?\" do\n    it \"delegates to the plan's repositories feature\" do\n      expected = stub(\"plan.has_feature?(:repositories)\")\n      plan = stub(\"plan\")\n      plan.stubs(:has_feature?).with(:repositories).returns(expected)\n      repository = Repository.new\n\n      result = repository.included_in_plan?(plan)\n\n      expect(result).to eq(expected)\n    end\n  end\n\n  describe \"#has_collaborator?\" do\n    context \"after #add_collaborator with that user\" do\n      it \"returns true\" do\n        collaborator = create(:user)\n        repository = create(:repository)\n        repository.add_collaborator(collaborator)\n\n        expect(repository).to have_collaborator(collaborator)\n      end\n    end\n\n    context \"after #add_collaborator with another user\" do\n      it \"returns false\" do\n        collaborator = create(:user)\n        non_collaborator = create(:user)\n        repository = create(:repository)\n        repository.add_collaborator(collaborator)\n\n        expect(repository).not_to have_collaborator(non_collaborator)\n      end\n    end\n\n    context \"after #remove_collaborator\" do\n      it \"returns false\" do\n        collaborator = create(:user)\n        repository = create(:repository)\n        repository.add_collaborator(collaborator)\n        repository.remove_collaborator(collaborator)\n\n        expect(repository).not_to have_collaborator(collaborator)\n      end\n    end\n  end\n\n  describe \"#add_collaborator\" do\n    it \"creates a GitHub fulfillment\" do\n      collaborator = build_stubbed(:user, :with_github)\n      repository = build_stubbed(:repository)\n      fulfillment = stub_fulfillment(repository, collaborator)\n\n      repository.add_collaborator(collaborator)\n\n      expect(fulfillment).to have_received(:fulfill)\n    end\n  end\n\n  describe \"#remove_collaborator\" do\n    context \"for an existing collaborator\" do\n      it \"removes fulfillment\" do\n        collaborator = create(:user)\n        repository = create(:repository)\n        fulfillment = stub_fulfillment(repository, collaborator)\n        repository.add_collaborator(collaborator)\n\n        repository.remove_collaborator(collaborator)\n\n        expect(fulfillment).to have_received(:remove)\n      end\n    end\n\n    context \"for a non-collaborator\" do\n      it \"doesn't remove fulfillment\" do\n        collaborator = create(:user)\n        repository = create(:repository)\n        fulfillment = stub_fulfillment(repository, collaborator)\n\n        repository.remove_collaborator(collaborator)\n\n        expect(fulfillment).to have_received(:remove).never\n      end\n    end\n  end\n\n  describe \"#has_github_member?\" do\n    context \"with GitHub access\" do\n      it \"returns true\" do\n        user = build_stubbed(:user)\n        repository = build_stubbed(:repository)\n        client = stub_github_client\n        client.\n          stubs(:team_member?).\n          with(repository.github_team, user.github_username).\n          returns(true)\n\n        expect(repository).to have_github_member(user)\n      end\n    end\n\n    context \"without GitHub access\" do\n      it \"returns false\" do\n        user = build_stubbed(:user)\n        repository = build_stubbed(:repository)\n        client = stub_github_client\n        client.stubs(:team_member?).returns(false)\n\n        expect(repository).not_to have_github_member(user)\n      end\n    end\n\n    def stub_github_client\n      stub(\"github_client\").tap do |client|\n        Octokit::Client.\n          stubs(:new).\n          with(login: GITHUB_USER, password: GITHUB_PASSWORD).returns(client)\n      end\n    end\n  end\n\n  def stub_fulfillment(repository, user)\n    stub(\"fulfillment\").tap do |fulfillment|\n      fulfillment.stubs(:fulfill)\n      fulfillment.stubs(:remove)\n      GithubFulfillment.\n        stubs(:new).with(repository, user).\n        returns(fulfillment)\n    end\n  end\nend\n","lang_cluster":"Ruby","length":141,"code_uid":"9f744fbd57d84dadbc06c4e8be147807"}
{"diff_hunk":"@@ -5,6 +5,7 @@ require 'bolt\/result'\n require 'bolt\/config'\n require 'bolt\/notifier'\n require 'bolt\/node'\n+require 'bolt\/result_set'\n \n module Bolt\n   class Executor","old_code":"require 'json'\nrequire 'concurrent'\nrequire 'logging'\nrequire 'bolt\/result'\nrequire 'bolt\/config'\nrequire 'bolt\/notifier'\nrequire 'bolt\/node'\n\nmodule Bolt\n  class Executor\n    attr_reader :noop\n\n    def initialize(config = Bolt::Config.new, noop = nil, plan_logging = false)\n      @config = config\n      @logger = Logging.logger[self]\n\n      # If a specific elevated log level has been requested, honor that.\n      # Otherwise, escalate the log level to \"info\" if running in plan mode, so\n      # that certain progress messages will be visible.\n      default_log_level = plan_logging ? :info : :notice\n      @logger.level = @config[:log_level] || default_log_level\n      @noop = noop\n      @notifier = Bolt::Notifier.new\n    end\n\n    def from_targets(targets)\n      targets.map do |target|\n        Bolt::Node.from_target(target, config: @config)\n      end\n    end\n    private :from_targets\n\n    def on(nodes, callback = nil)\n      results = Concurrent::Map.new\n\n      poolsize = [nodes.length, @config[:concurrency]].min\n      pool = Concurrent::FixedThreadPool.new(poolsize)\n      @logger.debug { \"Started with #{poolsize} thread(s)\" }\n\n      nodes.map(&:class).uniq.each do |klass|\n        klass.initialize_transport(@logger)\n      end\n\n      nodes.each { |node|\n        pool.post do\n          result =\n            begin\n              @notifier.notify(callback, type: :node_start, target: node.target) if callback\n              node.connect\n              yield node\n            rescue StandardError => ex\n              Bolt::Result.from_exception(node.target, ex)\n            ensure\n              begin\n                node.disconnect\n              rescue StandardError => ex\n                @logger.info(\"Failed to close connection to #{node.uri} : #{ex.message}\")\n              end\n            end\n          results[node] = result\n          @notifier.notify(callback, type: :node_result, result: result) if callback\n          result\n        end\n      }\n      pool.shutdown\n      pool.wait_for_termination\n\n      @notifier.shutdown\n\n      results_to_hash(results)\n    end\n    private :on\n\n    def summary(action, object, result)\n      fc = result.select { |_, r| r.error }.length\n      npl = result.length == 1 ? '' : 's'\n      fpl = fc == 1 ? '' : 's'\n      \"Ran #{action} '#{object}' on #{result.length} node#{npl} with #{fc} failure#{fpl}\"\n    end\n    private :summary\n\n    def run_command(targets, command)\n      nodes = from_targets(targets)\n      @logger.info(\"Starting command run '#{command}' on #{nodes.map(&:uri)}\")\n      callback = block_given? ? Proc.new : nil\n\n      r = on(nodes, callback) do |node|\n        @logger.debug(\"Running command '#{command}' on #{node.uri}\")\n        node_result = node.run_command(command)\n        @logger.debug(\"Result on #{node.uri}: #{JSON.dump(node_result.to_result)}\")\n        node_result\n      end\n      @logger.info(summary('command', command, r))\n      r\n    end\n\n    def run_script(targets, script, arguments)\n      nodes = from_targets(targets)\n      @logger.info(\"Starting script run #{script} on #{nodes.map(&:uri)}\")\n      @logger.debug(\"Arguments: #{arguments}\")\n      callback = block_given? ? Proc.new : nil\n\n      r = on(nodes, callback) do |node|\n        @logger.debug { \"Running script '#{script}' on #{node.uri}\" }\n        node_result = node.run_script(script, arguments)\n        @logger.debug(\"Result on #{node.uri}: #{JSON.dump(node_result.to_result)}\")\n        node_result\n      end\n      @logger.info(summary('script', script, r))\n      r\n    end\n\n    def run_task(targets, task, input_method, arguments)\n      nodes = from_targets(targets)\n      @logger.info(\"Starting task #{task} on #{nodes.map(&:uri)}\")\n      @logger.debug(\"Arguments: #{arguments} Input method: #{input_method}\")\n      callback = block_given? ? Proc.new : nil\n\n      r = on(nodes, callback) do |node|\n        @logger.debug { \"Running task run '#{task}' on #{node.uri}\" }\n        node_result = node.run_task(task, input_method, arguments)\n        @logger.debug(\"Result on #{node.uri}: #{JSON.dump(node_result.to_result)}\")\n        node_result\n      end\n      @logger.info(summary('task', task, r))\n      r\n    end\n\n    def file_upload(targets, source, destination)\n      nodes = from_targets(targets)\n      @logger.info(\"Starting file upload from #{source} to #{destination} on #{nodes.map(&:uri)}\")\n      callback = block_given? ? Proc.new : nil\n\n      r = on(nodes, callback) do |node|\n        @logger.debug { \"Uploading: '#{source}' to #{node.uri}\" }\n        node_result = node.upload(source, destination)\n        @logger.debug(\"Result on #{node.uri}: #{JSON.dump(node_result.to_result)}\")\n        node_result\n      end\n      @logger.info(summary('upload', source, r))\n      r\n    end\n\n    private\n\n    def results_to_hash(results)\n      result_hash = {}\n      results.each_pair { |k, v| result_hash[k] = v }\n      result_hash\n    end\n  end\nend\n","lang_cluster":"Ruby","length":152,"code_uid":"100d40c6da9c4e0ba038e8c635b168e0"}
{"diff_hunk":"@@ -19,7 +19,7 @@ require 'aws-sdk-s3'\n # @param target_key [String] The name of the copied object.\n # @return [Boolean] true if the object was copied; otherwise, false.\n # @example\n-#   s3_client = Aws::S3::Client.new(region: 'us-east-1')\n+#   s3_client = Aws::S3::Client.new(region: 'us-west-2')\n #   exit 1 unless object_copied?(\n #     s3_client,\n #     'doc-example-bucket1',","old_code":"# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX - License - Identifier: Apache - 2.0\n\n# snippet-start:[s3.ruby.copy_object_between_buckets.rb]\nrequire 'aws-sdk-s3'\n\n# Copies an object from one Amazon S3 bucket to another.\n#\n# Prerequisites:\n#\n# - Two S3 buckets (a source bucket and a target bucket).\n# - An object in the source bucket to be copied.\n#\n# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.\n# @param source_bucket_name [String] The source bucket's name.\n# @param source_key [String] The name of the object\n#   in the source bucket to be copied.\n# @param target_bucket_name [String] The target bucket's name.\n# @param target_key [String] The name of the copied object.\n# @return [Boolean] true if the object was copied; otherwise, false.\n# @example\n#   s3_client = Aws::S3::Client.new(region: 'us-east-1')\n#   exit 1 unless object_copied?(\n#     s3_client,\n#     'doc-example-bucket1',\n#     'my-source-file.txt',\n#     'doc-example-bucket2',\n#     'my-target-file.txt'\n#   )\ndef object_copied?(\n  s3_client,\n  source_bucket_name,\n  source_key,\n  target_bucket_name,\n  target_key)\n\n  return true if s3_client.copy_object(\n    bucket: target_bucket_name,\n    copy_source: source_bucket_name + '\/' + source_key,\n    key: target_key\n  )\nrescue StandardError => e\n  puts \"Error while copying object: #{e.message}\"\nend\n# snippet-end:[s3.ruby.copy_object_between_buckets.rb]\n\n# Full example call:\ndef run_me\n  source_bucket_name = 'doc-example-bucket1'\n  source_key = 'my-source-file.txt'\n  target_bucket_name = 'doc-example-bucket2'\n  target_key = 'my-target-file.txt'\n  region = 'us-east-1'\n\n  s3_client = Aws::S3::Client.new(region: region)\n\n  puts \"Copying object '#{source_key}' from bucket '#{source_bucket_name}' \" \\\n    \"to bucket '#{target_bucket_name}'...\"\n\n  if object_copied?(\n    s3_client,\n    source_bucket_name,\n    source_key,\n    target_bucket_name,\n    target_key)\n    puts 'The object was copied.'\n  else\n    puts 'The object was not copied. Stopping program.'\n    exit 1\n  end\nend\n\nrun_me if $PROGRAM_NAME == __FILE__\n","lang_cluster":"Ruby","length":73,"code_uid":"4768eddd111c47698975b841ba845ff9"}
{"diff_hunk":"@@ -99,6 +99,21 @@ RailsAdmin.config do |config|\n     edit do\n       include_all_fields\n \n+      group :seo do\n+        field :description do\n+          help DESCRIPTION_HELP\n+        end\n+        field :extended_description do\n+          help EXTENDED_DESCRIPTION_HELP\n+        end\n+        field :meta_description do\n+          help META_DESCRIPTION_HELP\n+        end\n+        field :page_title do\n+          help PAGE_TITLE_HELP\n+        end\n+      end\n+\n       field :steps do\n         orderable true\n       end","old_code":"RailsAdmin.config do |config|\n  config.authenticate_with do\n    unless current_user\n      session[:return_to] = request.url\n      redirect_to \"\/sign_in\", :alert => \"You must first log in or sign up before accessing this page.\"\n    end\n  end\n\n  config.authorize_with do\n    redirect_to \"\/\", :alert => \"You are not authorized to access that page\" unless current_user.admin?\n  end\n\n  config.current_user_method { current_user }\n\n  config.main_app_name = ['Upcase', 'Admin']\n\n  config.yell_for_non_accessible_fields = false\n\n  config.actions do\n    init_actions!\n  end\n\n  config.model Show do\n    list do\n      field :name\n      field :slug\n    end\n\n    edit do\n      field :name\n      field :slug\n      field :meta_description\n    end\n  end\n\n  config.model User do\n    list do\n      field :id\n      field :name\n      field :email\n      field :github_username\n      field :subscription\n      field :masquerade do\n        pretty_value do\n          bindings[:view].link_to(\n            'Masquerade',\n            bindings[:view].main_app.user_masquerade_path(bindings[:object]),\n            method: :post\n          )\n        end\n      end\n    end\n\n    edit do\n      field :email\n      field :name\n      field :admin\n      field :bio\n      field :github_username\n      field :stripe_customer_id\n    end\n  end\n\n  config.model Product do\n    list do\n      field :id\n      field :name\n      field :sku\n      field :type\n\n      sort_by :name\n    end\n  end\n\n  config.model Exercise do\n    list do\n      field :id\n      field :name\n    end\n\n    edit do\n      field :whetstone_edit_url do\n        label false\n        help false\n        partial \"whetstone_edit_url\"\n      end\n\n      field :trail\n    end\n  end\n\n  config.model Trail do\n    list do\n      field :id\n      field :name\n      field :published\n    end\n\n    edit do\n      include_all_fields\n\n      field :steps do\n        orderable true\n      end\n\n      exclude_fields :exercises, :videos, :statuses, :users\n     end\n   end\n\n  config.model Video do\n    list do\n      field :id\n\n      field :watchable do\n        label \"Product name\"\n\n        pretty_value do\n          value.name\n        end\n      end\n\n      field :name\n      field :created_at\n    end\n\n    edit do\n      group :default do\n        field :name\n        field :slug\n        field :summary\n        field :notes\n        field :topics\n        field :meta_description\n\n        field :watchable do\n          partial \"form_watchable_association\"\n        end\n\n        field :position\n        field :published_on\n        field :users\n        field :length_in_minutes\n        field :markers\n      end\n\n      group :wistia do\n        field :wistia_id\n        field :preview_wistia_id\n      end\n    end\n  end\n\n  config.model Marker do\n    field :video\n    field :anchor\n    field :time do\n      label \"Time (seconds)\"\n    end\n  end\n\n  config.model Topic do\n    list do\n      field :name\n      field :slug\n    end\n\n    edit do\n      field :name\n      field :slug\n      field :summary\n      field :extended_description\n      field :meta_description\n      field :page_title\n    end\n  end\nend\n","lang_cluster":"Ruby","length":176,"code_uid":"368342031e6e437db57904381bd187cf"}
{"diff_hunk":"@@ -108,9 +108,11 @@ class UsageController < ApplicationController\n \n     args = default_query_args\n     args[:start_date] = first_plan_date\n+    sep = params[\"sep\"]\n+    {:col_sep => sep}\n \n     plan_data(args: args, sort: :desc)\n-    data_csvified = StatCreatedPlan.to_csv(@plans_per_month, details: { by_template: true })\n+    data_csvified = StatCreatedPlan.to_csv(@plans_per_month, details: { by_template: true, sep: sep })\n     send_data(data_csvified, filename: \"created_plan_by_template.csv\")\n   end\n ","old_code":"# frozen_string_literal: true\n\nclass UsageController < ApplicationController\n\n  after_action :verify_authorized\n\n  # GET \/usage\n  def index\n    authorize :usage\n\n    args = default_query_args\n    user_data(args: args, as_json: true)\n    plan_data(args: args, as_json: true)\n    total_plans(args: min_max_dates(args: args))\n    total_users(args: min_max_dates(args: args))\n  end\n\n  # POST \/usage_plans_by_template\n  def plans_by_template\n    # This action is triggered when a user changes the timeframe for the\n    # plans by template chart\n    authorize :usage\n\n    args = default_query_args\n    if usage_params[\"template_plans_range\"].present?\n      args[:start_date] = usage_params[\"template_plans_range\"]\n    end\n    plan_data(args: args, as_json: true)\n  end\n\n  # GET\n  def global_statistics\n    # This action is triggered when a user clicks on the 'download csv' button\n    # for global usage\n    authorize :usage\n\n    data = Org::TotalCountStatService.call\n    data_csvified = Csvable.from_array_of_hashes(data)\n\n    send_data(data_csvified, filename: \"totals.csv\")\n  end\n\n  # POST \/usage_filter\n  # rubocop:disable Metrics\/MethodLength\n  def filter\n    # This action is triggered when a user specifies a date range\n    authorize :usage\n\n    args = args_from_params\n    plan_data(args: args)\n    user_data(args: args)\n    total_plans(args: min_max_dates(args: args))\n    total_users(args: min_max_dates(args: args))\n\n    @topic = usage_params[:topic]\n    case @topic\n    when \"plans\"\n      @total = @total_org_plans\n      @ranged = @plans_per_month.sum(:count)\n    else\n      @total = @total_org_users\n      @ranged = @users_per_month.sum(:count)\n    end\n  end\n  # rubocop:enable Metrics\/MethodLength\n\n  # GET \/usage_yearly_users\n  def yearly_users\n    # This action is triggered when a user clicks on the 'download csv' button\n    # for the annual users chart\n    authorize :usage\n\n    user_data(args: default_query_args)\n    send_data(CSV.generate do |csv|\n      csv << [_(\"Month\"), _(\"No. Users joined\")]\n      total = 0\n      @users_per_month.each do |data|\n        csv << [data.date.strftime(\"%b-%y\"), data.count]\n        total += data.count\n      end\n      csv << [_(\"Total\"), total]\n    end, filename: \"users_joined.csv\")\n  end\n\n  # GET \/usage_yearly_plans\n  def yearly_plans\n    # This action is triggered when a user clicks on the 'download csv' button\n    # for the annual plans chart\n    authorize :usage\n\n    plan_data(args: default_query_args)\n    send_data(CSV.generate do |csv|\n      csv << [_(\"Month\"), _(\"No. Completed Plans\")]\n      total = 0\n      @plans_per_month.each do |data|\n        csv << [data.date.strftime(\"%b-%y\"), data.count]\n        total += data.count\n      end\n      csv << [_(\"Total\"), total]\n    end, filename: \"completed_plans.csv\")\n  end\n\n  # GET \/usage_all_plans_by_template\n  def all_plans_by_template\n    # This action is triggered when a user clicks on the 'download csv' button\n    # for the plans by template chart\n    authorize :usage\n\n    args = default_query_args\n    args[:start_date] = first_plan_date\n\n    plan_data(args: args, sort: :desc)\n    data_csvified = StatCreatedPlan.to_csv(@plans_per_month, details: { by_template: true })\n    send_data(data_csvified, filename: \"created_plan_by_template.csv\")\n  end\n\n  private\n\n  def usage_params\n    params.require(:usage).permit(:template_plans_range, :org_id, :start_date,\n                                  :end_date, :topic)\n  end\n\n  # rubocop:disable Metrics\/AbcSize, Metrics\/CyclomaticComplexity\n  # rubocop:disable Metrics\/MethodLength\n  def args_from_params\n    org = current_user.org\n    if current_user.can_super_admin? && usage_params[:org_id].present?\n      org = Org.find_by(id: usage_params[:org_id])\n    end\n\n    start_date = usage_params[:start_date] if usage_params[:start_date].present?\n    end_date = usage_params[:end_date] if usage_params[:end_date].present?\n\n    {\n      org: org,\n      start_date: start_date.present? ? start_date : first_plan_date.strftime(\"%Y-%m-%d\"),\n      end_date: end_date.present? ? end_date : Date.today.strftime(\"%Y-%m-%d\")\n    }\n  end\n  # rubocop:enable Metrics\/AbcSize, Metrics\/CyclomaticComplexity\n  # rubocop:enable Metrics\/MethodLength\n\n  def default_query_args\n    # Stats are generated at the beginning of each month, so our reference\n    # point would be the end of the prior month. For example if it is December\n    # 15th 2019 then the most recent stats would be for the month of November 2019.\n    # That means we want our date range to be 11\/30\/2018 to 11\/30\/2019\n    {\n      org: current_user.org,\n      start_date: Date.today.months_ago(12).end_of_month.strftime(\"%Y-%m-%d\"),\n      end_date: Date.today.last_month.end_of_month.strftime(\"%Y-%m-%d\")\n    }\n  end\n\n  def min_max_dates(args:)\n    args[:start_date] = first_plan_date.strftime(\"%Y-%m-%d\")\n    args[:end_date] = Date.today.strftime(\"%Y-%m-%d\")\n    args\n  end\n\n  def user_data(args:, as_json: false, sort: :asc)\n    @users_per_month = StatJoinedUser.monthly_range(args)\n                                     .order(date: sort)\n    @users_per_month = @users_per_month.map { |rec| rec.to_json } if as_json\n  end\n\n  def plan_data(args:, as_json: false, sort: :asc)\n    @plans_per_month = StatCreatedPlan.monthly_range(args)\n                                      .where.not(details: \"{\\\"by_template\\\":[]}\")\n                                      .order(date: sort)\n    @plans_per_month = @plans_per_month.map { |rec| rec.to_json } if as_json\n  end\n\n  def total_plans(args:)\n    @total_org_plans = StatCreatedPlan.monthly_range(args).sum(:count)\n  end\n\n  def total_users(args:)\n    @total_org_users = StatJoinedUser.monthly_range(args).sum(:count)\n  end\n\n  def first_plan_date\n    StatCreatedPlan.all.order(:date).limit(1).pluck(:date).first\n  end\n\nend\n","lang_cluster":"Ruby","length":187,"code_uid":"eba0e2127ec74d278a505a43f207b11b"}
{"diff_hunk":"@@ -25,6 +25,10 @@ module Travis\n         private def wait_retries\n           @wait_retries ||= Integer(Travis::Build.config.network.wait_retries)\n         end\n+\n+        private def wait?\n+          @wait ||= Travis::Build.config.network.wait?\n+        end\n       end\n     end\n   end","old_code":"require 'travis\/build\/appliances\/base'\n\nmodule Travis\n  module Build\n    module Appliances\n      class WaitForNetwork < Base\n        def apply\n          sh.raw bash('travis_wait_for_network')\n          sh.cmd(\n            \"travis_wait_for_network #{wait_retries} #{check_urls.map(&:inspect).join(' ')}\",\n            echo: false\n          )\n        end\n\n        private def check_urls\n          @check_urls ||= Travis::Build.config.network.check_urls.map do |tmpl|\n            (tmpl % {\n              app_host: app_host,\n              job_id: data.job[:id],\n              repo: data.slug\n            }).output_safe\n          end\n        end\n\n        private def wait_retries\n          @wait_retries ||= Integer(Travis::Build.config.network.wait_retries)\n        end\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":31,"code_uid":"ff91a17d7466472993e0d1ea25ebb1c9"}
{"diff_hunk":"@@ -1,5 +1,5 @@\n <% if @prefs %>\n-  <%= form_tag( update_email_preferences_user_path(@user), method: :put, html: { id: \"preferences_registration_form\", class: \"form-horizontal\" }) do |f| %>\n+  <%= form_tag( update_email_preferences_user_path(@user), method: :put, id: \"preferences_registration_form\", class: \"form-horizontal\") do |f| %>\n     <div class=\"preferences\">\n   \n     <p>","old_code":"<% if @prefs %>\n  <%= form_tag( update_email_preferences_user_path(@user), method: :put, html: { id: \"preferences_registration_form\", class: \"form-horizontal\" }) do |f| %>\n    <div class=\"preferences\">\n  \n    <p>\n      <%= link_to _('Select all'), '#', id: 'select_all' %> |\n      <%= link_to _('Deselect all'), '#', id: 'deselect_all' %>\n    <\/p>\n  \n    <p class=\"form-control-static\"><strong>All Users<\/strong><\/p>\n    <div class=\"checkbox\">\n      <label for=\"prefs[users][new_comment]\">\n        <%= hidden_field_tag 'prefs[users][new_comment]', false %>\n        <%= check_box_tag 'prefs[users][new_comment]', true, @prefs[:users][:new_comment] %><%= _('A new comment has been added to my DMP')  %>\n      <\/label>\n    <\/div>\n    <div class=\"checkbox\">\n      <label for=\"prefs[users][added_as_coowner]\">\n        <%= hidden_field_tag 'prefs[users][added_as_coowner]', false %>\n        <%= check_box_tag 'prefs[users][added_as_coowner]', true, @prefs[:users][:added_as_coowner] %><%= _('A plan has been shared with me')  %>\n      <\/label>\n    <\/div>\n    <div class=\"checkbox\">\n      <label for=\"prefs[users][admin_privileges]\">\n        <%= hidden_field_tag 'prefs[users][admin_privileges]', false %>\n        <%= check_box_tag 'prefs[users][admin_privileges]', true, @prefs[:users][:admin_privileges] %><%= _('Admin privileges granted to me')  %>\n      <\/label>\n    <\/div>\n    <% if @user.org.present? && @user.org.feedback_enabled %>\n      <div class=\"checkbox\">\n        <label for=\"prefs[users][feedback_requested]\">\n          <%= hidden_field_tag 'prefs[users][feedback_requested]', false %>\n          <%= check_box_tag 'prefs[users][feedback_requested]', true, @prefs[:users][:feedback_requested] %><%= _('Feedback has been requested for my DMP') %>\n        <\/label>\n      <\/div>\n      <div class=\"checkbox\">\n        <label for=\"prefs[users][feedback_provided]\">\n          <%= hidden_field_tag 'prefs[users][feedback_provided]', false %>\n          <%= check_box_tag 'prefs[users][feedback_provided]', true, @prefs[:users][:feedback_provided] %><%= _('Feedback has been provided for my DMP')  %>\n        <\/label>\n      <\/div>\n    <% end %>\n\n    <p class=\"form-control-static\"><strong>DMP owners and co-owners<\/strong><\/p>\n  \n    <div class=\"checkbox\">\n      <label for=\"prefs[owners_and_coowners][visibility_changed]\">\n        <%= hidden_field_tag 'prefs[owners_and_coowners][visibility_changed]', false %>\n        <%= check_box_tag 'prefs[owners_and_coowners][visibility_changed]', true, @prefs[:owners_and_coowners][:visibility_changed] %><%= _(\"My DMP's visibility has changed\")  %>\n      <\/label>          \n    <\/div>\n\n    <% if @user.can_org_admin? %>\n      <p class=\"form-control-static\"><strong>DMP administrators<\/strong><\/p>\n    \n      <div class=\"checkbox\">\n        <label for=\"prefs[admins][feedback_requested]\">\n          <%= hidden_field_tag 'prefs[admins][feedback_requested]', false %>\n          <%= check_box_tag 'prefs[admins][feedback_requested]', true, @prefs[:admins][:feedback_requested] %><%= _('A user has requested feedback on a DMP')  %>\n        <\/label>\n      <\/div>\n    <% end %>\n    <div class='form-group'>\n      <%= submit_tag _('Save'), class: 'btn btn-default' %>\n    <\/div>\n  \n  <% end %>\n\n<% end %>\n","lang_cluster":"Ruby","length":69,"code_uid":"28eb2726a2df4f4cb94be716fe1631b6"}
{"diff_hunk":"@@ -4,6 +4,7 @@ class Video < ActiveRecord::Base\n   belongs_to :watchable, polymorphic: true\n   has_many :classifications, as: :classifiable\n   has_many :topics, through: :classifications\n+  has_many :statuses, as: :completeable, dependent: :destroy\n \n   validates :published_on, presence: true\n   validates :slug, presence: true, uniqueness: true","old_code":"class Video < ActiveRecord::Base\n  extend FriendlyId\n\n  belongs_to :watchable, polymorphic: true\n  has_many :classifications, as: :classifiable\n  has_many :topics, through: :classifications\n\n  validates :published_on, presence: true\n  validates :slug, presence: true, uniqueness: true\n  validates :title, presence: true\n  validates :watchable_id, presence: true\n  validates :watchable_type, presence: true\n  validates :wistia_id, presence: true\n\n  delegate :included_in_plan?, to: :watchable\n  delegate :name, to: :watchable, prefix: true\n\n  friendly_id :title, use: [:slugged, :finders]\n\n  def self.ordered\n    order('position asc')\n  end\n\n  def self.published\n    where('published_on <= ?', Time.zone.today)\n  end\n\n  def self.recently_published_first\n    order('published_on desc')\n  end\n\n  def clip\n    @video ||= Clip.new(wistia_id)\n  end\n\n  def preview\n    if preview_wistia_id.present?\n      Clip.new(preview_wistia_id)\n    else\n      VideoThumbnail.new(clip)\n    end\n  end\n\n  def has_notes?\n    notes.present?\n  end\n\n  def notes_html\n    BlueCloth.new(notes).to_html\n  end\n\n  def to_param\n    slug\n  end\nend\n","lang_cluster":"Ruby","length":55,"code_uid":"b6d92ac45a594315b5424b28703ec46f"}
{"diff_hunk":"@@ -31,6 +31,10 @@ import java.io.File;\n import java.io.IOException;\n import java.util.Map;\n import java.util.Objects;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import static java.nio.file.StandardCopyOption.REPLACE_EXISTING;\n+\n \n public class UploadFile implements CommandHandler {\n ","old_code":"\/\/ Licensed to the Software Freedom Conservancy (SFC) under one\n\/\/ or more contributor license agreements.  See the NOTICE file\n\/\/ distributed with this work for additional information\n\/\/ regarding copyright ownership.  The SFC licenses this file\n\/\/ to you under the Apache License, Version 2.0 (the\n\/\/ \"License\"); you may not use this file except in compliance\n\/\/ with the License.  You may obtain a copy of the License at\n\/\/\n\/\/   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing,\n\/\/ software distributed under the License is distributed on an\n\/\/ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\/\/ KIND, either express or implied.  See the License for the\n\/\/ specific language governing permissions and limitations\n\/\/ under the License.\n\npackage org.openqa.selenium.remote.server.commandhandler;\n\nimport org.openqa.selenium.WebDriverException;\nimport org.openqa.selenium.grid.session.ActiveSession;\nimport org.openqa.selenium.grid.web.CommandHandler;\nimport org.openqa.selenium.io.Zip;\nimport org.openqa.selenium.json.Json;\nimport org.openqa.selenium.remote.ErrorCodes;\nimport org.openqa.selenium.remote.Response;\nimport org.openqa.selenium.remote.http.HttpRequest;\nimport org.openqa.selenium.remote.http.HttpResponse;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.Map;\nimport java.util.Objects;\n\npublic class UploadFile implements CommandHandler {\n\n  private final Json json;\n  private final ActiveSession session;\n\n  public UploadFile(Json json, ActiveSession session) {\n    this.json = Objects.requireNonNull(json);\n    this.session = Objects.requireNonNull(session);\n  }\n\n  @Override\n  public void execute(HttpRequest req, HttpResponse resp) throws IOException {\n    Map<String, Object> args = json.toType(req.getContentString(), Json.MAP_TYPE);\n    String file = (String) args.get(\"file\");\n\n    File tempDir = session.getFileSystem().createTempDir(\"upload\", \"file\");\n\n    Zip.unzip(file, tempDir);\n    \/\/ Select the first file\n    File[] allFiles = tempDir.listFiles();\n\n    Response response = new Response(session.getId());\n    if (allFiles == null || allFiles.length != 1) {\n      response.setStatus(ErrorCodes.UNHANDLED_ERROR);\n      response.setValue(new WebDriverException(\n          \"Expected there to be only 1 file. There were: \" +\n          (allFiles == null ? 0 : allFiles.length)));\n    } else {\n      response.setStatus(ErrorCodes.SUCCESS);\n      response.setValue(allFiles[0].getAbsolutePath());\n    }\n\n    session.getDownstreamDialect().getResponseCodec().encode(() -> resp, response);\n  }\n}\n","lang_cluster":"Ruby","length":69,"code_uid":"49fed162d0794dcf95ec8f5fd623b943"}
{"diff_hunk":"@@ -47,6 +47,7 @@ import static org.openqa.selenium.remote.DriverCommand.MOUSE_UP;\n import static org.openqa.selenium.remote.DriverCommand.MOVE_TO;\n import static org.openqa.selenium.remote.DriverCommand.REMOVE_LOCAL_STORAGE_ITEM;\n import static org.openqa.selenium.remote.DriverCommand.REMOVE_SESSION_STORAGE_ITEM;\n+import static org.openqa.selenium.remote.DriverCommand.SEND_COMMAND_TO_BROWSER;\n import static org.openqa.selenium.remote.DriverCommand.SEND_KEYS_TO_ACTIVE_ELEMENT;\n import static org.openqa.selenium.remote.DriverCommand.SET_ALERT_VALUE;\n import static org.openqa.selenium.remote.DriverCommand.SET_CURRENT_WINDOW_POSITION;","old_code":"\/\/ Licensed to the Software Freedom Conservancy (SFC) under one\n\/\/ or more contributor license agreements.  See the NOTICE file\n\/\/ distributed with this work for additional information\n\/\/ regarding copyright ownership.  The SFC licenses this file\n\/\/ to you under the Apache License, Version 2.0 (the\n\/\/ \"License\"); you may not use this file except in compliance\n\/\/ with the License.  You may obtain a copy of the License at\n\/\/\n\/\/   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing,\n\/\/ software distributed under the License is distributed on an\n\/\/ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\/\/ KIND, either express or implied.  See the License for the\n\/\/ specific language governing permissions and limitations\n\/\/ under the License.\n\npackage org.openqa.selenium.remote.http;\n\nimport static org.openqa.selenium.remote.DriverCommand.ACCEPT_ALERT;\nimport static org.openqa.selenium.remote.DriverCommand.CLEAR_LOCAL_STORAGE;\nimport static org.openqa.selenium.remote.DriverCommand.CLEAR_SESSION_STORAGE;\nimport static org.openqa.selenium.remote.DriverCommand.CLICK;\nimport static org.openqa.selenium.remote.DriverCommand.DISMISS_ALERT;\nimport static org.openqa.selenium.remote.DriverCommand.DOUBLE_CLICK;\nimport static org.openqa.selenium.remote.DriverCommand.EXECUTE_ASYNC_SCRIPT;\nimport static org.openqa.selenium.remote.DriverCommand.EXECUTE_SCRIPT;\nimport static org.openqa.selenium.remote.DriverCommand.GET_ACTIVE_ELEMENT;\nimport static org.openqa.selenium.remote.DriverCommand.GET_ALERT_TEXT;\nimport static org.openqa.selenium.remote.DriverCommand.GET_CURRENT_WINDOW_HANDLE;\nimport static org.openqa.selenium.remote.DriverCommand.GET_CURRENT_WINDOW_POSITION;\nimport static org.openqa.selenium.remote.DriverCommand.GET_CURRENT_WINDOW_SIZE;\nimport static org.openqa.selenium.remote.DriverCommand.GET_ELEMENT_ATTRIBUTE;\nimport static org.openqa.selenium.remote.DriverCommand.GET_ELEMENT_LOCATION_ONCE_SCROLLED_INTO_VIEW;\nimport static org.openqa.selenium.remote.DriverCommand.GET_LOCAL_STORAGE_ITEM;\nimport static org.openqa.selenium.remote.DriverCommand.GET_LOCAL_STORAGE_KEYS;\nimport static org.openqa.selenium.remote.DriverCommand.GET_LOCAL_STORAGE_SIZE;\nimport static org.openqa.selenium.remote.DriverCommand.GET_PAGE_SOURCE;\nimport static org.openqa.selenium.remote.DriverCommand.GET_SESSION_STORAGE_ITEM;\nimport static org.openqa.selenium.remote.DriverCommand.GET_SESSION_STORAGE_KEYS;\nimport static org.openqa.selenium.remote.DriverCommand.GET_SESSION_STORAGE_SIZE;\nimport static org.openqa.selenium.remote.DriverCommand.GET_WINDOW_HANDLES;\nimport static org.openqa.selenium.remote.DriverCommand.IS_ELEMENT_DISPLAYED;\nimport static org.openqa.selenium.remote.DriverCommand.MAXIMIZE_CURRENT_WINDOW;\nimport static org.openqa.selenium.remote.DriverCommand.MOUSE_DOWN;\nimport static org.openqa.selenium.remote.DriverCommand.MOUSE_UP;\nimport static org.openqa.selenium.remote.DriverCommand.MOVE_TO;\nimport static org.openqa.selenium.remote.DriverCommand.REMOVE_LOCAL_STORAGE_ITEM;\nimport static org.openqa.selenium.remote.DriverCommand.REMOVE_SESSION_STORAGE_ITEM;\nimport static org.openqa.selenium.remote.DriverCommand.SEND_KEYS_TO_ACTIVE_ELEMENT;\nimport static org.openqa.selenium.remote.DriverCommand.SET_ALERT_VALUE;\nimport static org.openqa.selenium.remote.DriverCommand.SET_CURRENT_WINDOW_POSITION;\nimport static org.openqa.selenium.remote.DriverCommand.SET_CURRENT_WINDOW_SIZE;\nimport static org.openqa.selenium.remote.DriverCommand.SET_LOCAL_STORAGE_ITEM;\nimport static org.openqa.selenium.remote.DriverCommand.SET_SESSION_STORAGE_ITEM;\nimport static org.openqa.selenium.remote.DriverCommand.SUBMIT_ELEMENT;\nimport static org.openqa.selenium.remote.DriverCommand.TOUCH_DOUBLE_TAP;\nimport static org.openqa.selenium.remote.DriverCommand.TOUCH_DOWN;\nimport static org.openqa.selenium.remote.DriverCommand.TOUCH_FLICK;\nimport static org.openqa.selenium.remote.DriverCommand.TOUCH_LONG_PRESS;\nimport static org.openqa.selenium.remote.DriverCommand.TOUCH_MOVE;\nimport static org.openqa.selenium.remote.DriverCommand.TOUCH_SCROLL;\nimport static org.openqa.selenium.remote.DriverCommand.TOUCH_SINGLE_TAP;\nimport static org.openqa.selenium.remote.DriverCommand.TOUCH_UP;\n\nimport com.google.common.collect.ImmutableMap;\n\nimport org.openqa.selenium.InvalidArgumentException;\nimport org.openqa.selenium.remote.DriverCommand;\n\nimport java.util.Map;\n\n\/**\n * A command codec that adheres to the Selenium project's JSON\/HTTP wire protocol.\n *\n * @see <a href=\"https:\/\/github.com\/SeleniumHQ\/selenium\/wiki\/JsonWireProtocol\">\n *   JSON wire protocol<\/a>\n *\/\npublic class JsonHttpCommandCodec extends AbstractHttpCommandCodec {\n\n  public JsonHttpCommandCodec() {\n    defineCommand(GET_ELEMENT_ATTRIBUTE, get(\"\/session\/:sessionId\/element\/:id\/attribute\/:name\"));\n    defineCommand(GET_ELEMENT_LOCATION_ONCE_SCROLLED_INTO_VIEW, get(\"\/session\/:sessionId\/element\/:id\/location_in_view\"));\n    defineCommand(IS_ELEMENT_DISPLAYED, get(\"\/session\/:sessionId\/element\/:id\/displayed\"));\n    defineCommand(SUBMIT_ELEMENT, post(\"\/session\/:sessionId\/element\/:id\/submit\"));\n\n    defineCommand(EXECUTE_SCRIPT, post(\"\/session\/:sessionId\/execute\"));\n    defineCommand(EXECUTE_ASYNC_SCRIPT, post(\"\/session\/:sessionId\/execute_async\"));\n\n    defineCommand(GET_PAGE_SOURCE, get(\"\/session\/:sessionId\/source\"));\n\n    defineCommand(MAXIMIZE_CURRENT_WINDOW, post(\"\/session\/:sessionId\/window\/:windowHandle\/maximize\"));\n    defineCommand(GET_CURRENT_WINDOW_POSITION, get(\"\/session\/:sessionId\/window\/:windowHandle\/position\"));\n    defineCommand(SET_CURRENT_WINDOW_POSITION, post(\"\/session\/:sessionId\/window\/:windowHandle\/position\"));\n    defineCommand(GET_CURRENT_WINDOW_SIZE, get(\"\/session\/:sessionId\/window\/:windowHandle\/size\"));\n    defineCommand(SET_CURRENT_WINDOW_SIZE, post(\"\/session\/:sessionId\/window\/:windowHandle\/size\"));\n    defineCommand(GET_CURRENT_WINDOW_HANDLE, get(\"\/session\/:sessionId\/window_handle\"));\n    defineCommand(GET_WINDOW_HANDLES, get(\"\/session\/:sessionId\/window_handles\"));\n\n    defineCommand(ACCEPT_ALERT, post(\"\/session\/:sessionId\/accept_alert\"));\n    defineCommand(DISMISS_ALERT, post(\"\/session\/:sessionId\/dismiss_alert\"));\n    defineCommand(GET_ALERT_TEXT, get(\"\/session\/:sessionId\/alert_text\"));\n    defineCommand(SET_ALERT_VALUE, post(\"\/session\/:sessionId\/alert_text\"));\n\n    defineCommand(GET_ACTIVE_ELEMENT, post(\"\/session\/:sessionId\/element\/active\"));\n\n    defineCommand(CLEAR_LOCAL_STORAGE, delete(\"\/session\/:sessionId\/local_storage\"));\n    defineCommand(GET_LOCAL_STORAGE_KEYS, get(\"\/session\/:sessionId\/local_storage\"));\n    defineCommand(SET_LOCAL_STORAGE_ITEM, post(\"\/session\/:sessionId\/local_storage\"));\n    defineCommand(REMOVE_LOCAL_STORAGE_ITEM, delete(\"\/session\/:sessionId\/local_storage\/key\/:key\"));\n    defineCommand(GET_LOCAL_STORAGE_ITEM, get(\"\/session\/:sessionId\/local_storage\/key\/:key\"));\n    defineCommand(GET_LOCAL_STORAGE_SIZE, get(\"\/session\/:sessionId\/local_storage\/size\"));\n\n    defineCommand(CLEAR_SESSION_STORAGE, delete(\"\/session\/:sessionId\/session_storage\"));\n    defineCommand(GET_SESSION_STORAGE_KEYS, get(\"\/session\/:sessionId\/session_storage\"));\n    defineCommand(SET_SESSION_STORAGE_ITEM, post(\"\/session\/:sessionId\/session_storage\"));\n    defineCommand(REMOVE_SESSION_STORAGE_ITEM, delete(\"\/session\/:sessionId\/session_storage\/key\/:key\"));\n    defineCommand(GET_SESSION_STORAGE_ITEM, get(\"\/session\/:sessionId\/session_storage\/key\/:key\"));\n    defineCommand(GET_SESSION_STORAGE_SIZE, get(\"\/session\/:sessionId\/session_storage\/size\"));\n\n    \/\/ Interactions-related commands.\n    defineCommand(MOUSE_DOWN, post(\"\/session\/:sessionId\/buttondown\"));\n    defineCommand(MOUSE_UP, post(\"\/session\/:sessionId\/buttonup\"));\n    defineCommand(CLICK, post(\"\/session\/:sessionId\/click\"));\n    defineCommand(DOUBLE_CLICK, post(\"\/session\/:sessionId\/doubleclick\"));\n    defineCommand(MOVE_TO, post(\"\/session\/:sessionId\/moveto\"));\n    defineCommand(SEND_KEYS_TO_ACTIVE_ELEMENT, post(\"\/session\/:sessionId\/keys\"));\n    defineCommand(TOUCH_SINGLE_TAP, post(\"\/session\/:sessionId\/touch\/click\"));\n    defineCommand(TOUCH_DOUBLE_TAP, post(\"\/session\/:sessionId\/touch\/doubleclick\"));\n    defineCommand(TOUCH_DOWN, post(\"\/session\/:sessionId\/touch\/down\"));\n    defineCommand(TOUCH_FLICK, post(\"\/session\/:sessionId\/touch\/flick\"));\n    defineCommand(TOUCH_LONG_PRESS, post(\"\/session\/:sessionId\/touch\/longclick\"));\n    defineCommand(TOUCH_MOVE, post(\"\/session\/:sessionId\/touch\/move\"));\n    defineCommand(TOUCH_SCROLL, post(\"\/session\/:sessionId\/touch\/scroll\"));\n    defineCommand(TOUCH_UP, post(\"\/session\/:sessionId\/touch\/up\"));\n  }\n\n  @Override\n  protected Map<String, ?> amendParameters(String name, Map<String, ?> parameters) {\n    switch (name) {\n      case DriverCommand.GET_CURRENT_WINDOW_SIZE:\n      case DriverCommand.MAXIMIZE_CURRENT_WINDOW:\n      case DriverCommand.SET_CURRENT_WINDOW_SIZE:\n      case DriverCommand.SET_CURRENT_WINDOW_POSITION:\n        return ImmutableMap.<String, Object>builder()\n          .putAll(parameters)\n          .put(\"windowHandle\", \"current\")\n          .put(\"handle\", \"current\")\n          .build();\n\n      case DriverCommand.SET_TIMEOUT:\n        if (parameters.size() != 1) {\n          throw new InvalidArgumentException(\n              \"The JSON wire protocol only supports setting one time out at a time\");\n        }\n        Map.Entry<String, ?> entry = parameters.entrySet().iterator().next();\n        String type = entry.getKey();\n        if (\"pageLoad\".equals(type)) {\n          type = \"page load\";\n        }\n        return ImmutableMap.of(\"type\", type, \"ms\", entry.getValue());\n\n      case DriverCommand.SWITCH_TO_WINDOW:\n        return ImmutableMap.<String, Object>builder()\n          .put(\"name\", parameters.get(\"handle\"))\n          .build();\n\n      default:\n        return parameters;\n    }\n  }\n}\n","lang_cluster":"Ruby","length":172,"code_uid":"e0072ca4bebe4bd980d0809b107e1e2b"}
{"diff_hunk":"@@ -37,7 +37,7 @@ module Faker\n       end\n \n       def paragraph(sentence_count = 3, supplemental = false, random_sentences_to_add = 3)\n-        sentences(resolve(sentence_count) + rand(random_sentences_to_add.to_i), supplemental).join(' ')\n+        sentences(resolve(sentence_count) + rand(random_sentences_to_add.to_i), supplemental).join(locale_space)\n       end\n \n       def paragraphs(paragraph_count = 3, supplemental = false)","old_code":"module Faker\n  # Based on Perl's Text::Lorem\n  class Lorem < Base\n    CHARACTERS = ('0'..'9').to_a + ('a'..'z').to_a\n\n    class << self\n      def word\n        sample(translate('faker.lorem.words'))\n      end\n\n      def words(num = 3, supplemental = false)\n        resolved_num = resolve(num)\n        word_list = (\n          translate('faker.lorem.words') +\n          (supplemental ? translate('faker.lorem.supplemental') : [])\n        )\n        word_list *= ((resolved_num \/ word_list.length) + 1)\n        shuffle(word_list)[0, resolved_num]\n      end\n\n      def character\n        sample(CHARACTERS)\n      end\n\n      def characters(char_count = 255)\n        char_count = resolve(char_count)\n        return '' if char_count.to_i < 1\n        Array.new(char_count) { sample(CHARACTERS) }.join\n      end\n\n      def sentence(word_count = 4, supplemental = false, random_words_to_add = 6)\n        words(word_count + rand(random_words_to_add.to_i), supplemental).join(' ').capitalize + '.'\n      end\n\n      def sentences(sentence_count = 3, supplemental = false)\n        1.upto(resolve(sentence_count)).collect { sentence(3, supplemental) }\n      end\n\n      def paragraph(sentence_count = 3, supplemental = false, random_sentences_to_add = 3)\n        sentences(resolve(sentence_count) + rand(random_sentences_to_add.to_i), supplemental).join(' ')\n      end\n\n      def paragraphs(paragraph_count = 3, supplemental = false)\n        1.upto(resolve(paragraph_count)).collect { paragraph(3, supplemental) }\n      end\n\n      def question(word_count = 4, supplemental = false, random_words_to_add = 6)\n        words(word_count + rand(random_words_to_add.to_i), supplemental).join(' ').capitalize + '?'\n      end\n\n      def questions(question_count = 3, supplemental = false)\n        1.upto(resolve(question_count)).collect { question(3, supplemental) }\n      end\n\n      private\n\n      # If an array or range is passed, a random value will be selected.\n      # All other values are simply returned.\n      def resolve(value)\n        case value\n        when Array then sample(value)\n        when Range then rand value\n        else value\n        end\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":68,"code_uid":"98c91e31aa6a42cea398c629586dbf5f"}
{"diff_hunk":"@@ -3,7 +3,11 @@ class PurchasesController < ApplicationController\n     @purchaseable = find_purchaseable\n \n     if current_user_has_active_subscription?\n-      render 'for_subscribers'\n+      if overlapping_sections?(@purchaseable)\n+        render 'overlapping'\n+      else\n+        render 'for_subscribers'\n+      end\n     else\n       @purchase = @purchaseable.purchases.build(variant: params[:variant])\n       @purchase.defaults_from_user(current_user)","old_code":"class PurchasesController < ApplicationController\n  def new\n    @purchaseable = find_purchaseable\n\n    if current_user_has_active_subscription?\n      render 'for_subscribers'\n    else\n      @purchase = @purchaseable.purchases.build(variant: params[:variant])\n      @purchase.defaults_from_user(current_user)\n      @active_card = retrieve_active_card\n\n      if @purchase.subscription? && signed_out?\n        deny_access(t('shared.subscriptions.user_required'))\n      end\n    end\n  end\n\n  def create\n    @purchaseable = find_purchaseable\n    @purchase = @purchaseable.purchases.build(params[:purchase])\n    @purchase.user = current_user\n\n    if use_coupon?\n      @purchase.coupon = Coupon.active.find_by_id(params[:coupon_id])\n    end\n\n    if use_existing_card?\n      @purchase.stripe_customer = current_user.stripe_customer\n    end\n\n    if @purchase.save\n      create_subscription if @purchaseable.subscription?\n\n      notify_kissmetrics_of(@purchase)\n\n      redirect_to success_url, notice: t('.purchase.flashes.success', name: @purchaseable.name)\n    else\n      @active_card = retrieve_active_card\n      render :new\n    end\n  end\n\n  def show\n    @purchase = Purchase.find_by_lookup(params[:id])\n    @purchaseable = @purchase.purchaseable\n    if @purchase.paid?\n      render polymorphic_purchaseable_template\n    else\n      redirect_to @purchaseable\n    end\n  end\n\n  def paypal\n    flash.keep\n    @purchase = Purchase.find_by_lookup(params[:id])\n    @purchase.complete_paypal_payment!(params[:token], params[:PayerID])\n    notify_kissmetrics_of(@purchase)\n    redirect_to @purchase\n  end\n\n  private\n\n  def create_subscription\n    current_user.create_subscription\n  end\n\n  def notify_kissmetrics_of(purchase)\n    event_notifier = KissmetricsEventNotifier.new(km_http_client)\n    event_notifier.notify_of(purchase)\n  end\n\n  def use_existing_card?\n    params[:use_existing_card] == 'on'\n  end\n\n  def use_coupon?\n    params[:coupon_id].present?\n  end\n\n  def retrieve_active_card\n    if current_user && current_user.stripe_customer\n      Stripe::Customer.retrieve(current_user.stripe_customer)['active_card']\n    end\n  end\n\n  def current_purchase\n    @current_purchase ||= Purchase.find_by_lookup(params[:id])\n  end\n\n  def current_purchaseable\n    @current_purchaseable ||= purchaseable\n  end\n\n  def success_url\n    if @purchase.paypal?\n      @purchase.paypal_url\n    elsif @purchase.subscription?\n      products_path\n    else\n      purchase_path @purchase\n    end\n  end\n\n  def url_after_denied_access_when_signed_out\n    sign_up_url\n  end\n\n  def polymorphic_purchaseable_template\n    \"#{@purchaseable.product_type.pluralize}\/#{@purchaseable.product_type}_purchase_show\"\n  end\nend\n","lang_cluster":"Ruby","length":111,"code_uid":"9aa332f0962a4ecfbf2abe873e9fd482"}
{"diff_hunk":"@@ -1,9 +1,12 @@\n module Ncr\n   class ProposalsController < ApplicationController\n     before_filter :authenticate_user!\n+    before_filter :not_approved, only: [:edit, :update]\n+    before_filter :cart_owner, only: [:edit, :update]\n \n     def new\n       @proposal_form = Ncr::ProposalForm.new\n+      @form_url, @form_method = {action: \"create\"}, \"post\"\n       approver = self.suggested_approver\n       if approver\n         @proposal_form.approver_email = approver.email_address","old_code":"module Ncr\n  class ProposalsController < ApplicationController\n    before_filter :authenticate_user!\n\n    def new\n      @proposal_form = Ncr::ProposalForm.new\n      approver = self.suggested_approver\n      if approver\n        @proposal_form.approver_email = approver.email_address\n      end\n    end\n\n    def create\n      @proposal_form = Ncr::ProposalForm.new(params[:ncr_proposal])\n      @proposal_form.requester = current_user\n      if @proposal_form.valid?\n        cart = @proposal_form.create_cart\n        if cart.persisted?\n          Dispatcher.deliver_new_cart_emails(cart)\n          flash[:success] = \"Proposal submitted!\"\n          redirect_to cart_path(cart)\n        else\n          flash[:error] = cart.errors.full_messages\n          render 'new'\n        end\n      else\n        flash[:error] = @proposal_form.errors.full_messages\n        render 'new'\n      end\n    end\n\n    protected\n\n    def last_cart\n      current_user.last_requested_cart\n    end\n\n    def last_approvers\n      last_cart.try(:approvers)\n    end\n\n    def suggested_approver\n      last_approvers.try(:first)\n    end\n  end\nend\n","lang_cluster":"Ruby","length":46,"code_uid":"2179c0d8fd164ee78d72860bd33092a1"}
{"diff_hunk":"@@ -8,12 +8,13 @@ class StatCreatedPlan\n \n       def do(start_date:, end_date:, org:)\n         count = count_plans(start_date: start_date, end_date: end_date, org: org)\n-        by_template = by_template(start_date: start_date, end_date: end_date, org: org)\n+        by_template = plan_statistics(start_date: start_date, end_date: end_date, org: org)\n+        using_template = plan_statistics(start_date: start_date, end_date: end_date, org: org, own_templates: true)\n         attrs = {\n           date: end_date.to_date,\n           org_id: org.id,\n           count: count,\n-          details: { by_template: by_template }\n+          details: { by_template: by_template, using_template: using_template }\n         }\n         stat_created_plan = StatCreatedPlan.find_by(\n           date: attrs[:date],","old_code":"# frozen_string_literal: true\n\nclass StatCreatedPlan\n\n  class CreateOrUpdate\n\n    class << self\n\n      def do(start_date:, end_date:, org:)\n        count = count_plans(start_date: start_date, end_date: end_date, org: org)\n        by_template = by_template(start_date: start_date, end_date: end_date, org: org)\n        attrs = {\n          date: end_date.to_date,\n          org_id: org.id,\n          count: count,\n          details: { by_template: by_template }\n        }\n        stat_created_plan = StatCreatedPlan.find_by(\n          date: attrs[:date],\n          org_id: attrs[:org_id]\n        )\n\n        if stat_created_plan.present?\n          stat_created_plan.update(attrs)\n        else\n          StatCreatedPlan.create(attrs)\n        end\n      end\n\n      private\n\n      def users(org)\n        User.where(users: { org_id: org.id })\n      end\n\n      def plans(start_date:, end_date:)\n        Plan.where(plans: { created_at: start_date..end_date })\n      end\n\n      def count_plans(start_date:, end_date:, org:)\n        Role.joins(:plan, :user)\n          .administrator\n          .merge(users(org))\n          .merge(plans(start_date: start_date, end_date: end_date))\n          .select(:plan_id)\n          .distinct\n          .count\n      end\n\n      def by_template(start_date:, end_date:, org:)\n        roleable_plan_ids = Role.joins([:plan, :user])\n          .administrator\n          .merge(users(org))\n          .merge(plans(start_date: start_date, end_date: end_date))\n          .pluck(:plan_id)\n          .uniq\n\n        template_counts = Plan.joins(:template).where(id: roleable_plan_ids)\n          .group(\"templates.family_id\").count\n        most_recent_versions = Template.where(family_id: template_counts.keys)\n          .group(:family_id).maximum(\"version\")\n        most_recent_versions = most_recent_versions.map { |k, v| \"#{k}=#{v}\" }\n        template_names = Template.where(\"CONCAT(family_id, '=', version) IN (?)\",\n          most_recent_versions).pluck(:family_id, :title)\n        template_names.map do |t|\n          { name: t[1], count: template_counts[t[0]] }\n        end\n      end\n\n    end\n\n  end\n\nend\n","lang_cluster":"Ruby","length":74,"code_uid":"a472a3c028b04e8bb88140d725ad017c"}
{"diff_hunk":"@@ -78,10 +78,11 @@ def get_random_aes_256_gcm_key\n end\n \n # Full example call:\n+# Replace us-west-2 with the AWS Region you're using for Amazon S3.\n def run_me\n   bucket_name = 'doc-example-bucket'\n   object_key = 'my-file.txt'\n-  region = 'us-east-1'\n+  region = 'us-west-2'\n   object_content = File.read(object_key)\n \n   # The following call generates a random AES256-GCM key. Alternatively, you can","old_code":"# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\r\n# SPDX - License - Identifier: Apache - 2.0\r\n\r\nrequire 'aws-sdk-s3'\r\nrequire 'openssl'\r\n\r\n# Uploads an encrypted object to an Amazon S3 bucket.\r\n#\r\n# Prerequisites:\r\n#\r\n# - An Amazon S3 bucket.\r\n#\r\n# @param s3_encryption_client [Aws::S3::EncryptionV2::Client]\r\n#   An initialized Amazon S3 V2 encryption client.\r\n# @param bucket_name [String] The name of the bucket.\r\n# @param object_key [String] The name of the object to upload.\r\n# @param object_content [String] The content of the object to upload.\r\n# @return [Boolean] true if the object was encrypted and uploaded;\r\n#   otherwise, false.\r\n# @example\r\n#   s3_encryption_client = Aws::S3::EncryptionV2::Client.new(\r\n#     region: 'us-east-1',\r\n#     encryption_key: get_random_aes_256_gcm_key, # See later in this file.\r\n#     key_wrap_schema: :aes_gcm,\r\n#     content_encryption_schema: :aes_gcm_no_padding,\r\n#     security_profile: :v2\r\n#   )\r\n#   if encrypted_object_uploaded?(\r\n#     s3_encryption_client,\r\n#     'doc-example-bucket',\r\n#     'my-file.txt',\r\n#     'This is the content of my-file.txt.'\r\n#   )\r\n#     puts 'Uploaded.'\r\n#   else\r\n#     puts 'Not uploaded.'\r\n#   end\r\ndef encrypted_object_uploaded?(\r\n  s3_encryption_client,\r\n  bucket_name,\r\n  object_key,\r\n  object_content\r\n)\r\n  s3_encryption_client.put_object(\r\n    bucket: bucket_name,\r\n    key: object_key,\r\n    body: object_content\r\n  )\r\n  return true\r\nrescue StandardError => e\r\n  puts \"Error uploading object: #{e.message}\"\r\n  return false\r\nend\r\n\r\n# Generates a random AES256-GCM key. Call this function if you do not\r\n#   already have an AES256-GCM key that you want to use to encrypt the\r\n#   object.\r\n#\r\n# @ return [String] The generated AES256-GCM key. You must keep a record of\r\n#   the key string that is reported. You will not be able to later decrypt the\r\n#   contents of any object that is encrypted with this key unless you\r\n#   have this key.\r\n# @ example\r\n#     get_random_aes_256_gcm_key\r\ndef get_random_aes_256_gcm_key\r\n  cipher = OpenSSL::Cipher.new('aes-256-gcm')\r\n  cipher.encrypt\r\n  random_key = cipher.random_key\r\n  random_key_64_string = [random_key].pack('m')\r\n  random_key_64 = random_key_64_string.unpack('m')[0]\r\n  puts 'The base 64-encoded string representation of the randomly-' \\\r\n    'generated AES256-GCM key is:'\r\n  puts random_key_64_string\r\n  puts 'Keep a record of this key string. You will not be able to later ' \\\r\n    'decrypt the contents of any object that is encrypted with this key ' \\\r\n    'unless you have this key.'\r\n  return random_key_64\r\nend\r\n\r\n# Full example call:\r\ndef run_me\r\n  bucket_name = 'doc-example-bucket'\r\n  object_key = 'my-file.txt'\r\n  region = 'us-east-1'\r\n  object_content = File.read(object_key)\r\n\r\n  # The following call generates a random AES256-GCM key. Alternatively, you can\r\n  # provide a base64-encoded string representation of an existing key that\r\n  # you want to use to encrypt the object. For example:#\r\n  # encryption_key_string = 'XSiKrmzhtDKR9tTwJRSLjgwLhiMA82TC2z3GEXAMPLE='\r\n  # encryption_key = encryption_key_string.unpack('m')[0]\r\n  encryption_key = get_random_aes_256_gcm_key\r\n\r\n  # Note that in the following call:\r\n  # - key_wrap_schema must be aes_gcm for symmetric keys.\r\n  # - To allow reading and decrypting objects that are encrypted by the\r\n  #   Amazon S3 V1 encryption client instead, use :v2_and_legacy instead of :v2.\r\n  s3_encryption_client = Aws::S3::EncryptionV2::Client.new(\r\n    region: region,\r\n    encryption_key: encryption_key,\r\n    key_wrap_schema: :aes_gcm,\r\n    content_encryption_schema: :aes_gcm_no_padding,\r\n    security_profile: :v2\r\n  )\r\n\r\n  if encrypted_object_uploaded?(\r\n    s3_encryption_client,\r\n    bucket_name,\r\n    object_key,\r\n    object_content\r\n  )\r\n    puts 'Uploaded.'\r\n  else\r\n    puts 'Not uploaded.'\r\n  end\r\nend\r\n\r\nrun_me if $PROGRAM_NAME == __FILE__\r\n","lang_cluster":"Ruby","length":118,"code_uid":"e3bf378560c84c23b1687957dedb6f7f"}
{"diff_hunk":"@@ -52,12 +52,13 @@\n                   <div class=\"clearfix\"><\/div>\n                 <\/div>\n               <\/div>\n-              <div id=\"<%= \"collapsePhase#{phase.id}\" %>\" class=\"panel-collapse collapse\" role=\"tabpanel\" aria-labelledby=\"<%= \"headingPhase#{phase.id}\" %>\">\n+              <div id=\"<%= \"collapsePhase#{phase.id}\" %>\" class=\"panel-collapse collapse<%= i == 0 ? 'in' : '' %>\" role=\"tabpanel\" aria-labelledby=\"<%= \"headingPhase#{phase.id}\" %>\"<%= i == 0 ? 'aria-expanded=\"true\"' : '' %>>\n                 <div class=\"panel-body\">\n                   <%= render partial: 'org_admin\/templates\/show_phases_sections', locals: { phase: phase, phase_hash: phase_hash, template: template, current: current } %>\n                 <\/div>\n               <\/div>\n             <\/div>\n+            <% i += 1 %>\n         <% end %>\n       <% end %>\n     <\/div>","old_code":"<div class=\"row\">\n  <div class=\"col-md-12\">\n    <% if template == current && template.customization_of.nil? %>\n      <div class=\"template_edit\" style=\"display: none;\">\n        <%= render partial: \"org_admin\/templates\/edit_template\", locals: { template: template, template_hash: template_hash } %>\n      <\/div>\n    <% end %>\n  <\/div>\n<\/div>\n<div class=\"row\">\n  <div class=\"col-md-12\">\n    <div class=\"template_show\">\n      <%= render partial: \"org_admin\/templates\/show_template\", locals: { template: template, current: current, template_hash: template_hash }%>\n    <\/div>\n  <\/div>\n<\/div>\n<div class=\"row\">\n  <div class=\"col-md-12\">\n    <% if template_hash[:template][:phases].present? && template_hash[:template][:phases].length > 1 %>\n      <div id=\"sections-accordion-controls\">\n        <div class=\"accordion-controls\" data-parent=\"phases_accordion\">\n          <a href=\"#\" data-toggle-direction=\"show\"><%= _('expand all') %><\/a>\n          <span>|<\/span>\n          <a href=\"#\" data-toggle-direction=\"hide\"><%= _('collapse all') %><\/a>\n        <\/div>\n      <\/div>\n    <% end %>\n  <\/div>\n<\/div>\n<!-- Accordion for phases -->\n<div class=\"row\">\n  <div class=\"col-md-12\">\n    <div class=\"panel-group\" id=\"phases_accordion\" role=\"tablist\">\n      <!-- If template has phases-->\n      <% if template_hash[:template][:phases].present? %>\n        <% template_hash[:template][:phases].each do |phase_no, phase_hash| %>\n          <% phase = phase_hash[:data] %>\n            <div class=\"panel panel-default\">\n              <div class=\"heading-button\" role=\"button\" data-toggle=\"collapse\"\n                   data-parent=\"phases_accordion\" \n                   href=\"#collapsePhase<%= phase.id %>\"\n                   aria-expanded=\"false\" \n                   aria-controls=\"#collapsePhase<%= phase.id %>\">\n                   \n                <div class=\"panel-heading\" role=\"tab\" id=\"<%= \"headingPhase#{phase.id}\" %>\">\n                  <div class=\"panel-title pull-left\">\n                    <%= phase.title %>\n                  <\/div>\n                  <div class=\"pull-right\">\n                    <i class=\"fa fa-plus pull-right\" aria-hidden=\"true\"><\/i>\n                  <\/div>\n                  <div class=\"clearfix\"><\/div>\n                <\/div>\n              <\/div>\n              <div id=\"<%= \"collapsePhase#{phase.id}\" %>\" class=\"panel-collapse collapse\" role=\"tabpanel\" aria-labelledby=\"<%= \"headingPhase#{phase.id}\" %>\">\n                <div class=\"panel-body\">\n                  <%= render partial: 'org_admin\/templates\/show_phases_sections', locals: { phase: phase, phase_hash: phase_hash, template: template, current: current } %>\n                <\/div>\n              <\/div>\n            <\/div>\n        <% end %>\n      <% end %>\n    <\/div>\n  <\/div>\n<\/div>","lang_cluster":"Ruby","length":65,"code_uid":"2a44b96a183f40d38cb443c01103854d"}
{"diff_hunk":"@@ -81,7 +81,7 @@ describe 'Canceling a request' do\n \n       visit proposal_path(proposal)\n       click_on('Cancel my request')\n-      fill_in \"reason_input\", with: ''\n+      fill_in 'reason_input', with: ''\n       click_on('Yes, cancel this request')\n \n       expect(page).to have_content('A reason for cancellation is required. Please indicate why this request needs to be cancelled.')","old_code":"describe 'Canceling a request' do\n  it 'shows a cancel link for the requester' do\n    proposal = create(:proposal)\n    login_as(proposal.requester)\n\n    visit proposal_path(proposal)\n\n    expect(page).to have_content('Cancel my request')\n  end\n\n  it 'does not show a cancel link for non-requesters' do\n    proposal = create(:proposal, :with_approver)\n    login_as(proposal.approvers.first)\n\n    visit proposal_path(proposal)\n\n    expect(page).to_not have_content('Cancel my request')\n  end\n\n  it 'prompts the requester for a reason' do\n    proposal = create(:proposal)\n    login_as(proposal.requester)\n\n    visit proposal_path(proposal)\n    click_on('Cancel my request')\n\n    expect(current_path).to eq(\"\/proposals\/#{proposal.id}\/cancel_form\")\n  end\n\n  context 'email' do\n    context 'proposal without approver' do\n      it 'sends cancellation email to requester' do\n        ActionMailer::Base.deliveries.clear\n        proposal = create(:proposal)\n\n        login_as(proposal.requester)\n        visit proposal_path(proposal)\n        click_on('Cancel my request')\n        fill_in 'reason_input', with: 'This is a good reason for the cancellation.'\n        click_on('Yes, cancel this request')\n\n        expect(deliveries.length).to eq(1)\n      end\n    end\n\n   context 'proposal with approver cancelled with reason' do\n      it 'sends comment email in addition to cancellation emails' do\n        ActionMailer::Base.deliveries.clear\n        proposal = create(:proposal, :with_approver)\n\n        login_as(proposal.requester)\n        visit proposal_path(proposal)\n        click_on('Cancel my request')\n        fill_in 'reason_input', with: 'This is a good reason for the cancellation.'\n        click_on('Yes, cancel this request')\n\n        expect(deliveries.length).to eq(3)\n      end\n   end\n  end\n\n  context 'entering in a reason cancellation' do\n    it 'successfully saves comments, changes the request status' do\n      proposal = create(:proposal)\n      login_as(proposal.requester)\n\n      visit proposal_path(proposal)\n      click_on('Cancel my request')\n      fill_in 'reason_input', with: 'This is a good reason for the cancellation.'\n      click_on('Yes, cancel this request')\n\n      expect(current_path).to eq(\"\/proposals\/#{proposal.id}\")\n      expect(page).to have_content('Your request has been cancelled')\n      expect(proposal.reload.status).to eq('cancelled')\n      expect(proposal.reload.comments.last.comment_text).to eq('Request cancelled with comments: This is a good reason for the cancellation.')\n    end\n\n    it 'displays an error if the reason is blank' do\n      proposal = create(:proposal)\n      login_as(proposal.requester)\n\n      visit proposal_path(proposal)\n      click_on('Cancel my request')\n      fill_in \"reason_input\", with: ''\n      click_on('Yes, cancel this request')\n\n      expect(page).to have_content('A reason for cancellation is required. Please indicate why this request needs to be cancelled.')\n    end\n  end\n\n  context 'Cancel landing page' do\n    it 'succesfully opens the page for a requester' do\n      proposal = create(:proposal)\n      login_as(proposal.requester)\n\n      visit cancel_form_proposal_path(proposal)\n\n      expect(page).to have_content(\"Cancellation: #{proposal.name}\")\n      expect(current_path).to eq(\"\/proposals\/#{proposal.id}\/cancel_form\")\n    end\n\n    it 'redirects for non-requesters' do\n      proposal = create(:proposal, :with_approver)\n      login_as(proposal.approvers.first)\n\n      visit cancel_form_proposal_path(proposal)\n\n      expect(page).to have_content('You are not the requester')\n      expect(current_path).to eq(\"\/proposals\/#{proposal.id}\")\n    end\n  end\nend\n","lang_cluster":"Ruby","length":112,"code_uid":"9d1e2a32f494408fb1bd61cf84db330e"}
{"diff_hunk":"@@ -40,5 +40,10 @@ module BoltSpec\n       expect(result['_error'] || (result['items'] && result['items'][0] && result['items'][0]['status'] != 'success'))\n       result['items'][0]['result']\n     end\n+\n+    def error_support\n+      minor = RUBY_VERSION.split('.')[1].to_i\n+      minor >= 1\n+    end\n   end\n end","old_code":"module BoltSpec\n  module Integration\n    def run_cli(arguments)\n      cli = Bolt::CLI.new(arguments)\n\n      # prevent tests from reading users config\n      allow(cli.config).to receive(:default_paths).and_return([File.join('.', 'path', 'does not exist')])\n      allow(Bolt::Inventory).to receive(:default_paths).and_return([File.join('.', 'path', 'does not exist')])\n      output =  StringIO.new\n      outputter = Bolt::Outputter::JSON.new(output)\n      allow(cli).to receive(:outputter).and_return(outputter)\n\n      opts = cli.parse\n      cli.execute(opts)\n      output.string\n    end\n\n    def run_cli_json(arguments)\n      output = run_cli(arguments)\n\n      begin\n        result = JSON.parse(output)\n      rescue JSON::ParserError\n        expect(output.string).to eq(\"Output should be JSON\")\n      end\n      result\n    end\n\n    def run_one_node(arguments)\n      result = run_cli_json(arguments)\n      if result['_error'] ||\n         (result['items'] && result['items'][0] && result['items'][0]['status'] != 'success')\n        expect(result).to eq(\"Should have succeed on node\" => true)\n      end\n      result['items'][0]['result']\n    end\n\n    def run_failed_node(arguments)\n      result = run_cli_json(arguments)\n      expect(result['_error'] || (result['items'] && result['items'][0] && result['items'][0]['status'] != 'success'))\n      result['items'][0]['result']\n    end\n  end\nend\n","lang_cluster":"Ruby","length":44,"code_uid":"a6f27a4fbcfc4038b9d9f0b8f5aa1548"}
{"diff_hunk":"@@ -59,3 +59,4 @@ def run_me\n end\n \n run_me if $PROGRAM_NAME == __FILE__\n+# snippet-end:[ec2.Ruby.rebootInstances]","old_code":"# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\r\n# SPDX - License - Identifier: Apache - 2.0\r\n\r\nrequire 'aws-sdk-ec2'\r\n\r\n# Reboots an Amazon Elastic Compute Cloud (Amazon EC2) instance.\r\n#\r\n# Prerequisites:\r\n#\r\n# - An Amazon EC2 instance.\r\n#\r\n# @param ec2_client [Aws::EC2::Client] An initialized EC2 client.\r\n# @param instance_id [String] The ID of the instance.\r\n# @example\r\n#   request_instance_reboot(\r\n#     Aws::EC2::Resource.new(region: 'us-east-1'),\r\n#     'i-123abc'\r\n#   )\r\ndef request_instance_reboot(ec2_client, instance_id)\r\n  response = ec2_client.describe_instances(instance_ids: [instance_id])\r\n  if response.count.zero?\r\n    puts 'Error requesting reboot: no matching instance found.'\r\n  else\r\n    instance = response.reservations[0].instances[0]\r\n    if instance.state.name == 'terminated'\r\n      puts 'Error requesting reboot: the instance is already terminated.'\r\n    else\r\n      ec2_client.reboot_instances(instance_ids: [instance_id])\r\n      puts 'Reboot request sent.'\r\n    end\r\n  end\r\nrescue StandardError => e\r\n  puts \"Error requesting reboot: #{e.message}\"\r\nend\r\n\r\n# Full example call:\r\ndef run_me\r\n  instance_id = ''\r\n  region = ''\r\n  # Print usage information and then stop.\r\n  if ARGV[0] == '--help' || ARGV[0] == '-h'\r\n    puts 'Usage:   ruby ec2-ruby-example-reboot-instance-i-123abc.rb ' \\\r\n      'INSTANCE_ID REGION'\r\n    puts 'Example: ruby ec2-ruby-example-reboot-instance-i-123abc.rb ' \\\r\n      'i-123abc us-east-1'\r\n    exit 1\r\n  # If no values are specified at the command prompt, use these default values.\r\n  elsif ARGV.count.zero?\r\n    instance_id = 'i-123abc'\r\n    region = 'us-east-1'\r\n  # Otherwise, use the values as specified at the command prompt.\r\n  else\r\n    instance_id = ARGV[0]\r\n    region = ARGV[1]\r\n  end\r\n\r\n  ec2_client = Aws::EC2::Client.new(region: region)\r\n  request_instance_reboot(ec2_client, instance_id)\r\nend\r\n\r\nrun_me if $PROGRAM_NAME == __FILE__\r\n","lang_cluster":"Ruby","length":61,"code_uid":"fa305cf3c3a54a3e9724c84555f22e3f"}
{"diff_hunk":"@@ -23,11 +23,10 @@ class VsphereHelper\n     # support Fog\/Cloud Provisioner layout\n     # (ie, someplace besides my made up conf)\n     vsphere_credentials = nil\n-    if File.exists? '\/etc\/plharness\/vsphere'\n-      vsphere_credentials = load_legacy_credentials\n-\n-    elsif File.exists?( dot_fog )\n+    if File.exists?( dot_fog )\n       vsphere_credentials = load_fog_credentials(dot_fog)\n+    else\n+      raise ArgumentError, \".fog file '#{dot_fog}' does not exist\"\n     end\n \n     return vsphere_credentials","old_code":"require 'yaml' unless defined?(YAML)\nbegin\n  require 'beaker\/logger'\nrescue LoadError\n  require File.expand_path(File.join(File.dirname(__FILE__), '..', 'logger.rb'))\nend\n\nclass VsphereHelper\n  def initialize vInfo = {}\n    @logger = vInfo[:logger] || Beaker::Logger.new\n    begin\n      require 'rbvmomi'\n    rescue LoadError\n      raise \"Unable to load RbVmomi, please ensure its installed\"\n    end\n    @connection = RbVmomi::VIM.connect :host     => vInfo[:server],\n                                       :user     => vInfo[:user],\n                                       :password => vInfo[:pass],\n                                       :insecure => true\n  end\n\n  def self.load_config(dot_fog = '.fog')\n    # support Fog\/Cloud Provisioner layout\n    # (ie, someplace besides my made up conf)\n    vsphere_credentials = nil\n    if File.exists? '\/etc\/plharness\/vsphere'\n      vsphere_credentials = load_legacy_credentials\n\n    elsif File.exists?( dot_fog )\n      vsphere_credentials = load_fog_credentials(dot_fog)\n    end\n\n    return vsphere_credentials\n  end\n\n  def self.load_fog_credentials(dot_fog = '.fog')\n    vInfo = YAML.load_file( dot_fog )\n\n    vsphere_credentials = {}\n    vsphere_credentials[:server] = vInfo[:default][:vsphere_server]\n    vsphere_credentials[:user]   = vInfo[:default][:vsphere_username]\n    vsphere_credentials[:pass]   = vInfo[:default][:vsphere_password]\n\n    return vsphere_credentials\n  end\n\n  def self.load_legacy_credentials\n    vInfo = YAML.load_file '\/etc\/plharness\/vsphere'\n\n    puts(\n      \"Use of \/etc\/plharness\/vsphere as a config file is deprecated.\\n\" +\n      \"Please use ~\/.fog instead\\n\" +\n      \"See http:\/\/docs.puppetlabs.com\/pe\/2.0\/\" +\n      \"cloudprovisioner_configuring.html for format\"\n    )\n\n    vsphere_credentials = {}\n    vsphere_credentials[:server] = vInfo['location']\n    vsphere_credentials[:user]   = vInfo['user']\n    vsphere_credentials[:pass]   = vInfo['pass']\n\n    return vsphere_credentials\n  end\n\n  def find_snapshot vm, snapname\n    search_child_snaps vm.snapshot.rootSnapshotList, snapname\n  end\n\n  def search_child_snaps tree, snapname\n    snapshot = nil\n    tree.each do |child|\n      if child.name == snapname\n        snapshot ||= child.snapshot\n      else\n        snapshot ||= search_child_snaps child.childSnapshotList, snapname\n      end\n    end\n    snapshot\n  end\n\n  def find_customization name\n    csm = @connection.serviceContent.customizationSpecManager\n\n    begin\n      customizationSpec = csm.GetCustomizationSpec({:name => name}).spec\n    rescue\n      customizationSpec = nil\n    end\n\n    return customizationSpec\n  end\n\n  # an easier wrapper around the horrid PropertyCollector interface,\n  # necessary for searching VMs in all Datacenters that may be nested\n  # within folders of arbitrary depth\n  # returns a hash array of <name> => <VirtualMachine ManagedObjects>\n  def find_vms names, connection = @connection\n    names = names.is_a?(Array) ? names : [ names ]\n    containerView = get_base_vm_container_from connection\n    propertyCollector = connection.propertyCollector\n\n    objectSet = [{\n      :obj => containerView,\n      :skip => true,\n      :selectSet => [ RbVmomi::VIM::TraversalSpec.new({\n          :name => 'gettingTheVMs',\n          :path => 'view',\n          :skip => false,\n          :type => 'ContainerView'\n      }) ]\n    }]\n\n    propSet = [{\n      :pathSet => [ 'name' ],\n      :type => 'VirtualMachine'\n    }]\n\n    results = propertyCollector.RetrievePropertiesEx({\n      :specSet => [{\n        :objectSet => objectSet,\n        :propSet   => propSet\n      }],\n      :options => { :maxObjects => nil }\n    })\n\n    vms = {}\n    results.objects.each do |result|\n      name = result.propSet.first.val\n      next unless names.include? name\n      vms[name] = result.obj\n    end\n\n    while results.token do\n      results = propertyCollector.ContinueRetrievePropertiesEx({:token => results.token})\n      results.objects.each do |result|\n        name = result.propSet.first.val\n        next unless names.include? name\n        vms[name] = result.obj\n      end\n    end\n    vms\n  end\n\n  def find_datastore datastorename\n    datacenter = @connection.serviceInstance.find_datacenter\n    datacenter.find_datastore(datastorename)\n  end\n\n  def find_folder foldername\n    datacenter = @connection.serviceInstance.find_datacenter\n    base = datacenter.vmFolder\n    folders = foldername.split('\/')\n    folders.each do |folder|\n      case base\n        when RbVmomi::VIM::Folder\n          base = base.childEntity.find { |f| f.name == folder }\n        else\n          abort \"Unexpected object type encountered (#{base.class}) while finding folder\"\n      end\n    end\n\n    base\n  end\n\n  def find_pool poolname\n    datacenter = @connection.serviceInstance.find_datacenter\n    base = datacenter.hostFolder\n    pools = poolname.split('\/')\n    pools.each do |pool|\n      case base\n        when RbVmomi::VIM::Folder\n          base = base.childEntity.find { |f| f.name == pool }\n        when RbVmomi::VIM::ClusterComputeResource\n          base = base.resourcePool.resourcePool.find { |f| f.name == pool }\n        when RbVmomi::VIM::ResourcePool\n          base = base.resourcePool.find { |f| f.name == pool }\n        else\n          abort \"Unexpected object type encountered (#{base.class}) while finding resource pool\"\n      end\n    end\n\n    base = base.resourcePool unless base.is_a?(RbVmomi::VIM::ResourcePool) and base.respond_to?(:resourcePool)\n    base\n  end\n\n  def get_base_vm_container_from connection\n    viewManager = connection.serviceContent.viewManager\n    viewManager.CreateContainerView({\n      :container => connection.serviceContent.rootFolder,\n      :recursive => true,\n      :type      => [ 'VirtualMachine' ]\n    })\n  end\n\n  def close\n    @connection.close\n  end\nend\n\n","lang_cluster":"Ruby","length":199,"code_uid":"a411848b267841b2bf2b50cb6ba440d8"}
{"diff_hunk":"@@ -2,7 +2,7 @@ class UsersController < ApplicationController\n   helper PaginableHelper\n   helper PermsHelper\n   include ConditionalUserMailer\n-  after_action :verify_authorized\n+  after_action :verify_authorized, except: ['update_email_preferences']\n   respond_to :html\n \n   ##","old_code":"class UsersController < ApplicationController\n  helper PaginableHelper\n  helper PermsHelper\n  include ConditionalUserMailer\n  after_action :verify_authorized\n  respond_to :html\n\n  ##\n  # GET - List of all users for an organisation\n  # Displays number of roles[was project_group], name, email, and last sign in\n  def admin_index\n    authorize User\n    if current_user.can_super_admin?\n      @users = User.page(1)\n    else\n      @users = current_user.org.users.page(1)\n    end\n  end\n\n  ##\n  # GET - Displays the permissions available to the selected user\n  # Permissions which the user already has are pre-selected\n  # Selecting new permissions and saving calls the admin_update_permissions action\n  def admin_grant_permissions\n    user = User.find(params[:id])\n    authorize user\n\n    # Super admin can grant any Perm, org admins can only grant Perms they\n    # themselves have access to\n    if current_user.can_super_admin?\n      perms = Perm.all\n    else\n      perms = current_user.perms\n    end\n\n    render json: {\n      \"user\" => {\n        \"id\" => user.id,\n        \"html\" => render_to_string(partial: 'users\/admin_grant_permissions',\n                                   locals: { user: user, perms: perms },\n                                   formats: [:html])\n      }\n    }.to_json\n  end\n\n  ##\n  # POST - updates the permissions for a user\n  # redirects to the admin_index action\n  # should add validation that the perms given are current perms of the current_user\n  def admin_update_permissions\n    @user = User.find(params[:id])\n    authorize @user\n    perms_ids = params[:perm_ids].blank? ? [] : params[:perm_ids].map(&:to_i)\n    perms = Perm.where( id: perms_ids)\n    privileges_changed = false\n    current_user.perms.each do |perm|\n      if @user.perms.include? perm\n        if ! perms.include? perm\n          @user.perms.delete(perm)\n          if perm.id == Perm.use_api.id\n            @user.remove_token!\n          end\n          privileges_changed = true\n        end\n      else\n        if perms.include? perm\n          @user.perms << perm\n          if perm.id == Perm.use_api.id\n            @user.keep_or_generate_token!\n            privileges_changed = true\n          end\n        end\n      end\n    end\n\n    if @user.save!\n      if privileges_changed\n        deliver_if(recipients: @user, key: 'users.admin_privileges') do |r|\n          UserMailer.admin_privileges(r).deliver_now\n        end\n      end\n      render(json: {\n        code: 1,\n        msg: success_message(_('permissions'), _('saved')),\n        current_privileges: render_to_string(partial: 'users\/current_privileges', locals: { user: @user }, formats: [:html])\n        })\n    else\n      render(json: { code: 0, msg: failed_update_error(@user, _('user')) })\n    end\n  end\n\n  def update_email_preferences\n    prefs = params[:prefs]\n    authorize current_user, :update?\n    pref = current_user.pref\n    # does user not have prefs?\n    if pref.blank?\n      pref = Pref.new\n      pref.settings = {}\n      pref.user = current_user\n    end\n    pref.settings[:email] = booleanize_hash(prefs)\n    pref.save\n\n    # Include active tab in redirect path\n    redirect_to \"#{edit_user_registration_path}\\#notification-preferences\", notice: success_message(_('preferences'), _('saved'))\n  end\n\n  # PUT \/users\/:id\/org_swap\n  # -----------------------------------------------------\n  def org_swap\n    # Allows the user to swap their org affiliation on the fly\n    authorize current_user\n    begin\n      org = Org.find(org_swap_params[:org_id])\n    rescue ActiveRecord::RecordNotFound\n      redirect_to(request.referer, alert: _('Please select an organisation from the list')) and return\n    end\n    if org.present?\n      current_user.org = org\n      if current_user.save!\n        redirect_to request.referer, notice: _('Your organisation affiliation has been changed. You may now edit templates for %{org_name}.') % {org_name: current_user.org.name}\n      else\n        redirect_to request.referer, alert: _('Unable to change your organisation affiliation at this time.')\n      end\n    else\n      redirect_to request.referer, alert: _('Unknown organisation.')\n    end\n  end\n\n  # PUT \/users\/:id\/activate\n  # -----------------------------------------------------\n  def activate\n    authorize current_user\n\n    user = User.find(params[:id])\n    if user.present?\n      begin\n        user.active = !user.active\n        user.save!\n        render json: {\n          code: 1,\n          msg: _('Successfully %{action} %{username}\\'s account.') % { action: user.active ? _('activated') : _('deactivated'), username: user.name(false) }\n        }\n      rescue Exception\n        render json: {\n          code: 0,\n          msg: _('Unable to %{action} %{username}') % { action: user.active ? _('activate') : _('deactivate'), username: user.name(false) }\n        }\n      end\n    end\n  end\n\n  # POST \/users\/acknowledge_notification\n  def acknowledge_notification\n    authorize current_user\n    @notification = Notification.find(params[:notification_id])\n    current_user.acknowledge(@notification)\n    render nothing: true\n  end\n\n  private\n  def org_swap_params\n    params.require(:superadmin_user).permit(:org_id, :org_name)\n  end\n\n  ##\n  # html forms return our boolean values as strings, this converts them to true\/false\n  def booleanize_hash(node)\n    #leaf: convert to boolean and return\n    #hash: iterate over leaves\n    unless node.is_a?(Hash)\n      return node == \"true\"\n    end\n    node.each do |key, value|\n      node[key] = booleanize_hash(value)\n    end\n  end\n\nend\n","lang_cluster":"Ruby","length":180,"code_uid":"d2a118750ad1440ba598cf973b9ca506"}
{"diff_hunk":"@@ -1,5 +1,6 @@\n class StripeCustomerFinder\n-  ERROR_MESSAGE = \"a similar object exists in live mode\".freeze\n+  SIMILAR_OBJECT_ERROR = \"a similar object exists in live mode\".freeze\n+  NO_CUSTOMER_ERROR = \"No such customer\".freeze\n \n   def self.retrieve(customer_id)\n     new.retrieve(customer_id)","old_code":"class StripeCustomerFinder\n  ERROR_MESSAGE = \"a similar object exists in live mode\".freeze\n\n  def self.retrieve(customer_id)\n    new.retrieve(customer_id)\n  end\n\n  def retrieve(customer_id)\n    Stripe::Customer.retrieve(customer_id)\n  rescue Stripe::InvalidRequestError => e\n    if Rails.env.development? && stripe_environent_error?(e)\n      generate_fake_customer(customer_id)\n    else\n      raise\n    end\n  end\n\n  private\n\n  def stripe_environent_error?(error)\n    error.message.include? ERROR_MESSAGE\n  end\n\n  def generate_fake_customer(customer_id)\n    Stripe::Customer.construct_from(\n      \"id\" => customer_id.to_s,\n      \"object\" => \"customer\",\n      \"default_card\" => \"card_12345\",\n      \"default_source\" => \"card_12345\",\n      \"cards\" => Stripe::ListObject.construct_from(\n        \"data\" => [Stripe::Card.construct_from(\n          \"id\" => \"card_12345\",\n          \"object\" => \"card\",\n          \"address_city\" => nil,\n          \"address_country\" => nil,\n          \"address_line1\" => nil,\n          \"address_line1_check\" => nil,\n          \"address_line2\" => nil,\n          \"address_state\" => nil,\n          \"address_zip\" => nil,\n          \"address_zip_check\" => nil,\n          \"brand\" => \"FakeVisa\",\n          \"country\" => \"US\",\n          \"customer\" => customer_id.to_s,\n          \"cvc_check\" => \"pass\",\n          \"dynamic_last4\" => nil,\n          \"exp_month\" => 1,\n          \"exp_year\" => 2100,\n          \"fingerprint\" => \"61N1s4XuvJOoLAnb\",\n          \"funding\" => \"credit\",\n          \"last4\" => \"4242\",\n          \"metadata\" => {},\n          \"name\" => nil,\n          \"tokenization_method\" => nil,\n        )],\n        \"has_more\" => false,\n        \"total_count\" => 1,\n        \"url\" => \"\/v1\/customers\/cus_12345\/cards\",\n      ),\n    )\n  end\nend\n","lang_cluster":"Ruby","length":62,"code_uid":"944cc1b749ff47419553907d6abcd555"}
{"diff_hunk":"@@ -19,10 +19,6 @@ class User < ActiveRecord::Base\n     where(id: user_id).first || mentors.sample\n   end\n \n-  def mentor_name\n-    mentor.try(:name)\n-  end\n-\n   def subscription_purchases\n     paid_purchases.where(payment_method: 'subscription')\n   end","old_code":"class User < ActiveRecord::Base\n  include Clearance::User\n\n  has_many :paid_purchases, -> { where paid: true }, class_name: 'Purchase'\n  has_many :purchases\n  has_many :completions\n  has_many :notes, -> { order 'created_at DESC' }\n  has_one :subscription\n  belongs_to :mentor, class_name: 'User'\n  has_many :mentees, class_name: 'User', foreign_key: 'mentor_id'\n\n  validates :name, presence: true\n\n  def self.mentors\n    where(available_to_mentor: true)\n  end\n\n  def self.find_or_sample_mentor(user_id)\n    where(id: user_id).first || mentors.sample\n  end\n\n  def mentor_name\n    mentor.try(:name)\n  end\n\n  def subscription_purchases\n    paid_purchases.where(payment_method: 'subscription')\n  end\n\n  def paid_products\n    paid_purchases.where(\"purchaseable_type != 'IndividualPlan'\")\n  end\n\n  def first_name\n    name.split(\" \").first\n  end\n\n  def last_name\n    name.split(\" \").last\n  end\n\n  def external_auth?\n    auth_provider.present?\n  end\n\n  def has_purchased?\n    paid_purchases.present?\n  end\n\n  def inactive_subscription\n    if subscription.present? && !subscription.active?\n      subscription\n    end\n  end\n\n  def has_subscription_with_mentor?\n    subscription.try(:includes_mentor?)\n  end\n\n  def has_logged_in_to_forum?\n    OauthAccessToken.for_user(self).present?\n  end\n\n  def has_active_subscription?\n    subscription.present? && subscription.active?\n  end\n\n  def subscribed_at\n    subscription.try(:created_at)\n  end\n\n  def credit_card\n    if stripe_customer\n      stripe_customer['active_card']\n    end\n  end\n\n  def assign_mentor(user)\n    update_attribute(:mentor_id, user.id)\n  end\n\n  def plan_name\n    subscription.try(:plan).try(:name)\n  end\n\n  private\n\n  def stripe_customer\n    if stripe_customer_id.present?\n      Stripe::Customer.retrieve(stripe_customer_id)\n    end\n  end\n\n  def password_optional?\n    super || external_auth?\n  end\nend\n","lang_cluster":"Ruby","length":97,"code_uid":"2a89c499405a41f7ab659975c334b43c"}
{"diff_hunk":"@@ -163,7 +163,7 @@ module RSpec\n     #\n     # As of rspec 2.14.1, we no longer require `rspec\/mocks` and\n     # `rspec\/expectations` when `rspec` is required, so we want\n-    # to make them available as an autoload. For more info, see:\n+    # to make them available as an autoload.\n     require MODULES_TO_AUTOLOAD.fetch(name) { return super }\n     ::RSpec.const_get(name)\n   end","old_code":"require 'time'\nrequire 'rbconfig'\n\nrequire \"rspec\/support\"\nRSpec::Support.require_rspec_support \"caller_filter\"\n\nRSpec::Support.define_optimized_require_for_rspec(:core) { |f| require_relative f }\n\n%w[\n  version\n  warnings\n\n  flat_map\n  filter_manager\n  dsl\n  notifications\n  reporter\n\n  hooks\n  memoized_helpers\n  metadata\n  metadata_filter\n  pending\n  formatters\n  ordering\n\n  world\n  configuration\n  option_parser\n  configuration_options\n  command_line\n  runner\n  example\n  shared_example_group\/collection\n  shared_example_group\n  example_group\n].each { |name| RSpec::Support.require_rspec_core name }\n\nmodule RSpec\n  autoload :SharedContext, 'rspec\/core\/shared_context'\n\n  extend RSpec::Core::Warnings\n\n  # @private\n  # Used internally to ensure examples get reloaded between multiple runs in\n  # the same process.\n  def self.reset\n    @world = nil\n    @configuration = nil\n  end\n\n  # Returns the global [Configuration](RSpec\/Core\/Configuration) object. While you\n  # _can_ use this method to access the configuration, the more common\n  # convention is to use [RSpec.configure](RSpec#configure-class_method).\n  #\n  # @example\n  #     RSpec.configuration.drb_port = 1234\n  # @see RSpec.configure\n  # @see Core::Configuration\n  def self.configuration\n    @configuration ||= begin\n                         config = RSpec::Core::Configuration.new\n                         config.expose_dsl_globally = true\n                         config\n                       end\n\n  end\n\n  # @private\n  # Used internally to set the global object\n  def self.configuration=(new_configuration)\n    @configuration = new_configuration\n  end\n\n  # Yields the global configuration to a block.\n  # @yield [Configuration] global configuration\n  #\n  # @example\n  #     RSpec.configure do |config|\n  #       config.add_formatter 'documentation'\n  #     end\n  # @see Core::Configuration\n  def self.configure\n    yield configuration if block_given?\n  end\n\n  # The example being executed.\n  #\n  # The primary audience for this method is library authors who need access\n  # to the example currently being executed and also want to support all\n  # versions of RSpec 2 and 3.\n  #\n  # @example\n  #\n  #     RSpec.configure do |c|\n  #       # context.example is deprecated, but RSpec.current_example is not\n  #       # available until RSpec 3.0.\n  #       fetch_current_example = RSpec.respond_to?(:current_example) ?\n  #         proc { RSpec.current_example } : proc { |context| context.example }\n  #\n  #       c.before(:example) do\n  #         example = fetch_current_example.call(self)\n  #\n  #         # ...\n  #       end\n  #     end\n  #\n  def self.current_example\n    Thread.current[:_rspec_current_example]\n  end\n\n  # Set the current example being executed.\n  # @api private\n  def self.current_example=(example)\n    Thread.current[:_rspec_current_example] = example\n  end\n\n  # @private\n  # Internal container for global non-configuration data\n  def self.world\n    @world ||= RSpec::Core::World.new\n  end\n\n  # @private\n  # Used internally to set the global object\n  def self.world=(new_world)\n    @world = new_world\n  end\n\n  module Core\n    # @private\n    # This avoids issues with reporting time caused by examples that\n    # change the value\/meaning of Time.now without properly restoring\n    # it.\n    class Time\n      class << self\n        define_method(:now, &::Time.method(:now))\n      end\n    end\n\n    # @private path to executable file\n    def self.path_to_executable\n      @path_to_executable ||= File.expand_path('..\/..\/..\/exe\/rspec', __FILE__)\n    end\n  end\n\n  # @private\n  MODULES_TO_AUTOLOAD = {\n    :Matchers     => \"rspec\/expectations\",\n    :Expectations => \"rspec\/expectations\",\n    :Mocks        => \"rspec\/mocks\"\n  }\n\n  # @private\n  def self.const_missing(name)\n    # Load rspec-expectations when RSpec::Matchers is referenced. This allows\n    # people to define custom matchers (using `RSpec::Matchers.define`) before\n    # rspec-core has loaded rspec-expectations (since it delays the loading of\n    # it to allow users to configure a different assertion\/expectation\n    # framework). `autoload` can't be used since it works with ruby's built-in\n    # require (e.g. for files that are available relative to a load path dir),\n    # but not with rubygems' extended require.\n    #\n    # As of rspec 2.14.1, we no longer require `rspec\/mocks` and\n    # `rspec\/expectations` when `rspec` is required, so we want\n    # to make them available as an autoload. For more info, see:\n    require MODULES_TO_AUTOLOAD.fetch(name) { return super }\n    ::RSpec.const_get(name)\n  end\nend\n","lang_cluster":"Ruby","length":170,"code_uid":"0b8381f62f2a42519b7f26eef68350d5"}
{"diff_hunk":"@@ -4,7 +4,7 @@ require 'spec_helper'\n require 'bolt_spec\/conn'\n require 'bolt_spec\/transport'\n require 'bolt\/transport\/docker'\n-require 'bolt\/target'\n+require 'bolt\/inventory'\n \n require 'shared_examples\/transport'\n ","old_code":"# frozen_string_literal: true\n\nrequire 'spec_helper'\nrequire 'bolt_spec\/conn'\nrequire 'bolt_spec\/transport'\nrequire 'bolt\/transport\/docker'\nrequire 'bolt\/target'\n\nrequire 'shared_examples\/transport'\n\ndescribe Bolt::Transport::Docker, docker: true do\n  include BoltSpec::Conn\n  include BoltSpec::Transport\n\n  let(:hostname) { conn_info('docker')[:host] }\n  let(:docker) { Bolt::Transport::Docker.new }\n  let(:target) { Bolt::Target.new(\"docker:\/\/#{hostname}\", transport_conf) }\n\n  context 'with docker' do\n    let(:transport) { :docker }\n    let(:os_context) { posix_context }\n\n    it \"can test whether the target is available\" do\n      expect(runner.connected?(target)).to eq(true)\n    end\n\n    it \"returns false if the target is not available\" do\n      expect(runner.connected?(Bolt::Target.new('unknownfoo'))).to eq(false)\n    end\n\n    include_examples 'transport api'\n\n    context 'file errors' do\n      before(:each) do\n        allow_any_instance_of(Bolt::Transport::Docker::Connection).to receive(:write_remote_file).and_raise(\n          Bolt::Node::FileError.new(\"no write\", \"WRITE_ERROR\")\n        )\n        allow_any_instance_of(Bolt::Transport::Docker::Connection).to receive(:make_tempdir).and_raise(\n          Bolt::Node::FileError.new(\"no tmpdir\", \"TEMDIR_ERROR\")\n        )\n      end\n\n      include_examples 'transport failures'\n    end\n  end\n\n  context 'with_connection' do\n    it \"fails with an unknown host\" do\n      # Test fails differently on Windows due to issues in the docker-api gem.\n      expect {\n        docker.with_connection(Bolt::Target.new('not_a_target')) {}\n      }.to raise_error(Bolt::Node::ConnectError, \/Could not find a container with name or ID matching \\'not_a_target\\'\/)\n    end\n  end\n\n  context 'when url is specified' do\n    let(:transport_conf) { { 'service-url' => 'tcp:\/\/localhost:55555' } }\n\n    it 'uses the url' do\n      expect {\n        docker.with_connection(target) {}\n      }.to raise_error(Bolt::Node::ConnectError, \/Could not find a container with name or ID matching\/)\n    end\n  end\n\n  context 'when there is no host in the target' do\n    let(:target) { Bolt::Target.new(nil, \"name\" => \"hostless\") }\n\n    it 'errors' do\n      expect { docker.run_command(target, 'whoami') }.to raise_error(\/does not have a host\/)\n    end\n  end\nend\n","lang_cluster":"Ruby","length":73,"code_uid":"708eb9a127de4431b41e8b4f976657eb"}
{"diff_hunk":"@@ -46,7 +46,7 @@ describe \"admin\" do\n   end\n \n   it \"contains reindex button link\" do\n-    user = login_as_admin_user\n+    login_as_admin_user\n \n     visit admin_dashboard_path\n ","old_code":"describe \"admin\" do\n  it \"does not allow Delete of Users\" do\n    user = login_as_admin_user\n\n    visit admin_users_path\n\n    expect(page).to_not have_content(\"Delete\")\n  end\n\n  it \"does not allow editing of user delegates\" do\n    user = login_as_admin_user\n    other_user = create(:user)\n    user_delegate = create(:user_delegate, assigner: user, assignee: other_user)\n\n    visit edit_admin_user_path(user)\n    visit admin_user_delegate_path(user_delegate)\n\n    expect(page).not_to have_content(\"Edit User Delegate\")\n  end\n\n  it \"does not allow delete of proposals\" do\n    user = login_as_admin_user\n    _proposal = create(:proposal)\n\n    visit admin_proposals_path\n\n    expect(page).to_not have_content(\"Delete\")\n  end\n\n  it \"does not allow edit of proposals\" do\n    user = login_as_admin_user\n    _proposal = create(:proposal)\n\n    visit admin_proposals_path\n\n    expect(page).not_to have_content(\"Edit\")\n  end\n\n  it \"shows user.display_name when viewing User records\" do\n    user = login_as_admin_user\n    proposal = create(:proposal, requester: user)\n\n    visit admin_proposals_path\n\n    expect(page).to have_content(user.display_name)\n  end\n\n  it \"contains reindex button link\" do\n    user = login_as_admin_user\n\n    visit admin_dashboard_path\n\n    expect(page).to have_content(\"Re-index Proposals\")\n  end\n\n  it \"creates new User\" do\n    user = login_as_admin_user\n\n    visit new_admin_user_path\n\n    fill_in \"user[first_name]\", with: \"test\"\n    fill_in \"user[last_name]\", with: \"user\"\n    fill_in \"user[email_address]\", with: \"testuser@example.com\"\n    select \"test\", from: \"user[client_slug]\"\n    select \"observer\", from: \"user[role_ids][]\"\n    click_button \"Create User\"\n\n    expect(page).to have_content(\"test user <testuser@example.com>\")\n  end\n\n  it \"triggers actions on Complete button click\" do\n    user = login_as_admin_user\n    proposal = create(:proposal, :with_serial_approvers)\n\n    deliveries.clear\n    visit admin_proposal_path(proposal)\n    click_link \"Complete\"\n\n    expect(deliveries.count).to eq(3)\n    proposal.reload\n    expect(proposal).to be_completed\n  end\n\n  it \"does not trigger actions on Complete Without Notifications button click\" do\n    user = login_as_admin_user\n    proposal = create(:proposal, :with_serial_approvers)\n\n    deliveries.clear\n    visit admin_proposal_path(proposal)\n    click_link \"Complete without notifications\"\n\n    expect(deliveries.count).to eq(0)\n    proposal.reload\n    expect(proposal).to be_completed\n  end\n\n  it \"triggers actions on Step edit\" do\n    user = login_as_admin_user\n    proposal = create(:proposal, :with_serial_approvers)\n    first_step = proposal.individual_steps.first\n\n    deliveries.clear\n    visit edit_admin_step_path(first_step)\n    expect(page).to have_content(\"actionable\")\n\n    select \"completed\", from: \"step[status]\"\n    click_button \"Update Approval\"\n\n    expect(page).to have_content(\"Approval was successfully updated\")\n    expect(deliveries.count).to eq(0)\n    first_step.reload\n    expect(first_step.completed_at).to_not be_nil\n    expect(first_step.completer).to eq(user)\n  end\n\n  def login_as_admin_user\n    user = create(:user, :admin)\n    login_as(user)\n    user\n  end\nend\n","lang_cluster":"Ruby","length":121,"code_uid":"ddf56a6d8c55448d826d31bc851c9b93"}
{"diff_hunk":"@@ -87,6 +87,7 @@ module Bolt\n         Set.new\n       end\n     end\n+    alias feature_set features\n \n     def plugin_hooks\n       if @inventory","old_code":"# frozen_string_literal: true\n\nrequire 'bolt\/error'\n\nmodule Bolt\n  class Target\n    attr_reader :options\n    # CODEREVIEW: this feels wrong. The altertative is threading inventory through the\n    # executor to the RemoteTransport\n    attr_accessor :uri, :inventory\n\n    PRINT_OPTS ||= %w[host user port protocol].freeze\n\n    # Satisfies the Puppet datatypes API\n    def self.from_asserted_hash(hash)\n      new(hash['uri'], hash['options'])\n    end\n\n    # URI can be passes as nil\n    def initialize(uri, options = nil)\n      # lazy-load expensive gem code\n      require 'addressable\/uri'\n\n      @uri = uri\n      @uri_obj = parse(uri)\n      @options = options || {}\n      @options.freeze\n\n      if @options['user']\n        @user = @options['user']\n      end\n\n      if @options['password']\n        @password = @options['password']\n      end\n\n      if @options['port']\n        @port = @options['port']\n      end\n\n      if @options['protocol']\n        @protocol = @options['protocol']\n      end\n\n      if @options['host']\n        @host = @options['host']\n      end\n\n      # WARNING: name should never be updated\n      @name = @options['name'] || @uri\n    end\n\n    def update_conf(conf)\n      @protocol = conf[:transport]\n\n      t_conf = conf[:transports][transport.to_sym] || {}\n      # Override url methods\n      @user = t_conf['user']\n      @password = t_conf['password']\n      @port = t_conf['port']\n      @host = t_conf['host']\n\n      # Preserve everything in options so we can easily create copies of a Target.\n      @options = t_conf.merge(@options)\n\n      self\n    end\n\n    def parse(string)\n      if string.nil?\n        nil\n      elsif string =~ %r{^[^:]+:\/\/}\n        Addressable::URI.parse(string)\n      else\n        # Initialize with an empty scheme to ensure we parse the hostname correctly\n        Addressable::URI.parse(\"\/\/#{string}\")\n      end\n    rescue Addressable::URI::InvalidURIError => e\n      raise Bolt::ParseError, \"Could not parse target URI: #{e.message}\"\n    end\n    private :parse\n\n    def features\n      if @inventory\n        @inventory.features(self)\n      else\n        Set.new\n      end\n    end\n\n    def plugin_hooks\n      if @inventory\n        @inventory.plugin_hooks(self)\n      else\n        {}\n      end\n    end\n\n    # TODO: WHAT does equality mean here?\n    # should we just compare names? is there something else that is meaninful?\n    def eql?(other)\n      if self.class.equal?(other.class)\n        if @uri\n          return @uri == other.uri\n        else\n          @name = other.name\n        end\n      end\n      false\n    end\n    alias == eql?\n\n    def hash\n      @uri.hash ^ @options.hash\n    end\n\n    def to_s\n      opts = @options.select { |k, _| PRINT_OPTS.include? k }\n      \"Target('#{@uri}', #{opts})\"\n    end\n\n    def to_h\n      options.merge(\n        'name' => name,\n        'uri' => uri,\n        'protocol' => protocol,\n        'user' => user,\n        'password' => password,\n        'host' => host,\n        'port' => port\n      )\n    end\n\n    def host\n      @uri_obj&.hostname || @host\n    end\n\n    def name\n      @name || @uri\n    end\n\n    def remote?\n      @uri_obj&.scheme == 'remote' || @protocol == 'remote'\n    end\n\n    def port\n      @uri_obj&.port || @port\n    end\n\n    # transport is separate from protocol for remote targets.\n    def transport\n      remote? ? 'remote' : protocol\n    end\n\n    def protocol\n      @uri_obj&.scheme || @protocol\n    end\n\n    def user\n      Addressable::URI.unencode_component(@uri_obj&.user) || @user\n    end\n\n    def password\n      Addressable::URI.unencode_component(@uri_obj&.password) || @password\n    end\n  end\nend\n","lang_cluster":"Ruby","length":167,"code_uid":"1d160e7ea4934b4281be3fb30a4cab02"}
{"diff_hunk":"@@ -34,10 +34,6 @@ class Template\n     class NoFunderTemplateError < StandardError\n     end\n \n-    # Exception raised when a Section cannot be transferred to a new Phase.\n-    class UntransferrablSectionError < StandardError\n-    end\n-\n \n     ##\n     # The Template we're upgrading","old_code":"# frozen_string_literal: true\n\nclass Template\n\n  # Service object to upgrade a customization Template with new changes from the original\n  # funder Template. Remember: {target_template} is a customization of funder Template.\n  #\n  # - Duplicate the init template (Duplication called {#customized_template})\n  #\n  # - Create a new customisation of funder template (Customization called\n  #   {#target_template})\n  #\n  # - Take each phase on the {#target_template} and iterate to find if there's a\n  #   corresponding one in {#customized_template}\n  #   - Test for each of a) no corresponding phase in source,\n  #                      b) corresponding found\n  #     a) Create a duplicate phase on the customized_template\n  #     b) Do nothing at this point\n  #   - Copy over each of the modifiable sections from source to the target\n  #     - Re-number the sections if necessary to keep the display order (number) the same\n  #     - Copy each of the questions and annotations exactly\n  #   - For each unmodifiable section, copy over any modifiable questions from target\n  #\n  # - Copy each of the modifiable sections from the {#customized_template} to the\n  #   {#target_template}\n  #\n  class UpgradeCustomizationService\n\n    # Exception raised when the Template is not a customization.\n    class NotACustomizationError < StandardError\n    end\n\n    # Exception raised when no published funder Template can be found.\n    class NoFunderTemplateError < StandardError\n    end\n\n    # Exception raised when a Section cannot be transferred to a new Phase.\n    class UntransferrablSectionError < StandardError\n    end\n\n\n    ##\n    # The Template we're upgrading\n    #\n    # Returns {Template}\n    attr_reader :init_template\n\n    # Initialize a new instance and run the script\n    #\n    # template - The Template we're upgrading\n    #\n    # Returns {Template}\n    def self.call(template)\n      new(template).call\n    end\n\n    private_class_method :new\n\n    # Initialize a new record\n    #\n    # template - The Template we're upgrading. Sets the value for {#init_template}\n    #\n    def initialize(template)\n      @init_template = template\n    end\n\n    # Run the script\n    #\n    # Returns {Template}\n    def call\n      if init_template.customization_of.blank?\n        raise NotACustomizationError,\n          _(\"upgrade_customization! requires a customised template\")\n      end\n      if funder_template.nil?\n        # rubocop:disable Metrics\/LineLength\n        raise NoFunderTemplateError,\n          _(\"upgrade cannot be carried out since there is no published template of its current funder\")\n        # rubocop:enable Metrics\/LineLength\n      end\n\n      # Merges modifiable sections or questions from source into target_template object\n      target_template.phases.map do |target_phase|\n        # Search for the phase in the source template whose versionable_id matches the\n        # customization_phase\n        #\n        # a) If the Org's template ({#customized_template}) has the Phase...\n        if source_phase = find_matching_record_in_collection(\n                            record: target_phase,\n                            collection: customized_template.phases)\n\n        # b) If the Org's template ({#customized_template}) doesn't have this Phase.\n        #    This is not a problem, since {#customization_template} should have this Phase\n        #    copied over from {#template_phase}.\n        else\n          next\n        end\n\n        copy_modifiable_sections_for_phase(source_phase, target_phase)\n\n        sort_sections_within_phase(target_phase)\n      end\n\n      copy_custom_annotations_for_questions\n\n      return target_template\n    end\n\n    private\n\n    # The funder Template for this {#template}\n    #\n    # Returns Template\n    def funder_template\n      @funder_template ||= Template.published(init_template.customization_of).first\n    end\n\n    # A copy of the Template we're currently upgrading. Preserves modifiable flags from\n    # the self template copied\n    #\n    #\n    # Returns {Template}\n    def customized_template\n      @customized_template ||= init_template.deep_copy(attributes: { version: init_template.version + 1, published: false })\n    end\n\n    # Creates a new customisation for the published template whose family_id {#template}\n    # is a customization of\n    #\n    # Returns {Template}\n    def target_template\n      @target_template ||= funder_template.deep_copy(\n        attributes: {\n          version: customized_template.version,\n          published: customized_template.published,\n          family_id: customized_template.family_id,\n          customization_of: customized_template.customization_of,\n          org: customized_template.org,\n          visibility: Template.visibilities[:organisationally_visible],\n          is_default: false\n        }, modifiable: false, save: true\n      )\n    end\n\n    # Find an item within collection that has the same versionable_id as record\n    #\n    # record      - The record we're searching for a match of\n    # collection  - The collection of records we're searching in\n    #\n    # Returns Positionable\n    #\n    # Returns nil\n    def find_matching_record_in_collection(record:, collection:)\n      collection.detect { |item| item.versionable_id == record.versionable_id }\n    end\n\n    # Attach modifiable sections into the customization_phase\n    #\n    # source_phase - A Phase to copy sections for.\n    # target_phase - A Phase to copy Sections to.\n    #\n    # Returns Array of Sections\n    def copy_modifiable_sections_for_phase(source_phase, target_phase)\n      target_section_ids = target_phase.sections.pluck(:versionable_id)\n      source_phase.sections.select(&:modifiable?).each do |section|\n        if section.number.in?(target_phase.sections.pluck(:number))\n          section.number = target_phase.sections.maximum(:number) + 1\n        end\n        target_phase.sections << section or raise(UntransferrablSectionError)\n      end\n    end\n\n    def copy_custom_annotations_for_questions\n      init_template.annotations.where(org: template_org).each do |custom_annotation|\n        target_question = target_template.questions.find_by(\n          versionable_id: custom_annotation.question.versionable_id\n        )\n        target_question.annotations << custom_annotation\n      end\n    end\n\n    def sort_sections_within_phase(phase)\n      phase.sections = SectionSorter.new(*phase.sections).sort!\n    end\n\n    def template_org\n      init_template.org\n    end\n\n  end\n\nend\n","lang_cluster":"Ruby","length":192,"code_uid":"25c157fb670a448987fd5179e08e6082"}
{"diff_hunk":"@@ -58,7 +58,21 @@ module Mongoid\n         #\n         # @return [ String, nil ] A String representing the object or nil.\n         def mongoize(object)\n-          object && object.numeric? ? object.to_s : nil\n+          unless object.nil?\n+            if object.is_a?(BSON::Decimal128)\n+              object\n+            elsif Mongoid.map_big_decimal_to_decimal128\n+              if object.is_a?(BigDecimal)\n+                BSON::Decimal128.new(object)\n+              elsif object.numeric?\n+                BSON::Decimal128.new(object.to_s)\n+              else\n+                object.mongoize\n+              end\n+            elsif object.numeric?\n+              object.to_s\n+            end\n+          end\n         end\n       end\n     end","old_code":"# frozen_string_literal: true\n\nmodule Mongoid\n  module Extensions\n    module BigDecimal\n\n      # Convert the big decimal to an $inc-able value.\n      #\n      # @example Convert the big decimal.\n      #   bd.__to_inc__\n      #\n      # @return [ Float ] The big decimal as a float.\n      def __to_inc__\n        to_f\n      end\n\n      # Turn the object from the ruby type we deal with to a Mongo friendly\n      # type.\n      #\n      # @example Mongoize the object.\n      #   object.mongoize\n      #\n      # @return [ Object ] The object.\n      def mongoize\n        to_s\n      end\n\n      # Is the BigDecimal a number?\n      #\n      # @example Is the object a number?.\n      #   object.numeric?\n      #\n      # @return [ true ] Always true.\n      def numeric?\n        true\n      end\n\n      module ClassMethods\n\n        # Convert the object from its mongo friendly ruby type to this type.\n        #\n        # @example Demongoize the object.\n        #   Object.demongoize(object)\n        #\n        # @param [ Object ] object The object to demongoize.\n        #\n        # @return [ BigDecimal, nil ] A BigDecimal derived from the object or nil.\n        def demongoize(object)\n          object && object.numeric? ? BigDecimal(object.to_s) : nil\n        end\n\n        # Mongoize an object of any type to how it's stored in the db as a String.\n        #\n        # @example Mongoize the object.\n        #   BigDecimal.mongoize(123)\n        #\n        # @param [ Object ] object The object to Mongoize\n        #\n        # @return [ String, nil ] A String representing the object or nil.\n        def mongoize(object)\n          object && object.numeric? ? object.to_s : nil\n        end\n      end\n    end\n  end\nend\n\n::BigDecimal.__send__(:include, Mongoid::Extensions::BigDecimal)\n::BigDecimal.extend(Mongoid::Extensions::BigDecimal::ClassMethods)\n","lang_cluster":"Ruby","length":69,"code_uid":"cf3508c48e2d41ff827759887386f13d"}
{"diff_hunk":"@@ -111,13 +111,14 @@ describe Travis::Build::Script::Csharp, :sexp do\n   describe 'install' do\n     it 'restores nuget from solution' do\n       data[:config][:solution] = 'foo.sln'\n+      data[:config][:os] = 'linux'\n       should include_sexp [:cmd, 'nuget restore foo.sln', assert: true, echo: true, timing: true, retry: true]\n     end\n   end\n \n   describe 'script' do\n     it 'throws an error when no script or solution is defined' do\n-      should include_sexp [:cmd, 'false']\n+      should include_sexp [:echo, 'No solution or script defined, exiting']\n     end\n \n     it 'builds specified solution' do","old_code":"require 'spec_helper'\n\ndescribe Travis::Build::Script::Csharp, :sexp do\n  let(:data)    { payload_for(:push, :csharp) }\n  let(:script) { described_class.new(data) }\n  subject { script.sexp }\n\n  it_behaves_like 'compiled script' do\n    let(:code) { ['TRAVIS_LANGUAGE=csharp'] }\n  end\n\n  it_behaves_like 'a build script sexp'\n\n  describe 'configure' do\n    it 'sets up package repository' do\n      should include_sexp [:cmd, 'sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF', assert: true]\n      should include_sexp [:cmd, \"sudo sh -c \\\"echo 'deb http:\/\/download.mono-project.com\/repo\/debian wheezy main' >> \/etc\/apt\/sources.list.d\/mono-xamarin.list\\\"\", assert: true]\n      should include_sexp [:cmd, \"sudo sh -c \\\"echo 'deb http:\/\/download.mono-project.com\/repo\/debian wheezy-libtiff-compat main' >> \/etc\/apt\/sources.list.d\/mono-xamarin.list\\\"\", assert: true]\n      should include_sexp [:cmd, 'sudo apt-get update -qq', timing: true, assert: true]\n    end\n\n    it 'installs mono' do\n      should include_sexp [:cmd, 'sudo apt-get install -qq mono-complete mono-vbnc fsharp nuget referenceassemblies-pcl', timing: true, assert: true]\n      should include_sexp [:cmd, 'mozroots --import --sync --quiet', timing: true]\n    end\n  end\n\n  describe 'version switching' do\n    it 'throws a error with a invalid version' do\n      data[:config][:mono] = 'foo'\n      should include_sexp [:echo, '\"foo\" is not a valid version of mono.', {:ansi=>:red}]\n    end\n\n    it 'throws a error with a invalid version' do\n      data[:config][:mono] = '12.55.523'\n      should include_sexp [:echo, '\"12.55.523\" is not a valid version of mono.', {:ansi=>:red}]\n    end\n\n    it 'throws a error for invalid version of mono 2' do\n      data[:config][:mono] = '2.1.1'\n      should include_sexp [:echo, '\"2.1.1\" is not a valid version of mono.', {:ansi=>:red}]\n    end\n\n    it 'throws a error for mono 1' do\n      data[:config][:mono] = '1.1.8'\n      should include_sexp [:echo, '\"1.1.8\" is not a valid version of mono.', {:ansi=>:red}]\n    end\n\n    it 'selects mono 2' do\n      data[:config][:mono] = '2.10.8'\n      should include_sexp [:cmd,'sudo apt-get install -qq mono-complete mono-vbnc', timing: true, assert: true]\n    end\n\n    it 'selects mono 3.2.8' do\n      data[:config][:mono] = '3.2.8'\n      should include_sexp [:cmd,'sudo apt-get install -qq mono-complete mono-vbnc fsharp', timing: true, assert: true]\n    end\n\n    it 'does not install PCL on mono 3.8.0' do\n      data[:config][:mono] = '3.8.0'\n      should include_sexp [:cmd, 'sudo apt-get install -qq mono-complete mono-vbnc fsharp nuget ', timing: true, assert: true]\n    end\n\n    it 'selects latest version by default' do\n      should include_sexp [:cmd, \"sudo sh -c \\\"echo 'deb http:\/\/download.mono-project.com\/repo\/debian wheezy main' >> \/etc\/apt\/sources.list.d\/mono-xamarin.list\\\"\", assert: true]\n    end\n\n    it 'selects correct version' do\n      data[:config][:mono] = '3.12.0'\n      should include_sexp [:cmd, \"sudo sh -c \\\"echo 'deb http:\/\/download.mono-project.com\/repo\/debian wheezy\/snapshots\/3.12.0 main' >> \/etc\/apt\/sources.list.d\/mono-xamarin.list\\\"\", assert: true]\n    end\n\n    it 'selects alpha version when specified' do\n      data[:config][:mono] = 'alpha'\n      should include_sexp [:cmd, \"sudo sh -c \\\"echo 'deb http:\/\/download.mono-project.com\/repo\/debian alpha main' >> \/etc\/apt\/sources.list.d\/mono-xamarin.list\\\"\", assert: true]\n    end\n\n    it 'selects beta version when specified' do\n      data[:config][:mono] = 'beta'\n      should include_sexp [:cmd, \"sudo sh -c \\\"echo 'deb http:\/\/download.mono-project.com\/repo\/debian beta main' >> \/etc\/apt\/sources.list.d\/mono-xamarin.list\\\"\", assert: true]\n    end\n\n    it 'selects nightly (which really is weekly, but kept to avoid breaking existing scripts) version when specified' do\n      data[:config][:mono] = 'nightly'\n      should include_sexp [:cmd, \"sudo sh -c \\\"echo 'deb http:\/\/download.mono-project.com\/repo\/debian nightly main' >> \/etc\/apt\/sources.list.d\/mono-xamarin.list\\\"\", assert: true]\n    end\n\n    it 'selects weekly version when specified' do\n      data[:config][:mono] = 'weekly'\n      should include_sexp [:cmd, \"sudo sh -c \\\"echo 'deb http:\/\/download.mono-project.com\/repo\/debian nightly main' >> \/etc\/apt\/sources.list.d\/mono-xamarin.list\\\"\", assert: true]\n    end\n  end\n\n  describe 'export' do\n    it 'sets TRAVIS_SOLUTION' do\n      data[:config][:solution] = 'foo.sln'\n      should include_sexp [:export, ['TRAVIS_SOLUTION', 'foo.sln'], echo: true]\n    end\n  end\n\n  describe 'announce' do\n    it 'announces mono version' do\n      should include_sexp [:cmd, 'mono --version', echo: true, timing: true]\n    end\n\n    it 'announces xbuild version' do\n      should include_sexp [:cmd, 'xbuild \/version', echo: true, timing: true]\n    end\n  end\n\n  describe 'install' do\n    it 'restores nuget from solution' do\n      data[:config][:solution] = 'foo.sln'\n      should include_sexp [:cmd, 'nuget restore foo.sln', assert: true, echo: true, timing: true, retry: true]\n    end\n  end\n\n  describe 'script' do\n    it 'throws an error when no script or solution is defined' do\n      should include_sexp [:cmd, 'false']\n    end\n\n    it 'builds specified solution' do\n      data[:config][:solution] = 'foo.sln'\n      should include_sexp [:cmd, 'xbuild \/p:Configuration=Release foo.sln', echo: true, timing: true]\n    end\n  end\nend\n","lang_cluster":"Ruby","length":128,"code_uid":"619941461eda4a04890b32e4a91a607c"}
{"diff_hunk":"@@ -1,21 +1,41 @@\n require 'spec_helper'\n \n describe 'Videos' do\n+  context 'get show' do\n+    it 'does not allow watching a video without paying first' do\n+      product = create(:video_product)\n+      video = create_available_video(product, 0, 'Video One')\n+      purchase = create(:unpaid_purchase, purchaseable: product)\n+      purchase.lookup = 'unpaid'\n+      purchase.save!\n+\n+      visit purchase_path(purchase)\n+\n+      expect(current_path).to eq product_path(product)\n+\n+      visit purchase_video_path(purchase, video)\n+\n+      expect(current_path).to eq product_path(product)\n+    end\n+  end\n+\n   context 'GET \/' do\n     it 'lists the videos for a workshop' do\n       workshop = create(:workshop)\n       section = create(:section, starts_on: Date.yesterday, ends_on: 1.month.from_now, workshop: workshop)\n       video_one = create_available_video(workshop, 0, 'Video One')\n       video_two = create_available_video(workshop, 2, 'Video Two')\n-      purchase = create(:paid_purchase, purchaseable: section)\n+      purchase = create_subscriber_purchase_from_purchaseable(section)\n \n       visit purchase_path(purchase)\n+\n       expect(page).to have_content(\"2 lessons in this workshop\")\n       expect(page).to have_content(video_one.title)\n       expect(page).to have_content(video_two.title)\n       expect(page.body.index(video_one.title) < page.body.index(video_two.title)).to be\n \n       visit purchase_video_path(purchase, video_two)\n+\n       expect(page).to have_css('iframe')\n     end\n ","old_code":"require 'spec_helper'\n\ndescribe 'Videos' do\n  context 'GET \/' do\n    it 'lists the videos for a workshop' do\n      workshop = create(:workshop)\n      section = create(:section, starts_on: Date.yesterday, ends_on: 1.month.from_now, workshop: workshop)\n      video_one = create_available_video(workshop, 0, 'Video One')\n      video_two = create_available_video(workshop, 2, 'Video Two')\n      purchase = create(:paid_purchase, purchaseable: section)\n\n      visit purchase_path(purchase)\n      expect(page).to have_content(\"2 lessons in this workshop\")\n      expect(page).to have_content(video_one.title)\n      expect(page).to have_content(video_two.title)\n      expect(page.body.index(video_one.title) < page.body.index(video_two.title)).to be\n\n      visit purchase_video_path(purchase, video_two)\n      expect(page).to have_css('iframe')\n    end\n\n    it 'lists the videos for a product' do\n      purchase = create(:video_purchase)\n      video_one = create_available_video(purchase.purchaseable, 0, 'Video One')\n      video_two = create_available_video(purchase.purchaseable, 0, 'Video Two')\n\n      visit purchase_path(purchase)\n\n      expect(page).to have_content(\"2 videos in the series\")\n      expect(page).to have_content(video_one.title)\n      expect(page).to have_content(video_two.title)\n    end\n\n    it \"doesn't say it's a series with one video\" do\n      purchase = create(:video_purchase)\n      video_one = create_available_video(purchase.purchaseable, 0, 'Video One')\n\n      visit purchase_path(purchase)\n      expect(page).not_to have_content(\"in the series\")\n      expect(page).not_to have_content(\"in this workshop\")\n    end\n\n    it \"doesn't say it includes support with no subscription\" do\n      purchase = create(:video_purchase)\n\n      visit purchase_path(purchase)\n      expect(page).not_to have_content(\"includes support\")\n    end\n\n    it 'includes support with a subscription' do\n      sign_in_as_user_with_subscription\n\n      purchase = create(:video_purchase)\n\n      visit purchase_path(purchase)\n      expect(page).to have_content(\"includes support\")\n    end\n\n    def create_available_video(watchable, active_on_day, title)\n      create(:video, watchable: watchable, active_on_day: active_on_day, title: title)\n    end\n  end\nend\n","lang_cluster":"Ruby","length":63,"code_uid":"9e18e8a3f81f4e75b2e1af277550d9a4"}
{"diff_hunk":"@@ -19,6 +19,7 @@ import (\n \n \tlog \"github.com\/sirupsen\/logrus\"\n \n+\t\"github.com\/projectcalico\/felix\/ipsets\"\n \t\"github.com\/projectcalico\/libcalico-go\/lib\/set\"\n )\n ","old_code":"\/\/ Copyright (c) 2017-2021 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage ipsets\n\nimport (\n\t\"strings\"\n\n\tlog \"github.com\/sirupsen\/logrus\"\n\n\t\"github.com\/projectcalico\/libcalico-go\/lib\/set\"\n)\n\ntype CallBackFunc func(ipSetId string)\n\n\/\/ IPSets manages a whole plane of IP sets, i.e. all the IPv4 sets, or all the IPv6 IP sets.\ntype IPSets struct {\n\tIPVersionConfig  *IPVersionConfig\n\tipSetIDToIPSet   map[string]*ipSet\n\tlogCxt           *log.Entry\n\tcallbackOnUpdate CallBackFunc\n}\n\nfunc NewIPSets(ipVersionConfig *IPVersionConfig) *IPSets {\n\treturn &IPSets{\n\t\tIPVersionConfig: ipVersionConfig,\n\t\tipSetIDToIPSet:  map[string]*ipSet{},\n\t\tlogCxt: log.WithFields(log.Fields{\n\t\t\t\"family\": ipVersionConfig.Family,\n\t\t}),\n\t}\n}\n\nfunc (s *IPSets) SetCallback(callback CallBackFunc) {\n\ts.callbackOnUpdate = callback\n}\n\n\/\/ AddOrReplaceIPSet is responsible for the creation (or replacement) of an IP set in the store\nfunc (s *IPSets) AddOrReplaceIPSet(setMetadata IPSetMetadata, members []string) {\n\tlog.WithFields(log.Fields{\n\t\t\"metadata\":   setMetadata,\n\t\t\"numMembers\": len(members),\n\t}).Info(\"Adding IP set to cache\")\n\ts.logCxt.WithFields(log.Fields{\n\t\t\"setID\":   setMetadata.SetID,\n\t\t\"setType\": setMetadata.Type,\n\t}).Info(\"Creating IP set\")\n\tfilteredMembers := s.filterMembers(members)\n\n\t\/\/ Create the IP set struct and stores it by id\n\tsetID := setMetadata.SetID\n\tipSet := &ipSet{\n\t\tIPSetMetadata: setMetadata,\n\t\tMembers:       filteredMembers,\n\t}\n\ts.ipSetIDToIPSet[setID] = ipSet\n\ts.callbackOnUpdate(setID)\n}\n\n\/\/ RemoveIPSet is responsible for the removal of an IP set from the store\nfunc (s *IPSets) RemoveIPSet(setID string) {\n\ts.logCxt.WithField(\"setID\", setID).Info(\"Removing IP set\")\n\tdelete(s.ipSetIDToIPSet, setID)\n\ts.callbackOnUpdate(setID)\n}\n\n\/\/ AddMembers adds a range of new members to an existing IP set in the store\nfunc (s *IPSets) AddMembers(setID string, newMembers []string) {\n\tif len(newMembers) == 0 {\n\t\treturn\n\t}\n\n\tipSet := s.ipSetIDToIPSet[setID]\n\tfilteredMembers := s.filterMembers(newMembers)\n\tif filteredMembers.Len() == 0 {\n\t\treturn\n\t}\n\ts.logCxt.WithFields(log.Fields{\n\t\t\"setID\":           setID,\n\t\t\"filteredMembers\": filteredMembers,\n\t}).Debug(\"Adding new members to IP set\")\n\tfilteredMembers.Iter(func(m interface{}) error {\n\t\tipSet.Members.Add(m)\n\t\treturn nil\n\t})\n\ts.callbackOnUpdate(setID)\n}\n\n\/\/ RemoveMembers removes a range of members from an existing IP set in the store\nfunc (s *IPSets) RemoveMembers(setID string, removedMembers []string) {\n\tif len(removedMembers) == 0 {\n\t\treturn\n\t}\n\n\tipSet := s.ipSetIDToIPSet[setID]\n\tfilteredMembers := s.filterMembers(removedMembers)\n\tif filteredMembers.Len() == 0 {\n\t\treturn\n\t}\n\ts.logCxt.WithFields(log.Fields{\n\t\t\"setID\":           setID,\n\t\t\"filteredMembers\": filteredMembers,\n\t}).Debug(\"Removing members from IP set\")\n\n\tfilteredMembers.Iter(func(m interface{}) error {\n\t\tipSet.Members.Discard(m)\n\t\treturn nil\n\t})\n\ts.callbackOnUpdate(setID)\n}\n\n\/\/ GetIPSetMembers returns all of the members for a given IP set\nfunc (s *IPSets) GetIPSetMembers(setID string) []string {\n\tvar retVal []string\n\n\tipSet := s.ipSetIDToIPSet[setID]\n\tif ipSet == nil {\n\t\treturn nil\n\t}\n\n\tipSet.Members.Iter(func(item interface{}) error {\n\t\tmember := item.(string)\n\t\tretVal = append(retVal, member)\n\t\treturn nil\n\t})\n\n\t\/\/ Note: It is very important that nil is returned if there is no ip in an ipset\n\t\/\/ so that policy rules related to this ipset won't be populated.\n\treturn retVal\n}\n\n\/\/ filterMembers filters out any members which are not of the correct\n\/\/ ip family for the IPSet\nfunc (s *IPSets) filterMembers(members []string) set.Set {\n\tfiltered := set.New()\n\twantIPV6 := s.IPVersionConfig.Family == IPFamilyV6\n\tfor _, member := range members {\n\t\tisIPV6 := strings.Contains(member, \":\")\n\t\tif wantIPV6 != isIPV6 {\n\t\t\tcontinue\n\t\t}\n\t\tfiltered.Add(member)\n\t}\n\treturn filtered\n}\n\nfunc (s *IPSets) GetIPFamily() IPFamily {\n\treturn s.IPVersionConfig.Family\n}\n\n\/\/ The following functions are no-ops on Windows.\nfunc (s *IPSets) QueueResync() {\n}\n\nfunc (m *IPSets) GetTypeOf(setID string) (IPSetType, error) {\n\tpanic(\"Not implemented\")\n}\n\nfunc (m *IPSets) GetMembers(setID string) (set.Set, error) {\n\t\/\/ GetMembers is only called from XDPState, and XDPState does not coexist with\n\t\/\/ config.BPFEnabled.\n\tpanic(\"Not implemented\")\n}\n\nfunc (m *IPSets) ApplyUpdates() {\n}\n\nfunc (m *IPSets) ApplyDeletions() {\n}\n\nfunc (s *IPSets) SetFilter(ipSetNames set.Set) {\n\t\/\/ Not needed for Windows.\n}\n","lang_cluster":"C","length":184,"code_uid":"652408a8f5b44d67bb4af688a50f73f6"}
{"diff_hunk":"@@ -107,6 +107,17 @@ class AnalyzeParseTestCase(unittest.TestCase):\n                 # replace timestamps\n                 line = re.sub(r'\\[\\w+ \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}\\]',\n                               '[]', line)\n+\n+                # Replace full path only to file name on the following\n+                # formatted lines:\n+                # [severity] \/a\/b\/x.cpp:line:col: message [checker]\n+                # The replacement on this line will be the following:\n+                # [severity] x.cpp:line:col: message [checker]\n+                sep = re.escape(os.sep)\n+                line = re.sub(r'^(\\[\\w+\\]\\s)(?P<path>.+{0})'\n+                              r'(.+\\:\\d+\\:\\d+\\:\\s.*\\s\\[.*\\])$'.format(sep),\n+                              r'\\1\\3', line)\n+\n                 if not any([line.startswith(prefix) for prefix\n                             in skip_prefixes]):\n                     post_processed_output.append(line)","old_code":"# -----------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -----------------------------------------------------------------------------\n\n\"\"\"This module tests the CodeChecker 'analyze' and 'parse' feature.\"\"\"\n\nimport glob\nimport os\nimport re\nimport subprocess\nimport unittest\n\nfrom subprocess import CalledProcessError\n\nfrom libtest import env\n\n\nclass AnalyzeParseTestCase(unittest.TestCase):\n    \"\"\"This class tests the CodeChecker 'analyze' and 'parse' feature.\"\"\"\n\n    @classmethod\n    def setup_class(cls):\n        \"\"\"Setup the class.\"\"\"\n\n        # TEST_WORKSPACE is automatically set by test package __init__.py\n        test_workspace = os.environ['TEST_WORKSPACE']\n        cls.test_workspaces = {'NORMAL': os.path.join(test_workspace,\n                                                      'NORMAL'),\n                               'CHECK': os.path.join(test_workspace,\n                                                     'CHECK')}\n\n        # Get an environment with CodeChecker command in it.\n        cls.env = env.codechecker_env()\n\n        cls.test_dir = os.path.join(\n            os.path.dirname(__file__), 'test_files')\n\n        # Change working dir to testfile dir so CodeChecker can be run easily.\n        cls.__old_pwd = os.getcwd()\n        os.chdir(cls.test_dir)\n\n    @classmethod\n    def teardown_class(cls):\n        \"\"\"Restore environment after tests have ran.\"\"\"\n        os.chdir(cls.__old_pwd)\n\n    def __check_one_file(self, path, mode):\n        \"\"\"\n        Test 'analyze' and 'parse' output on a \".output\" file.\n\n        The '.output' file is formatted as follows:\n          * >= 1 lines of CodeChecker commands to execute, prefixed by a 'mode'\n            usually containing commands to build, log, analyze and parse the\n            corresponding test file.\n          * A single line containing some - (dashes)\n          * The lines of the output which is expected to be produced by the\n            commands in the lines above the -------------.\n\n        mode specifies which command prefixes to execute.\n        \"\"\"\n        with open(path, 'r') as ofile:\n            lines = ofile.readlines()\n\n        only_dash = re.compile(r'^[-]+$')\n        dash_index = 0\n        for idx, line in enumerate(lines):\n            if re.match(only_dash, line):\n                # The current line is the first line only containing dashes,\n                # thus mark it as separator.\n                dash_index = idx\n                break\n\n        commands = [line.strip() for line in lines[:dash_index]]\n        correct_output = ''.join(lines[dash_index + 1:])\n\n        run_name = os.path.basename(path).replace(\".output\", \"\")\n        workspace = self.test_workspaces[mode]\n        os.makedirs(os.path.join(workspace, run_name, \"reports\"))\n\n        try:\n            output = []\n            for command in commands:\n                split = command.split('#')\n                cmode, command = split[0], ''.join(split[1:])\n\n                if cmode != mode:\n                    # Ignore the command if its MODE does not match.\n                    continue\n\n                command = command.replace(\"$LOGFILE$\",\n                                          os.path.join(workspace,\n                                                       run_name, \"build.json\"))\n                command = command.replace(\"$OUTPUT$\",\n                                          os.path.join(workspace,\n                                                       run_name, \"reports\"))\n                result = subprocess.check_output(\n                    ['bash', '-c', command], env=self.env, cwd=self.test_dir)\n                output += result.splitlines(True)\n\n            post_processed_output = []\n            skip_prefixes = [\"[] - Analysis length:\",\n                             \"[] - Previous analysis results\",\n                             \"[] - Skipping input file\"]\n            for line in output:\n                # replace timestamps\n                line = re.sub(r'\\[\\w+ \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}\\]',\n                              '[]', line)\n                if not any([line.startswith(prefix) for prefix\n                            in skip_prefixes]):\n                    post_processed_output.append(line)\n\n            print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Actual output below:\")\n            print(''.join(post_processed_output))\n            print(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Expected output below:\")\n            print(correct_output)\n\n            print(\"Test output file: \" + path)\n            self.assertEqual(''.join(post_processed_output), correct_output)\n            return 0\n        except CalledProcessError as cerr:\n            print(\"Failed to run: \" + ' '.join(cerr.cmd))\n            print(cerr.output)\n            return cerr.returncode\n\n    def test_analyze_and_parse_files(self):\n        \"\"\"\n        Iterate over the test directory and run all tests in it.\n        \"\"\"\n        for ofile in glob.glob(os.path.join(self.test_dir, '*.output')):\n            self.assertEqual(self.__check_one_file(ofile, 'NORMAL'), 0)\n\n    def test_check_files(self):\n        \"\"\"\n        Iterate over the test directory and run all check (wrapper) tests.\n        \"\"\"\n        for ofile in glob.glob(os.path.join(self.test_dir, '*.output')):\n            self.assertEqual(self.__check_one_file(ofile, 'CHECK'), 0)\n","lang_cluster":"C","length":139,"code_uid":"648fb8b12fd14f8780eb78c9a84ef3ea"}
{"diff_hunk":"@@ -24,7 +24,7 @@ from .credential_manager import SESSION_COOKIE_NAME\n LOG = get_logger('system')\n \n \n-class ThriftAuthHelper():\n+class ThriftAuthHelper:\n     def __init__(self, protocol, host, port, uri,\n                  session_token=None):\n         self.__host = host","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nHelper for tha authentication api.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nfrom thrift.transport import THttpClient\nfrom thrift.protocol import TJSONProtocol\n\nfrom Authentication_v6 import codeCheckerAuthentication\n\nfrom libcodechecker import util\nfrom libcodechecker.libclient.thrift_call import ThriftClientCall\nfrom libcodechecker.logger import get_logger\n\nfrom .credential_manager import SESSION_COOKIE_NAME\n\nLOG = get_logger('system')\n\n\nclass ThriftAuthHelper():\n    def __init__(self, protocol, host, port, uri,\n                 session_token=None):\n        self.__host = host\n        self.__port = port\n        url = util.create_product_url(protocol, host, port, uri)\n        self.transport = THttpClient.THttpClient(url)\n        self.protocol = TJSONProtocol.TJSONProtocol(self.transport)\n        self.client = codeCheckerAuthentication.Client(self.protocol)\n\n        if session_token:\n            headers = {'Cookie': SESSION_COOKIE_NAME + '=' + session_token}\n            self.transport.setCustomHeaders(headers)\n\n    @ThriftClientCall\n    def checkAPIVersion(self):\n        pass\n\n    # ============= Authentication and session handling =============\n    @ThriftClientCall\n    def getAuthParameters(self):\n        pass\n\n    @ThriftClientCall\n    def getAcceptedAuthMethods(self):\n        pass\n\n    @ThriftClientCall\n    def performLogin(self, auth_method, auth_string):\n        pass\n\n    @ThriftClientCall\n    def destroySession(self):\n        pass\n\n    # ============= Authorization, permission management =============\n    @ThriftClientCall\n    def getPermissions(self, scope):\n        pass\n\n    @ThriftClientCall\n    def getPermissionsForUser(self, scope, extra_params, filter):\n        pass\n\n    @ThriftClientCall\n    def getAuthorisedNames(self, permission, extra_params):\n        pass\n\n    @ThriftClientCall\n    def addPermission(self, permission, auth_name, is_group, extra_params):\n        pass\n\n    @ThriftClientCall\n    def removePermission(self, permission, auth_name, is_group, extra_params):\n        pass\n\n    @ThriftClientCall\n    def hasPermission(self, permission, extra_params):\n        pass\n\n    # ============= Token management =============\n\n    @ThriftClientCall\n    def newToken(self, description):\n        pass\n\n    @ThriftClientCall\n    def removeToken(self, token):\n        pass\n\n    @ThriftClientCall\n    def getTokens(self):\n        pass\n","lang_cluster":"C","length":99,"code_uid":"cb0b82e0a58b43d586dbbe0e963cb8ca"}
{"diff_hunk":"@@ -173,7 +173,7 @@ void h2o_get_timestamp(h2o_context_t *ctx, h2o_mem_pool_t *pool, h2o_timestamp_t\n             if (ctx->_timestamp_cache.value != NULL)\n                 h2o_mem_release_shared(ctx->_timestamp_cache.value);\n             ctx->_timestamp_cache.value = h2o_mem_alloc_shared(NULL, sizeof(h2o_timestamp_string_t), NULL);\n-            gmtime_r(&ctx->_timestamp_cache.tv_at.tv_sec, &gmt);\n+            gmtime_r((time_t *)&ctx->_timestamp_cache.tv_at.tv_sec, &gmt);\n             h2o_time2str_rfc1123(ctx->_timestamp_cache.value->rfc1123, &gmt);\n             h2o_time2str_log(ctx->_timestamp_cache.value->log, ctx->_timestamp_cache.tv_at.tv_sec);\n         }","old_code":"\/*\n * Copyright (c) 2014 DeNA Co., Ltd.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n *\/\n#include <stddef.h>\n#include <stdlib.h>\n#include <sys\/time.h>\n#include \"h2o.h\"\n\nvoid h2o_context_init_pathconf_context(h2o_context_t *ctx, h2o_pathconf_t *pathconf)\n{\n    \/* add pathconf to the inited list (or return if already inited) *\/\n    size_t i;\n    for (i = 0; i != ctx->_pathconfs_inited.size; ++i)\n        if (ctx->_pathconfs_inited.entries[i] == pathconf)\n            return;\n    h2o_vector_reserve(NULL, (void *)&ctx->_pathconfs_inited, sizeof(ctx->_pathconfs_inited.entries[0]),\n                       ctx->_pathconfs_inited.size + 1);\n    ctx->_pathconfs_inited.entries[ctx->_pathconfs_inited.size++] = pathconf;\n\n#define DOIT(type, list)                                                                                                           \\\n    do {                                                                                                                           \\\n        size_t i;                                                                                                                  \\\n        for (i = 0; i != pathconf->list.size; ++i) {                                                                               \\\n            type *o = pathconf->list.entries[i];                                                                                   \\\n            if (o->on_context_init != NULL)                                                                                        \\\n                o->on_context_init(o, ctx);                                                                                        \\\n        }                                                                                                                          \\\n    } while (0)\n\n    DOIT(h2o_handler_t, handlers);\n    DOIT(h2o_filter_t, filters);\n    DOIT(h2o_logger_t, loggers);\n\n#undef DOIT\n}\n\nvoid h2o_context_dispose_pathconf_context(h2o_context_t *ctx, h2o_pathconf_t *pathconf)\n{\n    \/* nullify pathconf in the inited list (or return if already disposed) *\/\n    size_t i;\n    for (i = 0; i != ctx->_pathconfs_inited.size; ++i)\n        if (ctx->_pathconfs_inited.entries[i] == pathconf)\n            break;\n    if (i == ctx->_pathconfs_inited.size)\n        return;\n    ctx->_pathconfs_inited.entries[i] = NULL;\n\n#define DOIT(type, list)                                                                                                           \\\n    do {                                                                                                                           \\\n        size_t i;                                                                                                                  \\\n        for (i = 0; i != pathconf->list.size; ++i) {                                                                               \\\n            type *o = pathconf->list.entries[i];                                                                                   \\\n            if (o->on_context_dispose != NULL)                                                                                     \\\n                o->on_context_dispose(o, ctx);                                                                                     \\\n        }                                                                                                                          \\\n    } while (0)\n\n    DOIT(h2o_handler_t, handlers);\n    DOIT(h2o_filter_t, filters);\n    DOIT(h2o_logger_t, loggers);\n\n#undef DOIT\n}\n\nvoid h2o_context_init(h2o_context_t *ctx, h2o_loop_t *loop, h2o_globalconf_t *config)\n{\n    size_t i, j;\n\n    assert(config->hosts[0] != NULL);\n\n    memset(ctx, 0, sizeof(*ctx));\n    ctx->loop = loop;\n    ctx->globalconf = config;\n    h2o_timeout_init(ctx->loop, &ctx->zero_timeout, 0);\n    h2o_timeout_init(ctx->loop, &ctx->one_sec_timeout, 1000);\n    ctx->queue = h2o_multithread_create_queue(loop);\n    h2o_multithread_register_receiver(ctx->queue, &ctx->receivers.hostinfo_getaddr, h2o_hostinfo_getaddr_receiver);\n\n    h2o_timeout_init(ctx->loop, &ctx->http1.req_timeout, config->http1.req_timeout);\n    h2o_timeout_init(ctx->loop, &ctx->http2.idle_timeout, config->http2.idle_timeout);\n    h2o_linklist_init_anchor(&ctx->http2._conns);\n    ctx->proxy.client_ctx.loop = loop;\n    h2o_timeout_init(ctx->loop, &ctx->proxy.io_timeout, config->proxy.io_timeout);\n    ctx->proxy.client_ctx.getaddr_receiver = &ctx->receivers.hostinfo_getaddr;\n    ctx->proxy.client_ctx.io_timeout = &ctx->proxy.io_timeout;\n\n    ctx->_module_configs = h2o_mem_alloc(sizeof(*ctx->_module_configs) * config->_num_config_slots);\n    memset(ctx->_module_configs, 0, sizeof(*ctx->_module_configs) * config->_num_config_slots);\n\n    static pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;\n    pthread_mutex_lock(&mutex);\n    for (i = 0; config->hosts[i] != NULL; ++i) {\n        h2o_hostconf_t *hostconf = config->hosts[i];\n        for (j = 0; j != hostconf->paths.size; ++j) {\n            h2o_pathconf_t *pathconf = hostconf->paths.entries + j;\n            h2o_context_init_pathconf_context(ctx, pathconf);\n        }\n        h2o_context_init_pathconf_context(ctx, &hostconf->fallback_path);\n    }\n    pthread_mutex_unlock(&mutex);\n}\n\nvoid h2o_context_dispose(h2o_context_t *ctx)\n{\n    h2o_globalconf_t *config = ctx->globalconf;\n    size_t i, j;\n\n    for (i = 0; config->hosts[i] != NULL; ++i) {\n        h2o_hostconf_t *hostconf = config->hosts[i];\n        for (j = 0; j != hostconf->paths.size; ++j) {\n            h2o_pathconf_t *pathconf = hostconf->paths.entries + j;\n            h2o_context_dispose_pathconf_context(ctx, pathconf);\n        }\n        h2o_context_dispose_pathconf_context(ctx, &hostconf->fallback_path);\n    }\n    free(ctx->_pathconfs_inited.entries);\n    free(ctx->_module_configs);\n    h2o_timeout_dispose(ctx->loop, &ctx->zero_timeout);\n    h2o_timeout_dispose(ctx->loop, &ctx->one_sec_timeout);\n    h2o_timeout_dispose(ctx->loop, &ctx->http1.req_timeout);\n    h2o_timeout_dispose(ctx->loop, &ctx->http2.idle_timeout);\n    h2o_timeout_dispose(ctx->loop, &ctx->proxy.io_timeout);\n    \/* what should we do here? assert(!h2o_linklist_is_empty(&ctx->http2._conns); *\/\n\n    \/* TODO assert that the all the getaddrinfo threads are idle *\/\n    h2o_multithread_unregister_receiver(ctx->queue, &ctx->receivers.hostinfo_getaddr);\n    h2o_multithread_destroy_queue(ctx->queue);\n\n#if H2O_USE_LIBUV\n    \/* make sure the handles released by h2o_timeout_dispose get freed *\/\n    uv_run(ctx->loop, UV_RUN_NOWAIT);\n#endif\n}\n\nvoid h2o_context_request_shutdown(h2o_context_t *ctx)\n{\n    ctx->shutdown_requested = 1;\n    if (ctx->globalconf->http1.callbacks.request_shutdown != NULL)\n        ctx->globalconf->http1.callbacks.request_shutdown(ctx);\n    if (ctx->globalconf->http2.callbacks.request_shutdown != NULL)\n        ctx->globalconf->http2.callbacks.request_shutdown(ctx);\n}\n\nvoid h2o_get_timestamp(h2o_context_t *ctx, h2o_mem_pool_t *pool, h2o_timestamp_t *ts)\n{\n    uint64_t now = h2o_now(ctx->loop);\n    struct tm gmt;\n\n    if (ctx->_timestamp_cache.uv_now_at != now) {\n        time_t prev_sec = ctx->_timestamp_cache.tv_at.tv_sec;\n        ctx->_timestamp_cache.uv_now_at = now;\n        gettimeofday(&ctx->_timestamp_cache.tv_at, NULL);\n        if (ctx->_timestamp_cache.tv_at.tv_sec != prev_sec) {\n            \/* update the string cache *\/\n            if (ctx->_timestamp_cache.value != NULL)\n                h2o_mem_release_shared(ctx->_timestamp_cache.value);\n            ctx->_timestamp_cache.value = h2o_mem_alloc_shared(NULL, sizeof(h2o_timestamp_string_t), NULL);\n            gmtime_r(&ctx->_timestamp_cache.tv_at.tv_sec, &gmt);\n            h2o_time2str_rfc1123(ctx->_timestamp_cache.value->rfc1123, &gmt);\n            h2o_time2str_log(ctx->_timestamp_cache.value->log, ctx->_timestamp_cache.tv_at.tv_sec);\n        }\n    }\n\n    ts->at = ctx->_timestamp_cache.tv_at;\n    h2o_mem_link_shared(pool, ctx->_timestamp_cache.value);\n    ts->str = ctx->_timestamp_cache.value;\n}\n","lang_cluster":"C","length":185,"code_uid":"c01149d1b3a4428591bf8ed18f4858cb"}
{"diff_hunk":"@@ -26,23 +26,30 @@ wlr_idle_inhibitor_v1_from_resource(struct wl_resource *resource) {\n \treturn wl_resource_get_user_data(resource);\n }\n \n-static void idle_inhibitor_destroy(struct wl_resource *resource) {\n-\tstruct wlr_idle_inhibitor_v1 *inhibitor =\n-\t\twlr_idle_inhibitor_v1_from_resource(resource);\n+static void idle_inhibitor_v1_destroy(struct wlr_idle_inhibitor_v1 *inhibitor) {\n+\tif (!inhibitor) {\n+\t\treturn;\n+\t}\n \n \twlr_signal_emit_safe(&inhibitor->events.destroy, inhibitor->surface);\n \n+\twl_resource_set_user_data(inhibitor->resource, NULL);\n \twl_list_remove(&inhibitor->link);\n \twl_list_remove(&inhibitor->surface_destroy.link);\n \tfree(inhibitor);\n }\n \n+static void idle_inhibitor_v1_handle_resource_destroy(struct wl_resource *resource) {\n+\tstruct wlr_idle_inhibitor_v1 *inhibitor =\n+\t\twlr_idle_inhibitor_v1_from_resource(resource);\n+\tidle_inhibitor_v1_destroy(inhibitor);\n+}\n+\n static void idle_inhibitor_handle_surface_destroy(\n \t\tstruct wl_listener *listener, void *data) {\n \tstruct wlr_idle_inhibitor_v1 *inhibitor =\n \t\twl_container_of(listener, inhibitor, surface_destroy);\n-\n-\twl_resource_destroy(inhibitor->resource);\n+\tidle_inhibitor_v1_destroy(inhibitor);\n }\n \n static void idle_inhibitor_v1_handle_destroy(struct wl_client *client,","old_code":"#include <assert.h>\n#include <stdlib.h>\n#include <wlr\/util\/log.h>\n#include <util\/signal.h>\n#include <wlr\/types\/wlr_surface.h>\n#include <wlr\/types\/wlr_idle_inhibit_v1.h>\n#include \"wayland-util.h\"\n#include \"wayland-server.h\"\n#include \"idle-inhibit-unstable-v1-protocol.h\"\n\nstatic const struct zwp_idle_inhibit_manager_v1_interface idle_inhibit_impl;\n\nstatic const struct zwp_idle_inhibitor_v1_interface idle_inhibitor_impl;\n\nstatic struct wlr_idle_inhibit_manager_v1 *\nwlr_idle_inhibit_manager_v1_from_resource(struct wl_resource *resource) {\n\tassert(wl_resource_instance_of(resource, &zwp_idle_inhibit_manager_v1_interface,\n\t\t&idle_inhibit_impl));\n\treturn wl_resource_get_user_data(resource);\n}\n\nstatic struct wlr_idle_inhibitor_v1 *\nwlr_idle_inhibitor_v1_from_resource(struct wl_resource *resource) {\n\tassert(wl_resource_instance_of(resource, &zwp_idle_inhibitor_v1_interface,\n\t\t&idle_inhibitor_impl));\n\treturn wl_resource_get_user_data(resource);\n}\n\nstatic void idle_inhibitor_destroy(struct wl_resource *resource) {\n\tstruct wlr_idle_inhibitor_v1 *inhibitor =\n\t\twlr_idle_inhibitor_v1_from_resource(resource);\n\n\twlr_signal_emit_safe(&inhibitor->events.destroy, inhibitor->surface);\n\n\twl_list_remove(&inhibitor->link);\n\twl_list_remove(&inhibitor->surface_destroy.link);\n\tfree(inhibitor);\n}\n\nstatic void idle_inhibitor_handle_surface_destroy(\n\t\tstruct wl_listener *listener, void *data) {\n\tstruct wlr_idle_inhibitor_v1 *inhibitor =\n\t\twl_container_of(listener, inhibitor, surface_destroy);\n\n\twl_resource_destroy(inhibitor->resource);\n}\n\nstatic void idle_inhibitor_v1_handle_destroy(struct wl_client *client,\n\t\tstruct wl_resource *manager_resource) {\n\twl_resource_destroy(manager_resource);\n}\n\nstatic const struct zwp_idle_inhibitor_v1_interface idle_inhibitor_impl = {\n\t.destroy = idle_inhibitor_v1_handle_destroy,\n};\n\nstatic void manager_create_inhibitor(struct wl_client *client,\n\t\tstruct wl_resource *resource, uint32_t id,\n\t\tstruct wl_resource *surface_resource) {\n\tstruct wlr_surface *surface = wlr_surface_from_resource(surface_resource);\n\tstruct wlr_idle_inhibit_manager_v1 *manager =\n\t\twlr_idle_inhibit_manager_v1_from_resource(resource);\n\n\tstruct wlr_idle_inhibitor_v1 *inhibitor =\n\t\tcalloc(1, sizeof(struct wlr_idle_inhibitor_v1));\n\tif (!inhibitor) {\n\t\twl_client_post_no_memory(client);\n\t\treturn;\n\t}\n\n\tstruct wl_resource *wl_resource = wl_resource_create(client,\n\t\t&zwp_idle_inhibitor_v1_interface, 1, id);\n\tif (!wl_resource) {\n\t\twl_client_post_no_memory(client);\n\t\tfree(inhibitor);\n\t\treturn;\n\t}\n\n\tinhibitor->resource = wl_resource;\n\tinhibitor->surface = surface;\n\twl_signal_init(&inhibitor->events.destroy);\n\n\tinhibitor->surface_destroy.notify = idle_inhibitor_handle_surface_destroy;\n\twl_signal_add(&surface->events.destroy, &inhibitor->surface_destroy);\n\n\n\twl_resource_set_implementation(wl_resource, &idle_inhibitor_impl,\n\t\tinhibitor, idle_inhibitor_destroy);\n\n\twl_list_insert(&manager->inhibitors, &inhibitor->link);\n\twlr_signal_emit_safe(&manager->events.new_inhibitor, inhibitor);\n}\n\n\nstatic void idle_inhibit_manager_v1_destroy(struct wl_resource *resource) {\n\twl_list_remove(wl_resource_get_link(resource));\n}\n\nstatic void manager_destroy(struct wl_client *client,\n\t\tstruct wl_resource *manager_resource) {\n\twl_resource_destroy(manager_resource);\n}\n\nstatic const struct zwp_idle_inhibit_manager_v1_interface idle_inhibit_impl = {\n\t.destroy = manager_destroy,\n\t.create_inhibitor = manager_create_inhibitor,\n};\n\nstatic void handle_display_destroy(struct wl_listener *listener, void *data) {\n\tstruct wlr_idle_inhibit_manager_v1 *idle_inhibit =\n\t\twl_container_of(listener, idle_inhibit, display_destroy);\n\n\twlr_idle_inhibit_v1_destroy(idle_inhibit);\n}\n\nstatic void idle_inhibit_bind(struct wl_client *wl_client, void *data,\n\t\tuint32_t version, uint32_t id) {\n\tstruct wlr_idle_inhibit_manager_v1 *idle_inhibit = data;\n\n\tstruct wl_resource *wl_resource  = wl_resource_create(wl_client,\n\t\t&zwp_idle_inhibit_manager_v1_interface, version, id);\n\n\tif (!wl_resource) {\n\t\twl_client_post_no_memory(wl_client);\n\t\treturn;\n\t}\n\n\twl_list_insert(&idle_inhibit->wl_resources, wl_resource_get_link(wl_resource));\n\n\twl_resource_set_implementation(wl_resource, &idle_inhibit_impl,\n\t\tidle_inhibit, idle_inhibit_manager_v1_destroy);\n\twlr_log(L_DEBUG, \"idle_inhibit bound\");\n}\n\nvoid wlr_idle_inhibit_v1_destroy(struct wlr_idle_inhibit_manager_v1 *idle_inhibit) {\n\tif (!idle_inhibit) {\n\t\treturn;\n\t}\n\n\twl_list_remove(&idle_inhibit->display_destroy.link);\n\n\tstruct wlr_idle_inhibitor_v1 *inhibitor;\n\tstruct wlr_idle_inhibitor_v1 *tmp;\n\twl_list_for_each_safe(inhibitor, tmp, &idle_inhibit->inhibitors, link) {\n\t\twl_resource_destroy(inhibitor->resource);\n\t}\n\n\tstruct wl_resource *resource;\n\tstruct wl_resource *tmp_resource;\n\twl_resource_for_each_safe(resource, tmp_resource, &idle_inhibit->wl_resources) {\n\t\twl_resource_destroy(inhibitor->resource);\n\t}\n\n\twl_global_destroy(idle_inhibit->global);\n\tfree(idle_inhibit);\n}\n\nstruct wlr_idle_inhibit_manager_v1 *wlr_idle_inhibit_v1_create(struct wl_display *display) {\n\tstruct wlr_idle_inhibit_manager_v1 *idle_inhibit =\n\t\tcalloc(1, sizeof(struct wlr_idle_inhibit_manager_v1));\n\n\tif (!idle_inhibit) {\n\t\treturn NULL;\n\t}\n\n\twl_list_init(&idle_inhibit->wl_resources);\n\twl_list_init(&idle_inhibit->inhibitors);\n\tidle_inhibit->display_destroy.notify = handle_display_destroy;\n\twl_display_add_destroy_listener(display, &idle_inhibit->display_destroy);\n\twl_signal_init(&idle_inhibit->events.new_inhibitor);\n\n\tidle_inhibit->global = wl_global_create(display,\n\t\t&zwp_idle_inhibit_manager_v1_interface, 1,\n\t\tidle_inhibit, idle_inhibit_bind);\n\n\tif (!idle_inhibit->global) {\n\t\twl_list_remove(&idle_inhibit->display_destroy.link);\n\t\tfree(idle_inhibit);\n\t\treturn NULL;\n\t}\n\n\twlr_log(L_DEBUG, \"idle_inhibit manager created\");\n\n\treturn idle_inhibit;\n}\n","lang_cluster":"C","length":185,"code_uid":"82d5321d3c97488dbae34dd3767ccb65"}
{"diff_hunk":"@@ -149,6 +149,20 @@ class Session(Base):\n         self.last_access = datetime.now()\n \n \n+class Configuration(Base):\n+    __tablename__ = 'server_configurations'\n+\n+    id = Column(Integer, autoincrement=True, primary_key=True)\n+    config_key = Column(String,\n+                        nullable=False)\n+\n+    config_value = Column(String)\n+\n+    def __init__(self, config_key, config_value):\n+        self.config_key = config_key\n+        self.config_value = config_value\n+\n+\n IDENTIFIER = {\n     'identifier': \"ConfigDatabase\",\n     'orm_meta': CC_META,","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nSQLAlchemy ORM model for the product configuration database.\n\"\"\"\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nfrom __future__ import division\nfrom __future__ import absolute_import\nfrom datetime import datetime\nimport sys\n\nfrom sqlalchemy import MetaData, Column, Integer, Enum, String, Boolean, \\\n    ForeignKey, CHAR, DateTime, Text\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.sql.expression import false, true\n\nfrom libcodechecker.server import permissions\n\nCC_META = MetaData(naming_convention={\n    \"ix\": 'ix_%(column_0_label)s',\n    \"uq\": \"uq_%(table_name)s_%(column_0_name)s\",\n    \"ck\": \"ck_%(table_name)s_%(column_0_name)s\",\n    \"fk\": \"fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s\",\n    \"pk\": \"pk_%(table_name)s\"\n})\n\n# Create base class for ORM classes.\nBase = declarative_base(metadata=CC_META)\n\n\nclass DBVersion(Base):\n    __tablename__ = 'db_version'\n    # TODO: constraint, only one line in this table!\n    major = Column(Integer, primary_key=True)\n    minor = Column(Integer, primary_key=True)\n\n    def __init__(self, major, minor):\n        self.major = major\n        self.minor = minor\n\n\nclass Product(Base):\n    __tablename__ = 'products'\n\n    id = Column(Integer, autoincrement=True, primary_key=True)\n    endpoint = Column(String, nullable=False, unique=True)\n    connection = Column(String, nullable=False)\n    display_name = Column(String, nullable=False)\n    description = Column(Text)\n    run_limit = Column(Integer)\n\n    # Disable review status change on UI.\n    is_review_status_change_disabled = Column(Boolean,\n                                              server_default=false())\n\n    def __init__(self, endpoint, conn_str, name=None, description=None,\n                 run_limit=None, is_review_status_change_disabled=False):\n        self.endpoint = endpoint\n        self.connection = conn_str\n        self.display_name = name if name else endpoint\n        self.description = description\n        self.run_limit = run_limit\n        self.is_review_status_change_disabled = \\\n            True if is_review_status_change_disabled else False\n\n\ndef __get_permission_names(scope=None):\n    \"\"\"\n    Returns a list of strings which contains permission names.\n\n    This function is used internally to set up the permission database.\n\n    :param scope: One of the Permission class strings (e.g. 'SYSTEM'), which\n      if given, filters the returned list of permissions to only definitions\n      of the given scope.\n    \"\"\"\n\n    return [perm.name for perm in permissions.get_permissions(scope)]\n\n\nclass SystemPermission(Base):\n    __tablename__ = 'permissions_system'\n\n    id = Column(Integer, autoincrement=True, primary_key=True)\n    permission = Column(Enum(\n        *sys.modules[__name__].__dict__['__get_permission_names']('SYSTEM'),\n        name='sys_perms'))\n    name = Column(String, nullable=False)\n    is_group = Column(Boolean, nullable=False)\n\n    def __init__(self, permission, name, is_group=False):\n        self.permission = permission\n        self.name = name\n        self.is_group = is_group\n\n\nclass ProductPermission(Base):\n    __tablename__ = 'permissions_product'\n\n    id = Column(Integer, autoincrement=True, primary_key=True)\n    permission = Column(Enum(\n        *sys.modules[__name__].__dict__['__get_permission_names']('PRODUCT'),\n        name='product_perms'))\n    product_id = Column(Integer, ForeignKey('products.id',\n                                            deferrable=False,\n                                            initially='IMMEDIATE',\n                                            ondelete='CASCADE'),\n                        nullable=False)\n    name = Column(String, nullable=False)\n    is_group = Column(Boolean, nullable=False)\n\n    def __init__(self, permission, product_id, name, is_group=False):\n        self.permission = permission\n        self.product_id = product_id\n        self.name = name\n        self.is_group = is_group\n\n\nclass Session(Base):\n    __tablename__ = 'auth_sessions'\n\n    id = Column(Integer, autoincrement=True, primary_key=True)\n\n    user_name = Column(String)\n    token = Column(CHAR(32), nullable=False, unique=True)\n\n    # List of group names separated by semicolons.\n    groups = Column(String)\n\n    last_access = Column(DateTime, nullable=False)\n\n    # Token description.\n    description = Column(String)\n\n    can_expire = Column(Boolean, server_default=true(), default=True)\n\n    def __init__(self, token, user_name, groups, description=None,\n                 can_expire=True):\n        self.token = token\n        self.user_name = user_name\n        self.groups = groups\n        self.description = description\n        self.can_expire = can_expire\n        self.last_access = datetime.now()\n\n\nIDENTIFIER = {\n    'identifier': \"ConfigDatabase\",\n    'orm_meta': CC_META,\n    'version_class': DBVersion\n}\n","lang_cluster":"C","length":156,"code_uid":"6de0fe06d33b44f086fe86189e0c06ef"}
{"diff_hunk":"@@ -17,6 +17,7 @@ package main\n import (\n \t. \"github.com\/onsi\/ginkgo\"\n \t. \"github.com\/onsi\/gomega\"\n+\n \t\"github.com\/projectcalico\/felix\/k8sfv\/leastsquares\"\n )\n ","old_code":"\/\/ Copyright (c) 2017 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage main\n\nimport (\n\t. \"github.com\/onsi\/ginkgo\"\n\t. \"github.com\/onsi\/gomega\"\n\t\"github.com\/projectcalico\/felix\/k8sfv\/leastsquares\"\n)\n\nvar _ = Context(\"least squares\", func() {\n\n\tIt(\"should fit a straight line\", func() {\n\t\tp := []leastsquares.Point{\n\t\t\t{1, 1},\n\t\t\t{2, 2},\n\t\t\t{3, 3},\n\t\t\t{4, 4},\n\t\t}\n\t\tgradient, constant := leastsquares.LeastSquaresMethod(p)\n\t\tExpect(gradient).To(BeNumerically(\"==\", 1))\n\t\tExpect(constant).To(BeNumerically(\"==\", 0))\n\t})\n})\n","lang_cluster":"C","length":36,"code_uid":"8dadc794d6c7441390d6fd1df009107b"}
{"diff_hunk":"@@ -42,6 +42,10 @@ static struct config default_config[] = {\n     { \"no_docs_path\",   INSTALLED_NO_DOCS_PATH,     INTREE_NO_DOCS_PATH },\n     { \"rundir\",         INSTALLED_RUNDIR,           NULL },\n     { \"bindir\",         INSTALLED_BINDIR,           INTREE_BINDIR },\n+    { \"jobspec_validate_path\", INSTALLED_JOBSPEC_VALIDATE_PATH,\n+                                            INTREE_JOBSPEC_VALIDATE_PATH },\n+    { \"jobspec_schema_path\", INSTALLED_JOBSPEC_SCHEMA_PATH,\n+                                            INTREE_JOBSPEC_SCHEMA_PATH },\n     { NULL, NULL, NULL },\n };\n ","old_code":"\/************************************************************\\\n * Copyright 2014 Lawrence Livermore National Security, LLC\n * (c.f. AUTHORS, NOTICE.LLNS, COPYING)\n *\n * This file is part of the Flux resource manager framework.\n * For details, see https:\/\/github.com\/flux-framework.\n *\n * SPDX-License-Identifier: LGPL-3.0\n\\************************************************************\/\n\n#if HAVE_CONFIG_H\n#include \"config.h\"\n#endif\n#include <stdio.h>\n#include <string.h>\n#include \"conf.h\"\n\nstruct config {\n    char *key;\n    const char *val;\n    const char *val_intree;\n};\n\nstatic struct config default_config[] = {\n    { \"lua_cpath_add\",  INSTALLED_LUA_CPATH_ADD,    INTREE_LUA_CPATH_ADD },\n    { \"lua_path_add\",   INSTALLED_LUA_PATH_ADD,     INTREE_LUA_PATH_ADD },\n    { \"python_path\",    INSTALLED_PYTHON_PATH,      INTREE_PYTHON_PATH },\n    { \"man_path\",       INSTALLED_MAN_PATH,         INTREE_MAN_PATH },\n    { \"exec_path\",      INSTALLED_EXEC_PATH,        INTREE_EXEC_PATH },\n    { \"connector_path\", INSTALLED_CONNECTOR_PATH,   INTREE_CONNECTOR_PATH },\n    { \"module_path\",    INSTALLED_MODULE_PATH,      INTREE_MODULE_PATH },\n    { \"rc1_path\",       INSTALLED_RC1_PATH,         INTREE_RC1_PATH },\n    { \"rc3_path\",       INSTALLED_RC3_PATH,         INTREE_RC3_PATH },\n    { \"cmdhelp_pattern\",INSTALLED_CMDHELP_PATTERN,  INTREE_CMDHELP_PATTERN },\n    { \"pmi_library_path\",\n                        INSTALLED_PMI_LIBRARY_PATH, INTREE_PMI_LIBRARY_PATH },\n    { \"wrexecd_path\",   INSTALLED_WREXECD_PATH,     INTREE_WREXECD_PATH },\n    { \"wreck_lua_pattern\",\n                        INSTALLED_WRECK_LUA_PATTERN,\n                                                    INTREE_WRECK_LUA_PATTERN },\n    { \"keydir\",         NULL,                       INTREE_KEYDIR },\n    { \"no_docs_path\",   INSTALLED_NO_DOCS_PATH,     INTREE_NO_DOCS_PATH },\n    { \"rundir\",         INSTALLED_RUNDIR,           NULL },\n    { \"bindir\",         INSTALLED_BINDIR,           INTREE_BINDIR },\n    { NULL, NULL, NULL },\n};\n\nconst char *flux_conf_get (const char *name, int flags)\n{\n    struct config *c;\n\n    for (c = &default_config[0]; c->key != NULL; c++) {\n        if (!strcmp (c->key, name))\n            return (flags & CONF_FLAG_INTREE) ? c->val_intree : c->val;\n    }\n    return NULL;\n}\n\n\/*\n * vi:tabstop=4 shiftwidth=4 expandtab\n *\/\n","lang_cluster":"C","length":61,"code_uid":"205fcdcf13684dbc93482475bd36bcc4"}
{"diff_hunk":"@@ -126,9 +126,11 @@ void TThreadedServer::onClientDisconnected(TConnectedClient* pClient) {\n   Synchronized sync(clientMonitor_);\n   drainDeadClients(); \/\/ use the outgoing thread to do some maintenance on our dead client backlog\n   ClientMap::iterator it = activeClientMap_.find(pClient);\n-  ClientMap::iterator end = it;\n-  deadClientMap_.insert(it, ++end);\n-  activeClientMap_.erase(it);\n+  if (it != activeClientMap_.end()) {\n+    ClientMap::iterator end = it;\n+    deadClientMap_.insert(it, ++end);\n+    activeClientMap_.erase(it);\n+  }\n   if (activeClientMap_.empty()) {\n     clientMonitor_.notify();\n   }","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements. See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership. The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License. You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\n#include <string>\n#include <thrift\/stdcxx.h>\n#include <thrift\/concurrency\/PlatformThreadFactory.h>\n#include <thrift\/server\/TThreadedServer.h>\n\nnamespace apache {\nnamespace thrift {\nnamespace server {\n\nusing apache::thrift::concurrency::Runnable;\nusing apache::thrift::concurrency::Synchronized;\nusing apache::thrift::concurrency::Thread;\nusing apache::thrift::concurrency::ThreadFactory;\nusing apache::thrift::protocol::TProtocol;\nusing apache::thrift::protocol::TProtocolFactory;\nusing apache::thrift::stdcxx::make_shared;\nusing apache::thrift::stdcxx::shared_ptr;\nusing apache::thrift::transport::TServerTransport;\nusing apache::thrift::transport::TTransport;\nusing apache::thrift::transport::TTransportException;\nusing apache::thrift::transport::TTransportFactory;\n\nTThreadedServer::TThreadedServer(const shared_ptr<TProcessorFactory>& processorFactory,\n                                 const shared_ptr<TServerTransport>& serverTransport,\n                                 const shared_ptr<TTransportFactory>& transportFactory,\n                                 const shared_ptr<TProtocolFactory>& protocolFactory,\n                                 const shared_ptr<ThreadFactory>& threadFactory)\n  : TServerFramework(processorFactory, serverTransport, transportFactory, protocolFactory),\n    threadFactory_(threadFactory) {\n}\n\nTThreadedServer::TThreadedServer(const shared_ptr<TProcessor>& processor,\n                                 const shared_ptr<TServerTransport>& serverTransport,\n                                 const shared_ptr<TTransportFactory>& transportFactory,\n                                 const shared_ptr<TProtocolFactory>& protocolFactory,\n                                 const shared_ptr<ThreadFactory>& threadFactory)\n  : TServerFramework(processor, serverTransport, transportFactory, protocolFactory),\n    threadFactory_(threadFactory) {\n}\n\nTThreadedServer::TThreadedServer(const shared_ptr<TProcessorFactory>& processorFactory,\n                                 const shared_ptr<TServerTransport>& serverTransport,\n                                 const shared_ptr<TTransportFactory>& inputTransportFactory,\n                                 const shared_ptr<TTransportFactory>& outputTransportFactory,\n                                 const shared_ptr<TProtocolFactory>& inputProtocolFactory,\n                                 const shared_ptr<TProtocolFactory>& outputProtocolFactory,\n                                 const shared_ptr<ThreadFactory>& threadFactory)\n  : TServerFramework(processorFactory,\n                     serverTransport,\n                     inputTransportFactory,\n                     outputTransportFactory,\n                     inputProtocolFactory,\n                     outputProtocolFactory),\n    threadFactory_(threadFactory) {\n}\n\nTThreadedServer::TThreadedServer(const shared_ptr<TProcessor>& processor,\n                                 const shared_ptr<TServerTransport>& serverTransport,\n                                 const shared_ptr<TTransportFactory>& inputTransportFactory,\n                                 const shared_ptr<TTransportFactory>& outputTransportFactory,\n                                 const shared_ptr<TProtocolFactory>& inputProtocolFactory,\n                                 const shared_ptr<TProtocolFactory>& outputProtocolFactory,\n                                 const shared_ptr<ThreadFactory>& threadFactory)\n  : TServerFramework(processor,\n                     serverTransport,\n                     inputTransportFactory,\n                     outputTransportFactory,\n                     inputProtocolFactory,\n                     outputProtocolFactory),\n    threadFactory_(threadFactory) {\n}\n\nTThreadedServer::~TThreadedServer() {\n}\n\nvoid TThreadedServer::serve() {\n  TServerFramework::serve();\n\n  \/\/ Ensure post-condition of no active clients\n  Synchronized s(clientMonitor_);\n  while (!activeClientMap_.empty()) {\n    clientMonitor_.wait();\n  }\n\n  drainDeadClients();\n}\n\nvoid TThreadedServer::drainDeadClients() {\n  \/\/ we're in a monitor here\n  while (!deadClientMap_.empty()) {\n    ClientMap::iterator it = deadClientMap_.begin();\n    it->second->join();\n    deadClientMap_.erase(it);\n  }\n}\n\nvoid TThreadedServer::onClientConnected(const shared_ptr<TConnectedClient>& pClient) {\n  Synchronized sync(clientMonitor_);\n  shared_ptr<TConnectedClientRunner> pRunnable = make_shared<TConnectedClientRunner>(pClient);\n  shared_ptr<Thread> pThread = threadFactory_->newThread(pRunnable);\n  pRunnable->thread(pThread);\n  activeClientMap_.insert(ClientMap::value_type(pClient.get(), pThread));\n  pThread->start();\n}\n\nvoid TThreadedServer::onClientDisconnected(TConnectedClient* pClient) {\n  Synchronized sync(clientMonitor_);\n  drainDeadClients(); \/\/ use the outgoing thread to do some maintenance on our dead client backlog\n  ClientMap::iterator it = activeClientMap_.find(pClient);\n  ClientMap::iterator end = it;\n  deadClientMap_.insert(it, ++end);\n  activeClientMap_.erase(it);\n  if (activeClientMap_.empty()) {\n    clientMonitor_.notify();\n  }\n}\n\nTThreadedServer::TConnectedClientRunner::TConnectedClientRunner(const shared_ptr<TConnectedClient>& pClient)\n  : pClient_(pClient) {\n}\n\nTThreadedServer::TConnectedClientRunner::~TConnectedClientRunner() {\n}\n\nvoid TThreadedServer::TConnectedClientRunner::run() \/* override *\/ {\n  pClient_->run();  \/\/ Run the client\n  pClient_.reset(); \/\/ The client is done - release it here rather than in the destructor for safety\n}\n\n}\n}\n} \/\/ apache::thrift::server\n","lang_cluster":"C","length":151,"code_uid":"81e1b8b9713d4adc8fb3bc5e57327e23"}
{"diff_hunk":"@@ -59,6 +59,13 @@ static int on_req(h2o_handler_t *_self, h2o_req_t *req)\n         authority = &self->upstream.authority;\n     }\n \n+    \/* rewrite headers *\/\n+    if (self->config.header_cmds.size != 0) {\n+        h2o_headers_command_t *cmd;\n+        for (cmd = self->config.header_cmds.entries; cmd->cmd != H2O_HEADERS_CMD_NULL; ++cmd)\n+            h2o_rewrite_headers(&req->pool, &req->headers, cmd);\n+    }\n+\n     \/* request reprocess *\/\n     h2o_reprocess_request(req, req->method, scheme, *authority,\n                           h2o_build_destination(req, self->upstream.path.base, self->upstream.path.len, 0), overrides, 0);","old_code":"\/*\n * Copyright (c) 2014,2015 DeNA Co., Ltd., Masahiro Nagano\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n *\/\n#include <sys\/un.h>\n#include \"h2o.h\"\n#include \"h2o\/socketpool.h\"\n\nstruct rp_handler_t {\n    h2o_handler_t super;\n    h2o_url_t upstream;         \/* host should be NULL-terminated *\/\n    h2o_socketpool_t *sockpool; \/* non-NULL if config.use_keepalive == 1 *\/\n    h2o_proxy_config_vars_t config;\n};\n\nstatic int on_req(h2o_handler_t *_self, h2o_req_t *req)\n{\n    struct rp_handler_t *self = (void *)_self;\n    h2o_req_overrides_t *overrides = h2o_mem_alloc_pool(&req->pool, sizeof(*overrides));\n    const h2o_url_scheme_t *scheme;\n    h2o_iovec_t *authority;\n\n    \/* setup overrides *\/\n    *overrides = (h2o_req_overrides_t){NULL};\n    if (self->sockpool != NULL) {\n        overrides->socketpool = self->sockpool;\n    } else if (self->config.preserve_host) {\n        overrides->hostport.host = self->upstream.host;\n        overrides->hostport.port = h2o_url_get_port(&self->upstream);\n    }\n    overrides->location_rewrite.match = &self->upstream;\n    overrides->location_rewrite.path_prefix = req->pathconf->path;\n    overrides->use_proxy_protocol = self->config.use_proxy_protocol;\n    overrides->client_ctx = h2o_context_get_handler_context(req->conn->ctx, &self->super);\n\n    \/* determine the scheme and authority *\/\n    if (self->config.preserve_host) {\n        scheme = req->scheme;\n        authority = &req->authority;\n    } else {\n        scheme = self->upstream.scheme;\n        authority = &self->upstream.authority;\n    }\n\n    \/* request reprocess *\/\n    h2o_reprocess_request(req, req->method, scheme, *authority,\n                          h2o_build_destination(req, self->upstream.path.base, self->upstream.path.len, 0), overrides, 0);\n\n    return 0;\n}\n\nstatic void on_context_init(h2o_handler_t *_self, h2o_context_t *ctx)\n{\n    struct rp_handler_t *self = (void *)_self;\n\n    \/* use the loop of first context for handling socketpool timeouts *\/\n    if (self->sockpool != NULL && self->sockpool->timeout == UINT64_MAX)\n        h2o_socketpool_set_timeout(self->sockpool, ctx->loop, self->config.keepalive_timeout);\n\n    \/* setup a specific client context only if we need to *\/\n    if (ctx->globalconf->proxy.io_timeout == self->config.io_timeout && !self->config.websocket.enabled &&\n        self->config.ssl_ctx == ctx->globalconf->proxy.ssl_ctx)\n        return;\n\n    h2o_http1client_ctx_t *client_ctx = h2o_mem_alloc(sizeof(*ctx));\n    client_ctx->loop = ctx->loop;\n    client_ctx->getaddr_receiver = &ctx->receivers.hostinfo_getaddr;\n    if (ctx->globalconf->proxy.io_timeout == self->config.io_timeout) {\n        client_ctx->io_timeout = &ctx->proxy.io_timeout;\n    } else {\n        client_ctx->io_timeout = h2o_mem_alloc(sizeof(*client_ctx->io_timeout));\n        h2o_timeout_init(client_ctx->loop, client_ctx->io_timeout, self->config.io_timeout);\n    }\n    if (self->config.websocket.enabled) {\n        \/* FIXME avoid creating h2o_timeout_t for every path-level context in case the timeout values are the same *\/\n        client_ctx->websocket_timeout = h2o_mem_alloc(sizeof(*client_ctx->websocket_timeout));\n        h2o_timeout_init(client_ctx->loop, client_ctx->websocket_timeout, self->config.websocket.timeout);\n    } else {\n        client_ctx->websocket_timeout = NULL;\n    }\n    client_ctx->ssl_ctx = self->config.ssl_ctx;\n\n    h2o_context_set_handler_context(ctx, &self->super, client_ctx);\n}\n\nstatic void on_context_dispose(h2o_handler_t *_self, h2o_context_t *ctx)\n{\n    struct rp_handler_t *self = (void *)_self;\n    h2o_http1client_ctx_t *client_ctx = h2o_context_get_handler_context(ctx, &self->super);\n\n    if (client_ctx == NULL)\n        return;\n\n    if (client_ctx->io_timeout != &ctx->proxy.io_timeout) {\n        h2o_timeout_dispose(client_ctx->loop, client_ctx->io_timeout);\n        free(client_ctx->io_timeout);\n    }\n    if (client_ctx->websocket_timeout != NULL) {\n        h2o_timeout_dispose(client_ctx->loop, client_ctx->websocket_timeout);\n        free(client_ctx->websocket_timeout);\n    }\n    free(client_ctx);\n}\n\nstatic void on_handler_dispose(h2o_handler_t *_self)\n{\n    struct rp_handler_t *self = (void *)_self;\n\n    if (self->config.ssl_ctx != NULL)\n        SSL_CTX_free(self->config.ssl_ctx);\n    free(self->upstream.host.base);\n    free(self->upstream.path.base);\n    if (self->sockpool != NULL) {\n        h2o_socketpool_dispose(self->sockpool);\n        free(self->sockpool);\n    }\n}\n\nvoid h2o_proxy_register_reverse_proxy(h2o_pathconf_t *pathconf, h2o_url_t *upstream, h2o_proxy_config_vars_t *config)\n{\n    struct rp_handler_t *self = (void *)h2o_create_handler(pathconf, sizeof(*self));\n    self->super.on_context_init = on_context_init;\n    self->super.on_context_dispose = on_context_dispose;\n    self->super.dispose = on_handler_dispose;\n    self->super.on_req = on_req;\n    if (config->keepalive_timeout != 0) {\n        self->sockpool = h2o_mem_alloc(sizeof(*self->sockpool));\n        struct sockaddr_un sa;\n        const char *to_sa_err;\n        int is_ssl = upstream->scheme == &H2O_URL_SCHEME_HTTPS;\n        if ((to_sa_err = h2o_url_host_to_sun(upstream->host, &sa)) == h2o_url_host_to_sun_err_is_not_unix_socket) {\n            h2o_socketpool_init_by_hostport(self->sockpool, upstream->host, h2o_url_get_port(upstream), is_ssl,\n                                            SIZE_MAX \/* FIXME *\/);\n        } else {\n            assert(to_sa_err == NULL);\n            h2o_socketpool_init_by_address(self->sockpool, (void *)&sa, sizeof(sa), is_ssl, SIZE_MAX \/* FIXME *\/);\n        }\n    }\n    h2o_url_copy(NULL, &self->upstream, upstream);\n    h2o_strtolower(self->upstream.host.base, self->upstream.host.len);\n    self->config = *config;\n    if (self->config.ssl_ctx != NULL)\n        SSL_CTX_up_ref(self->config.ssl_ctx);\n}\n","lang_cluster":"C","length":161,"code_uid":"296e53ad39a448c8bb4b537cf37f118a"}
{"diff_hunk":"@@ -54,6 +54,13 @@ class SourceAnalyzer(object):\n         \"\"\"\n         raise NotImplementedError(\"Subclasses should implement this!\")\n \n+    @classmethod\n+    def version_compatible(cls, configured_binary, environ):\n+        \"\"\"\n+        Checker the version compatibility of the given analyzer binary.\n+        \"\"\"\n+        raise NotImplementedError(\"Subclasses should implement this!\")\n+\n     @classmethod\n     def construct_config_handler(cls, args, context):\n         \"\"\" Should return a subclass of AnalyzerConfigHandler.\"\"\"","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nBase class for various source analyzers.\n\"\"\"\n\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nfrom abc import ABCMeta, abstractmethod\nimport os\nimport signal\nimport subprocess\nimport sys\n\nfrom codechecker_common.logger import get_logger\n\nLOG = get_logger('analyzer')\n\n\nclass SourceAnalyzer(object):\n    \"\"\"\n    Base class for different source analyzers.\n    \"\"\"\n    __metaclass__ = ABCMeta\n\n    def __init__(self, config_handler, buildaction):\n        self.__config_handler = config_handler\n        self.__build_action = buildaction\n        # Currently analyzed source file.\n        self.source_file = ''\n\n    @property\n    def buildaction(self):\n        return self.__build_action\n\n    @property\n    def config_handler(self):\n        return self.__config_handler\n\n    @abstractmethod\n    def construct_analyzer_cmd(self, result_handler):\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    @classmethod\n    def resolve_missing_binary(cls, configured_binary, environ):\n        \"\"\"\n        In case of the configured binary for the analyzer is not found in the\n        PATH, this method is used to find a callable binary.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    @classmethod\n    def construct_config_handler(cls, args, context):\n        \"\"\" Should return a subclass of AnalyzerConfigHandler.\"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    @abstractmethod\n    def get_analyzer_mentioned_files(self, output):\n        \"\"\"\n        Return a collection of files that were mentioned by the analyzer in\n        its standard outputs, which should be analyzer_stdout or\n        analyzer_stderr from a result handler.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    @abstractmethod\n    def construct_result_handler(self, buildaction, report_output,\n                                 severity_map, skiplist_handler):\n        \"\"\"\n        This method constructs the class that is responsible to handle the\n        results of the analysis. The result should be a subclass of\n        ResultHandler\n        \"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    def analyze(self, analyzer_cmd, res_handler, env=None, proc_callback=None):\n        \"\"\"\n        Run the analyzer.\n        \"\"\"\n        LOG.debug('Running analyzer ...')\n\n        LOG.debug_analyzer('\\n%s', ' '.join(analyzer_cmd))\n\n        res_handler.analyzer_cmd = analyzer_cmd\n        try:\n            ret_code, stdout, stderr \\\n                = SourceAnalyzer.run_proc(analyzer_cmd,\n                                          env,\n                                          res_handler.buildaction.directory,\n                                          proc_callback)\n            res_handler.analyzer_returncode = ret_code\n            res_handler.analyzer_stdout = stdout\n            res_handler.analyzer_stderr = stderr\n            return res_handler\n\n        except Exception as ex:\n            LOG.error(ex)\n            res_handler.analyzer_returncode = 1\n            return res_handler\n\n    @classmethod\n    def get_analyzer_checkers(cls, cfg_handler, environ):\n        \"\"\"\n        Return the checkers available in the analyzer.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    @staticmethod\n    def run_proc(command, env=None, cwd=None, proc_callback=None):\n        \"\"\"\n        Just run the given command and return the return code\n        and the stdout and stderr outputs of the process.\n        \"\"\"\n\n        def signal_handler(signum, frame):\n            # Clang does not kill its child processes, so I have to.\n            try:\n                g_pid = proc.pid\n                os.killpg(g_pid, signal.SIGTERM)\n            finally:\n                sys.exit(128 + signum)\n\n        signal.signal(signal.SIGINT, signal_handler)\n\n        proc = subprocess.Popen(command,\n                                bufsize=-1,\n                                env=env,\n                                preexec_fn=os.setsid,\n                                cwd=cwd,\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE,\n                                universal_newlines=True)\n\n        # Send the created analyzer process' object if somebody wanted it.\n        if proc_callback:\n            proc_callback(proc)\n\n        stdout, stderr = proc.communicate()\n        return proc.returncode, stdout, stderr\n","lang_cluster":"C","length":144,"code_uid":"77a111124b9440fa8c919f69c307f79e"}
{"diff_hunk":"@@ -20,9 +20,33 @@\n #include <jansson.h>\n \n #include \"src\/common\/libutil\/macros.h\"\n+#include \"src\/common\/libccan\/ccan\/base64\/base64.h\"\n \n #include \"ioencode.h\"\n \n+\n+static json_t *data_encode_base64 (const char *stream,\n+                                   const char *rank,\n+                                   const char *data,\n+                                   int len)\n+{\n+    ssize_t n;\n+    json_t *o = NULL;\n+    char *dest = NULL;\n+    size_t destlen = base64_encoded_length (len) + 1; \/* +1 for NUL *\/\n+\n+    if ((dest = calloc (1, destlen))\n+        && (n = base64_encode (dest, destlen, data, len)) >= 0) {\n+            o = json_pack (\"{s:s s:s s:s s:s#}\",\n+                           \"stream\", stream,\n+                           \"rank\", rank,\n+                           \"encoding\", \"base64\",\n+                           \"data\", dest, n);\n+    }\n+    free (dest);\n+    return o;\n+}\n+\n json_t *ioencode (const char *stream,\n                   const char *rank,\n                   const char *data,","old_code":"\/************************************************************\\\n * Copyright 2019 Lawrence Livermore National Security, LLC\n * (c.f. AUTHORS, NOTICE.LLNS, COPYING)\n *\n * This file is part of the Flux resource manager framework.\n * For details, see https:\/\/github.com\/flux-framework.\n *\n * SPDX-License-Identifier: LGPL-3.0\n\\************************************************************\/\n\n#if HAVE_CONFIG_H\n# include \"config.h\"\n#endif\n\n#include <stdarg.h>\n#include <string.h>\n#include <stdbool.h>\n#include <errno.h>\n\n#include <jansson.h>\n\n#include \"src\/common\/libutil\/macros.h\"\n\n#include \"ioencode.h\"\n\njson_t *ioencode (const char *stream,\n                  const char *rank,\n                  const char *data,\n                  int len,\n                  bool eof)\n{\n    json_t *o = NULL;\n    json_t *rv = NULL;\n\n    \/* data can be NULL and len == 0 if eof true *\/\n    if (!stream\n        || !rank\n        || (data && len <= 0)\n        || (!data && len != 0)\n        || (!data && !len && !eof)) {\n        errno = EINVAL;\n        return NULL;\n    }\n\n    if (data && len) {\n        if (!(o = json_pack (\"{s:s s:s s:s#}\",\n                             \"stream\", stream,\n                             \"rank\", rank,\n                             \"data\", data, len)))\n            goto error;\n    }\n    else {\n        if (!(o = json_pack (\"{s:s s:s}\",\n                             \"stream\", stream,\n                             \"rank\", rank)))\n            goto error;\n    }\n    if (eof) {\n        if (json_object_set_new (o, \"eof\", json_true ()) < 0)\n            goto error;\n    }\n    rv = o;\nerror:\n    return rv;\n}\n\nint iodecode (json_t *o,\n              const char **streamp,\n              const char **rankp,\n              char **datap,\n              int *lenp,\n              bool *eofp)\n{\n    const char *stream;\n    const char *rank;\n    char *bin_data = NULL;\n    size_t bin_len = 0;\n    int eof = 0;\n    bool has_data = false;\n    bool has_eof = false;\n    int rv = -1;\n\n    if (!o) {\n        errno = EINVAL;\n        return -1;\n    }\n\n    if (json_unpack (o, \"{s:s s:s}\",\n                     \"stream\", &stream,\n                     \"rank\", &rank) < 0)\n        goto cleanup;\n    if (json_unpack (o, \"{s:s%}\", \"data\", &bin_data, &bin_len) == 0) {\n        has_data = true;\n    }\n    if (json_unpack (o, \"{s:b}\", \"eof\", &eof) == 0)\n        has_eof = true;\n\n    if (!has_data && !has_eof) {\n        errno = EPROTO;\n        goto cleanup;\n    }\n\n    if (streamp)\n        (*streamp) = stream;\n    if (rankp)\n        (*rankp) = rank;\n    if (datap) {\n        *datap = NULL;\n        if (bin_data) {\n            if (!(*datap = malloc (bin_len)))\n                goto cleanup;\n            memcpy (*datap, bin_data, bin_len);\n            bin_data = NULL;\n        }\n    }\n    if (lenp)\n        (*lenp) = bin_len;\n    if (eofp)\n        (*eofp) = eof;\n\n    rv = 0;\ncleanup:\n    return rv;\n}\n\n\/*\n * vi: ts=4 sw=4 expandtab\n *\/\n","lang_cluster":"C","length":128,"code_uid":"ef796e4a7d004c8587701d206c6e1453"}
{"diff_hunk":"@@ -24,7 +24,7 @@\n #include \"h2o.h\"\n \n static ssize_t add_header(h2o_mem_pool_t *pool, h2o_headers_t *headers, h2o_iovec_t *name, const char *orig_name, const char *value,\n-                          size_t value_len, h2o_header_flags_t flags)\n+                          size_t value_len)\n {\n     h2o_header_t *slot;\n ","old_code":"\/*\n * Copyright (c) 2014 DeNA Co., Ltd.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n *\/\n#include <stddef.h>\n#include <stdio.h>\n#include \"h2o.h\"\n\nstatic ssize_t add_header(h2o_mem_pool_t *pool, h2o_headers_t *headers, h2o_iovec_t *name, const char *orig_name, const char *value,\n                          size_t value_len, h2o_header_flags_t flags)\n{\n    h2o_header_t *slot;\n\n    h2o_vector_reserve(pool, headers, headers->size + 1);\n    slot = headers->entries + headers->size++;\n\n    slot->name = name;\n    slot->value.base = (char *)value;\n    slot->value.len = value_len;\n    slot->orig_name = orig_name ? h2o_strdup(pool, orig_name, name->len).base : NULL;\n    slot->flags = flags;\n    return headers->size - 1;\n}\n\nssize_t h2o_find_header(const h2o_headers_t *headers, const h2o_token_t *token, ssize_t cursor)\n{\n    for (++cursor; cursor < headers->size; ++cursor) {\n        if (headers->entries[cursor].name == &token->buf) {\n            return cursor;\n        }\n    }\n    return -1;\n}\n\nssize_t h2o_find_header_by_str(const h2o_headers_t *headers, const char *name, size_t name_len, ssize_t cursor)\n{\n    for (++cursor; cursor < headers->size; ++cursor) {\n        h2o_header_t *t = headers->entries + cursor;\n        if (h2o_memis(t->name->base, t->name->len, name, name_len)) {\n            return cursor;\n        }\n    }\n    return -1;\n}\n\nssize_t h2o_add_header(h2o_mem_pool_t *pool, h2o_headers_t *headers, const h2o_token_t *token, const char *orig_name,\n                       const char *value, size_t value_len)\n{\n    return add_header(pool, headers, (h2o_iovec_t *)&token->buf, orig_name, value, value_len, (h2o_header_flags_t){0});\n}\n\nssize_t h2o_add_header_by_str(h2o_mem_pool_t *pool, h2o_headers_t *headers, const char *name, size_t name_len, int maybe_token,\n                              const char *orig_name, const char *value, size_t value_len)\n{\n    h2o_iovec_t *name_buf;\n\n    if (maybe_token) {\n        const h2o_token_t *token = h2o_lookup_token(name, name_len);\n        if (token != NULL) {\n            return add_header(pool, headers, (h2o_iovec_t *)token, orig_name, value, value_len, (h2o_header_flags_t){0});\n        }\n    }\n    name_buf = h2o_mem_alloc_pool(pool, *name_buf, 1);\n    name_buf->base = (char *)name;\n    name_buf->len = name_len;\n    return add_header(pool, headers, name_buf, orig_name, value, value_len, (h2o_header_flags_t){0});\n}\n\nssize_t h2o_set_header(h2o_mem_pool_t *pool, h2o_headers_t *headers, const h2o_token_t *token, const char *value, size_t value_len,\n                       int overwrite_if_exists)\n{\n    ssize_t cursor = h2o_find_header(headers, token, -1);\n    if (cursor != -1) {\n        if (overwrite_if_exists) {\n            h2o_iovec_t *slot = &headers->entries[cursor].value;\n            slot->base = (char *)value;\n            slot->len = value_len;\n        }\n        return cursor;\n    } else {\n        return h2o_add_header(pool, headers, token, NULL, value, value_len);\n    }\n}\n\nssize_t h2o_set_header_by_str(h2o_mem_pool_t *pool, h2o_headers_t *headers, const char *name, size_t name_len, int maybe_token,\n                              const char *value, size_t value_len, int overwrite_if_exists)\n{\n    ssize_t cursor;\n\n    if (maybe_token) {\n        const h2o_token_t *token = h2o_lookup_token(name, name_len);\n        if (token != NULL) {\n            return h2o_set_header(pool, headers, token, value, value_len, overwrite_if_exists);\n        }\n    }\n\n    cursor = h2o_find_header_by_str(headers, name, name_len, -1);\n    if (cursor != -1) {\n        if (overwrite_if_exists) {\n            h2o_iovec_t *slot = &headers->entries[cursor].value;\n            slot->base = (char *)value;\n            slot->len = value_len;\n        }\n        return cursor;\n    } else {\n        h2o_iovec_t *name_buf = h2o_mem_alloc_pool(pool, *name_buf, 1);\n        name_buf->base = (char *)name;\n        name_buf->len = name_len;\n        return add_header(pool, headers, name_buf, NULL, value, value_len, (h2o_header_flags_t){0});\n    }\n}\n\nssize_t h2o_set_header_token(h2o_mem_pool_t *pool, h2o_headers_t *headers, const h2o_token_t *token, const char *value,\n                             size_t value_len)\n{\n    size_t found = -1;\n    size_t i;\n    for (i = 0; i != headers->size; ++i) {\n        if (headers->entries[i].name == &token->buf) {\n            if (h2o_contains_token(headers->entries[i].value.base, headers->entries[i].value.len, value, value_len, ','))\n                return -1;\n            found = i;\n        }\n    }\n    if (found != -1) {\n        h2o_header_t *dest = headers->entries + found;\n        dest->value = h2o_concat(pool, dest->value, h2o_iovec_init(H2O_STRLIT(\", \")), h2o_iovec_init(value, value_len));\n        return found;\n    } else {\n        return h2o_add_header(pool, headers, token, NULL, value, value_len);\n    }\n}\n\nssize_t h2o_delete_header(h2o_headers_t *headers, ssize_t cursor)\n{\n    assert(cursor != -1);\n\n    --headers->size;\n    memmove(headers->entries + cursor, headers->entries + cursor + 1, sizeof(h2o_header_t) * (headers->size - cursor));\n\n    return cursor;\n}\n","lang_cluster":"C","length":159,"code_uid":"5f9030704871409a8b5452cd7ea0933a"}
{"diff_hunk":"@@ -32,7 +32,8 @@ SYNOPSIS\n                           const char *key);\n \n  int flux_kvs_txn_symlink (flux_kvs_txn_t *txn, int flags,\n-                           const char *key, const char *target);\n+                           const char *key, const char *ns,\n+                           const char *target);\n \n  int flux_kvs_txn_put_raw (flux_kvs_txn_t *txn, int flags,\n                            const char *key, const void *data, int len);","old_code":"flux_kvs_txn_create(3)\n======================\n:doctype: manpage\n\n\nNAME\n----\nflux_kvs_txn_create, flux_kvs_txn_destroy, flux_kvs_txn_put, flux_kvs_txn_pack, flux_kvs_txn_vpack, flux_kvs_txn_mkdir, flux_kvs_txn_unlink, flux_kvs_txn_symlink, flux_kvs_txn_put_raw, flux_kvs_txn_put_treeobj - operate on a KVS transaction object\n\n\nSYNOPSIS\n--------\n #include <flux\/core.h>\n\n flux_kvs_txn_t *flux_kvs_txn_create (void);\n\n void flux_kvs_txn_destroy (flux_kvs_txn_t *txn);\n\n int flux_kvs_txn_put (flux_kvs_txn_t *txn, int flags,\n                       const char *key, const char *value);\n\n int flux_kvs_txn_pack (flux_kvs_txn_t *txn, int flags,\n                        const char *key, const char *fmt, ...);\n\n int flux_kvs_txn_vpack (flux_kvs_txn_t *txn, int flags,\n                         const char *key, const char *fmt, va_list ap);\n\n int flux_kvs_txn_mkdir (flux_kvs_txn_t *txn, int flags,\n                         const char *key);\n\n int flux_kvs_txn_unlink (flux_kvs_txn_t *txn, int flags,\n                          const char *key);\n\n int flux_kvs_txn_symlink (flux_kvs_txn_t *txn, int flags,\n                           const char *key, const char *target);\n\n int flux_kvs_txn_put_raw (flux_kvs_txn_t *txn, int flags,\n                           const char *key, const void *data, int len);\n\n int flux_kvs_txn_put_treeobj (flux_kvs_txn_t *txn, int flags,\n                               const char *key, const char *treeobj);\n\n\nDESCRIPTION\n-----------\n\nThe Flux Key Value Store is a general purpose distributed storage\nservice used by Flux services.\n\n`flux_kvs_txn_create()` creates a KVS transaction object that may be\npassed to `flux_kvs_commit(3)` or `flux_kvs_fence(3)`.  The transaction\nconsists of a list of operations that are applied to the KVS together,\nin order.  The entire transaction either succeeds or fails.  After commit\nor fence, the object must be destroyed with `flux_kvs_txn_destroy()`.\n\nEach function below adds a single operation to _txn_.  _key_ is a\nhierarchical path name with period (\".\") used as path separator.\nWhen the transaction is committed, any existing keys or path components\nthat are in conflict with the requested operation are overwritten.\n_flags_ can modify the request as described below.\n\n`flux_kvs_txn_put()` sets _key_ to a NULL terminated string _value_.\n_value_ may be NULL indicating that an empty value should be stored.\n\n`flux_kvs_txn_pack()` sets _key_ to a NULL terminated string encoded\nfrom a JSON object built with `json_pack()` style arguments (see below).\n`flux_kvs_txn_vpack()` is a variant that accepts a _va_list_ argument.\n\n`flux_kvs_txn_mkdir()` sets _key_ to an empty directory.\n\n`flux_kvs_txn_unlink()` removes _key_.  If _key_ is a directory,\nall its contents are removed as well.\n\n`flux_kvs_txn_symlink()` sets _key_ to a symbolic link pointing to _target_,\nanother key.  _target_ need not exist.\n\n`flux_kvs_txn_put_raw()` sets _key_ to a value containing raw data\nreferred to by _data_ of length _len_.\n\n`flux_kvs_txn_put_treeobj()` sets _key_ to an RFC 11 object, encoded\nas a JSON string.\n\n\nFLAGS\n-----\n\nThe following are valid bits in a _flags_ mask passed as an argument\nto `flux_kvs_txn_put()` or `flux_kvs_txn_put_raw()`.\n\nFLUX_KVS_APPEND::\nAppend value instead of overwriting it.  If the key does not exist,\nit will be created with the value as the initial value.\n\n\ninclude::JSON_PACK.adoc[]\n\n\nRETURN VALUE\n------------\n\n`flux_kvs_txn_create()` returns a `flux_kvs_txn_t` object on success,\nor NULL on failure with errno set appropriately.\n\n`flux_kvs_txn_put()`, `flux_kvs_txn_pack()`, `flux_kvs_txn_mkdir()`,\n`flux_kvs_txn_unlink()`, `flux_kvs_txn_symlink()`, and `flux_kvs_txn_put_raw()`\nreturns 0 on success, or -1 on failure with errno set appropriately.\n\nERRORS\n------\n\nEINVAL::\nOne of the arguments was invalid.\n\nENOMEM::\nOut of memory.\n\n\nAUTHOR\n------\nThis page is maintained by the Flux community.\n\n\nRESOURCES\n---------\nGithub: <http:\/\/github.com\/flux-framework>\n\n\nCOPYRIGHT\n---------\ninclude::COPYRIGHT.adoc[]\n\n\nSEE ALSO\n---------\nflux_kvs_commit(3)\n\nhttps:\/\/github.com\/flux-framework\/rfc\/blob\/master\/spec_11.adoc[RFC 11: Key Value Store Tree Object Format v1]\n","lang_cluster":"C","length":137,"code_uid":"1296c67e4ad04da19c5978f73dc45e91"}
{"diff_hunk":"@@ -93,6 +93,26 @@ bool wlr_drm_format_set_add(struct wlr_drm_format_set *set, uint32_t format,\n \treturn true;\n }\n \n+bool wlr_drm_format_set_copy(struct wlr_drm_format_set *dst,\n+\t\tconst struct wlr_drm_format_set *src) {\n+\tdst->len = src->len;\n+\tdst->cap = src->cap;\n+\tdst->formats = calloc(dst->cap, sizeof(struct wlr_drm_format *));\n+\tif (dst->formats == NULL) {\n+\t\treturn false;\n+\t}\n+\tfor (size_t i = 0; i < src->len; i++) {\n+\t\tconst struct wlr_drm_format *fmt = src->formats[i];\n+\t\tsize_t fmt_size = sizeof(*fmt) + sizeof(fmt->modifiers[0]) * fmt->cap;\n+\t\tdst->formats[i] = malloc(fmt_size);\n+\t\tif (dst->formats[i] == NULL) {\n+\t\t\treturn false;\n+\t\t}\n+\t\tmemcpy(dst->formats[i], fmt, fmt_size);\n+\t}\n+\treturn true;\n+}\n+\n struct wlr_drm_format *wlr_drm_format_create(uint32_t format) {\n \tsize_t cap = 4;\n \tstruct wlr_drm_format *fmt =","old_code":"#include <assert.h>\n#include <drm_fourcc.h>\n#include <stdbool.h>\n#include <stdint.h>\n#include <stdlib.h>\n#include <string.h>\n#include <wlr\/render\/drm_format_set.h>\n#include <wlr\/util\/log.h>\n#include \"render\/drm_format_set.h\"\n\nvoid wlr_drm_format_set_finish(struct wlr_drm_format_set *set) {\n\tfor (size_t i = 0; i < set->len; ++i) {\n\t\tfree(set->formats[i]);\n\t}\n\tfree(set->formats);\n\n\tset->len = 0;\n\tset->cap = 0;\n\tset->formats = NULL;\n}\n\nstatic struct wlr_drm_format **format_set_get_ref(struct wlr_drm_format_set *set,\n\t\tuint32_t format) {\n\tfor (size_t i = 0; i < set->len; ++i) {\n\t\tif (set->formats[i]->format == format) {\n\t\t\treturn &set->formats[i];\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nconst struct wlr_drm_format *wlr_drm_format_set_get(\n\t\tconst struct wlr_drm_format_set *set, uint32_t format) {\n\tstruct wlr_drm_format **ptr =\n\t\tformat_set_get_ref((struct wlr_drm_format_set *)set, format);\n\treturn ptr ? *ptr : NULL;\n}\n\nbool wlr_drm_format_set_has(const struct wlr_drm_format_set *set,\n\t\tuint32_t format, uint64_t modifier) {\n\tconst struct wlr_drm_format *fmt = wlr_drm_format_set_get(set, format);\n\tif (!fmt) {\n\t\treturn false;\n\t}\n\n\tif (modifier == DRM_FORMAT_MOD_INVALID) {\n\t\treturn true;\n\t}\n\n\tfor (size_t i = 0; i < fmt->len; ++i) {\n\t\tif (fmt->modifiers[i] == modifier) {\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nbool wlr_drm_format_set_add(struct wlr_drm_format_set *set, uint32_t format,\n\t\tuint64_t modifier) {\n\tassert(format != DRM_FORMAT_INVALID);\n\n\tstruct wlr_drm_format **ptr = format_set_get_ref(set, format);\n\tif (ptr) {\n\t\treturn wlr_drm_format_add(ptr, modifier);\n\t}\n\n\tstruct wlr_drm_format *fmt = wlr_drm_format_create(format);\n\tif (!fmt) {\n\t\treturn false;\n\t}\n\tif (!wlr_drm_format_add(&fmt, modifier)) {\n\t\treturn false;\n\t}\n\n\tif (set->len == set->cap) {\n\t\tsize_t new = set->cap ? set->cap * 2 : 4;\n\n\t\tstruct wlr_drm_format **tmp = realloc(set->formats,\n\t\t\tsizeof(*fmt) + sizeof(fmt->modifiers[0]) * new);\n\t\tif (!tmp) {\n\t\t\twlr_log_errno(WLR_ERROR, \"Allocation failed\");\n\t\t\tfree(fmt);\n\t\t\treturn false;\n\t\t}\n\n\t\tset->cap = new;\n\t\tset->formats = tmp;\n\t}\n\n\tset->formats[set->len++] = fmt;\n\treturn true;\n}\n\nstruct wlr_drm_format *wlr_drm_format_create(uint32_t format) {\n\tsize_t cap = 4;\n\tstruct wlr_drm_format *fmt =\n\t\tcalloc(1, sizeof(*fmt) + sizeof(fmt->modifiers[0]) * cap);\n\tif (!fmt) {\n\t\twlr_log_errno(WLR_ERROR, \"Allocation failed\");\n\t\treturn NULL;\n\t}\n\tfmt->format = format;\n\tfmt->cap = cap;\n\treturn fmt;\n}\n\nbool wlr_drm_format_add(struct wlr_drm_format **fmt_ptr, uint64_t modifier) {\n\tstruct wlr_drm_format *fmt = *fmt_ptr;\n\n\tif (modifier == DRM_FORMAT_MOD_INVALID) {\n\t\treturn true;\n\t}\n\n\tfor (size_t i = 0; i < fmt->len; ++i) {\n\t\tif (fmt->modifiers[i] == modifier) {\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (fmt->len == fmt->cap) {\n\t\tsize_t cap = fmt->cap ? fmt->cap * 2 : 4;\n\n\t\tfmt = realloc(fmt, sizeof(*fmt) + sizeof(fmt->modifiers[0]) * cap);\n\t\tif (!fmt) {\n\t\t\twlr_log_errno(WLR_ERROR, \"Allocation failed\");\n\t\t\treturn false;\n\t\t}\n\n\t\tfmt->cap = cap;\n\t\t*fmt_ptr = fmt;\n\t}\n\n\tfmt->modifiers[fmt->len++] = modifier;\n\treturn true;\n}\n\nstruct wlr_drm_format *wlr_drm_format_dup(const struct wlr_drm_format *format) {\n\tassert(format->len <= format->cap);\n\tsize_t format_size = sizeof(struct wlr_drm_format) +\n\t\tformat->cap * sizeof(format->modifiers[0]);\n\tstruct wlr_drm_format *duped_format = malloc(format_size);\n\tif (duped_format == NULL) {\n\t\treturn NULL;\n\t}\n\tmemcpy(duped_format, format, format_size);\n\treturn duped_format;\n}\n\nstruct wlr_drm_format *wlr_drm_format_intersect(\n\t\tconst struct wlr_drm_format *a, const struct wlr_drm_format *b) {\n\tassert(a->format == b->format);\n\n\t\/\/ Special case: if a format only supports LINEAR and the other doesn't\n\t\/\/ support any modifier, force LINEAR. This will force the allocator to\n\t\/\/ create a buffer with a LINEAR layout instead of an implicit modifier.\n\tif (a->len == 0 && b->len == 1 && b->modifiers[0] == DRM_FORMAT_MOD_LINEAR) {\n\t\treturn wlr_drm_format_dup(b);\n\t}\n\tif (b->len == 0 && a->len == 1 && a->modifiers[0] == DRM_FORMAT_MOD_LINEAR) {\n\t\treturn wlr_drm_format_dup(a);\n\t}\n\n\tsize_t format_cap = a->len < b->len ? a->len : b->len;\n\tsize_t format_size = sizeof(struct wlr_drm_format) +\n\t\tformat_cap * sizeof(a->modifiers[0]);\n\tstruct wlr_drm_format *format = calloc(1, format_size);\n\tif (format == NULL) {\n\t\twlr_log_errno(WLR_ERROR, \"Allocation failed\");\n\t\treturn NULL;\n\t}\n\tformat->format = a->format;\n\tformat->cap = format_cap;\n\n\tfor (size_t i = 0; i < a->len; i++) {\n\t\tfor (size_t j = 0; j < b->len; j++) {\n\t\t\tif (a->modifiers[i] == b->modifiers[j]) {\n\t\t\t\tassert(format->len < format->cap);\n\t\t\t\tformat->modifiers[format->len] = a->modifiers[i];\n\t\t\t\tformat->len++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t\/\/ If both formats support modifiers, but the intersection is empty, then\n\t\/\/ the formats aren't compatible with each other\n\tif (format->len == 0 && a->len > 0 && b->len > 0) {\n\t\tfree(format);\n\t\treturn NULL;\n\t}\n\n\treturn format;\n}\n","lang_cluster":"C","length":195,"code_uid":"e663a95350844205b8437bd2aefd65b3"}
{"diff_hunk":"@@ -76,22 +76,22 @@ thrift_server_set_property (GObject *object, guint property_id,\n   switch (property_id)\n   {\n     case PROP_THRIFT_SERVER_PROCESSOR:\n-      server->processor = g_value_get_object (value);\n+      server->processor = g_value_dup_object (value);\n       break;\n     case PROP_THRIFT_SERVER_SERVER_TRANSPORT:\n-      server->server_transport = g_value_get_object (value);\n+      server->server_transport = g_value_dup_object (value);\n       break;\n     case PROP_THRIFT_SERVER_INPUT_TRANSPORT_FACTORY:\n-      server->input_transport_factory = g_value_get_object (value);\n+      server->input_transport_factory = g_value_dup_object (value);\n       break;\n     case PROP_THRIFT_SERVER_OUTPUT_TRANSPORT_FACTORY:\n-      server->output_transport_factory = g_value_get_object (value);\n+      server->output_transport_factory = g_value_dup_object (value);\n       break;\n     case PROP_THRIFT_SERVER_INPUT_PROTOCOL_FACTORY:\n-      server->input_protocol_factory = g_value_get_object (value);\n+      server->input_protocol_factory = g_value_dup_object (value);\n       break;\n     case PROP_THRIFT_SERVER_OUTPUT_PROTOCOL_FACTORY:\n-      server->output_protocol_factory = g_value_get_object (value);\n+      server->output_protocol_factory = g_value_dup_object (value);\n       break;\n   }\n }","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements. See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership. The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License. You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\n#include <thrift\/c_glib\/thrift.h>\n#include \"thrift_server.h\"\n\n\/* object properties *\/\nenum _ThriftServerProperties\n{\n  PROP_0,\n  PROP_THRIFT_SERVER_PROCESSOR,\n  PROP_THRIFT_SERVER_SERVER_TRANSPORT,\n  PROP_THRIFT_SERVER_INPUT_TRANSPORT_FACTORY,\n  PROP_THRIFT_SERVER_OUTPUT_TRANSPORT_FACTORY,\n  PROP_THRIFT_SERVER_INPUT_PROTOCOL_FACTORY,\n  PROP_THRIFT_SERVER_OUTPUT_PROTOCOL_FACTORY\n};\n\nG_DEFINE_ABSTRACT_TYPE(ThriftServer, thrift_server, G_TYPE_OBJECT)\n\nvoid\nthrift_server_get_property (GObject *object, guint property_id,\n                            GValue *value, GParamSpec *pspec)\n{\n  ThriftServer *server = THRIFT_SERVER (object);\n\n  THRIFT_UNUSED_VAR (pspec);\n\n  switch (property_id)\n  {\n    case PROP_THRIFT_SERVER_PROCESSOR:\n      g_value_set_object (value, server->processor);\n      break;\n    case PROP_THRIFT_SERVER_SERVER_TRANSPORT:\n      g_value_set_object (value, server->server_transport);\n      break;\n    case PROP_THRIFT_SERVER_INPUT_TRANSPORT_FACTORY:\n      g_value_set_object (value, server->input_transport_factory);\n      break;\n    case PROP_THRIFT_SERVER_OUTPUT_TRANSPORT_FACTORY:\n      g_value_set_object (value, server->output_transport_factory);\n      break;\n    case PROP_THRIFT_SERVER_INPUT_PROTOCOL_FACTORY:\n      g_value_set_object (value, server->input_protocol_factory);\n      break;\n    case PROP_THRIFT_SERVER_OUTPUT_PROTOCOL_FACTORY:\n      g_value_set_object (value, server->output_protocol_factory);\n      break;\n  }\n}\n\nvoid\nthrift_server_set_property (GObject *object, guint property_id,\n                            const GValue *value, GParamSpec *pspec)\n{\n  ThriftServer *server = THRIFT_SERVER (object);\n\n  THRIFT_UNUSED_VAR (pspec);\n\n  switch (property_id)\n  {\n    case PROP_THRIFT_SERVER_PROCESSOR:\n      server->processor = g_value_get_object (value);\n      break;\n    case PROP_THRIFT_SERVER_SERVER_TRANSPORT:\n      server->server_transport = g_value_get_object (value);\n      break;\n    case PROP_THRIFT_SERVER_INPUT_TRANSPORT_FACTORY:\n      server->input_transport_factory = g_value_get_object (value);\n      break;\n    case PROP_THRIFT_SERVER_OUTPUT_TRANSPORT_FACTORY:\n      server->output_transport_factory = g_value_get_object (value);\n      break;\n    case PROP_THRIFT_SERVER_INPUT_PROTOCOL_FACTORY:\n      server->input_protocol_factory = g_value_get_object (value);\n      break;\n    case PROP_THRIFT_SERVER_OUTPUT_PROTOCOL_FACTORY:\n      server->output_protocol_factory = g_value_get_object (value);\n      break;\n  }\n}\n\ngboolean\nthrift_server_serve (ThriftServer *server, GError **error)\n{\n  return THRIFT_SERVER_GET_CLASS (server)->serve (server, error);\n}\n\nvoid\nthrift_server_stop (ThriftServer *server)\n{\n  THRIFT_SERVER_GET_CLASS (server)->stop (server);\n}\n\n\/* instance initializer for Thrift Server *\/\nstatic void\nthrift_server_init (ThriftServer *server)\n{\n  server->processor = NULL;\n  server->server_transport = NULL;\n  server->input_transport_factory = NULL;\n  server->output_transport_factory = NULL;\n  server->input_protocol_factory = NULL;\n  server->output_protocol_factory = NULL;\n}\n\n\/* class initializer for ThriftServer\n * TODO: implement ServerEventHandler as a GClosure\n *\/\nstatic void\nthrift_server_class_init (ThriftServerClass *cls)\n{\n  GObjectClass *gobject_class = G_OBJECT_CLASS (cls);\n\n  gobject_class->get_property = thrift_server_get_property;\n  gobject_class->set_property = thrift_server_set_property;\n\n  g_object_class_install_property (gobject_class,\n      PROP_THRIFT_SERVER_PROCESSOR,\n      g_param_spec_object (\"processor\", \"Processor\", \"Thrift Processor\",\n                           THRIFT_TYPE_PROCESSOR,\n                           G_PARAM_READWRITE | G_PARAM_CONSTRUCT_ONLY));\n  g_object_class_install_property (gobject_class,\n      PROP_THRIFT_SERVER_SERVER_TRANSPORT,\n      g_param_spec_object (\"server_transport\", \"Server Transport\",\n                           \"Thrift Server Transport\",\n                           THRIFT_TYPE_SERVER_TRANSPORT,\n                           G_PARAM_READWRITE | G_PARAM_CONSTRUCT_ONLY));\n  g_object_class_install_property (gobject_class,\n      PROP_THRIFT_SERVER_INPUT_TRANSPORT_FACTORY,\n      g_param_spec_object (\"input_transport_factory\", \"Input Transport Factory\",\n                           \"Thrift Server Input Transport Factory\",\n                           THRIFT_TYPE_TRANSPORT_FACTORY,\n                           G_PARAM_READWRITE | G_PARAM_CONSTRUCT_ONLY));\n  g_object_class_install_property (gobject_class,\n      PROP_THRIFT_SERVER_OUTPUT_TRANSPORT_FACTORY,\n      g_param_spec_object (\"output_transport_factory\",\n                           \"Output Transport Factory\",\n                           \"Thrift Server Output Transport Factory\",\n                           THRIFT_TYPE_TRANSPORT_FACTORY,\n                           G_PARAM_READWRITE | G_PARAM_CONSTRUCT_ONLY));\n  g_object_class_install_property (gobject_class,\n      PROP_THRIFT_SERVER_INPUT_PROTOCOL_FACTORY,\n      g_param_spec_object (\"input_protocol_factory\", \"Input Protocol Factory\",\n                           \"Thrift Server Input Protocol Factory\",\n                           THRIFT_TYPE_PROTOCOL_FACTORY,\n                           G_PARAM_READWRITE | G_PARAM_CONSTRUCT_ONLY));\n  g_object_class_install_property (gobject_class,\n      PROP_THRIFT_SERVER_OUTPUT_PROTOCOL_FACTORY,\n      g_param_spec_object (\"output_protocol_factory\", \"Output Protocol Factory\",\n                           \"Thrift Server Output Protocol Factory\",\n                           THRIFT_TYPE_PROTOCOL_FACTORY,\n                           G_PARAM_READWRITE | G_PARAM_CONSTRUCT_ONLY));\n\n  \/* set these as virtual methods to be implemented by a subclass *\/\n  cls->serve = thrift_server_serve;\n  cls->stop = thrift_server_stop;\n}\n","lang_cluster":"C","length":174,"code_uid":"3d716a28e25940d193f0ddcaa26d2815"}
{"diff_hunk":"@@ -33,6 +33,8 @@ class SkipListHandler:\n         Process the lines of the skip file.\n         \"\"\"\n         self.__skip = []\n+        if not skip_file_content:\n+            skip_file_content = \"\"\n \n         self.__skip_file_lines = [line.strip() for line\n                                   in skip_file_content.splitlines()","old_code":"# -------------------------------------------------------------------------\n#\n#  Part of the CodeChecker project, under the Apache License v2.0 with\n#  LLVM Exceptions. See LICENSE for license information.\n#  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n#\n# -------------------------------------------------------------------------\n\"\"\"\n\"\"\"\n\n\nimport fnmatch\nimport re\nimport os\n\nfrom codechecker_common.logger import get_logger\n\nLOG = get_logger('system')\n\n\nclass SkipListHandler:\n    \"\"\"\n    Skiplist file format:\n\n    -\/skip\/all\/source\/in\/directory*\n    -\/do\/not\/check\/this.file\n    +\/dir\/check.this.file\n    -\/dir\/*\n    \"\"\"\n\n    def __init__(self, skip_file_content=\"\"):\n        \"\"\"\n        Process the lines of the skip file.\n        \"\"\"\n        self.__skip = []\n\n        self.__skip_file_lines = [line.strip() for line\n                                  in skip_file_content.splitlines()\n                                  if line.strip()]\n\n        valid_lines = self.__check_line_format(self.__skip_file_lines)\n        self.__gen_regex(valid_lines)\n\n    def __gen_regex(self, skip_lines):\n        \"\"\"\n        Generate a regular expression from the given skip lines\n        and collect them for later match.\n\n        The lines should be checked for validity before generating\n        the regular expressions.\n        \"\"\"\n        for skip_line in skip_lines:\n            norm_skip_path = os.path.normpath(skip_line[1:].strip())\n            rexpr = re.compile(\n                fnmatch.translate(norm_skip_path + '*'))\n            self.__skip.append((skip_line, rexpr))\n\n    def __check_line_format(self, skip_lines):\n        \"\"\"\n        Check if the skip line is given in a valid format.\n        Returns the list of valid lines.\n        \"\"\"\n        valid_lines = []\n        for line in skip_lines:\n            if len(line) < 2 or line[0] not in ['-', '+']:\n                LOG.warning(\"Skipping malformed skipfile pattern: %s\", line)\n                continue\n\n            valid_lines.append(line)\n\n        return valid_lines\n\n    @property\n    def skip_file_lines(self):\n        \"\"\"\n        List of the lines from the skip file without changes.\n        \"\"\"\n        return self.__skip_file_lines\n\n    def overwrite_skip_content(self, skip_lines):\n        \"\"\"\n        Cleans out the already collected skip regular expressions\n        and rebuilds the list from the given skip_lines.\n        \"\"\"\n        self.__skip = []\n        valid_lines = self.__check_line_format(skip_lines)\n        self.__gen_regex(valid_lines)\n\n    def should_skip(self, source):\n        \"\"\"\n        Check if the given source should be skipped.\n        Should the analyzer skip the given source file?\n        \"\"\"\n        if not self.__skip:\n            return False\n\n        for line, rexpr in self.__skip:\n            if rexpr.match(source):\n                sign = line[0]\n                return sign == '-'\n        return False\n","lang_cluster":"C","length":101,"code_uid":"a89ab6e3c9b44da18989987a7167c21d"}
{"diff_hunk":"@@ -31,15 +31,19 @@\n typedef struct {\n   const char *name;\n   gboolean (*fn) (int argc, char **argv, GCancellable *cancellable, GError **error);\n+  const char *description;\n } OstreeAdminInstUtilCommand;\n \n static OstreeAdminInstUtilCommand admin_instutil_subcommands[] = {\n #ifdef HAVE_SELINUX\n-  { \"selinux-ensure-labeled\", ot_admin_instutil_builtin_selinux_ensure_labeled },\n+  { \"selinux-ensure-labeled\", ot_admin_instutil_builtin_selinux_ensure_labeled,\n+    \"relabel all or part of a deployment\" },\n #endif\n-  { \"set-kargs\", ot_admin_instutil_builtin_set_kargs },\n-  { \"grub2-generate\", ot_admin_instutil_builtin_grub2_generate },\n-  { NULL, NULL }\n+  { \"set-kargs\", ot_admin_instutil_builtin_set_kargs,\n+    \"set new kernel command line arguments(Not stable) \"  },\n+  { \"grub2-generate\", ot_admin_instutil_builtin_grub2_generate,\n+    \"generate GRUB2 configuration from given BLS entries\" },\n+  { NULL, NULL, NULL }\n };\n \n static GOptionContext *","old_code":"\/*\n * Copyright (C) 2011,2014 Colin Walters <walters@verbum.org>\n *\n * This library is free software; you can redistribute it and\/or\n * modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2 of the License, or (at your option) any later version.\n *\n * This library is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public\n * License along with this library; if not, write to the\n * Free Software Foundation, Inc., 59 Temple Place - Suite 330,\n * Boston, MA 02111-1307, USA.\n *\/\n\n#include \"config.h\"\n\n#include \"ot-main.h\"\n#include \"ot-builtins.h\"\n#include \"ot-admin-instutil-builtins.h\"\n#include \"ot-admin-builtins.h\"\n#include \"ot-admin-functions.h\"\n#include \"ostree.h\"\n\n#include <glib\/gi18n.h>\n\ntypedef struct {\n  const char *name;\n  gboolean (*fn) (int argc, char **argv, GCancellable *cancellable, GError **error);\n} OstreeAdminInstUtilCommand;\n\nstatic OstreeAdminInstUtilCommand admin_instutil_subcommands[] = {\n#ifdef HAVE_SELINUX\n  { \"selinux-ensure-labeled\", ot_admin_instutil_builtin_selinux_ensure_labeled },\n#endif\n  { \"set-kargs\", ot_admin_instutil_builtin_set_kargs },\n  { \"grub2-generate\", ot_admin_instutil_builtin_grub2_generate },\n  { NULL, NULL }\n};\n\nstatic GOptionContext *\nostree_admin_instutil_option_context_new_with_commands (void)\n{\n  OstreeAdminInstUtilCommand *command = admin_instutil_subcommands;\n  GOptionContext *context = g_option_context_new (\"COMMAND\");\n\n  g_autoptr(GString) summary = g_string_new (\"Builtin \\\"admin instutil\\\" Commands:\");\n\n  while (command->name != NULL)\n    {\n      g_string_append_printf (summary, \"\\n  %s\", command->name);\n      command++;\n    }\n\n  g_option_context_set_summary (context, summary->str);\n\n  return context;\n}\n\ngboolean\not_admin_builtin_instutil (int argc, char **argv, GCancellable *cancellable, GError **error)\n{\n  const char *subcommand_name = NULL;\n  int in, out;\n\n  for (in = 1, out = 1; in < argc; in++, out++)\n    {\n      \/* The non-option is the command, take it out of the arguments *\/\n      if (argv[in][0] != '-')\n        {\n          if (subcommand_name == NULL)\n            {\n              subcommand_name = argv[in];\n              out--;\n              continue;\n            }\n        }\n\n      else if (g_str_equal (argv[in], \"--\"))\n        {\n          break;\n        }\n\n      argv[out] = argv[in];\n    }\n\n  argc = out;\n\n  OstreeAdminInstUtilCommand *subcommand = admin_instutil_subcommands;\n  while (subcommand->name)\n    {\n      if (g_strcmp0 (subcommand_name, subcommand->name) == 0)\n        break;\n      subcommand++;\n    }\n\n  if (!subcommand->name)\n    {\n      g_autoptr(GOptionContext) context =\n        ostree_admin_instutil_option_context_new_with_commands ();\n\n      \/* This will not return for some options (e.g. --version). *\/\n      if (ostree_admin_option_context_parse (context, NULL, &argc, &argv,\n                                             OSTREE_ADMIN_BUILTIN_FLAG_NO_SYSROOT,\n                                             NULL, cancellable, error))\n        {\n          if (subcommand_name == NULL)\n            {\n              g_set_error_literal (error, G_IO_ERROR, G_IO_ERROR_FAILED,\n                                   \"No \\\"admin instutil\\\" subcommand specified\");\n            }\n          else\n            {\n              g_set_error (error, G_IO_ERROR, G_IO_ERROR_NOT_SUPPORTED,\n                           \"Unknown \\\"admin instutil\\\" subcommand '%s'\", subcommand_name);\n            }\n        }\n\n      g_autofree char *help = g_option_context_get_help (context, FALSE, NULL);\n      g_printerr (\"%s\", help);\n      return FALSE;\n    }\n\n  g_autofree char *prgname = g_strdup_printf (\"%s %s\", g_get_prgname (), subcommand_name);\n  g_set_prgname (prgname);\n\n  if (!subcommand->fn (argc, argv, cancellable, error))\n    return FALSE;\n\n  return TRUE;\n}\n","lang_cluster":"C","length":135,"code_uid":"3245d53fb9f74b16b8a2833e3af1feae"}
{"diff_hunk":"@@ -78,6 +78,7 @@ int syslog_prot_process(struct syslog_conn *conn)\n \n         \/* Incomplete message *\/\n         if (eof == end || (*eof != '\\n' && *eof != '\\0')) {\n+            flb_debug(\"[syslog-prot] incomplete message! (Enable trace log to see the message.)\");\n             return 0;\n         }\n ","old_code":"\/* -*- Mode: C; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- *\/\n\n\/*  Fluent Bit\n *  ==========\n *  Copyright (C) 2019      The Fluent Bit Authors\n *  Copyright (C) 2015-2018 Treasure Data Inc.\n *\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n *\/\n\n#include <string.h>\n\n#include <fluent-bit\/flb_info.h>\n#include <fluent-bit\/flb_parser.h>\n#include <fluent-bit\/flb_time.h>\n\n#include \"syslog.h\"\n#include \"syslog_conn.h\"\n\nstatic inline void consume_bytes(char *buf, int bytes, int length)\n{\n    memmove(buf, buf + bytes, length - bytes);\n}\n\nstatic inline int pack_line(struct flb_syslog *ctx,\n                            struct flb_time *time, char *data, size_t data_size)\n{\n    msgpack_packer mp_pck;\n    msgpack_sbuffer mp_sbuf;\n\n    \/* Initialize local msgpack buffer *\/\n    msgpack_sbuffer_init(&mp_sbuf);\n    msgpack_packer_init(&mp_pck, &mp_sbuf, msgpack_sbuffer_write);\n\n    msgpack_pack_array(&mp_pck, 2);\n    flb_time_append_to_msgpack(time, &mp_pck, 0);\n    msgpack_sbuffer_write(&mp_sbuf, data, data_size);\n\n    flb_input_chunk_append_raw(ctx->i_ins, NULL, 0, mp_sbuf.data, mp_sbuf.size);\n    msgpack_sbuffer_destroy(&mp_sbuf);\n\n    return 0;\n}\n\nint syslog_prot_process(struct syslog_conn *conn)\n{\n    int len;\n    int ret;\n    char *p;\n    char *eof;\n    char *end;\n    void *out_buf;\n    size_t out_size;\n    struct flb_time out_time;\n    struct flb_syslog *ctx = conn->ctx;\n\n    eof = p = conn->buf_data;\n    end = conn->buf_data + conn->buf_len;\n\n    \/* Always parse while some remaining bytes exists *\/\n    while (eof < end) {\n\n        \/* Lookup the ending byte *\/\n        eof = conn->buf_data + conn->buf_parsed;\n        while (*eof != '\\n' && *eof != '\\0' && eof < end) {\n            eof++;\n        }\n\n        \/* Incomplete message *\/\n        if (eof == end || (*eof != '\\n' && *eof != '\\0')) {\n            return 0;\n        }\n\n        \/* No data ? *\/\n        len = (eof - p);\n        if (len == 0) {\n            consume_bytes(conn->buf_data, 1, conn->buf_len);\n            conn->buf_len--;\n            conn->buf_parsed = 0;\n            conn->buf_data[conn->buf_len] = '\\0';\n            end = conn->buf_data + conn->buf_len;\n\n            if (conn->buf_len == 0) {\n                return 0;\n            }\n\n            continue;\n        }\n\n        \/* Process the string *\/\n        ret = flb_parser_do(ctx->parser, p, len,\n                            &out_buf, &out_size, &out_time);\n        if (ret >= 0) {\n            pack_line(ctx, &out_time, out_buf, out_size);\n            flb_free(out_buf);\n        }\n        else {\n            flb_warn(\"[in_syslog] error parsing log message\");\n        }\n\n        conn->buf_parsed += len + 1;\n        end = conn->buf_data + conn->buf_len;\n        eof = p = conn->buf_data + conn->buf_parsed;\n    }\n\n    consume_bytes(conn->buf_data, conn->buf_parsed, conn->buf_len);\n    conn->buf_len -= conn->buf_parsed;\n    conn->buf_parsed = 0;\n    conn->buf_data[conn->buf_len] = '\\0';\n\n    return 0;\n}\n\nint syslog_prot_process_udp(char *buf, size_t size, struct flb_syslog *ctx)\n{\n    int ret;\n    void *out_buf;\n    size_t out_size;\n    struct flb_time out_time = {0};\n\n    ret = flb_parser_do(ctx->parser, buf, size,\n                        &out_buf, &out_size, &out_time);\n    if (ret >= 0) {\n        if (flb_time_to_double(&out_time) == 0) {\n            flb_time_get(&out_time);\n        }\n        pack_line(ctx, &out_time, out_buf, out_size);\n        flb_free(out_buf);\n    }\n    else {\n        flb_warn(\"[in_syslog] error parsing log message\");\n        return -1;\n    }\n\n    return 0;\n}\n","lang_cluster":"C","length":146,"code_uid":"12f78acfe39742c9947af9d11bae4015"}
{"diff_hunk":"@@ -28,47 +28,27 @@ namespace Thrift.Transport.Server\n     public class TServerSocketTransport : TServerTransport\n     {\n         private readonly int _clientTimeout;\n-        private readonly int _port;\n         private readonly bool _useBufferedSockets;\n         private readonly bool _useFramedTransport;\n         private TcpListener _server;\n \n-        public TServerSocketTransport(TcpListener listener)\n-            : this(listener, 0)\n-        {\n-        }\n-\n-        public TServerSocketTransport(TcpListener listener, int clientTimeout)\n+        public TServerSocketTransport(TcpListener listener, int clientTimeout = 0, bool useBufferedSockets = false, bool useFramedTransport = false)\n         {\n             _server = listener;\n             _clientTimeout = clientTimeout;\n+            _useBufferedSockets = useBufferedSockets;\n+            _useFramedTransport = useFramedTransport;\n         }\n \n-        public TServerSocketTransport(int port)\n-            : this(port, 0)\n-        {\n-        }\n-\n-        public TServerSocketTransport(int port, int clientTimeout)\n-            : this(port, clientTimeout, false)\n-        {\n-        }\n-\n-        public TServerSocketTransport(int port, int clientTimeout, bool useBufferedSockets):\n-            this(port, clientTimeout, useBufferedSockets, false)\n-        {\n-        }\n-        \n-        public TServerSocketTransport(int port, int clientTimeout, bool useBufferedSockets, bool useFramedTransport)\n+        public TServerSocketTransport(int port, int clientTimeout = 0, bool useBufferedSockets = false, bool useFramedTransport = false)\n         {\n-            _port = port;\n             _clientTimeout = clientTimeout;\n             _useBufferedSockets = useBufferedSockets;\n             _useFramedTransport = useFramedTransport;\n             try\n             {\n                 \/\/ Make server socket\n-                _server = new TcpListener(IPAddress.Any, _port);\n+                _server = new TcpListener(IPAddress.Any, port);\n                 _server.Server.NoDelay = true;\n             }\n             catch (Exception)","old_code":"\/\/ Licensed to the Apache Software Foundation(ASF) under one\n\/\/ or more contributor license agreements.See the NOTICE file\n\/\/ distributed with this work for additional information\n\/\/ regarding copyright ownership.The ASF licenses this file\n\/\/ to you under the Apache License, Version 2.0 (the\n\/\/ \"License\"); you may not use this file except in compliance\n\/\/ with the License. You may obtain a copy of the License at\n\/\/ \n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/ \n\/\/ Unless required by applicable law or agreed to in writing,\n\/\/ software distributed under the License is distributed on an\n\/\/ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\/\/ KIND, either express or implied. See the License for the\n\/\/ specific language governing permissions and limitations\n\/\/ under the License.\n\nusing System;\nusing System.Net;\nusing System.Net.Sockets;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Thrift.Transport.Client;\n\nnamespace Thrift.Transport.Server\n{\n    \/\/ ReSharper disable once InconsistentNaming\n    public class TServerSocketTransport : TServerTransport\n    {\n        private readonly int _clientTimeout;\n        private readonly int _port;\n        private readonly bool _useBufferedSockets;\n        private readonly bool _useFramedTransport;\n        private TcpListener _server;\n\n        public TServerSocketTransport(TcpListener listener)\n            : this(listener, 0)\n        {\n        }\n\n        public TServerSocketTransport(TcpListener listener, int clientTimeout)\n        {\n            _server = listener;\n            _clientTimeout = clientTimeout;\n        }\n\n        public TServerSocketTransport(int port)\n            : this(port, 0)\n        {\n        }\n\n        public TServerSocketTransport(int port, int clientTimeout)\n            : this(port, clientTimeout, false)\n        {\n        }\n\n        public TServerSocketTransport(int port, int clientTimeout, bool useBufferedSockets):\n            this(port, clientTimeout, useBufferedSockets, false)\n        {\n        }\n        \n        public TServerSocketTransport(int port, int clientTimeout, bool useBufferedSockets, bool useFramedTransport)\n        {\n            _port = port;\n            _clientTimeout = clientTimeout;\n            _useBufferedSockets = useBufferedSockets;\n            _useFramedTransport = useFramedTransport;\n            try\n            {\n                \/\/ Make server socket\n                _server = new TcpListener(IPAddress.Any, _port);\n                _server.Server.NoDelay = true;\n            }\n            catch (Exception)\n            {\n                _server = null;\n                throw new TTransportException(\"Could not create ServerSocket on port \" + port + \".\");\n            }\n        }\n\n        public override void Listen()\n        {\n            \/\/ Make sure not to block on accept\n            if (_server != null)\n            {\n                try\n                {\n                    _server.Start();\n                }\n                catch (SocketException sx)\n                {\n                    throw new TTransportException(\"Could not accept on listening socket: \" + sx.Message);\n                }\n            }\n        }\n\n        public override bool IsClientPending()\n        {\n            return _server.Pending();\n        }\n\n        protected override async Task<TTransport> AcceptImplementationAsync(CancellationToken cancellationToken)\n        {\n            if (cancellationToken.IsCancellationRequested)\n            {\n                return await Task.FromCanceled<TTransport>(cancellationToken);\n            }\n\n            if (_server == null)\n            {\n                throw new TTransportException(TTransportException.ExceptionType.NotOpen, \"No underlying server socket.\");\n            }\n\n            try\n            {\n                TTransport tSocketTransport = null;\n                var tcpClient = await _server.AcceptTcpClientAsync();\n\n                try\n                {\n                    tSocketTransport = new TSocketTransport(tcpClient)\n                    {\n                        Timeout = _clientTimeout\n                    };\n\n                    if (_useBufferedSockets)\n                    {\n                        tSocketTransport = new TBufferedTransport(tSocketTransport);\n                    }\n\n                    if (_useFramedTransport)\n                    {\n                        tSocketTransport = new TFramedTransport(tSocketTransport);\n                    }\n\n                    return tSocketTransport;\n                }\n                catch (Exception)\n                {\n                    if (tSocketTransport != null)\n                    {\n                        tSocketTransport.Dispose();\n                    }\n                    else \/\/  Otherwise, clean it up ourselves.\n                    {\n                        ((IDisposable) tcpClient).Dispose();\n                    }\n\n                    throw;\n                }\n            }\n            catch (Exception ex)\n            {\n                throw new TTransportException(ex.ToString());\n            }\n        }\n\n        public override void Close()\n        {\n            if (_server != null)\n            {\n                try\n                {\n                    _server.Stop();\n                }\n                catch (Exception ex)\n                {\n                    throw new TTransportException(\"WARNING: Could not close server socket: \" + ex);\n                }\n                _server = null;\n            }\n        }\n    }\n}","lang_cluster":"C","length":174,"code_uid":"5e3500b57f1b4a6c8042aab972f17818"}
{"diff_hunk":"@@ -27,7 +27,8 @@ class ClangTidyConfigHandler(config_handler.AnalyzerConfigHandler):\n         \"\"\"\n         Enable checker, keep description if already set.\n         \"\"\"\n-        if checker_name.startswith(\"Wno-\") or checker_name.startswith(\"W\"):\n+        if checker_name.startswith('W') or \\\n+           checker_name.startswith('clang-diagnostic-'):\n             self.add_checker(checker_name)\n \n         super(ClangTidyConfigHandler, self).set_checker_enabled(checker_name,","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\nConfig handler for Clang Tidy analyzer.\n\"\"\"\n\n\nfrom codechecker_common.logger import get_logger\n\nfrom .. import config_handler\n\nLOG = get_logger('analyzer.tidy')\n\n\nclass ClangTidyConfigHandler(config_handler.AnalyzerConfigHandler):\n    \"\"\"\n    Configuration handler for Clang-tidy analyzer.\n    \"\"\"\n\n    def __init__(self):\n        super(ClangTidyConfigHandler, self).__init__()\n\n    def set_checker_enabled(self, checker_name, enabled=True):\n        \"\"\"\n        Enable checker, keep description if already set.\n        \"\"\"\n        if checker_name.startswith(\"Wno-\") or checker_name.startswith(\"W\"):\n            self.add_checker(checker_name)\n\n        super(ClangTidyConfigHandler, self).set_checker_enabled(checker_name,\n                                                                enabled)\n","lang_cluster":"C","length":34,"code_uid":"0fd87662dba34276bd55aabf83cd7bee"}
{"diff_hunk":"@@ -54,6 +54,7 @@ void handle_keyboard_key(struct libinput_event *event,\n \tstruct libinput_event_keyboard *kbevent =\n \t\tlibinput_event_get_keyboard_event(event);\n \tstruct wlr_event_keyboard_key wlr_event = { 0 };\n+\twlr_event.device = wlr_dev;\n \twlr_event.time_msec =\n \t\tusec_to_msec(libinput_event_keyboard_get_time_usec(kbevent));\n \twlr_event.keycode = libinput_event_keyboard_get_key(kbevent);","old_code":"#include <stdlib.h>\n#include <assert.h>\n#include <libinput.h>\n#include <wlr\/backend\/session.h>\n#include <wlr\/types\/wlr_input_device.h>\n#include <wlr\/interfaces\/wlr_keyboard.h>\n#include <wlr\/util\/log.h>\n#include \"backend\/libinput.h\"\n\nstruct wlr_libinput_keyboard {\n\tstruct wlr_keyboard wlr_keyboard;\n\tstruct libinput_device *libinput_dev;\n};\n\nstatic void wlr_libinput_keyboard_set_leds(struct wlr_keyboard *wlr_kb, uint32_t leds) {\n\tstruct wlr_libinput_keyboard *wlr_libinput_kb = (struct wlr_libinput_keyboard *)wlr_kb;\n\tlibinput_device_led_update(wlr_libinput_kb->libinput_dev, leds);\n}\n\nstatic void wlr_libinput_keyboard_destroy(struct wlr_keyboard *wlr_kb) {\n\tstruct wlr_libinput_keyboard *wlr_libinput_kb =\n\t\t(struct wlr_libinput_keyboard *)wlr_kb;\n\tlibinput_device_unref(wlr_libinput_kb->libinput_dev);\n}\n\nstruct wlr_keyboard_impl impl = {\n\t.destroy = wlr_libinput_keyboard_destroy,\n\t.led_update = wlr_libinput_keyboard_set_leds\n};\n\nstruct wlr_keyboard *wlr_libinput_keyboard_create(\n\t\tstruct libinput_device *libinput_dev) {\n\tassert(libinput_dev);\n\tstruct wlr_libinput_keyboard *wlr_libinput_kb;\n\tif (!(wlr_libinput_kb= calloc(1, sizeof(struct wlr_libinput_keyboard)))) {\n\t\treturn NULL;\n\t}\n\twlr_libinput_kb->libinput_dev = libinput_dev;\n\tlibinput_device_ref(libinput_dev);\n\tlibinput_device_led_update(libinput_dev, 0);\n\tstruct wlr_keyboard *wlr_kb = &wlr_libinput_kb->wlr_keyboard;\n\twlr_keyboard_init(wlr_kb, &impl);\n\treturn wlr_kb;\n}\n\nvoid handle_keyboard_key(struct libinput_event *event,\n\t\tstruct libinput_device *libinput_dev) {\n\tstruct wlr_input_device *wlr_dev =\n\t\tget_appropriate_device(WLR_INPUT_DEVICE_KEYBOARD, libinput_dev);\n\tif (!wlr_dev) {\n\t\twlr_log(L_DEBUG, \"Got a keyboard event for a device with no keyboards?\");\n\t\treturn;\n\t}\n\tstruct libinput_event_keyboard *kbevent =\n\t\tlibinput_event_get_keyboard_event(event);\n\tstruct wlr_event_keyboard_key wlr_event = { 0 };\n\twlr_event.time_msec =\n\t\tusec_to_msec(libinput_event_keyboard_get_time_usec(kbevent));\n\twlr_event.keycode = libinput_event_keyboard_get_key(kbevent);\n\tenum libinput_key_state state = \n\t\tlibinput_event_keyboard_get_key_state(kbevent);\n\tswitch (state) {\n\tcase LIBINPUT_KEY_STATE_RELEASED:\n\t\twlr_event.state = WLR_KEY_RELEASED;\n\t\tbreak;\n\tcase LIBINPUT_KEY_STATE_PRESSED:\n\t\twlr_event.state = WLR_KEY_PRESSED;\n\t\tbreak;\n\t}\n\twlr_event.update_state = true;\n\twlr_keyboard_notify_key(wlr_dev->keyboard, &wlr_event);\n}\n","lang_cluster":"C","length":72,"code_uid":"3f9857b2e2d744f392e8700018e9db0f"}
{"diff_hunk":"@@ -105,21 +105,14 @@ func createNetworkPolicy(clientset *kubernetes.Clientset, namespace string) {\n \t\t\tName:      \"test-syncer-basic-net-policy\",\n \t\t},\n \t\tSpec: networkingv1.NetworkPolicySpec{\n-\t\t\tPodSelector: metav1.LabelSelector{\n-\t\t\t\tMatchLabels: map[string]string{\"calico\/k8s_ns\": namespace},\n-\t\t\t},\n+\t\t\t\/\/ An empty PodSelector selects all pods in this Namespace.\n+\t\t\tPodSelector: metav1.LabelSelector{},\n \t\t\tIngress: []networkingv1.NetworkPolicyIngressRule{\n \t\t\t\tnetworkingv1.NetworkPolicyIngressRule{\n-\t\t\t\t\tPorts: []networkingv1.NetworkPolicyPort{\n-\t\t\t\t\t\tnetworkingv1.NetworkPolicyPort{},\n-\t\t\t\t\t},\n \t\t\t\t\tFrom: []networkingv1.NetworkPolicyPeer{\n \t\t\t\t\t\tnetworkingv1.NetworkPolicyPeer{\n-\t\t\t\t\t\t\tPodSelector: &metav1.LabelSelector{\n-\t\t\t\t\t\t\t\tMatchLabels: map[string]string{\n-\t\t\t\t\t\t\t\t\t\"calico\/k8s_ns\": namespace,\n-\t\t\t\t\t\t\t\t},\n-\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t\/\/ An empty PodSelector selects all pods in this Namespace.\n+\t\t\t\t\t\t\tPodSelector: &metav1.LabelSelector{},\n \t\t\t\t\t\t},\n \t\t\t\t\t},\n \t\t\t\t},","old_code":"\/\/ Copyright (c) 2017 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage main\n\nimport (\n\t\"encoding\/json\"\n\t\"fmt\"\n\t\"strings\"\n\n\tlog \"github.com\/sirupsen\/logrus\"\n\t\"k8s.io\/api\/core\/v1\"\n\tnetworkingv1 \"k8s.io\/api\/networking\/v1\"\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n\t\"k8s.io\/client-go\/kubernetes\"\n)\n\nvar nsPrefixNum = 0\n\nfunc getNamespacePrefix() (nsPrefix string) {\n\tnsPrefixNum++\n\tnsPrefix = fmt.Sprintf(\"ns%d-\", nsPrefixNum)\n\treturn\n}\n\ntype namespacePolicy struct {\n\tIngress struct {\n\t\tIsolation string `json:\"isolation\"`\n\t} `json:\"ingress\"`\n}\n\nfunc createNamespace(clientset *kubernetes.Clientset, name string, labels map[string]string) {\n\tcreateNamespaceInt(clientset, name, labels, \"\")\n}\n\nfunc createIsolatedNamespace(clientset *kubernetes.Clientset, name string, labels map[string]string) {\n\tcreateNamespaceInt(clientset, name, labels, \"DefaultDeny\")\n}\n\nfunc createNamespaceInt(\n\tclientset *kubernetes.Clientset,\n\tname string,\n\tlabels map[string]string,\n\tisolation string,\n) {\n\tns_in := &v1.Namespace{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:   name,\n\t\t\tLabels: labels,\n\t\t},\n\t}\n\tif isolation != \"\" {\n\t\tnp := namespacePolicy{}\n\t\tnp.Ingress.Isolation = isolation\n\t\tannotation, _ := json.Marshal(np)\n\t\tns_in.ObjectMeta.Annotations = map[string]string{\n\t\t\t\"net.beta.kubernetes.io\/network-policy\": string(annotation),\n\t\t}\n\t}\n\tlog.WithField(\"ns_in\", ns_in).Debug(\"Namespace defined\")\n\tns_out, err := clientset.CoreV1().Namespaces().Create(ns_in)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"ns_out\", ns_out).Debug(\"Created namespace\")\n}\n\nfunc cleanupAllNamespaces(clientset *kubernetes.Clientset, nsPrefix string) {\n\tlog.Info(\"Cleaning up all namespaces...\")\n\tnsList, err := clientset.CoreV1().Namespaces().List(metav1.ListOptions{})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"count\", len(nsList.Items)).Info(\"Namespaces present\")\n\tfor _, ns := range nsList.Items {\n\t\tif strings.HasPrefix(ns.ObjectMeta.Name, nsPrefix) {\n\t\t\terr = clientset.CoreV1().Namespaces().Delete(ns.ObjectMeta.Name, deleteImmediately)\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t} else {\n\t\t\tlog.WithField(\"name\", ns.ObjectMeta.Name).Debug(\"Namespace skipped\")\n\t\t}\n\t}\n\tlog.Info(\"Cleaned up all namespaces\")\n}\n\n\/\/ Create a NetworkPolicy, for pods in the specified namespace, that allows ingress from other pods\n\/\/ in the same namespace.\nfunc createNetworkPolicy(clientset *kubernetes.Clientset, namespace string) {\n\tnp := networkingv1.NetworkPolicy{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tNamespace: namespace,\n\t\t\tName:      \"test-syncer-basic-net-policy\",\n\t\t},\n\t\tSpec: networkingv1.NetworkPolicySpec{\n\t\t\tPodSelector: metav1.LabelSelector{\n\t\t\t\tMatchLabels: map[string]string{\"calico\/k8s_ns\": namespace},\n\t\t\t},\n\t\t\tIngress: []networkingv1.NetworkPolicyIngressRule{\n\t\t\t\tnetworkingv1.NetworkPolicyIngressRule{\n\t\t\t\t\tPorts: []networkingv1.NetworkPolicyPort{\n\t\t\t\t\t\tnetworkingv1.NetworkPolicyPort{},\n\t\t\t\t\t},\n\t\t\t\t\tFrom: []networkingv1.NetworkPolicyPeer{\n\t\t\t\t\t\tnetworkingv1.NetworkPolicyPeer{\n\t\t\t\t\t\t\tPodSelector: &metav1.LabelSelector{\n\t\t\t\t\t\t\t\tMatchLabels: map[string]string{\n\t\t\t\t\t\t\t\t\t\"calico\/k8s_ns\": namespace,\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\tclientset.NetworkingV1().NetworkPolicies(\"\").Create(&np)\n}\n","lang_cluster":"C","length":130,"code_uid":"2b6cd8ad96484efcb694576e4b87942d"}
{"diff_hunk":"@@ -32,8 +32,10 @@ bool wlr_backend_start(struct wlr_backend *backend) {\n }\n \n void wlr_backend_destroy(struct wlr_backend *backend) {\n-\tif (backend->impl->destroy) {\n+\tif (backend->impl && backend->impl->destroy) {\n \t\tbackend->impl->destroy(backend);\n+\t} else {\n+\t\tfree(backend);\n \t}\n }\n ","old_code":"#include <wayland-server.h>\n#include <unistd.h>\n#include <stdlib.h>\n#include <string.h>\n#include <errno.h>\n#include <assert.h>\n#include <libinput.h>\n#include <wlr\/backend\/session.h>\n#include <wlr\/backend\/interface.h>\n#include <wlr\/backend\/drm.h>\n#include <wlr\/backend\/libinput.h>\n#include <wlr\/backend\/wayland.h>\n#include <wlr\/backend\/multi.h>\n#include <wlr\/util\/log.h>\n#include \"backend\/udev.h\"\n\nvoid wlr_backend_init(struct wlr_backend *backend,\n\t\tconst struct wlr_backend_impl *impl) {\n\tassert(backend);\n\tbackend->impl = impl;\n\twl_signal_init(&backend->events.input_add);\n\twl_signal_init(&backend->events.input_remove);\n\twl_signal_init(&backend->events.output_add);\n\twl_signal_init(&backend->events.output_remove);\n}\n\nbool wlr_backend_start(struct wlr_backend *backend) {\n\tif (backend->impl->start) {\n\t\treturn backend->impl->start(backend);\n\t}\n\treturn true;\n}\n\nvoid wlr_backend_destroy(struct wlr_backend *backend) {\n\tif (backend->impl->destroy) {\n\t\tbackend->impl->destroy(backend);\n\t}\n}\n\nstruct wlr_egl *wlr_backend_get_egl(struct wlr_backend *backend) {\n\tif (backend->impl->get_egl) {\n\t\treturn backend->impl->get_egl(backend);\n\t}\n\treturn NULL;\n}\n\nstatic struct wlr_backend *attempt_wl_backend(struct wl_display *display) {\n\tstruct wlr_backend *backend = wlr_wl_backend_create(display);\n\tif (backend) {\n\t\tint outputs = 1;\n\t\tconst char *_outputs = getenv(\"WLR_WL_OUTPUTS\");\n\t\tif (_outputs) {\n\t\t\tchar *end;\n\t\t\toutputs = (int)strtol(_outputs, &end, 10);\n\t\t\tif (*end) {\n\t\t\t\twlr_log(L_ERROR, \"WLR_WL_OUTPUTS specified with invalid integer, ignoring\");\n\t\t\t\toutputs = 1;\n\t\t\t} else if (outputs < 0) {\n\t\t\t\twlr_log(L_ERROR, \"WLR_WL_OUTPUTS specified with negative outputs, ignoring\");\n\t\t\t\toutputs = 1;\n\t\t\t}\n\t\t}\n\t\twhile (outputs--) {\n\t\t\twlr_wl_output_create(backend);\n\t\t}\n\t}\n\treturn backend;\n}\n\nstruct wlr_backend *wlr_backend_autocreate(struct wl_display *display) {\n\tstruct wlr_backend *backend;\n\tif (getenv(\"WAYLAND_DISPLAY\") || getenv(\"_WAYLAND_DISPLAY\")) {\n\t\tbackend = attempt_wl_backend(display);\n\t\tif (backend) {\n\t\t\treturn backend;\n\t\t}\n\t}\n\n\tif (getenv(\"DISPLAY\")) {\n\t\twlr_log(L_ERROR, \"X11 backend is not implemented\"); \/\/ TODO\n\t\treturn NULL;\n\t}\n\n\t\/\/ Attempt DRM+libinput\n\n\tstruct wlr_session *session = wlr_session_start(display);\n\tif (!session) {\n\t\twlr_log(L_ERROR, \"Failed to start a DRM session\");\n\t\treturn NULL;\n\t}\n\n\tstruct wlr_udev *udev = wlr_udev_create(display);\n\tif (!udev) {\n\t\twlr_log(L_ERROR, \"Failed to start udev\");\n\t\tgoto error_session;\n\t}\n\n\tint gpu = wlr_udev_find_gpu(udev, session);\n\tif (gpu == -1) {\n\t\twlr_log(L_ERROR, \"Failed to open DRM device\");\n\t\tgoto error_udev;\n\t}\n\n\tbackend = wlr_multi_backend_create(session, udev);\n\tif (!backend) {\n\t\tgoto error_gpu;\n\t}\n\n\tstruct wlr_backend *libinput = wlr_libinput_backend_create(display, session, udev);\n\tif (!libinput) {\n\t\tgoto error_multi;\n\t}\n\n\tstruct wlr_backend *drm = wlr_drm_backend_create(display, session, udev, gpu);\n\tif (!drm) {\n\t\tgoto error_libinput;\n\t}\n\n\twlr_multi_backend_add(backend, libinput);\n\twlr_multi_backend_add(backend, drm);\n\treturn backend;\n\nerror_libinput:\n\twlr_backend_destroy(libinput);\nerror_multi:\n\twlr_backend_destroy(backend);\nerror_gpu:\n\twlr_session_close_file(session, gpu);\nerror_udev:\n\twlr_udev_destroy(udev);\nerror_session:\n\twlr_session_finish(session);\n\treturn NULL;\n}\n","lang_cluster":"C","length":134,"code_uid":"e1e5d59004bd473c9ca7a19deb930975"}
{"diff_hunk":"@@ -50,6 +50,24 @@\n -- @see rule\n -- @see except\n \n+--- A table which content will be compared to the target object current properties.\n+--\n+-- The comparison will be made using the lesser (`<`) operator.\n+--\n+-- @rulecomponent rule_lesser\n+-- @param table\n+-- @see rule\n+-- @see except\n+\n+--- A table which content will be compared to the target object current properties.\n+--\n+-- The comparison will be made using the greater (`>`) operator.\n+--\n+-- @rulecomponent rule_greater\n+-- @param table\n+-- @see rule\n+-- @see except\n+\n --- An identifier for this rule.\n --\n -- It can be anything. It will be compared with the `==` operator. Strings are","old_code":"---\n\n--- A table which content will be used to set the target object properties.\n--\n-- @rulecomponent properties\n-- @param table\n-- @see callbacks\n\n--TODO add ^\n-- @DOC_text_gears_matcher_properties_EXAMPLE@\n\n--- A list of callback function to call *after* the properties have been apploed.\n-- @rulecomponent callbacks\n-- @param table\n-- @see properties\n\n--- A table which content will be compared to the target object current properties.\n--\n-- @rulecomponent rule\n-- @param table\n-- @see rule_any\n-- @see except\n\n--- Similar to `rule`, but each entry is a table with multiple values.\n--\n--\n-- @rulecomponent rule_any\n-- @param table\n-- @see rule\n-- @see except_any\n\n--- The negative equivalent of `rule`.\n--\n-- @rulecomponent except\n-- @param table\n-- @see rule\n-- @see except_any\n\n--- The negative equivalent of `rule_any`.\n--\n-- @rulecomponent except_any\n-- @param table\n-- @see rule\n-- @see except\n\n--- Matches when one of every \"category\" of components match.\n--\n-- @rulecomponent rule_every\n-- @param table\n-- @see rule\n-- @see except\n\n--- An identifier for this rule.\n--\n-- It can be anything. It will be compared with the `==` operator. Strings are\n-- highly recommended.\n--\n-- Setting an `id` is useful to be able to remove the rule by using its id\n-- instead of a table reference. Modules can also listen to `rule::appended` and\n-- modify or disable a rule.\n--\n-- @rulecomponent id\n-- @param table|string|number|function\n","lang_cluster":"C","length":63,"code_uid":"97e2dffeeda4454c9969e22a9e20cbdb"}
{"diff_hunk":"@@ -74,6 +74,12 @@ proc_num_simd_saved(void)\n     return num_simd_saved;\n }\n \n+void\n+proc_set_num_simd_saved(int num)\n+{\n+    num_simd_saved = num;\n+}\n+\n DR_API\n int\n proc_num_simd_registers(void)","old_code":"\/* **********************************************************\n * Copyright (c) 2016 ARM Limited. All rights reserved.\n * **********************************************************\/\n\n\/*\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * * Redistributions of source code must retain the above copyright notice,\n *   this list of conditions and the following disclaimer.\n *\n * * Redistributions in binary form must reproduce the above copyright notice,\n *   this list of conditions and the following disclaimer in the documentation\n *   and\/or other materials provided with the distribution.\n *\n * * Neither the name of ARM Limited nor the names of its contributors may be\n *   used to endorse or promote products derived from this software without\n *   specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL ARM LIMITED OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n *\/\n\n#include \"..\/globals.h\"\n#include \"proc.h\"\n#include \"instr.h\"\n\nstatic int num_simd_saved;\nstatic int num_simd_registers;\n\nvoid\nproc_init_arch(void)\n{\n    num_simd_saved = MCXT_NUM_SIMD_SLOTS;\n    num_simd_registers = MCXT_NUM_SIMD_SLOTS;\n\n    \/* FIXME i#1569: NYI *\/\n}\n\nbool\nproc_has_feature(feature_bit_t f)\n{\n    ASSERT_NOT_IMPLEMENTED(false); \/* FIXME i#1569 *\/\n    return false;\n}\n\nvoid\nmachine_cache_sync(void *pc_start, void *pc_end, bool flush_icache)\n{\n    clear_icache(pc_start, pc_end);\n}\n\nDR_API\nsize_t\nproc_fpstate_save_size(void)\n{\n    ASSERT_NOT_IMPLEMENTED(false); \/* FIXME i#1569 *\/\n    return 0;\n}\n\nDR_API\nint\nproc_num_simd_saved(void)\n{\n    return num_simd_saved;\n}\n\nDR_API\nint\nproc_num_simd_registers(void)\n{\n    return num_simd_registers;\n}\n\nint\nproc_num_simd_sse_avx_registers(void)\n{\n    CLIENT_ASSERT(false, \"Incorrect usage for ARM\/AArch64.\");\n    return 0;\n}\n\nint\nproc_num_simd_sse_avx_saved(void)\n{\n    CLIENT_ASSERT(false, \"Incorrect usage for ARM\/AArch64.\");\n    return 0;\n}\n\nDR_API\nsize_t\nproc_save_fpstate(byte *buf)\n{\n    \/* All registers are saved by insert_push_all_registers so nothing extra\n     * needs to be saved here.\n     *\/\n    return DR_FPSTATE_BUF_SIZE;\n}\n\nDR_API\nvoid\nproc_restore_fpstate(byte *buf)\n{\n    \/* Nothing to restore. *\/\n}\n\nvoid\ndr_insert_save_fpstate(void *drcontext, instrlist_t *ilist, instr_t *where, opnd_t buf)\n{\n    ASSERT_NOT_IMPLEMENTED(false); \/* FIXME i#1569 *\/\n}\n\nvoid\ndr_insert_restore_fpstate(void *drcontext, instrlist_t *ilist, instr_t *where, opnd_t buf)\n{\n    ASSERT_NOT_IMPLEMENTED(false); \/* FIXME i#1569 *\/\n}\n\nuint64\nproc_get_timestamp(void)\n{\n    ASSERT_NOT_IMPLEMENTED(false); \/* FIXME i#1569 *\/\n    return 0;\n}\n","lang_cluster":"C","length":132,"code_uid":"93e1786ac72a4d1d8d24430b4e0f3521"}
{"diff_hunk":"@@ -41,6 +41,7 @@ def add_verbose_arguments(parser):\n \n # ------------------------------------------------------------------------------\n logging.DEBUG_ANALYZER = 15\n+DEBUG_ANALYZER = logging.DEBUG_ANALYZER\n logging.addLevelName(logging.DEBUG_ANALYZER, 'DEBUG_ANALYZER')\n \n ","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\n\"\"\"\n\nimport logging\nimport os\nimport sys\nimport time\n\n# The logging leaves can be accesses without\n# importing the logging module in other modules.\nDEBUG = logging.DEBUG\nINFO = logging.INFO\nWARNING = logging.WARNING\nERROR = logging.ERROR\nCRITICAL = logging.CRITICAL\nNOTSET = logging.NOTSET\n\n\nclass BColors(object):\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n\n\ndef add_verbose_arguments(parser):\n    \"\"\"\n    Verbosity level arguments.\n    \"\"\"\n    parser.add_argument('--verbose', type=str, dest='verbose',\n                        choices=['info', 'debug', 'debug_analyzer'],\n                        default='info',\n                        help='Set verbosity level.')\n\n# ------------------------------------------------------------------------------\nlogging.DEBUG_ANALYZER = 15\nlogging.addLevelName(logging.DEBUG_ANALYZER, 'DEBUG_ANALYZER')\n\n\nclass CCLogger(logging.Logger):\n    def __init__(self, name, level=NOTSET):\n        super(CCLogger, self).__init__(name, level)\n\n    def debug_analyzer(self, msg, *args, **kwargs):\n        if self.isEnabledFor(logging.DEBUG_ANALYZER):\n            self._log(logging.DEBUG_ANALYZER, msg, args, **kwargs)\n\n\nlogging.setLoggerClass(CCLogger)\n\n\nclass CustomFormatter(logging.Formatter):\n    \"\"\"\n    Custom formatter to print log level in case of ERROR, WARNING\n    or CRITICAL.\n    \"\"\"\n\n    info_fmt = '[%(asctime)s] - %(message)s'\n    error_fmt = '[%(levelname)s] [%(asctime)s] - %(message)s'\n    debug_fmt = '[%(asctime)s] [%(process)d] <%(thread)d> - ' \\\n        '%(filename)s:%(lineno)d %(funcName)s() - %(message)s'\n\n    def formatTime(self, record, datefmt=None):\n        if LoggerFactory.log_level == logging.DEBUG or \\\n                LoggerFactory.log_level == logging.DEBUG_ANALYZER:\n            return time.strftime('%H:%M:%S')\n        else:\n            return time.strftime('%H:%M')\n\n    def format(self, record):\n\n        # Save the original format\n        format_orig = self._fmt\n\n        # Replace the original format\n        if record.levelno == logging.DEBUG:\n            self._fmt = CustomFormatter.debug_fmt\n        if record.levelno == logging.DEBUG_ANALYZER:\n            self._fmt = CustomFormatter.debug_fmt\n        elif record.levelno == logging.INFO:\n            self._fmt = CustomFormatter.info_fmt\n        elif record.levelno == logging.ERROR:\n            self._fmt = CustomFormatter.error_fmt\n        elif record.levelno == logging.WARNING:\n            self._fmt = CustomFormatter.error_fmt\n        elif record.levelno == logging.CRITICAL:\n            self._fmt = CustomFormatter.error_fmt\n\n        result = logging.Formatter.format(self, record)\n\n        self._fmt = format_orig\n\n        return result\n\n\nclass LoggerFactory(object):\n    log_level = logging.INFO\n    loggers = []\n\n    short_format_handler = logging.StreamHandler(stream=sys.stdout)\n    long_format_handler = logging.StreamHandler(stream=sys.stdout)\n\n    short_format_handler.setFormatter(CustomFormatter())\n    long_format_handler.setFormatter(CustomFormatter())\n\n    handlers = {\n        logging.INFO: short_format_handler,\n        logging.DEBUG: long_format_handler,\n        logging.DEBUG_ANALYZER: short_format_handler}\n\n    @classmethod\n    def get_log_level(cls):\n        return cls.log_level\n\n    @classmethod\n    def set_log_level(cls, level):\n        for logger in cls.loggers:\n            logger.removeHandler(cls.handlers[LoggerFactory.log_level])\n\n        if level == 'debug':\n            cls.log_level = logging.DEBUG\n        elif level == 'info':\n            cls.log_level = logging.INFO\n        elif level == 'debug_analyzer':\n            cls.log_level = logging.DEBUG_ANALYZER\n        else:\n            cls.log_level = logging.INFO\n\n        cls.short_format_handler.setLevel(cls.log_level)\n        cls.long_format_handler.setLevel(cls.log_level)\n\n        for logger in cls.loggers:\n            logger.setLevel(cls.log_level)\n            logger.addHandler(cls.handlers[cls.log_level])\n\n    @classmethod\n    def get_new_logger(cls, logger_name):\n        logger = logging.getLogger('[' + logger_name + ']')\n\n        logger.setLevel(cls.log_level)\n        logger.addHandler(cls.handlers[cls.log_level])\n\n        cls.loggers.append(logger)\n\n        return logger\n","lang_cluster":"C","length":152,"code_uid":"e77745dce17043208243ecb33a25557b"}
{"diff_hunk":"@@ -34,6 +34,7 @@\n  *\/\n \n #include \"globals.h\"\n+#include \"arch.h\"\n \n void\n unit_test_io(void);","old_code":"\/* **********************************************************\n * Copyright (c) 2012-2013 Google, Inc.  All rights reserved.\n * **********************************************************\/\n\n\/*\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * * Redistributions of source code must retain the above copyright notice,\n *   this list of conditions and the following disclaimer.\n *\n * * Redistributions in binary form must reproduce the above copyright notice,\n *   this list of conditions and the following disclaimer in the documentation\n *   and\/or other materials provided with the distribution.\n *\n * * Neither the name of Google, Inc. nor the names of its contributors may be\n *   used to endorse or promote products derived from this software without\n *   specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL GOOGLE, INC. OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n *\/\n\n\/* Simple main that just calls each unit test in turn.\n *\/\n\n#include \"globals.h\"\n\nvoid\nunit_test_io(void);\n#ifdef UNIX\nvoid\nunit_test_string(void);\nvoid\nunit_test_os(void);\n#endif\nvoid\nunit_test_options(void);\nvoid\nunit_test_vmareas(void);\nvoid\nunit_test_utils(void);\n#ifdef WINDOWS\nvoid\nunit_test_drwinapi(void);\n#endif\nvoid\nunit_test_asm(dcontext_t *dc);\nvoid\nunit_test_atomic_ops(void);\nvoid\nunit_test_jit_fragment_tree(void);\n\nint\nmain(int argc, char **argv, char **envp)\n{\n    dcontext_t *dc = standalone_init();\n\n    \/* Each test will abort if it fails, so we just call each in turn and return\n     * 0 for success.  If we want to be able to call each test independently, it\n     * might be worth looking into gtest, which already does this.\n     *\/\n    unit_test_io();\n#ifdef UNIX\n    unit_test_string();\n    unit_test_os();\n#endif\n    unit_test_utils();\n    unit_test_options();\n    unit_test_vmareas();\n#ifdef WINDOWS\n    unit_test_drwinapi();\n#endif\n    unit_test_asm(dc);\n    unit_test_atomic_ops();\n    unit_test_jit_fragment_tree();\n    print_file(STDERR, \"all done\\n\");\n    return 0;\n}\n","lang_cluster":"C","length":88,"code_uid":"865d8fe9859b4298ba34f08f626f10fc"}
{"diff_hunk":"@@ -73,13 +73,15 @@ void handle_x11_input_event(struct wlr_x11_backend *x11,\n \n \t\tif (ev->detail == XCB_BUTTON_INDEX_4 ||\n \t\t\t\tev->detail == XCB_BUTTON_INDEX_5) {\n-\t\t\tdouble delta = (ev->detail == XCB_BUTTON_INDEX_4 ? -15 : 15);\n+\t\t\tint32_t delta_discrete = ev->detail == XCB_BUTTON_INDEX_4 ? -1 : 1;\n \t\t\tstruct wlr_event_pointer_axis axis = {\n \t\t\t\t.device = &output->pointer_dev,\n \t\t\t\t.time_msec = ev->time,\n \t\t\t\t.source = WLR_AXIS_SOURCE_WHEEL,\n \t\t\t\t.orientation = WLR_AXIS_ORIENTATION_VERTICAL,\n-\t\t\t\t.delta = delta,\n+\t\t\t\t\/\/ 15 is a typical value libinput sends for one scroll\n+\t\t\t\t.delta = delta_discrete * 15,\n+\t\t\t\t.delta_discrete = delta_discrete,\n \t\t\t};\n \t\t\twlr_signal_emit_safe(&output->pointer.events.axis, &axis);\n \t\t\tx11->time = ev->time;","old_code":"#include <stdlib.h>\n#include <wlr\/config.h>\n#include <wlr\/interfaces\/wlr_input_device.h>\n#include <wlr\/interfaces\/wlr_keyboard.h>\n#include <wlr\/interfaces\/wlr_pointer.h>\n#include <wlr\/util\/log.h>\n#include <xcb\/xcb.h>\n#ifdef __linux__\n#include <linux\/input-event-codes.h>\n#elif __FreeBSD__\n#include <dev\/evdev\/input-event-codes.h>\n#endif\n#ifdef WLR_HAS_XCB_XKB\n#include <xcb\/xkb.h>\n#endif\n#include \"backend\/x11.h\"\n#include \"util\/signal.h\"\n\nstatic uint32_t xcb_button_to_wl(uint32_t button) {\n\tswitch (button) {\n\tcase XCB_BUTTON_INDEX_1: return BTN_LEFT;\n\tcase XCB_BUTTON_INDEX_2: return BTN_MIDDLE;\n\tcase XCB_BUTTON_INDEX_3: return BTN_RIGHT;\n\t\/\/ XXX: I'm not sure the scroll-wheel direction is right\n\tcase XCB_BUTTON_INDEX_4: return BTN_GEAR_UP;\n\tcase XCB_BUTTON_INDEX_5: return BTN_GEAR_DOWN;\n\tdefault: return 0;\n\t}\n}\n\nstatic void x11_handle_pointer_position(struct wlr_x11_output *output,\n\t\tint16_t x, int16_t y, xcb_timestamp_t time) {\n\tstruct wlr_x11_backend *x11 = output->x11;\n\tstruct wlr_output *wlr_output = &output->wlr_output;\n\tstruct wlr_event_pointer_motion_absolute event = {\n\t\t.device = &output->pointer_dev,\n\t\t.time_msec = time,\n\t\t.x = (double)x \/ wlr_output->width,\n\t\t.y = (double)y \/ wlr_output->height,\n\t};\n\twlr_signal_emit_safe(&output->pointer.events.motion_absolute, &event);\n\n\tx11->time = time;\n}\n\nvoid handle_x11_input_event(struct wlr_x11_backend *x11,\n\t\txcb_generic_event_t *event) {\n\tswitch (event->response_type & XCB_EVENT_RESPONSE_TYPE_MASK) {\n\tcase XCB_KEY_PRESS:\n\tcase XCB_KEY_RELEASE: {\n\t\txcb_key_press_event_t *ev = (xcb_key_press_event_t *)event;\n\t\tstruct wlr_event_keyboard_key key = {\n\t\t\t.time_msec = ev->time,\n\t\t\t.keycode = ev->detail - 8,\n\t\t\t.state = event->response_type == XCB_KEY_PRESS ?\n\t\t\t\tWLR_KEY_PRESSED : WLR_KEY_RELEASED,\n\t\t\t.update_state = true,\n\t\t};\n\n\t\t\/\/ TODO use xcb-xkb for more precise modifiers state?\n\t\twlr_keyboard_notify_key(&x11->keyboard, &key);\n\t\tx11->time = ev->time;\n\t\treturn;\n\t}\n\tcase XCB_BUTTON_PRESS: {\n\t\txcb_button_press_event_t *ev = (xcb_button_press_event_t *)event;\n\n\t\tstruct wlr_x11_output *output =\n\t\t\tget_x11_output_from_window_id(x11, ev->event);\n\t\tif (output == NULL) {\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ev->detail == XCB_BUTTON_INDEX_4 ||\n\t\t\t\tev->detail == XCB_BUTTON_INDEX_5) {\n\t\t\tdouble delta = (ev->detail == XCB_BUTTON_INDEX_4 ? -15 : 15);\n\t\t\tstruct wlr_event_pointer_axis axis = {\n\t\t\t\t.device = &output->pointer_dev,\n\t\t\t\t.time_msec = ev->time,\n\t\t\t\t.source = WLR_AXIS_SOURCE_WHEEL,\n\t\t\t\t.orientation = WLR_AXIS_ORIENTATION_VERTICAL,\n\t\t\t\t.delta = delta,\n\t\t\t};\n\t\t\twlr_signal_emit_safe(&output->pointer.events.axis, &axis);\n\t\t\tx11->time = ev->time;\n\t\t\tbreak;\n\t\t}\n\t}\n\t\/* fallthrough *\/\n\tcase XCB_BUTTON_RELEASE: {\n\t\txcb_button_press_event_t *ev = (xcb_button_press_event_t *)event;\n\n\t\tstruct wlr_x11_output *output =\n\t\t\tget_x11_output_from_window_id(x11, ev->event);\n\t\tif (output == NULL) {\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ev->detail != XCB_BUTTON_INDEX_4 &&\n\t\t\t\tev->detail != XCB_BUTTON_INDEX_5) {\n\t\t\tstruct wlr_event_pointer_button button = {\n\t\t\t\t.device = &output->pointer_dev,\n\t\t\t\t.time_msec = ev->time,\n\t\t\t\t.button = xcb_button_to_wl(ev->detail),\n\t\t\t\t.state = event->response_type == XCB_BUTTON_PRESS ?\n\t\t\t\t\tWLR_BUTTON_PRESSED : WLR_BUTTON_RELEASED,\n\t\t\t};\n\n\t\t\twlr_signal_emit_safe(&output->pointer.events.button, &button);\n\t\t}\n\t\tx11->time = ev->time;\n\t\treturn;\n\t}\n\tcase XCB_MOTION_NOTIFY: {\n\t\txcb_motion_notify_event_t *ev = (xcb_motion_notify_event_t *)event;\n\n\t\tstruct wlr_x11_output *output =\n\t\t\tget_x11_output_from_window_id(x11, ev->event);\n\t\tif (output != NULL) {\n\t\t\tx11_handle_pointer_position(output, ev->event_x, ev->event_y, ev->time);\n\t\t}\n\t\treturn;\n\t}\n\tdefault:\n#ifdef WLR_HAS_XCB_XKB\n\t\tif (x11->xkb_supported && event->response_type == x11->xkb_base_event) {\n\t\t\txcb_xkb_state_notify_event_t *ev =\n\t\t\t\t(xcb_xkb_state_notify_event_t *)event;\n\t\t\twlr_keyboard_notify_modifiers(&x11->keyboard, ev->baseMods,\n\t\t\t\tev->latchedMods, ev->lockedMods, ev->lockedGroup);\n\t\t\treturn;\n\t\t}\n#endif\n\t\tbreak;\n\t}\n}\n\nstatic void input_device_destroy(struct wlr_input_device *wlr_device) {\n\t\/\/ Don't free the input device, it's on the stack\n}\n\nconst struct wlr_input_device_impl input_device_impl = {\n\t.destroy = input_device_destroy,\n};\n\nstatic void keyboard_destroy(struct wlr_keyboard *wlr_keyboard) {\n\t\/\/ Don't free the keyboard, it's on the stack\n}\n\nconst struct wlr_keyboard_impl keyboard_impl = {\n\t.destroy = keyboard_destroy,\n};\n\nstatic void pointer_destroy(struct wlr_pointer *wlr_pointer) {\n\t\/\/ Don't free the pointer, it's on the stack\n}\n\nconst struct wlr_pointer_impl pointer_impl = {\n\t.destroy = pointer_destroy,\n};\n\nvoid update_x11_pointer_position(struct wlr_x11_output *output,\n\t\txcb_timestamp_t time) {\n\tstruct wlr_x11_backend *x11 = output->x11;\n\n\txcb_query_pointer_cookie_t cookie =\n\t\txcb_query_pointer(x11->xcb_conn, output->win);\n\txcb_query_pointer_reply_t *reply =\n\t\txcb_query_pointer_reply(x11->xcb_conn, cookie, NULL);\n\tif (!reply) {\n\t\treturn;\n\t}\n\n\tx11_handle_pointer_position(output, reply->win_x, reply->win_y, time);\n\n\tfree(reply);\n}\n\nbool wlr_input_device_is_x11(struct wlr_input_device *wlr_dev) {\n\treturn wlr_dev->impl == &input_device_impl;\n}\n","lang_cluster":"C","length":181,"code_uid":"1193c67f9066469ca69b9103119d6404"}
{"diff_hunk":"@@ -19,6 +19,23 @@ from libcodechecker.logger import LoggerFactory\n LOG = LoggerFactory.get_new_logger('PROFILER')\n \n \n+class Timer(object):\n+    \"\"\"\n+    Simple timer context manager\n+    to measure code block execution time.\n+    \"\"\"\n+    def __init__(self, block_name=''):\n+        self.block_name = block_name\n+\n+    def __enter__(self):\n+        self.before = datetime.datetime.now()\n+\n+    def __exit__(self, type, value, traceback):\n+        after = datetime.datetime.now()\n+        time_diff = after - self.before\n+        LOG.debug(self.block_name + \" \" + str(time_diff.total_seconds())+'s')\n+\n+\n def timeit(function):\n     \"\"\"\n     Decorator to measure function call time.","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\nfrom datetime import datetime\nimport cProfile\nimport pstats\n\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import BytesIO as StringIO\n\nfrom libcodechecker import logger\nfrom libcodechecker.logger import LoggerFactory\n\nLOG = LoggerFactory.get_new_logger('PROFILER')\n\n\ndef timeit(function):\n    \"\"\"\n    Decorator to measure function call time.\n    \"\"\"\n\n    func_name = function.__name__\n\n    def debug_wrapper(*args, **kwargs):\n        \"\"\"\n        Log measured time.\n        \"\"\"\n        before = datetime.now()\n        res = function(*args, **kwargs)\n        after = datetime.now()\n        timediff = after - before\n        diff = timediff.microseconds\/1000\n        LOG.debug('['+str(diff)+'ms] ' + func_name)\n        return res\n\n    def release_wrapper(*args, **kwargs):\n        \"\"\"\n        No logging and measuring.\n        \"\"\"\n        res = function(*args, **kwargs)\n        return res\n\n    if LoggerFactory.get_log_level() == logger.DEBUG:\n        return debug_wrapper\n    else:\n        return release_wrapper\n\n\ndef profileit(function):\n    \"\"\"\n    Decorator to pofile function calls.\n    \"\"\"\n\n    function_name = function.__name__\n\n    def wrapper(*args, **kwargs):\n\n        prof = cProfile.Profile()\n        LOG.debug('Profiling: ' + function_name)\n        prof.enable()\n        res = function(*args, **kwargs)\n        prof.disable()\n        sortby = 'cumulative'\n\n        prof_data = StringIO.StringIO()\n        ps = pstats.Stats(prof, stream=prof_data).sort_stats(sortby)\n        ps.print_stats()\n        LOG.debug(prof_data.getvalue())\n        prof_data.close()\n        return res\n\n    return wrapper\n","lang_cluster":"C","length":77,"code_uid":"b6cd9e19e11c4e1b9372e634312f4a43"}
{"diff_hunk":"@@ -13,7 +13,10 @@ void wlr_tablet_pad_init(struct wlr_tablet_pad *pad,\n }\n \n void wlr_tablet_pad_destroy(struct wlr_tablet_pad *pad) {\n-\tif (!pad) return;\n+\tif (!pad) {\n+\t\treturn;\n+\t}\n+\t\n \tif (pad->impl && pad->impl->destroy) {\n \t\tpad->impl->destroy(pad);\n \t} else {","old_code":"#include <stdlib.h>\n#include <string.h>\n#include <wayland-server.h>\n#include <wlr\/types\/wlr_tablet_pad.h>\n#include <wlr\/interfaces\/wlr_tablet_pad.h>\n\nvoid wlr_tablet_pad_init(struct wlr_tablet_pad *pad,\n\t\tstruct wlr_tablet_pad_impl *impl) {\n\tpad->impl = impl;\n\twl_signal_init(&pad->events.button);\n\twl_signal_init(&pad->events.ring);\n\twl_signal_init(&pad->events.strip);\n}\n\nvoid wlr_tablet_pad_destroy(struct wlr_tablet_pad *pad) {\n\tif (!pad) return;\n\tif (pad->impl && pad->impl->destroy) {\n\t\tpad->impl->destroy(pad);\n\t} else {\n\t\tfree(pad);\n\t}\n}\n","lang_cluster":"C","length":22,"code_uid":"0a56323435644a9183e021f284e06c4f"}
{"diff_hunk":"@@ -1,18 +1,23 @@\n #define BOOST_TEST_MODULE TQTcpServerTest\n #include <QTest>\n-#include <boost\/smart_ptr.hpp>\n+\n+#ifndef Q_MOC_RUN\n+  #include <boost\/smart_ptr.hpp>\n+#endif\n #include <iostream>\n \n #include <QTcpServer>\n #include <QTcpSocket>\n #include <QHostAddress>\n \n-#include \"thrift\/protocol\/TBinaryProtocol.h\"\n-#include \"thrift\/async\/TAsyncProcessor.h\"\n-#include \"thrift\/qt\/TQTcpServer.h\"\n-#include \"thrift\/qt\/TQIODeviceTransport.h\"\n-\n-#include \"gen-cpp\/ParentService.h\"\n+#ifndef Q_MOC_RUN\n+  #include \"thrift\/protocol\/TBinaryProtocol.h\"\n+  #include \"thrift\/async\/TAsyncProcessor.h\"\n+  #include \"thrift\/qt\/TQTcpServer.h\"\n+  #include \"thrift\/qt\/TQIODeviceTransport.h\"\n+  \n+  #include \"gen-cpp\/ParentService.h\"\n+#endif\n \n using namespace apache::thrift;\n ","old_code":"#define BOOST_TEST_MODULE TQTcpServerTest\n#include <QTest>\n#include <boost\/smart_ptr.hpp>\n#include <iostream>\n\n#include <QTcpServer>\n#include <QTcpSocket>\n#include <QHostAddress>\n\n#include \"thrift\/protocol\/TBinaryProtocol.h\"\n#include \"thrift\/async\/TAsyncProcessor.h\"\n#include \"thrift\/qt\/TQTcpServer.h\"\n#include \"thrift\/qt\/TQIODeviceTransport.h\"\n\n#include \"gen-cpp\/ParentService.h\"\n\nusing namespace apache::thrift;\n\nstruct AsyncHandler : public test::ParentServiceCobSvIf {\n  std::vector<std::string> strings;\n  virtual void addString(tcxx::function<void()> cob, const std::string& s) {\n    strings.push_back(s);\n    cob();\n  }\n  virtual void getStrings(tcxx::function<void(std::vector<std::string> const& _return)> cob) {\n    cob(strings);\n  }\n\n  \/\/ Overrides not used in this test\n  virtual void incrementGeneration(tcxx::function<void(int32_t const& _return)> cob) {}\n  virtual void getGeneration(tcxx::function<void(int32_t const& _return)> cob) {}\n  virtual void getDataWait(tcxx::function<void(std::string const& _return)> cob,\n                           const int32_t length) {}\n  virtual void onewayWait(tcxx::function<void()> cob) {}\n  virtual void exceptionWait(\n      tcxx::function<void()> cob,\n      tcxx::function<void(::apache::thrift::TDelayedException* _throw)> \/* exn_cob *\/,\n      const std::string& message) {}\n  virtual void unexpectedExceptionWait(tcxx::function<void()> cob, const std::string& message) {}\n};\n\nclass TQTcpServerTest : public QObject {\n  void init() {\n    \/\/ setup server\n    serverSocket.reset(new QTcpServer);\n    server.reset(new async::TQTcpServer(serverSocket,\n                                        boost::make_shared<test::ParentServiceAsyncProcessor>(\n                                            boost::make_shared<AsyncHandler>()),\n                                        boost::make_shared<protocol::TBinaryProtocolFactory>()));\n    QVERIFY(serverSocket->listen(QHostAddress::LocalHost));\n    int port = serverSocket->serverPort();\n    QVERIFY(port > 0);\n\n    \/\/ setup client\n    socket.reset(new QTcpSocket);\n    client.reset(new test::ParentServiceClient(boost::make_shared<protocol::TBinaryProtocol>(\n        boost::make_shared<transport::TQIODeviceTransport>(socket))));\n    socket->connectToHost(QHostAddress::LocalHost, port);\n    QVERIFY(socket->waitForConnected());\n  }\n\n  void cleanup() {\n    socket->close();\n    serverSocket->close();\n  }\n\n  void test_communicate() {\n    client->addString(\"foo\");\n    client->addString(\"bar\");\n\n    std::vector<std::string> reply;\n    client->getStrings(reply);\n    QCOMPARE(reply[0], \"foo\");\n    QCOMPARE(reply[1], \"foo\");\n  }\n\n  boost::shared_ptr<QTcpServer> serverSocket;\n  boost::shared_ptr<async::TQTcpServer> server;\n  boost::shared_ptr<QTcpSocket> socket;\n  boost::shared_ptr<test::ParentServiceClient> client;\n};\n\n#if (QT_VERSION >= QT_VERSION_CHECK(5, 0, 0))\nQTEST_GUILESS_MAIN(TQTcpServerTest);\n#else\n#undef QT_GUI_LIB\nQTEST_MAIN(TQTcpServerTest);\n#endif\n#include \"TQTcpServerTest.moc\"\n","lang_cluster":"C","length":89,"code_uid":"fca87a28469e4871a27f818fe091537d"}
{"diff_hunk":"@@ -4,11 +4,13 @@\n #   This file is distributed under the University of Illinois Open Source\n #   License. See LICENSE.TXT for details.\n # -----------------------------------------------------------------------------\n+from time import sleep\n \n \"\"\"Setup for the test package comment.\"\"\"\n \n from subprocess import CalledProcessError\n \n+import json\n import multiprocessing\n import os\n import shutil","old_code":"# coding=utf-8\n# -----------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -----------------------------------------------------------------------------\n\n\"\"\"Setup for the test package comment.\"\"\"\n\nfrom subprocess import CalledProcessError\n\nimport multiprocessing\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport time\nimport uuid\n\nfrom libtest import codechecker\nfrom libtest import env\nfrom libtest import project\n\n# Stopping event for CodeChecker server.\n__STOP_SERVER = multiprocessing.Event()\n\n# Test workspace should be initialized in this module.\nTEST_WORKSPACE = None\n\n\ndef setup_package():\n    \"\"\"Setup the environment for the tests. \"\"\"\n\n    global TEST_WORKSPACE\n    TEST_WORKSPACE = env.get_workspace('comment')\n\n    os.environ['TEST_WORKSPACE'] = TEST_WORKSPACE\n\n    test_project = 'cpp'\n\n    test_project_path = project.path(test_project)\n\n    pg_db_config = env.get_postgresql_cfg()\n\n    test_config = {}\n\n    project_info = project.get_info(test_project)\n\n    test_config['test_project'] = project_info\n\n    suppress_file = None\n\n    skip_list_file = None\n\n    # Setup environment variabled for test cases.\n    host_port_cfg = env.get_host_port_cfg()\n\n    test_env = env.test_env()\n\n    codechecker_cfg = {\n        'suppress_file': suppress_file,\n        'skip_list_file': skip_list_file,\n        'check_env': test_env,\n        'workspace': TEST_WORKSPACE,\n        'reportdir': os.path.join(TEST_WORKSPACE, 'reports'),\n        'pg_db_config': pg_db_config,\n        'checkers': ['-d', 'core.CallAndMessage',\n                     '-e', 'core.StackAddressEscape']\n    }\n\n    codechecker_cfg.update(host_port_cfg)\n\n    test_config['codechecker_cfg'] = codechecker_cfg\n\n    ret = project.clean(test_project, test_env)\n    if ret:\n        sys.exit(ret)\n\n    # Start the CodeChecker server.\n    print(\"Starting server to get results\")\n    _start_server(codechecker_cfg, test_config, False)\n\n    # Check the test project for the first time.\n    test_project_name = project_info['name'] + '_' + uuid.uuid4().hex\n\n    ret = codechecker.check(codechecker_cfg,\n                            test_project_name,\n                            test_project_path)\n    if ret:\n        sys.exit(1)\n    print(\"Analyzing test project was succcessful.\")\n\n    # Check the test project again.\n    test_project_name = project_info['name'] + '_' + uuid.uuid4().hex\n\n    ret = codechecker.check(codechecker_cfg,\n                            test_project_name,\n                            test_project_path)\n    if ret:\n        sys.exit(1)\n    print(\"Analyzing test project was succcessful.\")\n\n    if pg_db_config:\n        print(\"Waiting for PotgreSQL to stop.\")\n        codechecker.wait_for_postgres_shutdown(TEST_WORKSPACE)\n\n    codechecker_cfg['run_names'] = [test_project_name]\n\n    test_config['codechecker_cfg'] = codechecker_cfg\n\n    env.export_test_cfg(TEST_WORKSPACE, test_config)\n\n\ndef teardown_package():\n    \"\"\"Stop the CodeChecker server.\"\"\"\n    __STOP_SERVER.set()\n\n    # TODO: If environment variable is set keep the workspace\n    # and print out the path.\n    global TEST_WORKSPACE\n\n    print(\"Removing: \" + TEST_WORKSPACE)\n    shutil.rmtree(TEST_WORKSPACE)\n\n\ndef _start_server(codechecker_cfg, test_config, auth=False):\n    \"\"\"Start the CodeChecker server.\"\"\"\n\n    def start_server_proc(event, server_cmd, checking_env):\n        \"\"\"Target function for starting the CodeChecker server.\"\"\"\n\n        proc = subprocess.Popen(server_cmd, env=checking_env)\n\n        # Blocking termination until event is set.\n        event.wait()\n\n        # If proc is still running, stop it.\n        if proc.poll() is None:\n            proc.terminate()\n\n    server_cmd = codechecker.serv_cmd(codechecker_cfg, test_config)\n\n    server_proc = multiprocessing.Process(\n        name='server',\n        target=start_server_proc,\n        args=(__STOP_SERVER, server_cmd, codechecker_cfg['check_env']))\n\n    server_proc.start()\n\n    # Wait for server to start and connect to database.\n    time.sleep(20)\n","lang_cluster":"C","length":151,"code_uid":"749ed7f1a6f64ad99b9077de6cdcae31"}
{"diff_hunk":"@@ -38,6 +38,8 @@\n #include <flux\/core.h>\n #include \"heaptrace.h\"\n \n+static flux_msg_handler_t **handlers = NULL;\n+\n static void start_cb (flux_t *h, flux_msg_handler_t *mh,\n                       const flux_msg_t *msg, void *arg)\n {","old_code":"\/*****************************************************************************\\\n *  Copyright (c) 2014 Lawrence Livermore National Security, LLC.  Produced at\n *  the Lawrence Livermore National Laboratory (cf, AUTHORS, DISCLAIMER.LLNS).\n *  LLNL-CODE-658032 All rights reserved.\n *\n *  This file is part of the Flux resource manager framework.\n *  For details, see https:\/\/github.com\/flux-framework.\n *\n *  This program is free software; you can redistribute it and\/or modify it\n *  under the terms of the GNU General Public License as published by the Free\n *  Software Foundation; either version 2 of the license, or (at your option)\n *  any later version.\n *\n *  Flux is distributed in the hope that it will be useful, but WITHOUT\n *  ANY WARRANTY; without even the IMPLIED WARRANTY OF MERCHANTABILITY or\n *  FITNESS FOR A PARTICULAR PURPOSE.  See the terms and conditions of the\n *  GNU General Public License for more details.\n *\n *  You should have received a copy of the GNU General Public License along\n *  with this program; if not, write to the Free Software Foundation, Inc.,\n *  59 Temple Place, Suite 330, Boston, MA 02111-1307 USA.\n *  See also:  http:\/\/www.gnu.org\/licenses\/\n\\*****************************************************************************\/\n\n#if HAVE_CONFIG_H\n#include \"config.h\"\n#endif\n#if WITH_TCMALLOC\n#if HAVE_GPERFTOOLS_HEAP_PROFILER_H\n  #include <gperftools\/heap-profiler.h>\n#elif HAVE_GOOGLE_HEAP_PROFILER_H\n  #include <google\/heap-profiler.h>\n#else\n  #error gperftools headers not configured\n#endif\n#endif \/* WITH_TCMALLOC *\/\n\n#include <flux\/core.h>\n#include \"heaptrace.h\"\n\nstatic void start_cb (flux_t *h, flux_msg_handler_t *mh,\n                      const flux_msg_t *msg, void *arg)\n{\n    const char *filename;\n\n    if (flux_request_unpack (msg, NULL, \"{s:s}\", \"filename\", &filename) < 0)\n        goto error;\n#if WITH_TCMALLOC\n    if (IsHeapProfilerRunning ()) {\n        errno = EINVAL;\n        goto error;\n    }\n    HeapProfilerStart (filename);\n#else\n    errno = ENOSYS;\n    goto error;\n#endif\n    if (flux_respond (h, msg, 0, NULL) < 0)\n        FLUX_LOG_ERROR (h);\n    return;\nerror:\n    if (flux_respond (h, msg, errno, NULL) < 0)\n        FLUX_LOG_ERROR (h);\n}\n\nstatic void dump_cb (flux_t *h, flux_msg_handler_t *mh,\n                     const flux_msg_t *msg, void *arg)\n{\n    const char *reason;\n\n    if (flux_request_unpack (msg, NULL, \"{s:s}\", \"reason\", &reason) < 0)\n        goto error;\n#if WITH_TCMALLOC\n    if (!IsHeapProfilerRunning ()) {\n        errno = EINVAL;\n        goto error;\n    }\n    HeapProfilerDump (reason);\n#else\n    errno = ENOSYS;\n    goto error;\n#endif\n    if (flux_respond (h, msg, 0, NULL) < 0)\n        FLUX_LOG_ERROR (h);\n    return;\nerror:\n    if (flux_respond (h, msg, errno, NULL) < 0)\n        FLUX_LOG_ERROR (h);\n}\n\nstatic void stop_cb (flux_t *h, flux_msg_handler_t *mh,\n                     const flux_msg_t *msg, void *arg)\n{\n    if (flux_request_decode (msg, NULL, NULL) < 0)\n        goto error;\n#if WITH_TCMALLOC\n    if (!IsHeapProfilerRunning ()) {\n        errno = EINVAL;\n        goto error;\n    }\n    HeapProfilerStop();\n#else\n    errno = ENOSYS;\n    goto error;\n#endif \/* WITH_TCMALLOC *\/\n    if (flux_respond (h, msg, 0, NULL) < 0)\n        FLUX_LOG_ERROR (h);\n    return;\nerror:\n    if (flux_respond (h, msg, errno, NULL) < 0)\n        FLUX_LOG_ERROR (h);\n}\n\nstatic struct flux_msg_handler_spec handlers[] = {\n    { FLUX_MSGTYPE_REQUEST, \"heaptrace.start\",  start_cb, 0, NULL },\n    { FLUX_MSGTYPE_REQUEST, \"heaptrace.dump\",   dump_cb, 0, NULL },\n    { FLUX_MSGTYPE_REQUEST, \"heaptrace.stop\",   stop_cb, 0, NULL },\n    FLUX_MSGHANDLER_TABLE_END,\n};\n\nstatic void heaptrace_finalize (void *arg)\n{\n    flux_msg_handler_delvec (handlers);\n}\n\nint heaptrace_initialize (flux_t *h)\n{\n    char *dummy = \"hello\";\n    if (flux_msg_handler_addvec (h, handlers, NULL) < 0)\n        return -1;\n    flux_aux_set (h, \"flux::heaptrace\", dummy, heaptrace_finalize);\n    return 0;\n}\n\n\/*\n * vi:tabstop=4 shiftwidth=4 expandtab\n *\/\n","lang_cluster":"C","length":137,"code_uid":"33e375160b51435089f411a860498d8e"}
{"diff_hunk":"@@ -47,7 +47,7 @@ bool legacy_crtc_set_cursor(struct wlr_drm_backend *drm,\n \n \tif (drmModeSetCursor(drm->fd, crtc->id, gbm_bo_get_handle(bo).u32,\n \t\t\tplane->surf.width, plane->surf.height)) {\n-\t\twlr_log_errno(L_ERROR, \"Failed to set hardware cursor\");\n+\t\twlr_log_errno(L_DEBUG, \"Failed to set hardware cursor\");\n \t\treturn false;\n \t}\n ","old_code":"#include <gbm.h>\n#include <wlr\/util\/log.h>\n#include <xf86drm.h>\n#include <xf86drmMode.h>\n#include \"backend\/drm\/drm.h\"\n#include \"backend\/drm\/iface.h\"\n#include \"backend\/drm\/util.h\"\n\nstatic bool legacy_crtc_pageflip(struct wlr_drm_backend *drm,\n\t\tstruct wlr_drm_connector *conn, struct wlr_drm_crtc *crtc,\n\t\tuint32_t fb_id, drmModeModeInfo *mode) {\n\tif (mode) {\n\t\tif (drmModeSetCrtc(drm->fd, crtc->id, fb_id, 0, 0,\n\t\t\t\t&conn->id, 1, mode)) {\n\t\t\twlr_log_errno(L_ERROR, \"%s: Failed to set CRTC\", conn->output.name);\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tif (drmModePageFlip(drm->fd, crtc->id, fb_id, DRM_MODE_PAGE_FLIP_EVENT, conn)) {\n\t\twlr_log_errno(L_ERROR, \"%s: Failed to page flip\", conn->output.name);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool legacy_conn_enable(struct wlr_drm_backend *drm,\n\t\tstruct wlr_drm_connector *conn, bool enable) {\n\tint ret = drmModeConnectorSetProperty(drm->fd, conn->id, conn->props.dpms,\n\t\tenable ? DRM_MODE_DPMS_ON : DRM_MODE_DPMS_OFF);\n\treturn ret >= 0;\n}\n\nbool legacy_crtc_set_cursor(struct wlr_drm_backend *drm,\n\t\tstruct wlr_drm_crtc *crtc, struct gbm_bo *bo) {\n\tif (!crtc || !crtc->cursor) {\n\t\treturn true;\n\t}\n\n\tif (!bo) {\n\t\tdrmModeSetCursor(drm->fd, crtc->id, 0, 0, 0);\n\t\treturn true;\n\t}\n\n\tstruct wlr_drm_plane *plane = crtc->cursor;\n\n\tif (drmModeSetCursor(drm->fd, crtc->id, gbm_bo_get_handle(bo).u32,\n\t\t\tplane->surf.width, plane->surf.height)) {\n\t\twlr_log_errno(L_ERROR, \"Failed to set hardware cursor\");\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nbool legacy_crtc_move_cursor(struct wlr_drm_backend *drm,\n\t\tstruct wlr_drm_crtc *crtc, int x, int y) {\n\treturn !drmModeMoveCursor(drm->fd, crtc->id, x, y);\n}\n\nbool legacy_crtc_set_gamma(struct wlr_drm_backend *drm,\n\t\tstruct wlr_drm_crtc *crtc, uint16_t *r, uint16_t *g, uint16_t *b,\n\t\tuint32_t size) {\n\treturn !drmModeCrtcSetGamma(drm->fd, crtc->id, size, r, g, b);\n}\n\nuint32_t legacy_crtc_get_gamma_size(struct wlr_drm_backend *drm,\n\t\tstruct wlr_drm_crtc *crtc) {\n\treturn crtc->legacy_crtc->gamma_size;\n}\n\nconst struct wlr_drm_interface legacy_iface = {\n\t.conn_enable = legacy_conn_enable,\n\t.crtc_pageflip = legacy_crtc_pageflip,\n\t.crtc_set_cursor = legacy_crtc_set_cursor,\n\t.crtc_move_cursor = legacy_crtc_move_cursor,\n\t.crtc_set_gamma = legacy_crtc_set_gamma,\n\t.crtc_get_gamma_size = legacy_crtc_get_gamma_size,\n};\n","lang_cluster":"C","length":80,"code_uid":"33fbb86c221145999ac4a8114458c9f4"}
{"diff_hunk":"@@ -54,6 +54,26 @@ var _ = Describe(\"NAT\", func() {\n \t\t\t},\n \t\t}))\n \t})\n+\tIt(\"should render rules when active with explicit port range\", func() {\n+\n+\t\t\/\/copy struct\n+\t\tlocalConfig := rrConfigNormal\n+\t\tlocalConfig.IptablesNATOutgoingInterfaceFilter = \"cali-123\"\n+\t\trenderer = NewRenderer(localConfig)\n+\n+\t\tExpect(renderer.NATOutgoingChain(true, 4)).To(Equal(&Chain{\n+\t\t\tName: \"cali-nat-outgoing\",\n+\t\t\tRules: []Rule{\n+\t\t\t\t{\n+\t\t\t\t\tAction: MasqAction{},\n+\t\t\t\t\tMatch: Match().\n+\t\t\t\t\t\tSourceIPSet(\"cali4-masq-ipam-pools\").\n+\t\t\t\t\t\tNotDestIPSet(\"cali4-all-ipam-pools\").\n+\t\t\t\t\t\tOutInterface(\"cali-123\"),\n+\t\t\t\t},\n+\t\t\t},\n+\t\t}))\n+\t})\n \tIt(\"should render nothing when inactive\", func() {\n \t\tExpect(renderer.NATOutgoingChain(false, 4)).To(Equal(&Chain{\n \t\t\tName:  \"cali-nat-outgoing\",","old_code":"\/\/ Copyright (c) 2017 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage rules_test\n\nimport (\n\t. \"github.com\/projectcalico\/felix\/rules\"\n\n\t. \"github.com\/onsi\/ginkgo\"\n\t. \"github.com\/onsi\/gomega\"\n\n\t\"github.com\/projectcalico\/felix\/ipsets\"\n\t. \"github.com\/projectcalico\/felix\/iptables\"\n)\n\nvar _ = Describe(\"NAT\", func() {\n\tvar rrConfigNormal = Config{\n\t\tIPIPEnabled:          true,\n\t\tIPIPTunnelAddress:    nil,\n\t\tIPSetConfigV4:        ipsets.NewIPVersionConfig(ipsets.IPFamilyV4, \"cali\", nil, nil),\n\t\tIPSetConfigV6:        ipsets.NewIPVersionConfig(ipsets.IPFamilyV6, \"cali\", nil, nil),\n\t\tIptablesMarkAccept:   0x8,\n\t\tIptablesMarkPass:     0x10,\n\t\tIptablesMarkScratch0: 0x20,\n\t\tIptablesMarkScratch1: 0x40,\n\t}\n\n\tvar renderer RuleRenderer\n\tBeforeEach(func() {\n\t\trenderer = NewRenderer(rrConfigNormal)\n\t})\n\n\tIt(\"should render rules when active\", func() {\n\t\tExpect(renderer.NATOutgoingChain(true, 4)).To(Equal(&Chain{\n\t\t\tName: \"cali-nat-outgoing\",\n\t\t\tRules: []Rule{\n\t\t\t\t{\n\t\t\t\t\tAction: MasqAction{},\n\t\t\t\t\tMatch: Match().\n\t\t\t\t\t\tSourceIPSet(\"cali4-masq-ipam-pools\").\n\t\t\t\t\t\tNotDestIPSet(\"cali4-all-ipam-pools\"),\n\t\t\t\t},\n\t\t\t},\n\t\t}))\n\t})\n\tIt(\"should render nothing when inactive\", func() {\n\t\tExpect(renderer.NATOutgoingChain(false, 4)).To(Equal(&Chain{\n\t\t\tName:  \"cali-nat-outgoing\",\n\t\t\tRules: nil,\n\t\t}))\n\t})\n})\n","lang_cluster":"C","length":63,"code_uid":"3728a594880247488dfcc05fc7a75374"}
{"diff_hunk":"@@ -24,6 +24,19 @@\n \n const char h2o_httpclient_error_is_eos[] = \"end of stream\";\n const char h2o_httpclient_error_refused_stream[] = \"refused stream\";\n+const char h2o_httpclient_error_unknown_alpn_protocol[] = \"unknown alpn protocol\";\n+const char h2o_httpclient_error_io[] = \"I\/O error\";\n+const char h2o_httpclient_error_connection_timeout[] = \"connection timeout\";\n+const char h2o_httpclient_error_first_byte_timeout[] = \"first byte timeout\";\n+const char h2o_httpclient_error_io_timeout[] = \"I\/O timeout\";\n+const char h2o_httpclient_error_http1_line_folding[] = \"line folding of header fields is not supported\";\n+const char h2o_httpclient_error_http1_unexpected_transfer_encoding[] = \"unexpected type of transfer-encoding\";\n+const char h2o_httpclient_error_http1_invalid_content_length[] = \"invalid content-length\";\n+const char h2o_httpclient_error_http1_parse_failed[] = \"failed to parse the response\";\n+const char h2o_httpclient_error_http2_upstream_protocol[] = \"upstream protocol error\";\n+const char h2o_httpclient_error_http2_goaway_received[] = \"GOAWAY received\";\n+const char h2o_httpclient_error_http2_flow_control_window_overflow[] = \"flow control window overflow\";\n+const char h2o_httpclient_error_internal[] = \"internal error\";\n \n void h2o_httpclient_connection_pool_init(h2o_httpclient_connection_pool_t *connpool, h2o_socketpool_t *sockpool)\n {","old_code":"\/*\n * Copyright (c) 2018 Ichito Nagata, Fastly, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n *\/\n\n#include \"h2o\/httpclient.h\"\n\nconst char h2o_httpclient_error_is_eos[] = \"end of stream\";\nconst char h2o_httpclient_error_refused_stream[] = \"refused stream\";\n\nvoid h2o_httpclient_connection_pool_init(h2o_httpclient_connection_pool_t *connpool, h2o_socketpool_t *sockpool)\n{\n    connpool->socketpool = sockpool;\n    h2o_linklist_init_anchor(&connpool->http2.conns);\n}\n\nstatic void close_client(h2o_httpclient_t *client)\n{\n    if (client->_connect_req != NULL) {\n        h2o_socketpool_cancel_connect(client->_connect_req);\n        client->_connect_req = NULL;\n    }\n\n    if (h2o_timer_is_linked(&client->_timeout))\n        h2o_timer_unlink(&client->_timeout);\n\n    free(client);\n}\n\nstatic void on_connect_error(h2o_httpclient_t *client, const char *errstr)\n{\n    assert(errstr != NULL);\n    client->_cb.on_connect(client, errstr, NULL, NULL, NULL, 0, NULL, NULL, NULL, NULL);\n    close_client(client);\n}\n\nstatic void on_connect_timeout(h2o_timer_t *entry)\n{\n    h2o_httpclient_t *client = H2O_STRUCT_FROM_MEMBER(h2o_httpclient_t, _timeout, entry);\n    on_connect_error(client, \"connection timeout\");\n}\n\nstatic void do_cancel(h2o_httpclient_t *_client)\n{\n    h2o_httpclient_t *client = (void *)_client;\n    close_client(client);\n}\n\nstatic h2o_httpclient_t *create_client(h2o_mem_pool_t *pool, void *data, h2o_httpclient_ctx_t *ctx, h2o_httpclient_connect_cb cb)\n{\n#define SZ_MAX(x, y) ((x) > (y) ? (x) : (y))\n    size_t sz = SZ_MAX(h2o_httpclient__h1_size, h2o_httpclient__h2_size);\n#undef SZ_MAX\n    h2o_httpclient_t *client = h2o_mem_alloc(sz);\n    memset(client, 0, sz);\n    client->pool = pool;\n    client->ctx = ctx;\n    client->data = data;\n    client->cancel = do_cancel;\n    client->steal_socket = NULL;\n    client->get_socket = NULL;\n    client->update_window = NULL;\n    client->write_req = NULL;\n    client->_cb.on_connect = cb;\n    client->_timeout.cb = on_connect_timeout;\n\n    return client;\n}\n\nstatic void on_pool_connect(h2o_socket_t *sock, const char *errstr, void *data, h2o_url_t *origin)\n{\n    h2o_httpclient_t *client = data;\n\n    h2o_timer_unlink(&client->_timeout);\n\n    client->_connect_req = NULL;\n\n    if (sock == NULL) {\n        assert(errstr != NULL);\n        on_connect_error(client, errstr);\n        return;\n    }\n\n    h2o_iovec_t alpn_proto;\n    if (sock->ssl == NULL || (alpn_proto = h2o_socket_ssl_get_selected_protocol(sock)).len == 0) {\n        h2o_httpclient__h1_on_connect(client, sock, origin);\n    } else {\n        if (h2o_memis(alpn_proto.base, alpn_proto.len, H2O_STRLIT(\"h2\"))) {\n            \/* detach this socket from the socketpool to count the number of h1 connections correctly *\/\n            h2o_socketpool_detach(client->connpool->socketpool, sock);\n            h2o_httpclient__h2_on_connect(client, sock, origin);\n        } else if (memcmp(alpn_proto.base, \"http\/1.1\", alpn_proto.len) == 0) {\n            h2o_httpclient__h1_on_connect(client, sock, origin);\n        } else {\n            on_connect_error(client, \"unknown alpn protocol\");\n        }\n    }\n}\n\nstatic int should_use_h2(int8_t ratio, int8_t *counter)\n{\n    \/* weighted fair queueing *\/\n    if (*counter < 0)\n        *counter = ratio == 0 ? 0 : 50 \/ ratio; \/* set initial counter value *\/\n    int use_h2 = (((int)ratio * *counter) % 100) + ratio >= 100;\n    if (++*counter == 100)\n        *counter = 0;\n    return use_h2;\n}\n\nvoid h2o_httpclient_connect(h2o_httpclient_t **_client, h2o_mem_pool_t *pool, void *data, h2o_httpclient_ctx_t *ctx,\n                            h2o_httpclient_connection_pool_t *connpool, h2o_url_t *origin, h2o_httpclient_connect_cb cb)\n{\n    static const h2o_iovec_t both_protos = {H2O_STRLIT(\"\\x02\"\n                                                       \"h2\"\n                                                       \"\\x08\"\n                                                       \"http\/1.1\")};\n    assert(connpool != NULL);\n    h2o_iovec_t alpn_protos = h2o_iovec_init(NULL, 0);\n\n    h2o_httpclient_t *client = create_client(pool, data, ctx, cb);\n    client->connpool = connpool;\n    if (_client != NULL)\n        *_client = client;\n\n    client->timings.start_at = h2o_gettimeofday(ctx->loop);\n\n    h2o_httpclient__h2_conn_t *http2_conn = NULL;\n    if (!h2o_linklist_is_empty(&connpool->http2.conns)) {\n        http2_conn = H2O_STRUCT_FROM_MEMBER(h2o_httpclient__h2_conn_t, link, connpool->http2.conns.next);\n        if (http2_conn->num_streams >= h2o_httpclient__h2_get_max_concurrent_streams(http2_conn))\n            http2_conn = NULL;\n    }\n\n    if (ctx->http2.ratio < 0) {\n        \/* mix mode *\/\n\n        if (http2_conn != NULL && connpool->socketpool->_shared.pooled_count != 0) {\n            \/* both of h1 and h2 connections exist, compare in-use ratio *\/\n            double http1_ratio = (double)(connpool->socketpool->_shared.count - connpool->socketpool->_shared.pooled_count) \/\n                                 connpool->socketpool->_shared.count;\n            double http2_ratio = http2_conn->num_streams \/ h2o_httpclient__h2_get_max_concurrent_streams(http2_conn);\n            if (http2_ratio <= http1_ratio) {\n                h2o_httpclient__h2_on_connect(client, http2_conn->sock, &http2_conn->origin_url);\n            } else {\n                goto UseSocketPool;\n            }\n        } else if (http2_conn != NULL) {\n            \/* h2 connection exists *\/\n            h2o_httpclient__h2_on_connect(client, http2_conn->sock, &http2_conn->origin_url);\n        } else if (connpool->socketpool->_shared.pooled_count != 0) {\n            \/* h1 connection exists *\/\n            goto UseSocketPool;\n        } else {\n            \/* no connections, connect using ALPN *\/\n            alpn_protos = both_protos;\n            goto UseSocketPool;\n        }\n    } else {\n        \/* fixed ratio mode *\/\n\n        if (should_use_h2(ctx->http2.ratio, &ctx->http2.counter)) {\n            if (http2_conn != NULL) {\n                h2o_httpclient__h2_on_connect(client, http2_conn->sock, &http2_conn->origin_url);\n            } else {\n                alpn_protos = both_protos;\n                goto UseSocketPool;\n            }\n        } else {\n            goto UseSocketPool;\n        }\n    }\n\n    return;\n\nUseSocketPool:\n    h2o_timer_link(client->ctx->loop, client->ctx->connect_timeout, &client->_timeout);\n    h2o_socketpool_connect(&client->_connect_req, connpool->socketpool, origin, ctx->loop, ctx->getaddr_receiver, alpn_protos,\n                           on_pool_connect, client);\n}\n","lang_cluster":"C","length":197,"code_uid":"40ce5e4d987f4cc9af944e15a021102c"}
{"diff_hunk":"@@ -1,6 +1,6 @@\n Summary:        Open Programmable Acceleration Engine (OPAE) SDK\n Name:           opae\n-Version:        1.4.0\n+Version:        1.4.1\n Release:        1%{?dist}\n License:        BSD\n ExclusiveArch:  x86_64","old_code":"Summary:        Open Programmable Acceleration Engine (OPAE) SDK\nName:           opae\nVersion:        1.4.0\nRelease:        1%{?dist}\nLicense:        BSD\nExclusiveArch:  x86_64\n\nGroup:          Development\/Libraries\nVendor:         Intel Corporation\nRequires:       uuid, json-c, python\nURL:            https:\/\/github.com\/OPAE\/%{name}-sdk\nSource0:        https:\/\/github.com\/OPAE\/opae-sdk\/releases\/download\/%{version}-1\/%{name}.tar.gz\n\nBuildRequires:  gcc, gcc-c++\nBuildRequires:  cmake\nBuildRequires:  python3-devel\nBuildRequires:  json-c-devel\nBuildRequires:  libuuid-devel\nBuildRequires:  rpm-build\nBuildRequires:  hwloc-devel\nBuildRequires:  python-sphinx\nBuildRequires:  doxygen\nBuildRequires:  systemd-rpm-macros\nBuildRequires:  systemd\n\n%description\nOpen Programmable Acceleration Engine (OPAE) is a software framework\nfor managing and accessing programmable accelerators (FPGAs).\nIts main parts are:\n\n* OPAE Software Development Kit (OPAE SDK) (this package)\n* OPAE Linux driver for Intel(R) Xeon(R) CPU with\n  Integrated FPGAs and Intel(R) PAC with Arria(R) 10 GX FPGA\n* Basic Building Block (BBB) library for accelerating AFU\n\nOPAE SDK is a collection of libraries and tools to facilitate the\ndevelopment of software applications and accelerators using OPAE.\nIt provides a library implementing the OPAE C API for presenting a\nstreamlined and easy-to-use interface for software applications to\ndiscover, access, and manage FPGA devices and accelerators using\nthe OPAE software stack.\n\n%package devel\nSummary:    OPAE headers, sample source, and documentation\nRequires:   libuuid-devel, %{name}%{?_isa} = %{version}-%{release}\n\n%description devel\nOPAE headers, tools, sample source, and documentation\n\n\n\n\n%prep\n%setup -q -n %{name}\n\n%build\nrm -rf _build\nmkdir _build\ncd _build\n\n%cmake .. -DCMAKE_INSTALL_PREFIX=\/usr\n\n%make_build  opae-c \\\n         bitstream \\\n         xfpga \\\n         safestr \\\n         modbmc \\\n         opae-cxx-core \\\n         hello_cxxcore \\\n         board_rc \\\n         board_vc \\\n         fpgaconf \\\n         fpgainfo \\\n         userclk \\\n         object_api \\\n         hello_fpga \\\n         hello_events \\\n         mmlink \n\n%install\nmkdir -p %{buildroot}%{_datadir}\/opae\ncp RELEASE_NOTES.md %{buildroot}%{_datadir}\/opae\/RELEASE_NOTES.md\ncp LICENSE %{buildroot}%{_datadir}\/opae\/LICENSE\ncp COPYING %{buildroot}%{_datadir}\/opae\/COPYING\n\nmkdir -p %{buildroot}%{_usr}\/src\/opae\/cmake\/modules\n\nfor s in FindSphinx.cmake\ndo\n  cp \"cmake\/${s}\" %{buildroot}%{_usr}\/src\/opae\/cmake\/\ndone\n\n\nmkdir -p %{buildroot}%{_usr}\/src\/opae\/opae-libs\/cmake\/modules\nfor s in FindHwloc.cmake \\\n         OPAE.cmake \\\n         FindUUID.cmake \\\n         Findjson-c.cmake \\\n         OPAECompiler.cmake \\\n         OPAEGit.cmake \\\n         OPAEPackaging.cmake \ndo\n  cp \"opae-libs\/cmake\/modules\/${s}\" %{buildroot}%{_usr}\/src\/opae\/opae-libs\/cmake\/modules\ndone\n\nmkdir -p %{buildroot}%{_usr}\/src\/opae\/samples\nmkdir -p %{buildroot}%{_usr}\/src\/opae\/samples\/hello_fpga\/\nmkdir -p %{buildroot}%{_usr}\/src\/opae\/samples\/hello_events\/\nmkdir -p %{buildroot}%{_usr}\/src\/opae\/samples\/object_api\/\n\n\ncp samples\/hello_fpga\/hello_fpga.c %{buildroot}%{_usr}\/src\/opae\/samples\/hello_fpga\/\ncp samples\/hello_events\/hello_events.c %{buildroot}%{_usr}\/src\/opae\/samples\/hello_events\/\ncp samples\/object_api\/object_api.c %{buildroot}%{_usr}\/src\/opae\/samples\/object_api\/\n\n\ncd _build\n\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=safestrlib -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=opaeclib -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=opaecxxcorelib -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=samples -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=opaetoolslibs -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=toolfpgainfo -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=toolfpgaconf -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=tooluserclk -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=toolmmlink -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=samplebin -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=libopaeheaders -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=safestrheaders -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=toolpackager -P cmake_install.cmake\nDESTDIR=%{buildroot}  cmake -DCOMPONENT=jsonschema -P cmake_install.cmake\n\n\n%files\n%dir %{_datadir}\/opae\n%doc %{_datadir}\/opae\/RELEASE_NOTES.md\n%license %{_datadir}\/opae\/LICENSE\n%license %{_datadir}\/opae\/COPYING\n%{_libdir}\/libopae-c.so.%{version}\n%{_libdir}\/libopae-c.so.1\n%{_libdir}\/libopae-c.so\n\n%{_libdir}\/libbitstream.so.%{version}\n%{_libdir}\/libbitstream.so.1\n%{_libdir}\/libbitstream.so\n\n%{_libdir}\/libopae-cxx-core.so.%{version}\n%{_libdir}\/libopae-cxx-core.so.1\n%{_libdir}\/libopae-cxx-core.so\n\n\n%{_libdir}\/opae\/libxfpga.so*\n%{_libdir}\/opae\/libmodbmc.so*\n%{_libdir}\/libsafestr.a*\n\n\n\n%files devel\n%dir %{_includedir}\/opae\n%{_includedir}\/opae\/*\n%dir %{_includedir}\/safe_string\n%{_includedir}\/safe_string\/safe_string.h\n%{_libdir}\/libsafestr.a\n%dir %{_usr}\/src\/opae\n%{_usr}\/src\/opae\/samples\/hello_fpga\/hello_fpga.c\n%{_usr}\/src\/opae\/samples\/hello_events\/hello_events.c\n%{_usr}\/src\/opae\/samples\/object_api\/object_api.c\n%{_usr}\/src\/opae\/cmake\/*\n%{_usr}\/src\/opae\/opae-libs\/cmake\/modules\/*\n\n%{_libdir}\/opae\/libboard_rc.so*\n%{_libdir}\/opae\/libboard_vc.so*\n\n%{_bindir}\/fpgaconf\n%{_bindir}\/fpgainfo\n%{_bindir}\/mmlink\n%{_bindir}\/userclk\n%{_bindir}\/hello_fpga\n%{_bindir}\/object_api\n%{_bindir}\/hello_events\n%{_bindir}\/hello_cxxcore\n%{_bindir}\/afu_json_mgr\n%{_bindir}\/packager\n\n%{_usr}\/share\/opae\/*\n\n\n%changelog\n* Tue Dec 17 2019 Korde Nakul <nakul.korde@intel.com> 1.4.0-1\n- Added support to FPGA Linux kernel Device Feature List (DFL) driver patch set2.\n- Increased test cases and test coverage\n- Various bug fixes\n- Various compiler warning fixes\n- Various memory leak fixes\n- Various Static code scan bug fixes\n- Added new FPGA MMIO API to write 512 bits\n","lang_cluster":"C","length":197,"code_uid":"dec5908bfd5f46828c33cefbec3add4b"}
{"diff_hunk":"@@ -137,6 +137,18 @@ class ClangSA(analyzer_base.SourceAnalyzer):\n                                          '-analyzer-disable-checker',\n                                          '-Xclang', checker_name])\n \n+            if config.ctu_dir:\n+                analyzer_cmd.extend(['-Xclang', '-analyzer-config',\n+                                     '-Xclang', 'xtu-dir=' + config.ctu_dir,\n+                                     '-Xclang', '-analyzer-config',\n+                                     '-Xclang', 'reanalyze-xtu-visited=true'])\n+                if config.ctu_in_memory:\n+                    analyzer_cmd.extend(['-Xclang', '-analyzer-config',\n+                                         '-Xclang',\n+                                         'xtu-reparse=' +\n+                                         os.path.abspath(config.log_file[0])])\n+            # TODO Make it work on multiple logfiles\n+\n             # Set language.\n             analyzer_cmd.extend(['-x', self.buildaction.lang])\n ","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\nimport os\nimport re\nimport shlex\nimport subprocess\n\nfrom libcodechecker.analyze.analyzers import analyzer_base\nfrom libcodechecker.logger import LoggerFactory\nfrom libcodechecker.util import find_by_regex_in_envpath\n\nLOG = LoggerFactory.get_new_logger('CLANGSA')\n\n\nclass ClangSA(analyzer_base.SourceAnalyzer):\n    \"\"\"\n    Constructs clang static analyzer commands.\n    \"\"\"\n\n    def __parse_checkers(self, clangsa_output):\n        \"\"\"\n        Parse clang static analyzer checkers, store them to checkers.\n        \"\"\"\n\n        # Checker name and description in one line.\n        pattern = re.compile(\n            r'^\\s\\s(?P<checker_name>\\S*)\\s*(?P<description>.*)')\n\n        checker_name = None\n        for line in clangsa_output.splitlines():\n            if re.match(r'^CHECKERS:', line) or line == '':\n                continue\n            elif checker_name and not re.match(r'^\\s\\s\\S', line):\n                # Collect description for the checker name.\n                self.checkers.append((checker_name, line.strip()))\n                checker_name = None\n            elif re.match(r'^\\s\\s\\S+$', line.rstrip()):\n                # Only checker name is in the line.\n                checker_name = line.strip()\n            else:\n                # Checker name and description is in one line.\n                match = pattern.match(line.rstrip())\n                if match:\n                    current = match.groupdict()\n                    self.checkers.append((current['checker_name'],\n                                          current['description']))\n\n    def get_analyzer_checkers(self, config_handler, env):\n        \"\"\"\n        Return the list of the supported checkers.\n        \"\"\"\n        if not self.checkers:\n            analyzer_binary = config_handler.analyzer_binary\n\n            command = [analyzer_binary, \"-cc1\"]\n            for plugin in config_handler.analyzer_plugins:\n                command.extend([\"-load\", plugin])\n            command.append(\"-analyzer-checker-help\")\n\n            try:\n                command = shlex.split(' '.join(command))\n                result = subprocess.check_output(command,\n                                                 env=env)\n                self.__parse_checkers(result)\n            except (subprocess.CalledProcessError, OSError):\n                return {}\n\n        return self.checkers\n\n    def construct_analyzer_cmd(self, res_handler):\n        \"\"\"\n        Called by the analyzer method.\n        Construct the analyzer command.\n        \"\"\"\n        try:\n            # Get an output file from the result handler.\n            analyzer_output_file = res_handler.analyzer_result_file\n\n            # Get the checkers list from the config_handler.\n            # Checker order matters.\n            config = self.config_handler\n\n            analyzer_cmd = [config.analyzer_binary]\n\n            analyzer_cmd.extend(self.buildaction.compiler_defines)\n            analyzer_cmd.extend(self.buildaction.compiler_includes)\n\n            if len(config.compiler_resource_dir) > 0:\n                analyzer_cmd.extend(['-resource-dir',\n                                     config.compiler_resource_dir,\n                                     '-isystem',\n                                     config.compiler_resource_dir])\n\n            # Compiling is enough.\n            analyzer_cmd.append('-c')\n\n            analyzer_cmd.append('--analyze')\n\n            # Turn off clang hardcoded checkers list.\n            analyzer_cmd.append('--analyzer-no-default-checks')\n\n            for plugin in config.analyzer_plugins:\n                analyzer_cmd.extend([\"-Xclang\", \"-plugin\",\n                                     \"-Xclang\", \"checkercfg\",\n                                     \"-Xclang\", \"-load\",\n                                     \"-Xclang\", plugin])\n\n            analyzer_mode = 'plist-multi-file'\n            analyzer_cmd.extend(['-Xclang',\n                                 '-analyzer-opt-analyze-headers',\n                                 '-Xclang',\n                                 '-analyzer-output=' + analyzer_mode])\n\n            if config.compiler_sysroot:\n                analyzer_cmd.extend(['--sysroot', config.compiler_sysroot])\n\n            for path in config.system_includes:\n                analyzer_cmd.extend(['-isystem', path])\n\n            for path in config.includes:\n                analyzer_cmd.extend(['-I', path])\n\n            analyzer_cmd.extend(['-o', analyzer_output_file])\n\n            # Config handler stores which checkers are enabled or disabled.\n            for checker_name, value in config.checks().items():\n                enabled, _ = value\n                if enabled:\n                    analyzer_cmd.extend(['-Xclang',\n                                         '-analyzer-checker=' + checker_name])\n                else:\n                    analyzer_cmd.extend(['-Xclang',\n                                         '-analyzer-disable-checker',\n                                         '-Xclang', checker_name])\n\n            # Set language.\n            analyzer_cmd.extend(['-x', self.buildaction.lang])\n\n            analyzer_cmd.append(config.analyzer_extra_arguments)\n\n            analyzer_cmd.extend(self.buildaction.analyzer_options)\n\n            analyzer_cmd.append(self.source_file)\n\n            return analyzer_cmd\n\n        except Exception as ex:\n            LOG.error(ex)\n            return []\n\n    @classmethod\n    def resolve_missing_binary(cls, configured_binary, env):\n        \"\"\"\n        In case of the configured binary for the analyzer is not found in the\n        PATH, this method is used to find a callable binary.\n        \"\"\"\n\n        LOG.debug(configured_binary + \" not found in path for ClangSA!\")\n\n        if os.path.isabs(configured_binary):\n            # Do not autoresolve if the path is an absolute path as there\n            # is nothing we could auto-resolve that way.\n            return False\n\n        # clang, clang-5.0, clang++, clang++-5.1, ...\n        binaries = find_by_regex_in_envpath(\n            r'^clang(\\+\\+)?(-\\d+(\\.\\d+){0,2})?$', env)\n\n        if len(binaries) == 0:\n            return False\n        elif len(binaries) == 1:\n            # Return the first found (earliest in PATH) binary for the only\n            # found binary name group.\n            return binaries.values()[0][0]\n        else:\n            # Select the \"newest\" available clang version if there are multiple\n            keys = list(binaries.keys())\n            keys.sort()\n            files = binaries[keys[-1]]\n            return files[-1]\n","lang_cluster":"C","length":184,"code_uid":"c7b5bebe3ba8428e97b3ed7e40bff6ef"}
{"diff_hunk":"@@ -93,6 +93,11 @@ xfpga_fpgaOpen(fpga_token token, fpga_handle *handle, int flags)\n \t\/\/ Init workspace table\n \t_handle->wsid_root = NULL;\n \n+\t\/\/ Init metric enum\n+\t_handle->metric_enum_status = false;\n+\t_handle->dl_handle = NULL;\n+\t_handle->_bmc_metric_value = NULL;\n+\n \t\/\/ Open resources in exclusive mode unless FPGA_OPEN_SHARED is given\n \topen_flags = O_RDWR | ((flags & FPGA_OPEN_SHARED) ? 0 : O_EXCL);\n \tfddev = open(_token->devpath, open_flags);","old_code":"\/\/ Copyright(c) 2017-2018, Intel Corporation\n\/\/\n\/\/ Redistribution  and  use  in source  and  binary  forms,  with  or  without\n\/\/ modification, are permitted provided that the following conditions are met:\n\/\/\n\/\/ * Redistributions of  source code  must retain the  above copyright notice,\n\/\/   this list of conditions and the following disclaimer.\n\/\/ * Redistributions in binary form must reproduce the above copyright notice,\n\/\/   this list of conditions and the following disclaimer in the documentation\n\/\/   and\/or other materials provided with the distribution.\n\/\/ * Neither the name  of Intel Corporation  nor the names of its contributors\n\/\/   may be used to  endorse or promote  products derived  from this  software\n\/\/   without specific prior written permission.\n\/\/\n\/\/ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n\/\/ AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,  BUT NOT LIMITED TO,  THE\n\/\/ IMPLIED WARRANTIES OF  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n\/\/ ARE DISCLAIMED.  IN NO EVENT  SHALL THE COPYRIGHT OWNER  OR CONTRIBUTORS BE\n\/\/ LIABLE  FOR  ANY  DIRECT,  INDIRECT,  INCIDENTAL,  SPECIAL,  EXEMPLARY,  OR\n\/\/ CONSEQUENTIAL  DAMAGES  (INCLUDING,  BUT  NOT LIMITED  TO,  PROCUREMENT  OF\n\/\/ SUBSTITUTE GOODS OR SERVICES;  LOSS OF USE,  DATA, OR PROFITS;  OR BUSINESS\n\/\/ INTERRUPTION)  HOWEVER CAUSED  AND ON ANY THEORY  OF LIABILITY,  WHETHER IN\n\/\/ CONTRACT,  STRICT LIABILITY,  OR TORT  (INCLUDING NEGLIGENCE  OR OTHERWISE)\n\/\/ ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,  EVEN IF ADVISED OF THE\n\/\/ POSSIBILITY OF SUCH DAMAGE.\n\n#ifdef HAVE_CONFIG_H\n#include <config.h>\n#endif \/\/ HAVE_CONFIG_H\n\n#include \"common_int.h\"\n#include <opae\/access.h>\n#include <opae\/utils.h>\n#include \"types_int.h\"\n\n#include <string.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <ctype.h>\n\n#include \"safe_string\/safe_string.h\"\n\nfpga_result __FPGA_API__\nxfpga_fpgaOpen(fpga_token token, fpga_handle *handle, int flags)\n{\n\tfpga_result result = FPGA_NOT_FOUND;\n\tstruct _fpga_handle *_handle;\n\tstruct _fpga_token *_token;\n\tint fddev = -1;\n\tpthread_mutexattr_t mattr;\n\tint open_flags = 0;\n\n\tif (NULL == token) {\n\t\tFPGA_MSG(\"token is NULL\");\n\t\treturn FPGA_INVALID_PARAM;\n\t}\n\n\tif (NULL == handle) {\n\t\tFPGA_MSG(\"handle is NULL\");\n\t\treturn FPGA_INVALID_PARAM;\n\t}\n\n\tif (flags & ~FPGA_OPEN_SHARED) {\n\t\tFPGA_MSG(\"unrecognized flags\");\n\t\treturn FPGA_INVALID_PARAM;\n\t}\n\n\t_token = (struct _fpga_token *)token;\n\n\tif (_token->magic != FPGA_TOKEN_MAGIC) {\n\t\tFPGA_MSG(\"Invalid token\");\n\t\treturn FPGA_INVALID_PARAM;\n\t}\n\n\t_handle = malloc(sizeof(struct _fpga_handle));\n\tif (NULL == _handle) {\n\t\tFPGA_MSG(\"Failed to allocate memory for handle\");\n\t\treturn FPGA_NO_MEMORY;\n\t}\n\n\tmemset_s(_handle, sizeof(*_handle), 0);\n\n\t\/\/ mark data structure as valid\n\t_handle->magic = FPGA_HANDLE_MAGIC;\n\n\t_handle->token = token;\n\n\t_handle->fdfpgad = -1;\n\n\t\/\/ Init MMIO table\n\t_handle->mmio_root = NULL;\n\n\t\/\/ Init workspace table\n\t_handle->wsid_root = NULL;\n\n\t\/\/ Open resources in exclusive mode unless FPGA_OPEN_SHARED is given\n\topen_flags = O_RDWR | ((flags & FPGA_OPEN_SHARED) ? 0 : O_EXCL);\n\tfddev = open(_token->devpath, open_flags);\n\tif (-1 == fddev) {\n\t\tFPGA_MSG(\"open(%s) failed: %s\", _token->devpath, strerror(errno));\n\t\tswitch (errno) {\n\t\tcase EACCES:\n\t\t\tresult = FPGA_NO_ACCESS;\n\t\t\tbreak;\n\t\tcase EBUSY:\n\t\t\tresult = FPGA_BUSY;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tresult = FPGA_NO_DRIVER;\n\t\t\tbreak;\n\t\t}\n\t\tgoto out_free;\n\t}\n\n\t\/\/ Save the file descriptor for close.\n\t_handle->fddev = fddev;\n\n\tif (pthread_mutexattr_init(&mattr)) {\n\t\tFPGA_MSG(\"Failed to init handle mutex attributes\");\n\t\tresult = FPGA_EXCEPTION;\n\t\tgoto out_free;\n\t}\n\n\tif (pthread_mutexattr_settype(&mattr, PTHREAD_MUTEX_RECURSIVE) ||\n\t    pthread_mutex_init(&_handle->lock, &mattr)) {\n\t\tFPGA_MSG(\"Failed to init handle mutex\");\n\t\tresult = FPGA_EXCEPTION;\n\t\tgoto out_attr_destroy;\n\t}\n\n\tpthread_mutexattr_destroy(&mattr);\n\n\t\/\/ set handle return value\n\t*handle = (void *)_handle;\n\n\treturn FPGA_OK;\n\nout_attr_destroy:\n\tpthread_mutexattr_destroy(&mattr);\n\nout_free:\n\tfree(_handle);\n\n\tif (-1 != fddev) {\n\t\tclose(fddev);\n\t}\n\n\treturn result;\n}\n","lang_cluster":"C","length":149,"code_uid":"c250a325db0e4f72a312b1b65e24079e"}
{"diff_hunk":"@@ -15,22 +15,21 @@\n #include <string.h>\n #include \"parse.h\"\n \n+static tagRegexTable antTagRegexTable [] = {\n+\t{\"^[ \\t]*<[ \\t]*project[^>]+name=\\\"([^\\\"]+)\\\".*\", \"\\\\1\",\n+\t \"p,project,projects\", NULL},\n+\t{\"^[ \\t]*<[ \\t]*target[^>]+name=\\\"([^\\\"]+)\\\".*\", \"\\\\1\",\n+\t \"t,target,targets\", NULL},\n+\t{\"^[ \\t]*<[ \\t]*property[^>]+name=\\\"([^\\\"]+)\\\".*\", \"\\\\1\",\n+\t \"P,property,property\", NULL},\n+\t{\"^[ \\t]*<[ \\t]*import[^>]+file=\\\"([^\\\"]+)\\\".*\", \"\\\\1\",\n+\t \"i,import,imports\", NULL}\n+};\n+\n \/*\n *   FUNCTION DEFINITIONS\n *\/\n \n-static void installAntRegex (const langType language)\n-{\n-\taddTagRegex (language,\n-\t\t\"^[ \\t]*<[ \\t]*project[^>]+name=\\\"([^\\\"]+)\\\".*\", \"\\\\1\", \"p,project,projects\", NULL);\n-\taddTagRegex (language,\n-\t\t\"^[ \\t]*<[ \\t]*target[^>]+name=\\\"([^\\\"]+)\\\".*\", \"\\\\1\", \"t,target,targets\", NULL);\n-\taddTagRegex (language,\n-\t\t\"^[ \\t]*<[ \\t]*property[^>]+name=\\\"([^\\\"]+)\\\".*\", \"\\\\1\", \"P,property,property\", NULL);\n-\taddTagRegex (language,\n-\t\t\"^[ \\t]*<[ \\t]*import[^>]+file=\\\"([^\\\"]+)\\\".*\", \"\\\\1\", \"i,import,imports\", NULL);\n-}\n-\n extern parserDefinition* AntParser (void)\n {\n \tstatic const char *const extensions [] = { \"build.xml\", NULL };","old_code":"\/*\n*   Copyright (c) 2008, David Fishburn\n*\n*   This source code is released for free distribution under the terms of the\n*   GNU General Public License version 2 or (at your option) any later version.\n*\n*   This module contains functions for generating tags for Ant language files.\n*\/\n\n\/*\n*   INCLUDE FILES\n*\/\n#include \"general.h\"  \/* must always come first *\/\n\n#include <string.h>\n#include \"parse.h\"\n\n\/*\n*   FUNCTION DEFINITIONS\n*\/\n\nstatic void installAntRegex (const langType language)\n{\n\taddTagRegex (language,\n\t\t\"^[ \\t]*<[ \\t]*project[^>]+name=\\\"([^\\\"]+)\\\".*\", \"\\\\1\", \"p,project,projects\", NULL);\n\taddTagRegex (language,\n\t\t\"^[ \\t]*<[ \\t]*target[^>]+name=\\\"([^\\\"]+)\\\".*\", \"\\\\1\", \"t,target,targets\", NULL);\n\taddTagRegex (language,\n\t\t\"^[ \\t]*<[ \\t]*property[^>]+name=\\\"([^\\\"]+)\\\".*\", \"\\\\1\", \"P,property,property\", NULL);\n\taddTagRegex (language,\n\t\t\"^[ \\t]*<[ \\t]*import[^>]+file=\\\"([^\\\"]+)\\\".*\", \"\\\\1\", \"i,import,imports\", NULL);\n}\n\nextern parserDefinition* AntParser (void)\n{\n\tstatic const char *const extensions [] = { \"build.xml\", NULL };\n\tstatic const char *const patterns [] = { \"build.xml\", NULL };\n\tparserDefinition* const def = parserNew (\"Ant\");\n\tdef->extensions = extensions;\n\tdef->patterns = patterns;\n\tdef->initialize = installAntRegex;\n\tdef->method     = METHOD_NOT_CRAFTED|METHOD_REGEX;\n\treturn def;\n}\n\n\/* vi:set tabstop=4 shiftwidth=4: *\/\n","lang_cluster":"C","length":46,"code_uid":"c1767a6fbc9f401ca2fc894f7bfbecf2"}
{"diff_hunk":"@@ -10,32 +10,25 @@\n #include <stdlib.h>\n #include <dlfcn.h>\n #include <errno.h>\n+#include <fcntl.h>\n #include <string.h>\n #include <signal.h>\n #include <inttypes.h>\n #include <stdarg.h>\n #include <unistd.h>\n+#include <string.h>\n #include <sys\/types.h>\n+#include <sys\/stat.h>\n \n-ssize_t write(int fd, const void *buf, size_t count) {\n-\n-\t__typeof__(write) *original_write = dlsym(RTLD_NEXT, \"write\");\n-\n-\tint real_count;\n-\tint prev_errno = errno;\n-\terrno = 0;\n-\treal_count = original_write(fd, buf, count);\n-\n-\tif(real_count < 0 && errno == ENOSPC) {\n-\t\toriginal_write(STDERR_FILENO, \"WRITE ERROR: device capacity reached.\\n\", 39);\n-\t\treturn real_count;\n-\t}\n-\n-\tif(!errno) {\n-\t   errno = prev_errno;\n+char *strconcat(char *s1, char *s2)\n+{\n+    char *result = malloc(strlen(s1)+strlen(s2)+1);\n+\tif(!result) {\n+\t\treturn \"\";\n \t}\n-\n-\treturn real_count;\n+    strcpy(result, s1);\n+    strcat(result, s2);\n+    return result;\n }\n \n int open(const char *path, int flags, ...)","old_code":"#if defined(__linux__) && !defined(_GNU_SOURCE)\n#define _GNU_SOURCE \/\/ Aaaaaah!!\n#endif\n\n#if !defined(RTLD_NEXT)\n#define RTLD_NEXT 0\n#endif\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <dlfcn.h>\n#include <errno.h>\n#include <string.h>\n#include <signal.h>\n#include <inttypes.h>\n#include <stdarg.h>\n#include <unistd.h>\n#include <sys\/types.h>\n\nssize_t write(int fd, const void *buf, size_t count) {\n\n\t__typeof__(write) *original_write = dlsym(RTLD_NEXT, \"write\");\n\n\tint real_count;\n\tint prev_errno = errno;\n\terrno = 0;\n\treal_count = original_write(fd, buf, count);\n\n\tif(real_count < 0 && errno == ENOSPC) {\n\t\toriginal_write(STDERR_FILENO, \"WRITE ERROR: device capacity reached.\\n\", 39);\n\t\treturn real_count;\n\t}\n\n\tif(!errno) {\n\t   errno = prev_errno;\n\t}\n\n\treturn real_count;\n}\n\nint open(const char *path, int flags, ...)\n{\n\n\t__typeof__(open) *original_open = dlsym(RTLD_NEXT, \"open\");\n\n\tva_list ap;\n\tint fd;\n\tint mode;\n\tint prev_errno = errno;\n\n\tva_start(ap, flags);\n\tmode = va_arg(ap, int);\n\tva_end(ap);\n\n\tfd = original_open(path, flags, mode);\n\n\tif(fd == -1 && errno == ENOSPC) {\n\t\tfprintf(stderr, \"OPEN ERROR: inode capacity reached.\\n\");\n\t\treturn fd;\n\t}\n\n\tif(!errno) {\n\t\terrno = prev_errno;\n\t}\n\n\treturn fd;\n}\n","lang_cluster":"C","length":67,"code_uid":"ef02b938a60e46fa9fbff7253da91c28"}
{"diff_hunk":"@@ -93,6 +93,13 @@ def parse_compile_commands_json(logfile, add_compiler_defaults=False):\n     counter = 0\n     for entry in data:\n         sourcefile = entry['file']\n+\n+        if not os.path.isabs(sourcefile):\n+            # Newest versions of intercept-build can create the 'file' in the\n+            # JSON Compilation Database as a relative path.\n+            sourcefile = os.path.join(os.path.abspath(entry['directory']),\n+                                      sourcefile)\n+\n         lang = option_parser.get_language(sourcefile[sourcefile.rfind('.'):])\n \n         if not lang:","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\nimport os\nimport sys\nimport traceback\nimport subprocess\nimport shlex\n\n# TODO: This is a cross-subpackage import!\nfrom libcodechecker.log import build_action\nfrom libcodechecker.log import option_parser\nfrom libcodechecker.logger import LoggerFactory\n\nLOG = LoggerFactory.get_new_logger('LOG PARSER')\n\n\n# -----------------------------------------------------------------------------\ndef get_compiler_includes(compiler):\n    \"\"\"\n    Returns a list of default includes of the given compiler.\n    \"\"\"\n    LOG.debug('getting include paths for  ' + compiler)\n    start_mark = \"#include <...> search starts here:\"\n    end_mark = \"End of search list.\"\n\n    cmd = compiler + \" -E -x c++ - -v \"  # what if not c++?\n    include_paths = []\n    try:\n        proc = subprocess.Popen(shlex.split(cmd),\n                                stdin=subprocess.PIPE,\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE)\n\n        out, err = proc.communicate(\"\")\n\n        do_append = False\n        for line in err.splitlines(True):\n            line = line.strip()\n            if line.startswith(end_mark):\n                do_append = False\n            if do_append:\n                include_paths.append(\"-I\"+line)\n            if line.startswith(start_mark):\n                do_append = True\n\n    except OSError as oerr:\n        LOG.error(\"Cannot find include paths:\" + oerr.strerror+\"\\n\")\n    return include_paths\n\n\n# -----------------------------------------------------------------------------\ndef get_compiler_defines(compiler):\n    \"\"\"\n    Returns a list of default defines of the given compiler.\n    \"\"\"\n    cmd = compiler + \" -dM -E -\"\n    defines = []\n    try:\n        with open(os.devnull, 'r') as FNULL:\n            proc = subprocess.Popen(shlex.split(cmd),\n                                    stdin=FNULL,\n                                    stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE)\n            out, err = proc.communicate(\"\")\n            for line in out.splitlines(True):\n                LOG.debug(\"define:\"+line)\n                define = line.strip().split(\" \")[1:]\n                d = \"-D\"+define[0] + '=' + '\"' + ' '.join(define[1:]) + '\"'\n                defines.append(d)\n    except OSError as oerr:\n        LOG.error(\"Cannot find defines:\" + oerr.strerror+\"\\n\")\n    return defines\n\n\n# -----------------------------------------------------------------------------\ndef parse_compile_commands_json(logfile, add_compiler_defaults=False):\n    import json\n    LOG.debug('parse_compile_commands_json: ' + str(add_compiler_defaults))\n\n    actions = []\n    filtered_build_actions = {}\n\n    logfile.seek(0)\n    data = json.load(logfile)\n\n    compiler_defines = {}\n    compiler_includes = {}\n\n    counter = 0\n    for entry in data:\n        sourcefile = entry['file']\n        lang = option_parser.get_language(sourcefile[sourcefile.rfind('.'):])\n\n        if not lang:\n            continue\n\n        action = build_action.BuildAction(counter)\n\n        command = entry['command']\n        results = option_parser.parse_options(command)\n\n        action.original_command = command\n        action.analyzer_options = results.compile_opts\n        action.lang = results.lang\n        action.target = results.arch\n\n        # store the compiler built in include paths\n        # and defines\n        if add_compiler_defaults and results.compiler:\n            if not (results.compiler in compiler_defines):\n                compiler_defines[results.compiler] = \\\n                    get_compiler_defines(results.compiler)\n                compiler_includes[results.compiler] = \\\n                    get_compiler_includes(results.compiler)\n            action.compiler_defines = compiler_defines[results.compiler]\n            action.compiler_includes = compiler_includes[results.compiler]\n\n        if results.action == option_parser.ActionType.COMPILE or \\\n           results.action == option_parser.ActionType.LINK:\n            action.skip = False\n\n        # TODO: check arch.\n        action.directory = entry['directory']\n        action.sources = sourcefile\n        # Filter out duplicate compilation commands.\n        unique_key = action.cmp_key\n        if filtered_build_actions.get(unique_key) is None:\n            filtered_build_actions[unique_key] = action\n\n        del action\n        counter += 1\n\n    for ba_hash, ba in filtered_build_actions.items():\n        actions.append(ba)\n\n    return actions\n\n\n# -----------------------------------------------------------------------------\ndef parse_log(logfilepath, add_compiler_defaults=False):\n    LOG.debug('Parsing log file: ' + logfilepath)\n    actions = []\n\n    with open(logfilepath) as logfile:\n        try:\n            actions = \\\n                parse_compile_commands_json(logfile, add_compiler_defaults)\n        except (ValueError, KeyError, TypeError) as ex:\n            if os.stat(logfilepath).st_size == 0:\n                LOG.error('The compile database is empty.')\n            else:\n                LOG.error('The compile database is not valid.')\n            LOG.debug(traceback.format_exc())\n            LOG.debug(ex)\n            sys.exit(1)\n\n    LOG.debug('Parsing log file done.')\n    return actions\n","lang_cluster":"C","length":162,"code_uid":"3431ec3029b948639ec40605deb79d6e"}
{"diff_hunk":"@@ -12,6 +12,11 @@ import hashlib\n \n \n class BuildAction(object):\n+    LINK = 0\n+    COMPILE = 1\n+    PREPROCESS = 2\n+    INFO = 3\n+\n     def __init__(self, build_action_id=0):\n         self._id = build_action_id\n         # Filtered list of options.","old_code":"# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n\"\"\"\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport hashlib\n\n\nclass BuildAction(object):\n    def __init__(self, build_action_id=0):\n        self._id = build_action_id\n        # Filtered list of options.\n        self.analyzer_options = []\n        self.compiler_includes = []\n        self.analyzer_type = -1\n        self._original_command = ''\n        self.directory = ''\n        self.output = ''\n        self.lang = None\n        self.target = ''\n        self.source_count = 0\n        self._sources = []\n\n    def __str__(self):\n        # For debugging.\n        return ('Id: {0} ,\\nOriginal command: {1},\\n'\n                'Analyzer type: {2},\\n Analyzer options: {3},\\n'\n                'Directory: {4},\\nOutput: {5},\\nLang: {6},\\nTarget: {7},\\n'\n                'Source count {8},\\nSources: {9}'). \\\n            format(self._id, self._original_command,\n                   self.analyzer_type, self.analyzer_options,\n                   self.directory, self.output, self.lang, self.target,\n                   self.source_count, self._sources)\n\n    @property\n    def id(self):\n        return self._id\n\n    @property\n    def original_command(self):\n        return self._original_command\n\n    @property\n    def original_command_hash(self):\n        hash_object = hashlib.sha1(self._original_command)\n        hex_dig = hash_object.hexdigest()\n        return hex_dig\n\n    @original_command.setter\n    def original_command(self, value):\n        self._original_command = value\n\n    @property\n    def sources(self):\n        for source in self._sources:\n            yield source\n\n    @sources.setter\n    def sources(self, value):\n        self._sources.append(value)\n        self.source_count += 1\n\n    def __eq__(self, other):\n        return other._original_command == self._original_command\n\n    @property\n    def cmp_key(self):\n        \"\"\"\n        If the compilation database contains the same compilation action\n        multiple times it should be checked only once.\n        Use this key to compare compilation commands for the analysis.\n        \"\"\"\n        hash_content = []\n        hash_content.extend(self.analyzer_options)\n        hash_content.append(str(self.analyzer_type))\n        hash_content.append(self.target)\n        hash_content.extend(self.sources)\n        return hashlib.sha1(''.join(hash_content)).hexdigest()\n","lang_cluster":"C","length":83,"code_uid":"465202a09a4541cd8210b837a2b0a69a"}
{"diff_hunk":"@@ -23,17 +23,28 @@\n \/\/ win32\n #include <time.h>\n \n+#if defined(__MINGW32__)\n+  #include <sys\/time.h>\n+#endif\n+\n #if defined(_MSC_VER) || defined(_MSC_EXTENSIONS)\n #define DELTA_EPOCH_IN_MICROSECS 11644473600000000Ui64\n #else\n #define DELTA_EPOCH_IN_MICROSECS 11644473600000000ULL\n #endif\n \n+#if !defined(__MINGW32__)\n struct timezone {\n   int tz_minuteswest; \/* minutes W of Greenwich *\/\n   int tz_dsttime;     \/* type of dst correction *\/\n };\n+#endif\n \n+#if defined(__MINGW32__)\n+int thrift_gettimeofday(struct timeval* tv, struct timezone* tz) {\n+  return gettimeofday(tv,tz);\n+}\n+#else\n int thrift_gettimeofday(struct timeval* tv, struct timezone* tz) {\n   FILETIME ft;\n   unsigned __int64 tmpres(0);","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements. See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership. The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License. You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\n#include <thrift\/windows\/GetTimeOfDay.h>\n#include <thrift\/thrift-config.h>\n\n\/\/ win32\n#include <time.h>\n\n#if defined(_MSC_VER) || defined(_MSC_EXTENSIONS)\n#define DELTA_EPOCH_IN_MICROSECS 11644473600000000Ui64\n#else\n#define DELTA_EPOCH_IN_MICROSECS 11644473600000000ULL\n#endif\n\nstruct timezone {\n  int tz_minuteswest; \/* minutes W of Greenwich *\/\n  int tz_dsttime;     \/* type of dst correction *\/\n};\n\nint thrift_gettimeofday(struct timeval* tv, struct timezone* tz) {\n  FILETIME ft;\n  unsigned __int64 tmpres(0);\n  static int tzflag;\n\n  if (NULL != tv) {\n    GetSystemTimeAsFileTime(&ft);\n\n    tmpres |= ft.dwHighDateTime;\n    tmpres <<= 32;\n    tmpres |= ft.dwLowDateTime;\n\n    \/*converting file time to unix epoch*\/\n    tmpres -= DELTA_EPOCH_IN_MICROSECS;\n    tmpres \/= 10; \/*convert into microseconds*\/\n    tv->tv_sec = (long)(tmpres \/ 1000000UL);\n    tv->tv_usec = (long)(tmpres % 1000000UL);\n  }\n\n  if (NULL != tz) {\n    if (!tzflag) {\n      _tzset();\n      tzflag++;\n    }\n\n    long time_zone(0);\n    errno_t err(_get_timezone(&time_zone));\n    if (err == NO_ERROR) {\n      tz->tz_minuteswest = time_zone \/ 60;\n    } else {\n      return -1;\n    }\n\n    int day_light(0);\n    err = (_get_daylight(&day_light));\n    if (err == NO_ERROR) {\n      tz->tz_dsttime = day_light;\n      return 0;\n    } else {\n      return -1;\n    }\n  }\n\n  return 0;\n}\n\nint thrift_sleep(unsigned int seconds) {\n  ::Sleep(seconds * 1000);\n  return 0;\n}\nint thrift_usleep(unsigned int microseconds) {\n  unsigned int milliseconds = (microseconds + 999) \/ 1000;\n  ::Sleep(milliseconds);\n  return 0;\n}\n\nchar* thrift_ctime_r(const time_t* _clock, char* _buf) {\n  strcpy(_buf, ctime(_clock));\n  return _buf;\n}\n","lang_cluster":"C","length":96,"code_uid":"3cd4565d792d4cf69aa4ee13df9b5b45"}
{"diff_hunk":"@@ -57,11 +57,8 @@ static void handle_destroy(struct wl_listener *listener, void *data) {\n \tstruct roots_wl_shell_surface *roots_surface =\n \t\twl_container_of(listener, roots_surface, destroy);\n \twl_list_remove(&roots_surface->destroy.link);\n-\twl_list_remove(&roots_surface->ping_timeout.link);\n \twl_list_remove(&roots_surface->request_move.link);\n \twl_list_remove(&roots_surface->request_resize.link);\n-\twl_list_remove(&roots_surface->request_set_fullscreen.link);\n-\twl_list_remove(&roots_surface->request_set_maximized.link);\n \tview_destroy(roots_surface->view);\n \tfree(roots_surface);\n }","old_code":"#include <assert.h>\n#include <stdlib.h>\n#include <stdbool.h>\n#include <wayland-server.h>\n#include <wlr\/types\/wlr_box.h>\n#include <wlr\/types\/wlr_surface.h>\n#include <wlr\/types\/wlr_wl_shell.h>\n#include <wlr\/util\/log.h>\n#include \"rootston\/desktop.h\"\n#include \"rootston\/server.h\"\n#include \"rootston\/input.h\"\n\nstatic void resize(struct roots_view *view, uint32_t width, uint32_t height) {\n\tassert(view->type == ROOTS_WL_SHELL_VIEW);\n\tstruct wlr_wl_shell_surface *surf = view->wl_shell_surface;\n\twlr_wl_shell_surface_configure(surf, WL_SHELL_SURFACE_RESIZE_NONE, width,\n\t\theight);\n}\n\nstatic void close(struct roots_view *view) {\n\tassert(view->type == ROOTS_WL_SHELL_VIEW);\n\tstruct wlr_wl_shell_surface *surf = view->wl_shell_surface;\n\twl_client_destroy(surf->client);\n}\n\nstatic void handle_request_move(struct wl_listener *listener, void *data) {\n\tstruct roots_wl_shell_surface *roots_surface =\n\t\twl_container_of(listener, roots_surface, request_move);\n\tstruct roots_view *view = roots_surface->view;\n\tstruct roots_input *input = view->desktop->server->input;\n\tstruct wlr_wl_shell_surface_move_event *e = data;\n\tconst struct roots_input_event *event = get_input_event(input, e->serial);\n\tif (!event || input->mode != ROOTS_CURSOR_PASSTHROUGH) {\n\t\treturn;\n\t}\n\tview_begin_move(input, event->cursor, view);\n}\n\nstatic void handle_request_resize(struct wl_listener *listener, void *data) {\n\tstruct roots_wl_shell_surface *roots_surface =\n\t\twl_container_of(listener, roots_surface, request_resize);\n\tstruct roots_view *view = roots_surface->view;\n\tstruct roots_input *input = view->desktop->server->input;\n\tstruct wlr_wl_shell_surface_resize_event *e = data;\n\tconst struct roots_input_event *event = get_input_event(input, e->serial);\n\tif (!event || input->mode != ROOTS_CURSOR_PASSTHROUGH) {\n\t\treturn;\n\t}\n\tview_begin_resize(input, event->cursor, view, e->edges);\n}\n\nstatic void handle_surface_commit(struct wl_listener *listener, void *data) {\n\t\/\/ TODO do we need to do anything here?\n}\n\nstatic void handle_destroy(struct wl_listener *listener, void *data) {\n\tstruct roots_wl_shell_surface *roots_surface =\n\t\twl_container_of(listener, roots_surface, destroy);\n\twl_list_remove(&roots_surface->destroy.link);\n\twl_list_remove(&roots_surface->ping_timeout.link);\n\twl_list_remove(&roots_surface->request_move.link);\n\twl_list_remove(&roots_surface->request_resize.link);\n\twl_list_remove(&roots_surface->request_set_fullscreen.link);\n\twl_list_remove(&roots_surface->request_set_maximized.link);\n\tview_destroy(roots_surface->view);\n\tfree(roots_surface);\n}\n\nstatic int shell_surface_compare_equals(const void *item, const void *cmp_to) {\n\tconst struct roots_view *view = item;\n\tif (view->type == ROOTS_WL_SHELL_VIEW && view->wl_shell_surface == cmp_to) {\n\t\treturn 0;\n\t}\n\treturn -1;\n}\n\nvoid handle_wl_shell_surface(struct wl_listener *listener, void *data) {\n\tstruct roots_desktop *desktop =\n\t\twl_container_of(listener, desktop, wl_shell_surface);\n\n\tstruct wlr_wl_shell_surface *surface = data;\n\twlr_log(L_DEBUG, \"new shell surface: title=%s, class=%s\",\n\t\tsurface->title, surface->class);\n\twlr_wl_shell_surface_ping(surface);\n\n\tstruct roots_wl_shell_surface *roots_surface =\n\t\tcalloc(1, sizeof(struct roots_wl_shell_surface));\n\tif (!roots_surface) {\n\t\treturn;\n\t}\n\twl_list_init(&roots_surface->destroy.link);\n\troots_surface->destroy.notify = handle_destroy;\n\twl_signal_add(&surface->events.destroy, &roots_surface->destroy);\n\twl_list_init(&roots_surface->ping_timeout.link);\n\twl_list_init(&roots_surface->request_move.link);\n\troots_surface->request_move.notify = handle_request_move;\n\twl_signal_add(&surface->events.request_move, &roots_surface->request_move);\n\twl_list_init(&roots_surface->request_resize.link);\n\troots_surface->request_resize.notify = handle_request_resize;\n\twl_signal_add(&surface->events.request_resize,\n\t\t&roots_surface->request_resize);\n\twl_list_init(&roots_surface->request_set_fullscreen.link);\n\twl_list_init(&roots_surface->request_set_maximized.link);\n\twl_list_init(&roots_surface->surface_commit.link);\n\troots_surface->surface_commit.notify = handle_surface_commit;\n\twl_signal_add(&surface->surface->events.commit,\n\t\t&roots_surface->surface_commit);\n\n\tstruct roots_view *view = calloc(1, sizeof(struct roots_view));\n\tview->type = ROOTS_WL_SHELL_VIEW;\n\n\tview->wl_shell_surface = surface;\n\tview->roots_wl_shell_surface = roots_surface;\n\tview->wlr_surface = surface->surface;\n\tview->resize = resize;\n\tview->close = close;\n\tview->desktop = desktop;\n\troots_surface->view = view;\n\tlist_add(desktop->views, view);\n\tview_initialize(view);\n\n\tif (surface->state == WLR_WL_SHELL_SURFACE_STATE_TRANSIENT) {\n\t\t\/\/ we need to map it relative to the parent\n\t\tint i =\n\t\t\tlist_seq_find(desktop->views,\n\t\t\t\tshell_surface_compare_equals, surface->parent);\n\t\tif (i != -1) {\n\t\t\tstruct roots_view *parent = desktop->views->items[i];\n\t\t\tview_set_position(view,\n\t\t\t\tparent->x + surface->transient_state->x,\n\t\t\t\tparent->y + surface->transient_state->y);\n\t\t}\n\t}\n}\n","lang_cluster":"C","length":134,"code_uid":"bf5db1cbe3664a87b22909966c6388da"}
{"diff_hunk":"@@ -59,3 +59,37 @@ int h2o_http3_decode_priority_update_frame(h2o_http3_priority_update_frame_t *fr\n \n     return 0;\n }\n+\n+size_t h2o_http3_goaway_frame_capacity(quicly_stream_id_t stream_or_push_id)\n+{\n+    return 1   \/* type *\/\n+           + 1 \/* length field. length should be less than 64, so 1 byte should be enough to represent it *\/\n+           + quicly_encodev_capacity(stream_or_push_id);\n+}\n+\n+uint8_t *h2o_http3_encode_goaway_frame(uint8_t *dst, quicly_stream_id_t stream_or_push_id)\n+{\n+    *dst++ = H2O_HTTP3_FRAME_TYPE_GOAWAY;                \/* type *\/\n+    *dst++ = quicly_encodev_capacity(stream_or_push_id); \/* payload length *\/\n+    dst = quicly_encodev(dst, stream_or_push_id);\n+\n+    return dst;\n+}\n+\n+int h2o_http3_decode_goaway_frame(h2o_http3_goaway_frame_t *frame, const uint8_t *payload, size_t len, const char **err_desc)\n+{\n+    const uint8_t *src = payload, *end = src + len;\n+\n+    \/* quicly_decodev below will reject len == 0 case *\/\n+    if (len > PTLS_ENCODE_QUICINT_CAPACITY)\n+        goto Fail;\n+\n+    if ((frame->stream_or_push_id = quicly_decodev(&src, end)) == UINT64_MAX)\n+        goto Fail;\n+\n+    return 0;\n+\n+Fail:\n+    *err_desc = \"Invalid GOAWAY frame\";\n+    return H2O_HTTP3_ERROR_FRAME;\n+}","old_code":"\/*\n * Copyright (c) 2019 Fastly, Kazuho Oku\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n *\/\n#include \"h2o\/absprio.h\"\n#include \"h2o\/http3_common.h\"\n\nuint8_t *h2o_http3_encode_priority_update_frame(uint8_t *dst, const h2o_http3_priority_update_frame_t *frame)\n{\n    *dst++ = H2O_HTTP3_FRAME_TYPE_PRIORITY_UPDATE;\n    *dst++ = frame->element_is_push ? 0x80 : 0;\n    dst = quicly_encodev(dst, frame->element);\n    *dst++ = 'u';\n    *dst++ = '=';\n    *dst++ = '0' + frame->priority.urgency;\n    if (!frame->priority.incremental) {\n        static const h2o_iovec_t s = {H2O_STRLIT(\",i=1\")};\n        memcpy(dst, s.base, s.len);\n        dst += s.len;\n    }\n    return dst;\n}\n\nint h2o_http3_decode_priority_update_frame(h2o_http3_priority_update_frame_t *frame, const uint8_t *payload, size_t len,\n                                           const char **err_desc)\n{\n    const uint8_t *src = payload, *end = src + len;\n\n    if (src == end)\n        return H2O_HTTP3_ERROR_FRAME;\n    frame->element_is_push = (*src++ & 0x80) != 0;\n    if ((frame->element = quicly_decodev(&src, end)) == UINT64_MAX) {\n        *err_desc = \"invalid PRIORITY frame\";\n        return H2O_HTTP3_ERROR_FRAME;\n    }\n    if (!frame->element_is_push) {\n        if (!(quicly_stream_is_client_initiated(frame->element) && !quicly_stream_is_unidirectional(frame->element)))\n            return H2O_HTTP3_ERROR_FRAME;\n    }\n    frame->priority = h2o_absprio_default;\n    h2o_absprio_parse_priority((const char *)src, end - src, &frame->priority);\n\n    return 0;\n}\n","lang_cluster":"C","length":61,"code_uid":"fe7d18486d104a8f83b37d620fb63658"}
{"diff_hunk":"@@ -159,17 +159,17 @@ static struct flb_config_map config_map[] = {\n     {\n      FLB_CONFIG_MAP_STR, \"format\", NULL,\n      0, FLB_FALSE, 0,\n-     NULL\n+     \"Specifies the data format to be printed. Supported formats are msgpack json, json_lines and json_stream.\"\n     },\n     {\n      FLB_CONFIG_MAP_STR, \"json_date_format\", NULL,\n      0, FLB_FALSE, 0,\n-     NULL\n+    \"Specifies the name of the date field in output.\"\n     },\n     {\n      FLB_CONFIG_MAP_STR, \"json_date_key\", \"date\",\n      0, FLB_TRUE, offsetof(struct flb_stdout, json_date_key),\n-     NULL\n+    \"Specifies the format of the date. Supported formats are double, iso8601 and epoch.\"\n     },\n \n     \/* EOF *\/","old_code":"\/* -*- Mode: C; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- *\/\n\n\/*  Fluent Bit\n *  ==========\n *  Copyright (C) 2019-2020 The Fluent Bit Authors\n *  Copyright (C) 2015-2018 Treasure Data Inc.\n *\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n *\/\n\n#include <fluent-bit\/flb_output_plugin.h>\n#include <fluent-bit\/flb_utils.h>\n#include <fluent-bit\/flb_slist.h>\n#include <fluent-bit\/flb_time.h>\n#include <fluent-bit\/flb_pack.h>\n#include <fluent-bit\/flb_config_map.h>\n#include <msgpack.h>\n\n#include \"stdout.h\"\n\nstatic int cb_stdout_init(struct flb_output_instance *ins,\n                          struct flb_config *config, void *data)\n{\n    int ret;\n    const char *tmp;\n    struct flb_stdout *ctx = NULL;\n    (void) ins;\n    (void) config;\n    (void) data;\n\n    ctx = flb_calloc(1, sizeof(struct flb_stdout));\n    if (!ctx) {\n        flb_errno();\n        return -1;\n    }\n    ctx->ins = ins;\n\n    ret = flb_output_config_map_set(ins, (void *) ctx);\n    if (ret == -1) {\n        flb_free(ctx);\n        return -1;\n    }\n\n    ctx->out_format = FLB_PACK_JSON_FORMAT_NONE;\n    tmp = flb_output_get_property(\"format\", ins);\n    if (tmp) {\n        ret = flb_pack_to_json_format_type(tmp);\n        if (ret == -1) {\n            flb_plg_error(ctx->ins, \"unrecognized 'format' option. \"\n                          \"Using 'msgpack'\");\n        }\n        else {\n            ctx->out_format = ret;\n        }\n    }\n\n    \/* Date format for JSON output *\/\n    ctx->json_date_format = FLB_PACK_JSON_DATE_DOUBLE;\n    tmp = flb_output_get_property(\"json_date_format\", ins);\n    if (tmp) {\n        ret = flb_pack_to_json_date_type(tmp);\n        if (ret == -1) {\n            flb_plg_error(ctx->ins, \"invalid json_date_format '%s'. \"\n                          \"Using 'double' type\", tmp);\n        }\n        else {\n            ctx->json_date_format = ret;\n        }\n    }\n\n    \/* Export context *\/\n    flb_output_set_context(ins, ctx);\n\n    return 0;\n}\n\nstatic void cb_stdout_flush(const void *data, size_t bytes,\n                            const char *tag, int tag_len,\n                            struct flb_input_instance *i_ins,\n                            void *out_context,\n                            struct flb_config *config)\n{\n    msgpack_unpacked result;\n    size_t off = 0, cnt = 0;\n    struct flb_stdout *ctx = out_context;\n    flb_sds_t json;\n    char *buf = NULL;\n    (void) i_ins;\n    (void) config;\n    struct flb_time tmp;\n    msgpack_object *p;\n\n    if (ctx->out_format != FLB_PACK_JSON_FORMAT_NONE) {\n        json = flb_pack_msgpack_to_json_format(data, bytes,\n                                               ctx->out_format,\n                                               ctx->json_date_format,\n                                               ctx->json_date_key);\n        write(STDOUT_FILENO, json, flb_sds_len(json));\n        flb_sds_destroy(json);\n\n        \/*\n         * If we are 'not' in json_lines mode, we need to add an extra\n         * breakline.\n         *\/\n        if (ctx->out_format != FLB_PACK_JSON_FORMAT_LINES) {\n            printf(\"\\n\");\n        }\n        fflush(stdout);\n    }\n    else {\n        \/* A tag might not contain a NULL byte *\/\n        buf = flb_malloc(tag_len + 1);\n        if (!buf) {\n            flb_errno();\n            FLB_OUTPUT_RETURN(FLB_RETRY);\n        }\n        memcpy(buf, tag, tag_len);\n        buf[tag_len] = '\\0';\n        msgpack_unpacked_init(&result);\n        while (msgpack_unpack_next(&result, data, bytes, &off) == MSGPACK_UNPACK_SUCCESS) {\n            printf(\"[%zd] %s: [\", cnt++, buf);\n            flb_time_pop_from_msgpack(&tmp, &result, &p);\n            printf(\"%\"PRIu32\".%09lu, \", (uint32_t)tmp.tm.tv_sec, tmp.tm.tv_nsec);\n            msgpack_object_print(stdout, *p);\n            printf(\"]\\n\");\n        }\n        msgpack_unpacked_destroy(&result);\n        flb_free(buf);\n    }\n    fflush(stdout);\n\n    FLB_OUTPUT_RETURN(FLB_OK);\n}\n\nstatic int cb_stdout_exit(void *data, struct flb_config *config)\n{\n    struct flb_stdout *ctx = data;\n\n    if (!ctx) {\n        return 0;\n    }\n\n    flb_free(ctx);\n    return 0;\n}\n\n\/* Configuration properties map *\/\nstatic struct flb_config_map config_map[] = {\n    {\n     FLB_CONFIG_MAP_STR, \"format\", NULL,\n     0, FLB_FALSE, 0,\n     NULL\n    },\n    {\n     FLB_CONFIG_MAP_STR, \"json_date_format\", NULL,\n     0, FLB_FALSE, 0,\n     NULL\n    },\n    {\n     FLB_CONFIG_MAP_STR, \"json_date_key\", \"date\",\n     0, FLB_TRUE, offsetof(struct flb_stdout, json_date_key),\n     NULL\n    },\n\n    \/* EOF *\/\n    {0}\n};\n\n\/* Plugin registration *\/\nstruct flb_output_plugin out_stdout_plugin = {\n    .name         = \"stdout\",\n    .description  = \"Prints events to STDOUT\",\n    .cb_init      = cb_stdout_init,\n    .cb_flush     = cb_stdout_flush,\n    .cb_exit      = cb_stdout_exit,\n    .flags        = 0,\n    .config_map   = config_map\n};\n","lang_cluster":"C","length":188,"code_uid":"a4da6962b0e64de6ab1e5d3ad6bb42ea"}
{"diff_hunk":"@@ -77,9 +77,6 @@ struct flb_td *td_config_init(struct flb_output_instance *ins)\n             return NULL;\n         }\n     }\n-    else {\n-        ctx->region = FLB_TD_REGION_US;\n-    }\n \n     flb_plg_info(ctx->ins, \"Treasure Data \/ database='%s' table='%s'\",\n                  ctx->db_name, ctx->db_table);","old_code":"\/* -*- Mode: C; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- *\/\n\n\/*  Fluent Bit\n *  ==========\n *  Copyright (C) 2019-2020 The Fluent Bit Authors\n *  Copyright (C) 2015-2018 Treasure Data Inc.\n *\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n *\/\n\n#include <fluent-bit\/flb_output_plugin.h>\n#include \"td_config.h\"\n#include <stdlib.h>\n\nstruct flb_td *td_config_init(struct flb_output_instance *ins)\n{\n    const char *tmp;\n    const char *api;\n    const char *db_name;\n    const char *db_table;\n    struct flb_td *ctx;\n\n    \/* Validate TD section keys *\/\n    api = flb_output_get_property(\"API\", ins);\n    db_name = flb_output_get_property(\"Database\", ins);\n    db_table = flb_output_get_property(\"Table\", ins);\n\n    if (!api) {\n        flb_plg_error(ins, \"error reading API key value\");\n        return NULL;\n    }\n\n    if (!db_name) {\n        flb_plg_error(ins, \"error reading Database name\");\n        return NULL;\n    }\n\n    if (!db_table) {\n        flb_plg_error(ins, \"error reading Table name\");\n        return NULL;\n    }\n\n    \/* Allocate context *\/\n    ctx = flb_calloc(1, sizeof(struct flb_td));\n    if (!ctx) {\n        flb_errno();\n        return NULL;\n    }\n    ctx->ins      = ins;\n    ctx->fd       = -1;\n    ctx->api      = api;\n    ctx->db_name  = db_name;\n    ctx->db_table = db_table;\n\n    \/* Lookup desired region *\/\n    tmp = flb_output_get_property(\"region\", ins);\n    if (tmp) {\n        if (strcasecmp(tmp, \"us\") == 0) {\n            ctx->region = FLB_TD_REGION_US;\n        }\n        else if (strcasecmp(tmp, \"jp\") == 0) {\n            ctx->region = FLB_TD_REGION_JP;\n        }\n        else {\n            flb_plg_error(ctx->ins, \"invalid region in configuration\");\n            flb_free(ctx);\n            return NULL;\n        }\n    }\n    else {\n        ctx->region = FLB_TD_REGION_US;\n    }\n\n    flb_plg_info(ctx->ins, \"Treasure Data \/ database='%s' table='%s'\",\n                 ctx->db_name, ctx->db_table);\n\n    return ctx;\n}\n","lang_cluster":"C","length":88,"code_uid":"a50fce342f0a449e9703f7bc7398bc7c"}
{"diff_hunk":"@@ -21,5 +21,13 @@\n \n static CALI_BPF_INLINE int calico_unittest_entry (struct __sk_buff *skb)\n {\n-\treturn vxlan_v4_encap(skb, HOST_IP, 0x02020202);\n+\tstruct cali_tc_ctx ctx = {\n+\t\t.state = state_get(),\n+\t\t.skb = skb,\n+\t\t.fwd = {\n+\t\t\t.res = TC_ACT_UNSPEC,\n+\t\t\t.reason = CALI_REASON_UNKNOWN,\n+\t\t},\n+\t};\n+\treturn vxlan_v4_encap(&ctx, HOST_IP, 0x02020202);\n }","old_code":"\/\/ Project Calico BPF dataplane programs.\n\/\/ Copyright (c) 2020 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ This program is free software; you can redistribute it and\/or modify\n\/\/ it under the terms of the GNU General Public License as published by\n\/\/ the Free Software Foundation; either version 2 of the License, or\n\/\/ (at your option) any later version.\n\/\/\n\/\/ This program is distributed in the hope that it will be useful,\n\/\/ but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n\/\/ GNU General Public License for more details.\n\/\/\n\/\/ You should have received a copy of the GNU General Public License along\n\/\/ with this program; if not, write to the Free Software Foundation, Inc.,\n\/\/ 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\n#include \"ut.h\"\n#include \"bpf.h\"\n#include \"nat.h\"\n\nstatic CALI_BPF_INLINE int calico_unittest_entry (struct __sk_buff *skb)\n{\n\treturn vxlan_v4_encap(skb, HOST_IP, 0x02020202);\n}\n","lang_cluster":"C","length":25,"code_uid":"0ed1406360d544cfbe99471a749d079b"}
{"diff_hunk":"@@ -1,4 +1,4 @@\n-\ufeffusing System;\n+using System;\n using System.Collections.Generic;\n using System.Globalization;\n using System.Linq.Expressions;","old_code":"\ufeffusing System;\nusing System.Collections.Generic;\nusing System.Globalization;\nusing System.Linq.Expressions;\nusing System.Reflection;\nusing System.Text;\nusing MvvmCross.Platform.Logging;\n\nnamespace MvvmCross.Core.Platform.LogProviders\n{\n    internal sealed class ConsoleLogProvider : MvxBaseLogProvider\n    {\n        private static readonly Type ConsoleType;\n        private static readonly Type ConsoleColorType;\n        private static readonly Action<string> ConsoleWriteLine;\n        private static readonly Func<int> GetConsoleForeground;\n        private static readonly Action<int> SetConsoleForeground;\n        private static readonly IDictionary<MvxLogLevel, int> Colors;\n\n        static ConsoleLogProvider()\n        {\n            ConsoleType = Type.GetType(\"System.Console\");\n            ConsoleColorType = ConsoleColorValues.Type;\n\n            if (!IsLoggerAvailable())\n            {\n                throw new InvalidOperationException(\"System.Console or System.ConsoleColor type not found\");\n            }\n\n            MessageFormatter = DefaultMessageFormatter;\n            Colors = new Dictionary<MvxLogLevel, int>\n            {\n                {MvxLogLevel.Fatal, ConsoleColorValues.Red},\n                {MvxLogLevel.Error, ConsoleColorValues.Yellow},\n                {MvxLogLevel.Warn, ConsoleColorValues.Magenta},\n                {MvxLogLevel.Info, ConsoleColorValues.White},\n                {MvxLogLevel.Debug, ConsoleColorValues.Gray},\n                {MvxLogLevel.Trace, ConsoleColorValues.DarkGray},\n            };\n            ConsoleWriteLine = GetConsoleWrite();\n            GetConsoleForeground = GetGetConsoleForeground();\n            SetConsoleForeground = GetSetConsoleForeground();\n        }\n\n        internal static bool IsLoggerAvailable()\n            => ConsoleType != null && ConsoleColorType != null;\n\n        protected override Logger GetLogger(string name)\n            => new ColouredConsoleLogger(name, ConsoleWriteLine, GetConsoleForeground, SetConsoleForeground).Log;\n\n        internal delegate string MessageFormatterDelegate(\n            string loggerName,\n            MvxLogLevel level,\n            object message,\n            Exception e);\n\n        internal static MessageFormatterDelegate MessageFormatter { get; set; }\n\n        private static string DefaultMessageFormatter(string loggerName, MvxLogLevel level, object message, Exception e)\n        {\n            var stringBuilder = new StringBuilder();\n            stringBuilder.Append(DateTime.Now.ToString(\"yyyy-MM-dd hh:mm:ss\", CultureInfo.InvariantCulture));\n            stringBuilder.Append(\" \");\n\n            \/\/ Append a readable representation of the log level\n            stringBuilder.Append((\"[\" + level.ToString().ToUpper() + \"]\").PadRight(8));\n            stringBuilder.Append(\"(\" + loggerName + \") \");\n\n            \/\/ Append the message\n            stringBuilder.Append(message);\n\n            \/\/ Append stack trace if there is an exception\n            if (e != null)\n            {\n                stringBuilder.Append(Environment.NewLine).Append(e.GetType());\n                stringBuilder.Append(Environment.NewLine).Append(e.Message);\n                stringBuilder.Append(Environment.NewLine).Append(e.StackTrace);\n            }\n\n            return stringBuilder.ToString();\n        }\n\n        private static Action<string> GetConsoleWrite()\n        {\n            var messageParameter = Expression.Parameter(typeof(string), \"message\");\n\n            MethodInfo writeMethod = ConsoleType.GetMethodPortable(\"WriteLine\", typeof(string));\n            var writeExpression = Expression.Call(writeMethod, messageParameter);\n\n            return Expression.Lambda<Action<string>>(\n                writeExpression, messageParameter).Compile();\n        }\n\n        private static Func<int> GetGetConsoleForeground()\n        {\n            MethodInfo getForeground = ConsoleType.GetPropertyPortable(\"ForegroundColor\").GetGetMethod();\n            var getForegroundExpression = Expression.Convert(Expression.Call(getForeground), typeof(int));\n\n            return Expression.Lambda<Func<int>>(getForegroundExpression).Compile();\n        }\n\n        private static Action<int> GetSetConsoleForeground()\n        {\n            var colorParameter = Expression.Parameter(typeof(int), \"color\");\n\n            MethodInfo setForeground = ConsoleType.GetPropertyPortable(\"ForegroundColor\").GetSetMethod();\n            var setForegroundExpression = Expression.Call(setForeground,\n                Expression.Convert(colorParameter, ConsoleColorType));\n\n            return Expression.Lambda<Action<int>>(\n                setForegroundExpression, colorParameter).Compile();\n        }\n\n        public class ColouredConsoleLogger\n        {\n            private readonly string _name;\n            private readonly Action<string> _write;\n            private readonly Func<int> _getForeground;\n            private readonly Action<int> _setForeground;\n\n            public ColouredConsoleLogger(string name, Action<string> write,\n                Func<int> getForeground, Action<int> setForeground)\n            {\n                _name = name;\n                _write = write;\n                _getForeground = getForeground;\n                _setForeground = setForeground;\n            }\n\n            public bool Log(MvxLogLevel logLevel, Func<string> messageFunc, Exception exception,\n                params object[] formatParameters)\n            {\n                if (messageFunc == null)\n                {\n                    return true;\n                }\n\n                messageFunc = LogMessageFormatter.SimulateStructuredLogging(messageFunc, formatParameters);\n\n                Write(logLevel, messageFunc(), exception);\n                return true;\n            }\n\n            protected void Write(MvxLogLevel logLevel, string message, Exception e = null)\n            {\n                var formattedMessage = MessageFormatter(_name, logLevel, message, e);\n                int color;\n\n                if (Colors.TryGetValue(logLevel, out color))\n                {\n                    var originalColor = _getForeground();\n                    try\n                    {\n                        _setForeground(color);\n                        _write(formattedMessage);\n                    }\n                    finally\n                    {\n                        _setForeground(originalColor);\n                    }\n                }\n                else\n                {\n                    _write(formattedMessage);\n                }\n            }\n        }\n\n        private static class ConsoleColorValues\n        {\n            internal static readonly Type Type;\n            internal static readonly int Red;\n            internal static readonly int Yellow;\n            internal static readonly int Magenta;\n            internal static readonly int White;\n            internal static readonly int Gray;\n            internal static readonly int DarkGray;\n\n            static ConsoleColorValues()\n            {\n                Type = Type.GetType(\"System.ConsoleColor\");\n                if (Type == null) return;\n                Red = (int)Enum.Parse(Type, \"Red\", false);\n                Yellow = (int)Enum.Parse(Type, \"Yellow\", false);\n                Magenta = (int)Enum.Parse(Type, \"Magenta\", false);\n                White = (int)Enum.Parse(Type, \"White\", false);\n                Gray = (int)Enum.Parse(Type, \"Gray\", false);\n                DarkGray = (int)Enum.Parse(Type, \"DarkGray\", false);\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":192,"code_uid":"6ccb2ff96e104ca191f9af5e777758ba"}
{"diff_hunk":"@@ -53,6 +53,17 @@ namespace Examples.AspNet\n                         zipkinOptions.Endpoint = new Uri(ConfigurationManager.AppSettings[\"ZipkinEndpoint\"]);\n                     });\n                     break;\n+                case \"otlp\":\n+                    \/\/ Adding the OtlpExporter creates a GrpcChannel.\n+                    \/\/ This switch must be set before creating a GrpcChannel\/HttpClient when calling an insecure gRPC service.\n+                    \/\/ See: https:\/\/docs.microsoft.com\/aspnet\/core\/grpc\/troubleshoot#call-insecure-grpc-services-with-net-core-client\n+                    AppContext.SetSwitch(\"System.Net.Http.SocketsHttpHandler.Http2UnencryptedSupport\", true);\n+\n+                    builder.AddOtlpExporter(otlpOptions =>\n+                        {\n+                            otlpOptions.Endpoint = new Uri(ConfigurationManager.AppSettings[\"OtlpEndpoint\"]);\n+                        });\n+                    break;\n                 default:\n                     builder.AddConsoleExporter(options => options.Targets = ConsoleExporterOutputTargets.Debug);\n                     break;","old_code":"\/\/ <copyright file=\"Global.asax.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Configuration;\nusing System.Web;\nusing System.Web.Http;\nusing System.Web.Mvc;\nusing System.Web.Routing;\nusing OpenTelemetry;\nusing OpenTelemetry.Exporter;\nusing OpenTelemetry.Trace;\n\nnamespace Examples.AspNet\n{\n#pragma warning disable SA1649 \/\/ File name should match first type name\n    public class WebApiApplication : HttpApplication\n#pragma warning restore SA1649 \/\/ File name should match first type name\n    {\n        private IDisposable tracerProvider;\n\n        protected void Application_Start()\n        {\n            var builder = Sdk.CreateTracerProviderBuilder()\n                 .AddAspNetInstrumentation()\n                 .AddHttpClientInstrumentation();\n\n            switch (ConfigurationManager.AppSettings[\"UseExporter\"].ToLowerInvariant())\n            {\n                case \"jaeger\":\n                    builder.AddJaegerExporter(jaegerOptions =>\n                     {\n                         jaegerOptions.AgentHost = ConfigurationManager.AppSettings[\"JaegerHost\"];\n                         jaegerOptions.AgentPort = int.Parse(ConfigurationManager.AppSettings[\"JaegerPort\"]);\n                     });\n                    break;\n                case \"zipkin\":\n                    builder.AddZipkinExporter(zipkinOptions =>\n                    {\n                        zipkinOptions.Endpoint = new Uri(ConfigurationManager.AppSettings[\"ZipkinEndpoint\"]);\n                    });\n                    break;\n                default:\n                    builder.AddConsoleExporter(options => options.Targets = ConsoleExporterOutputTargets.Debug);\n                    break;\n            }\n\n            this.tracerProvider = builder.Build();\n\n            GlobalConfiguration.Configure(WebApiConfig.Register);\n\n            AreaRegistration.RegisterAllAreas();\n            RouteConfig.RegisterRoutes(RouteTable.Routes);\n        }\n\n        protected void Application_End()\n        {\n            this.tracerProvider?.Dispose();\n        }\n    }\n}\n","lang_cluster":"C#","length":74,"code_uid":"d257f62bfd4f4405bfbaa21d7a50ba74"}
{"diff_hunk":"@@ -15,7 +15,21 @@\n \/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n \/\/ \n \n+using System;\n+using System.Threading.Tasks;\n+using FluentAssertions;\n+using Nethermind.Blockchain;\n+using Nethermind.Core;\n+using Nethermind.Core.Crypto;\n+using Nethermind.Core.Extensions;\n+using Nethermind.Core.Test.Builders;\n+using Nethermind.Int256;\n+using Nethermind.Logging;\n+using Nethermind.Mev.Data;\n+using Nethermind.Mev.Execution;\n+using Nethermind.Mev.Source;\n using NUnit.Framework;\n+using NSubstitute;\n \n namespace Nethermind.Mev.Test\n {","old_code":"\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\/\/ \n\nusing NUnit.Framework;\n\nnamespace Nethermind.Mev.Test\n{\n    [TestFixture]\n    public class MetricsTests\n    {\n        [Test]\n        public void Can_update_metrics()\n        {\n            \n        }\n        \n        [Test]\n        public void Are_described()\n        {\n            Monitoring.Test.MetricsTests.ValidateMetricsDescriptions();\n        }\n    }\n}\n","lang_cluster":"C#","length":37,"code_uid":"d1b0efc7442a44988ecec20f0608e15b"}
{"diff_hunk":"@@ -50,11 +50,11 @@ namespace OpenTelemetry\n             return Resource.Empty;\n         }\n \n-        public static Func<Batch<Metric>> GetMetricCollect(this BaseProvider baseProvider)\n+        public static Action GetCollectObservableInstruments(this BaseProvider baseProvider)\n         {\n             if (baseProvider is MeterProviderSdk meterProviderSdk)\n             {\n-                return meterProviderSdk.Collect;\n+                return meterProviderSdk.CollectObservableInstruments;\n             }\n \n             return null;","old_code":"\/\/ <copyright file=\"ProviderExtensions.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing OpenTelemetry.Logs;\nusing OpenTelemetry.Metrics;\nusing OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\n\nnamespace OpenTelemetry\n{\n    \/\/\/ <summary>\n    \/\/\/ Contains provider extension methods.\n    \/\/\/ <\/summary>\n    public static class ProviderExtensions\n    {\n        \/\/\/ <summary>\n        \/\/\/ Gets the <see cref=\"Resource\"\/> associated with the <see cref=\"BaseProvider\"\/>.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"baseProvider\"><see cref=\"BaseProvider\"\/>.<\/param>\n        \/\/\/ <returns><see cref=\"Resource\"\/>if found otherwise <see cref=\"Resource.Empty\"\/>.<\/returns>\n        public static Resource GetResource(this BaseProvider baseProvider)\n        {\n            if (baseProvider is TracerProviderSdk tracerProviderSdk)\n            {\n                return tracerProviderSdk.Resource;\n            }\n            else if (baseProvider is OpenTelemetryLoggerProvider otelLoggerProvider)\n            {\n                return otelLoggerProvider.Resource;\n            }\n            else if (baseProvider is MeterProviderSdk meterProviderSdk)\n            {\n                return meterProviderSdk.Resource;\n            }\n\n            return Resource.Empty;\n        }\n\n        public static Func<Batch<Metric>> GetMetricCollect(this BaseProvider baseProvider)\n        {\n            if (baseProvider is MeterProviderSdk meterProviderSdk)\n            {\n                return meterProviderSdk.Collect;\n            }\n\n            return null;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the <see cref=\"Resource\"\/> associated with the <see cref=\"BaseProvider\"\/>.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"baseProvider\"><see cref=\"BaseProvider\"\/>.<\/param>\n        \/\/\/ <returns><see cref=\"Resource\"\/>if found otherwise <see cref=\"Resource.Empty\"\/>.<\/returns>\n        public static Resource GetDefaultResource(this BaseProvider baseProvider)\n        {\n            return ResourceBuilder.CreateDefault().Build();\n        }\n    }\n}\n","lang_cluster":"C#","length":73,"code_uid":"38fee9cb910244179c57d2f40c7e5895"}
{"diff_hunk":"@@ -35,7 +35,7 @@ namespace Nethermind.TxPool\n         void AddPeer(ITxPoolPeer peer);\n         void RemovePeer(PublicKey nodeId);\n         AddTxResult AddTransaction(Transaction tx, TxHandlingOptions handlingOptions);\n-        void RemoveTransaction(Keccak hash, long blockNumber);\n+        void RemoveTransaction(Keccak hash, long blockNumber, bool removeSmallerNonces);\n         bool TryGetPendingTransaction(Keccak hash, out Transaction transaction);\n         UInt256 ReserveOwnTransactionNonce(Address address);\n         event EventHandler<TxEventArgs> NewDiscovered;","old_code":"\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing System.Collections.Generic;\nusing Nethermind.Core;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Int256;\n\nnamespace Nethermind.TxPool\n{\n    public interface ITxPool\n    {\n        Transaction[] GetPendingTransactions();\n        Transaction[] GetOwnPendingTransactions();\n        \n        \/\/\/ <summary>\n        \/\/\/ Grouped by sender address, sorted by nonce and later tx pool sorting\n        \/\/\/ <\/summary>\n        \/\/\/ <returns><\/returns>\n        IDictionary<Address, Transaction[]> GetPendingTransactionsBySender();\n        void AddPeer(ITxPoolPeer peer);\n        void RemovePeer(PublicKey nodeId);\n        AddTxResult AddTransaction(Transaction tx, TxHandlingOptions handlingOptions);\n        void RemoveTransaction(Keccak hash, long blockNumber);\n        bool TryGetPendingTransaction(Keccak hash, out Transaction transaction);\n        UInt256 ReserveOwnTransactionNonce(Address address);\n        event EventHandler<TxEventArgs> NewDiscovered;\n        event EventHandler<TxEventArgs> NewPending;\n        event EventHandler<TxEventArgs> RemovedPending;\n    }\n}\n","lang_cluster":"C#","length":45,"code_uid":"8104200c87da4382aed6d25dcd29f4a2"}
{"diff_hunk":"@@ -83,7 +83,7 @@ namespace Nethermind.Core.Collections\n         public static int BinarySearch<TItem>(this IList<TItem> list, TItem value,  IComparer<TItem> comparer) => list.BinarySearch(value, comparer.Compare);\n         \n         public static bool TryGetSearchedItem<TComparable>(this IList<TComparable> list, in TComparable activation, out TComparable item) where TComparable : IComparable<TComparable> => \n-            list.TryGetSearchedItem(activation, (b, c) => b.CompareTo(c), out item);\n+            list.TryGetSearchedItem(activation, (b, c) => b.CompareTo(c), out item!);\n \n         public static bool TryGetForBlock(this IList<long> list, in long blockNumber, out long item) =>\n             list.TryGetSearchedItem(blockNumber, (b, c) => b.CompareTo(c), out item);","old_code":"\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing System.Collections.Generic;\n\nnamespace Nethermind.Core.Collections\n{\n    public static class ListExtensions\n    {\n        public static T? GetItemRoundRobin<T>(this IList<T> array, long index) => array.Count == 0 ? default : array[(int) (index % array.Count)];\n\n        \/\/\/ <summary>\n        \/\/\/ Performs a binary search on the specified collection.\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"TItem\">The type of the item.<\/typeparam>\n        \/\/\/ <typeparam name=\"TSearch\">The type of the searched item.<\/typeparam>\n        \/\/\/ <param name=\"list\">The list to be searched.<\/param>\n        \/\/\/ <param name=\"value\">The value to search for.<\/param>\n        \/\/\/ <param name=\"comparer\">The comparer that is used to compare the value\n        \/\/\/ with the list items.<\/param>\n        \/\/\/ <returns><\/returns>\n        public static int BinarySearch<TItem, TSearch>(this IList<TItem> list, TSearch value, Func<TSearch, TItem, int> comparer)\n        {\n            if (list is null) throw new ArgumentNullException(nameof(list));\n            if (comparer is null) throw new ArgumentNullException(nameof(comparer));\n\n            int lower = 0;\n            int upper = list.Count - 1;\n\n            while (lower <= upper)\n            {\n                int middle = lower + (upper - lower) \/ 2;\n                int comparisonResult = comparer(value, list[middle]);\n                if (comparisonResult < 0)\n                {\n                    upper = middle - 1;\n                }\n                else if (comparisonResult > 0)\n                {\n                    lower = middle + 1;\n                }\n                else\n                {\n                    return middle;\n                }\n            }\n\n            return ~lower;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Performs a binary search on the specified collection.\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"TItem\">The type of the item.<\/typeparam>\n        \/\/\/ <param name=\"list\">The list to be searched.<\/param>\n        \/\/\/ <param name=\"value\">The value to search for.<\/param>\n        \/\/\/ <returns><\/returns>\n        public static int BinarySearch<TItem>(this IList<TItem> list, TItem value) => BinarySearch(list, value, Comparer<TItem>.Default);\n\n        \/\/\/ <summary>\n        \/\/\/ Performs a binary search on the specified collection.\n        \/\/\/ <\/summary>\n        \/\/\/ <typeparam name=\"TItem\">The type of the item.<\/typeparam>\n        \/\/\/ <param name=\"list\">The list to be searched.<\/param>\n        \/\/\/ <param name=\"value\">The value to search for.<\/param>\n        \/\/\/ <param name=\"comparer\">The comparer that is used to compare the value\n        \/\/\/ with the list items.<\/param>\n        \/\/\/ <returns><\/returns>\n        public static int BinarySearch<TItem>(this IList<TItem> list, TItem value,  IComparer<TItem> comparer) => list.BinarySearch(value, comparer.Compare);\n        \n        public static bool TryGetSearchedItem<TComparable>(this IList<TComparable> list, in TComparable activation, out TComparable item) where TComparable : IComparable<TComparable> => \n            list.TryGetSearchedItem(activation, (b, c) => b.CompareTo(c), out item);\n\n        public static bool TryGetForBlock(this IList<long> list, in long blockNumber, out long item) =>\n            list.TryGetSearchedItem(blockNumber, (b, c) => b.CompareTo(c), out item);\n        \n        public static bool TryGetSearchedItem<T, TComparable>(this IList<T> list, in TComparable searchedItem, Func<TComparable, T, int> comparer, out T? item)\n        {\n            int index = list.BinarySearch(searchedItem, comparer);\n            return TryGetSearchedItem(list, index, out item);\n        }\n\n        private static bool TryGetSearchedItem<T>(this IList<T> list, int index, out T? item)\n        {\n            if (index >= 0)\n            {\n                item = list[index];\n                return true;\n            }\n            else\n            {\n                int largerIndex = ~index;\n                if (largerIndex != 0)\n                {\n                    item = list[largerIndex - 1];\n                    return true;\n                }\n                else\n                {\n                    item = default;\n                    return false;\n                }\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":120,"code_uid":"4dc2358c4d2d4a3e9a3c1d92d0ca6d6b"}
{"diff_hunk":"@@ -42,5 +42,12 @@ namespace OpenTelemetry.Trace.Export\n         \/\/\/ <param name=\"cancellationToken\">Cancellation token.<\/param>\n         \/\/\/ <returns>Returns <see cref=\"Task\"\/>.<\/returns>\n         public abstract Task ShutdownAsync(CancellationToken cancellationToken);\n+\n+        \/\/\/ <summary>\n+        \/\/\/ Flushes all activity objects that have been queued for processing.\n+        \/\/\/ <\/summary>\n+        \/\/\/ <param name=\"cancellationToken\">Cancellation token.<\/param>\n+        \/\/\/ <returns>Returns <see cref=\"Task\"\/>.<\/returns>\n+        public abstract Task ForceFlushAsync(CancellationToken cancellationToken);\n     }\n }","old_code":"\ufeff\/\/ <copyright file=\"ActivityProcessor.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\nusing System.Diagnostics;\nusing System.Threading;\nusing System.Threading.Tasks;\n\nnamespace OpenTelemetry.Trace.Export\n{\n    \/\/\/ <summary>\n    \/\/\/ Activity processor base class.\n    \/\/\/ <\/summary>\n    public abstract class ActivityProcessor\n    {\n        \/\/\/ <summary>\n        \/\/\/ Activity start hook.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"activity\">Instance of activity to process.<\/param>\n        public abstract void OnStart(Activity activity);\n\n        \/\/\/ <summary>\n        \/\/\/ Activity end hook.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"activity\">Instance of activity to process.<\/param>\n        public abstract void OnEnd(Activity activity);\n\n        \/\/\/ <summary>\n        \/\/\/ Shuts down Activity processor asynchronously.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"cancellationToken\">Cancellation token.<\/param>\n        \/\/\/ <returns>Returns <see cref=\"Task\"\/>.<\/returns>\n        public abstract Task ShutdownAsync(CancellationToken cancellationToken);\n    }\n}\n","lang_cluster":"C#","length":46,"code_uid":"2feaf09e4e4c48369b4747057efd31cf"}
{"diff_hunk":"@@ -35,7 +35,7 @@ namespace Examples.AspNet\n             this.openTelemetry = Sdk.CreateTracerProviderBuilder()\n                  .AddHttpClientInstrumentation()\n                  .AddAspNetInstrumentation()\n-                 .UseJaegerExporter(c =>\n+                 .AddJaegerExporter(c =>\n                 {\n                     c.AgentHost = \"localhost\";\n                     c.AgentPort = 6831;","old_code":"\ufeff\/\/ <copyright file=\"Global.asax.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Web;\nusing System.Web.Http;\nusing System.Web.Mvc;\nusing System.Web.Routing;\nusing OpenTelemetry;\nusing OpenTelemetry.Trace;\n\nnamespace Examples.AspNet\n{\n#pragma warning disable SA1649 \/\/ File name should match first type name\n    public class WebApiApplication : HttpApplication\n#pragma warning restore SA1649 \/\/ File name should match first type name\n    {\n        private IDisposable openTelemetry;\n\n        protected void Application_Start()\n        {\n            this.openTelemetry = Sdk.CreateTracerProviderBuilder()\n                 .AddHttpClientInstrumentation()\n                 .AddAspNetInstrumentation()\n                 .UseJaegerExporter(c =>\n                {\n                    c.AgentHost = \"localhost\";\n                    c.AgentPort = 6831;\n                })\n                 .Build();\n\n            GlobalConfiguration.Configure(WebApiConfig.Register);\n\n            AreaRegistration.RegisterAllAreas();\n            RouteConfig.RegisterRoutes(RouteTable.Routes);\n        }\n\n        protected void Application_End()\n        {\n            this.openTelemetry?.Dispose();\n        }\n    }\n}\n","lang_cluster":"C#","length":56,"code_uid":"e0311acc98b6415c8731cc8234a53b2a"}
{"diff_hunk":"@@ -48,7 +48,7 @@ namespace Nethermind.Api\n         IFilterStore? FilterStore { get; set; }\n         IFilterManager? FilterManager { get; set; }\n         IHeaderValidator? HeaderValidator { get; set; }\n-        ITrieStore? ReadOnlyTrieStore { get; set; }\n+        ReadOnlyTrieStore? ReadOnlyTrieStore { get; set; }\n         IRewardCalculatorSource? RewardCalculatorSource { get; set; }\n         ISealer? Sealer { get; set; }\n         ISealValidator? SealValidator { get; set; }","old_code":"\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\/\/ \n\n#nullable enable\nusing Nethermind.Blockchain;\nusing Nethermind.Blockchain.Filters;\nusing Nethermind.Blockchain.Processing;\nusing Nethermind.Blockchain.Rewards;\nusing Nethermind.Blockchain.Validators;\nusing Nethermind.Config;\nusing Nethermind.Consensus;\nusing Nethermind.Core;\nusing Nethermind.Evm;\nusing Nethermind.Facade;\nusing Nethermind.State;\nusing Nethermind.Trie.Pruning;\nusing Nethermind.TxPool;\n\nnamespace Nethermind.Api\n{\n    public interface IApiWithBlockchain : IApiWithStores, IBlockchainBridgeFactory\n    {\n        (IApiWithStores GetFromApi, IApiWithBlockchain SetInApi) ForInit => (this, this);\n        (IApiWithStores GetFromApi, IApiWithBlockchain SetInApi) ForBlockchain => (this, this);\n        (IApiWithBlockchain GetFromApi, IApiWithBlockchain SetInApi) ForProducer => (this, this);\n        \n        IBlockchainProcessor? BlockchainProcessor { get; set; }\n        CompositeBlockPreprocessorStep BlockPreprocessor { get; }\n        IBlockProcessingQueue? BlockProcessingQueue { get; set; }\n        IBlockProcessor? MainBlockProcessor { get; set; }\n        IBlockProducer? BlockProducer { get; set; }\n        IBlockValidator? BlockValidator { get; set; }\n        IEnode? Enode { get; set; }\n        IFilterStore? FilterStore { get; set; }\n        IFilterManager? FilterManager { get; set; }\n        IHeaderValidator? HeaderValidator { get; set; }\n        ITrieStore? ReadOnlyTrieStore { get; set; }\n        IRewardCalculatorSource? RewardCalculatorSource { get; set; }\n        ISealer? Sealer { get; set; }\n        ISealValidator? SealValidator { get; set; }\n        \n        \/\/\/ <summary>\n        \/\/\/ Can be used only for processing blocks, on all other contexts use <see cref=\"StateReader\"\/> or <see cref=\"ChainHeadStateProvider\"\/>.\n        \/\/\/ <\/summary>\n        \/\/\/ <remarks>\n        \/\/\/ DO NOT USE OUTSIDE OF PROCESSING BLOCK CONTEXT!\n        \/\/\/ <\/remarks>\n        IStateProvider? StateProvider { get; set; }\n        IKeyValueStoreWithBatching? MainStateDbWithCache { get; set; }\n        IReadOnlyStateProvider? ChainHeadStateProvider { get; set; }\n        IStateReader? StateReader { get; set; }\n        IStorageProvider? StorageProvider { get; set; }\n        ITransactionProcessor? TransactionProcessor { get; set; }\n        ITrieStore? TrieStore { get; set; }\n        ITxSender? TxSender { get; set; }\n        ITxPool? TxPool { get; set; }\n        ITxPoolInfoProvider? TxPoolInfoProvider { get; set; }\n        IWitnessCollector? WitnessCollector { get; set; }\n        IHealthHintService? HealthHintService { get; set; }\n    }\n}\n","lang_cluster":"C#","length":75,"code_uid":"6bc3001eb42c42b5869579537fa09fe1"}
{"diff_hunk":"@@ -37,14 +37,17 @@ namespace OpenTelemetry.Metrics\n \n             var options = new PrometheusExporterOptions();\n             configure?.Invoke(options);\n-            var exporter = new PrometheusExporter(options);\n \n-            var metricReader = new BaseExportingMetricReader(exporter);\n-            exporter.CollectMetric = metricReader.Collect;\n+            \/\/ var exporter = new PrometheusExporter(options);\n \n-            var metricsHttpServer = new PrometheusExporterMetricsHttpServer(exporter);\n-            metricsHttpServer.Start();\n-            return builder.AddMetricReader(metricReader);\n+            \/\/ var metricReader = new BaseExportingMetricReader(exporter);\n+            \/\/ exporter.CollectMetric = metricReader.Collect;\n+\n+            \/\/ var metricsHttpServer = new PrometheusExporterMetricsHttpServer(exporter);\n+            \/\/ metricsHttpServer.Start();\n+            \/\/ return builder.AddMetricReader(metricReader);\n+\n+            return builder;\n         }\n     }\n }","old_code":"\/\/ <copyright file=\"MeterProviderBuilderExtensions.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing OpenTelemetry.Exporter;\n\nnamespace OpenTelemetry.Metrics\n{\n    public static class MeterProviderBuilderExtensions\n    {\n        \/\/\/ <summary>\n        \/\/\/ Adds Console exporter to the TracerProvider.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"builder\"><see cref=\"MeterProviderBuilder\"\/> builder to use.<\/param>\n        \/\/\/ <param name=\"configure\">Exporter configuration options.<\/param>\n        \/\/\/ <returns>The instance of <see cref=\"MeterProviderBuilder\"\/> to chain the calls.<\/returns>\n        [System.Diagnostics.CodeAnalysis.SuppressMessage(\"Reliability\", \"CA2000:Dispose objects before losing scope\", Justification = \"The objects should not be disposed.\")]\n        public static MeterProviderBuilder AddPrometheusExporter(this MeterProviderBuilder builder, Action<PrometheusExporterOptions> configure = null)\n        {\n            if (builder == null)\n            {\n                throw new ArgumentNullException(nameof(builder));\n            }\n\n            var options = new PrometheusExporterOptions();\n            configure?.Invoke(options);\n            var exporter = new PrometheusExporter(options);\n\n            var metricReader = new BaseExportingMetricReader(exporter);\n            exporter.CollectMetric = metricReader.Collect;\n\n            var metricsHttpServer = new PrometheusExporterMetricsHttpServer(exporter);\n            metricsHttpServer.Start();\n            return builder.AddMetricReader(metricReader);\n        }\n    }\n}\n","lang_cluster":"C#","length":50,"code_uid":"8f053494748640cab47e778b2b2c75ec"}
{"diff_hunk":"@@ -92,7 +92,7 @@ namespace System.Diagnostics.Metrics\n             T delta,\n             params KeyValuePair<string, object?>[] tags)\n         {\n-            this.RecordMeasurement(delta, tags);\n+            this.RecordMeasurement(delta, new ReadOnlySpan<KeyValuePair<string, object?>>(tags));\n         }\n     }\n }","old_code":"\/\/ <copyright file=\"Counter.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System.Collections.Generic;\n\n#nullable enable\n\nnamespace System.Diagnostics.Metrics\n{\n    \/\/\/ <summary>\n    \/\/\/ The counter is a non-observable Instrument that supports non-negative increments.\n    \/\/\/ e.g. Number of completed requests.\n    \/\/\/ <\/summary>\n    \/\/\/ <typeparam name=\"T\">TBD.<\/typeparam>\n    public sealed class Counter<T> : Instrument<T>\n        where T : struct\n    {\n        internal Counter(Meter meter, string name, string? unit, string? description)\n            : base(meter, name, unit, description)\n        {\n            this.Publish();\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ TBD.\n        \/\/\/ <\/summary>\n        public void Add(T delta)\n        {\n            this.RecordMeasurement(delta);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ TBD.\n        \/\/\/ <\/summary>\n        public void Add(\n            T delta,\n            KeyValuePair<string, object?> tag1)\n        {\n            this.RecordMeasurement(delta, tag1);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ TBD.\n        \/\/\/ <\/summary>\n        public void Add(\n            T delta,\n            KeyValuePair<string, object?> tag1,\n            KeyValuePair<string, object?> tag2)\n        {\n            this.RecordMeasurement(delta, tag1, tag2);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ TBD.\n        \/\/\/ <\/summary>\n        public void Add(\n            T delta,\n            KeyValuePair<string, object?> tag1,\n            KeyValuePair<string, object?> tag2,\n            KeyValuePair<string, object?> tag3)\n        {\n            this.RecordMeasurement(delta, tag1, tag2, tag3);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ TBD.\n        \/\/\/ <\/summary>\n        public void Add(\n            T delta,\n            ReadOnlySpan<KeyValuePair<string, object?>> tags)\n        {\n            this.RecordMeasurement(delta, tags);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ TBD.\n        \/\/\/ <\/summary>\n        public void Add(\n            T delta,\n            params KeyValuePair<string, object?>[] tags)\n        {\n            this.RecordMeasurement(delta, tags);\n        }\n    }\n}\n","lang_cluster":"C#","length":98,"code_uid":"ed87f21f3d044211b79a17d8610f1221"}
{"diff_hunk":"@@ -36,14 +36,13 @@ namespace Nethermind.Runner\n         protected override (CommandLineApplication, Func<IConfigProvider>, Func<string>) BuildCommandLineApp()\n         {\n             string pluginsDirectory = Path.Combine(AppDomain.CurrentDomain.BaseDirectory, \"plugins\");\n-            Console.WriteLine($\"Loading plugins from: {pluginsDirectory}\");\n             if (Directory.Exists(pluginsDirectory))\n             {\n                 var plugins = Directory.GetFiles(pluginsDirectory, \"*.dll\");\n                 foreach (string plugin in plugins)\n                 {\n-                    Console.WriteLine($\"Loading plugin {plugin} from {pluginsDirectory}\");\n                     string pluginName = plugin.Contains(\"\/\") ? plugin.Split(\"\/\").Last() : plugin.Split(\"\\\\\").Last();\n+                    Console.WriteLine($\"Loading plugin: {pluginName}\");\n                     AssemblyLoadContext.Default.LoadFromAssemblyPath(plugin);\n                 }\n             }","old_code":"\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Linq;\nusing System.Reflection;\nusing System.Runtime.Loader;\nusing Microsoft.Extensions.CommandLineUtils;\nusing Nethermind.Config;\nusing Nethermind.Logging;\nusing NLog;\nusing NLog.Config;\n\nnamespace Nethermind.Runner\n{\n    public class RunnerApp : RunnerAppBase, IRunnerApp\n    {\n        private const string DefaultConfigsDirectory = \"configs\";\n        private readonly string _defaultConfigFile = Path.Combine(DefaultConfigsDirectory, \"mainnet.cfg\");\n\n        protected override (CommandLineApplication, Func<IConfigProvider>, Func<string>) BuildCommandLineApp()\n        {\n            string pluginsDirectory = Path.Combine(AppDomain.CurrentDomain.BaseDirectory, \"plugins\");\n            Console.WriteLine($\"Loading plugins from: {pluginsDirectory}\");\n            if (Directory.Exists(pluginsDirectory))\n            {\n                var plugins = Directory.GetFiles(pluginsDirectory, \"*.dll\");\n                foreach (string plugin in plugins)\n                {\n                    Console.WriteLine($\"Loading plugin {plugin} from {pluginsDirectory}\");\n                    string pluginName = plugin.Contains(\"\/\") ? plugin.Split(\"\/\").Last() : plugin.Split(\"\\\\\").Last();\n                    AssemblyLoadContext.Default.LoadFromAssemblyPath(plugin);\n                }\n            }\n\n            var loadedAssemblies = AppDomain.CurrentDomain.GetAssemblies().ToList();\n            loadedAssemblies\n                .SelectMany(x => x.GetReferencedAssemblies())\n                .Distinct()\n                .Where(y => loadedAssemblies.Any((a) => a.FullName == y.FullName) == false)\n                .ToList()\n                .ForEach(x => loadedAssemblies.Add(AppDomain.CurrentDomain.Load(x)));\n            \n            Type configurationType = typeof(IConfig);\n            var configs = AppDomain.CurrentDomain.GetAssemblies()\n                .SelectMany(a => a.GetTypes())\n                .Where(t => configurationType.IsAssignableFrom(t) && !t.IsInterface)\n                .ToList();\n            \n            CommandLineApplication app = new CommandLineApplication {Name = \"Nethermind.Runner\"};\n            app.HelpOption(\"-?|-h|--help\");\n            CommandOption configFile = app.Option(\"-c|--config <configFile>\", \"config file path\", CommandOptionType.SingleValue);\n            CommandOption dbBasePath = app.Option(\"-d|--baseDbPath <baseDbPath>\", \"base db path\", CommandOptionType.SingleValue);\n            CommandOption logLevelOverride = app.Option(\"-l|--log <logLevel>\", \"log level\", CommandOptionType.SingleValue);\n            \n            foreach (Type configType in configs)\n            {\n                foreach (PropertyInfo propertyInfo in configType.GetProperties(BindingFlags.Public | BindingFlags.Instance))\n                {\n                    app.Option($\"--{configType.Name.Replace(\"Config\", String.Empty)}.{propertyInfo.Name}\", $\"{configType.Name}.{propertyInfo.Name}\", CommandOptionType.SingleValue);\n                }\n            }\n\n            IConfigProvider BuildConfigProvider()\n            {\n                \/\/ TODO: dynamically switch log levels from CLI!\n                if (logLevelOverride.HasValue())\n                {\n                    string logLevel = logLevelOverride.Value();\n                    NLog.LogLevel nLogLevel = NLog.LogLevel.Info;\n                    switch (logLevel.ToUpperInvariant())\n                    {\n                        case \"OFF\":\n                            nLogLevel = NLog.LogLevel.Off;\n                            break;\n                        case \"ERROR\":\n                            nLogLevel = NLog.LogLevel.Error;\n                            break;\n                        case \"WARN\":\n                            nLogLevel = NLog.LogLevel.Warn;\n                            break;\n                        case \"INFO\":\n                            nLogLevel = NLog.LogLevel.Info;\n                            break;\n                        case \"DEBUG\":\n                            nLogLevel = NLog.LogLevel.Debug;\n                            break;\n                        case \"TRACE\":\n                            nLogLevel = NLog.LogLevel.Trace;\n                            break;\n                    }\n                    \n                    Console.WriteLine($\"Enabling log level override: {logLevel.ToUpperInvariant()}\");\n\n                    foreach (LoggingRule rule in LogManager.Configuration.LoggingRules)\n                    {\n                        rule.DisableLoggingForLevels(NLog.LogLevel.Trace, nLogLevel);\n                        rule.EnableLoggingForLevels(nLogLevel, NLog.LogLevel.Off);\n                    }\n\n                    \/\/Call to update existing Loggers created with GetLogger() or \/\/GetCurrentClassLogger()\n                    LogManager.ReconfigExistingLoggers();\n                }\n\n                ConfigProvider configProvider = new ConfigProvider();\n                Dictionary<string, string> args = new Dictionary<string, string>();\n                foreach (CommandOption commandOption in app.Options)\n                {\n                    if (commandOption.HasValue())\n                    {\n                        args.Add(commandOption.LongName, commandOption.Value());\n                    }\n                }\n\n                IConfigSource argsSource = new ArgsConfigSource(args);\n                configProvider.AddSource(argsSource);\n                configProvider.AddSource(new EnvConfigSource());\n\n                string configFilePath = configFile.HasValue() ? configFile.Value() : _defaultConfigFile;\n                string configPathVariable = Environment.GetEnvironmentVariable(\"NETHERMIND_CONFIG\");\n                if (!string.IsNullOrWhiteSpace(configPathVariable))\n                {\n                    configFilePath = configPathVariable;\n                }\n\n                configFilePath = configFilePath.GetApplicationResourcePath();\n\n                if (!Path.HasExtension(configFilePath) && !configFilePath.Contains(Path.DirectorySeparatorChar))\n                {\n                    string redirectedConfigPath = Path.Combine(DefaultConfigsDirectory, string.Concat(configFilePath, \".cfg\"));\n                    Console.WriteLine($\"Redirecting config {configFilePath} to {redirectedConfigPath}\");\n                    configFilePath = redirectedConfigPath;\n                    if (!File.Exists(configFilePath))\n                    {\n                        throw new InvalidOperationException($\"Configuration: {configFilePath} was not found.\");\n                    }\n                }\n\n                if (!Path.HasExtension(configFilePath))\n                {\n                    configFilePath = string.Concat(configFilePath, \".cfg\");\n                }\n                \n                \/\/ Fallback to \"{executingDirectory}\/configs\/{configFile}\" if \"configs\" catalog was not specified.\n                if (!File.Exists(configFilePath))\n                {\n                    string configName = Path.GetFileName(configFilePath);\n                    string configDirectory = Path.GetDirectoryName(configFilePath);\n                    string redirectedConfigPath = Path.Combine(configDirectory, DefaultConfigsDirectory, configName);\n                    Console.WriteLine($\"Redirecting config {configFilePath} to {redirectedConfigPath}\");\n                    configFilePath = redirectedConfigPath;\n                    if (!File.Exists(configFilePath))\n                    {\n                        throw new InvalidOperationException($\"Configuration: {configFilePath} was not found.\");\n                    }\n                }\n\n                Console.WriteLine($\"Reading config file from {configFilePath}\");\n                configProvider.AddSource(new JsonConfigSource(configFilePath));\n                return configProvider;\n            }\n\n            string GetBaseDbPath()\n            {\n                return dbBasePath.HasValue() ? dbBasePath.Value() : null;\n            }\n\n            return (app, BuildConfigProvider, GetBaseDbPath);\n        }\n    }\n}","lang_cluster":"C#","length":186,"code_uid":"51bbae273d1142cd84182d2023b6894b"}
{"diff_hunk":"@@ -25,7 +25,12 @@ namespace Datadog.Trace.ClrProfiler.Integrations\n         \/\/\/ <returns>A task with the result<\/returns>\n         public static object ExecuteAsync(object apiController, object controllerContext, object cancellationTokenSource)\n         {\n-            var cancellationToken = ((CancellationTokenSource)cancellationTokenSource).Token;\n+            if (apiController == null) { throw new ArgumentNullException(nameof(apiController)); }\n+\n+            if (controllerContext == null) { throw new ArgumentNullException(nameof(controllerContext)); }\n+\n+            var tokenSource = cancellationTokenSource as CancellationTokenSource;\n+            var cancellationToken = tokenSource?.Token ?? CancellationToken.None;\n             return ExecuteAsyncInternal(apiController, controllerContext, cancellationToken);\n         }\n ","old_code":"#if !NETSTANDARD2_0\n\nusing System;\nusing System.Collections.Generic;\nusing System.Net.Http;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Datadog.Trace.ExtensionMethods;\n\nnamespace Datadog.Trace.ClrProfiler.Integrations\n{\n    \/\/\/ <summary>\n    \/\/\/ AspNetWeb5Integration wraps the Web API.\n    \/\/\/ <\/summary>\n    public static class AspNetWebApi2Integration\n    {\n        internal const string OperationName = \"aspnet-web-api.request\";\n\n        \/\/\/ <summary>\n        \/\/\/ Calls the underlying ExecuteAsync and traces the request.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"apiController\">The Api Controller<\/param>\n        \/\/\/ <param name=\"controllerContext\">The controller context for the call<\/param>\n        \/\/\/ <param name=\"cancellationTokenSource\">The cancellation token source<\/param>\n        \/\/\/ <returns>A task with the result<\/returns>\n        public static object ExecuteAsync(object apiController, object controllerContext, object cancellationTokenSource)\n        {\n            var cancellationToken = ((CancellationTokenSource)cancellationTokenSource).Token;\n            return ExecuteAsyncInternal(apiController, controllerContext, cancellationToken);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Calls the underlying ExecuteAsync and traces the request.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"apiController\">The Api Controller<\/param>\n        \/\/\/ <param name=\"controllerContext\">The controller context for the call<\/param>\n        \/\/\/ <param name=\"cancellationToken\">The cancellation token<\/param>\n        \/\/\/ <returns>A task with the result<\/returns>\n        private static async Task<HttpResponseMessage> ExecuteAsyncInternal(object apiController, object controllerContext, CancellationToken cancellationToken)\n        {\n            Type controllerType = apiController.GetType();\n\n            \/\/ in some cases, ExecuteAsync() is an explicit interface implementation,\n            \/\/ which is not public and has a different name, so try both\n            var executeAsyncFunc =\n                DynamicMethodBuilder<Func<object, object, CancellationToken, Task<HttpResponseMessage>>>\n                   .GetOrCreateMethodCallDelegate(controllerType, \"ExecuteAsync\") ??\n                DynamicMethodBuilder<Func<object, object, CancellationToken, Task<HttpResponseMessage>>>\n                   .GetOrCreateMethodCallDelegate(controllerType, \"System.Web.Http.Controllers.IHttpController.ExecuteAsync\");\n\n            using (Scope scope = CreateScope(controllerContext))\n            {\n                try\n                {\n                    var responseMessage = await executeAsyncFunc(apiController, controllerContext, cancellationToken).ConfigureAwait(false);\n\n                    \/\/ some fields aren't set till after execution, so populate anything missing\n                    UpdateSpan(controllerContext, scope.Span);\n\n                    return responseMessage;\n                }\n                catch (Exception ex)\n                {\n                    scope.Span.SetException(ex);\n                    throw;\n                }\n            }\n        }\n\n        private static Scope CreateScope(object controllerContext)\n        {\n            var scope = Tracer.Instance.StartActive(OperationName);\n            UpdateSpan(controllerContext, scope.Span);\n            return scope;\n        }\n\n        private static void UpdateSpan(dynamic controllerContext, Span span)\n        {\n            var req = controllerContext?.Request;\n\n            string host = req?.Headers?.Host ?? string.Empty;\n            string rawUrl = req?.RequestUri?.ToString()?.ToLowerInvariant() ?? string.Empty;\n            string method = controllerContext?.Request?.Method?.Method?.ToUpperInvariant() ?? \"GET\";\n            string route = null;\n            try\n            {\n                route = controllerContext?.RouteData?.Route?.RouteTemplate;\n            }\n            catch\n            {\n            }\n\n            string resourceName = $\"{method} {rawUrl}\";\n            if (route != null)\n            {\n                resourceName = $\"{method} {route}\";\n            }\n\n            string controller = string.Empty;\n            string action = string.Empty;\n            try\n            {\n                if (controllerContext?.RouteData?.Values is IDictionary<string, object> routeValues)\n                {\n                    controller = (routeValues.GetValueOrDefault(\"controller\") as string)?.ToLowerInvariant();\n                    action = (routeValues.GetValueOrDefault(\"action\") as string)?.ToLowerInvariant();\n                }\n            }\n            catch\n            {\n            }\n\n            span.ResourceName = resourceName;\n            span.Type = SpanTypes.Web;\n            span.SetTag(Tags.AspNetAction, action);\n            span.SetTag(Tags.AspNetController, controller);\n            span.SetTag(Tags.AspNetRoute, route);\n            span.SetTag(Tags.HttpMethod, method);\n            span.SetTag(Tags.HttpRequestHeadersHost, host);\n            span.SetTag(Tags.HttpUrl, rawUrl);\n        }\n    }\n}\n\n#endif\n","lang_cluster":"C#","length":125,"code_uid":"236bdd9706b44dd38eac5d2fffb04eec"}
{"diff_hunk":"@@ -37,14 +37,16 @@ namespace Nethermind.JsonRpc.Modules.Trace\n         private readonly ITracer _tracer;\n         private readonly IBlockFinder _blockFinder;\n         private readonly TransactionDecoder _txDecoder = new TransactionDecoder();\n-        private readonly CancellationToken _cancellationToken; \n+        private readonly IJsonRpcConfig _jsonRpcConfig;\n+        private readonly TimeSpan _cancellationTokenTimeout;\n \n-        public TraceModule(IReceiptFinder receiptFinder, ITracer tracer, IBlockFinder blockFinder, CancellationToken cancellationToken = default(CancellationToken))\n+        public TraceModule(IReceiptFinder receiptFinder, ITracer tracer, IBlockFinder blockFinder, IJsonRpcConfig jsonRpcConfig)\n         {\n-            _cancellationToken = cancellationToken;\n             _receiptFinder = receiptFinder ?? throw new ArgumentNullException(nameof(receiptFinder));\n             _tracer = tracer ?? throw new ArgumentNullException(nameof(tracer));\n             _blockFinder = blockFinder ?? throw new ArgumentNullException(nameof(blockFinder));\n+            _jsonRpcConfig = jsonRpcConfig ?? throw new ArgumentNullException(nameof(jsonRpcConfig));\n+            _cancellationTokenTimeout = TimeSpan.FromMilliseconds(_jsonRpcConfig.TracerTimeout);\n         }\n \n         private static ParityTraceTypes GetParityTypes(string[] types)","old_code":"\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Nethermind.Blockchain;\nusing Nethermind.Blockchain.Find;\nusing Nethermind.Blockchain.Receipts;\nusing Nethermind.Blockchain.Tracing;\nusing Nethermind.Core;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Evm.Tracing.ParityStyle;\nusing Nethermind.JsonRpc.Data;\nusing Nethermind.Serialization.Rlp;\n\nnamespace Nethermind.JsonRpc.Modules.Trace\n{\n    public class TraceModule : ITraceModule\n    {\n        private readonly IReceiptFinder _receiptFinder;\n        private readonly ITracer _tracer;\n        private readonly IBlockFinder _blockFinder;\n        private readonly TransactionDecoder _txDecoder = new TransactionDecoder();\n        private readonly CancellationToken _cancellationToken; \n\n        public TraceModule(IReceiptFinder receiptFinder, ITracer tracer, IBlockFinder blockFinder, CancellationToken cancellationToken = default(CancellationToken))\n        {\n            _cancellationToken = cancellationToken;\n            _receiptFinder = receiptFinder ?? throw new ArgumentNullException(nameof(receiptFinder));\n            _tracer = tracer ?? throw new ArgumentNullException(nameof(tracer));\n            _blockFinder = blockFinder ?? throw new ArgumentNullException(nameof(blockFinder));\n        }\n\n        private static ParityTraceTypes GetParityTypes(string[] types)\n        {\n            return types.Select(s => (ParityTraceTypes) Enum.Parse(typeof(ParityTraceTypes), s, true)).Aggregate((t1, t2) => t1 | t2);\n        }\n\n        public ResultWrapper<ParityTxTraceFromReplay> trace_call(TransactionForRpc message, string[] traceTypes, BlockParameter blockParameter)\n        {\n            Transaction tx = message.ToTransaction();\n            return TraceTx(tx, traceTypes, blockParameter);\n        }\n\n        public ResultWrapper<ParityTxTraceFromReplay[]> trace_callMany((TransactionForRpc message, string[] traceTypes, BlockParameter numberOrTag)[] a)\n        {\n            throw new NotImplementedException();\n        }\n\n        public ResultWrapper<ParityTxTraceFromReplay> trace_rawTransaction(byte[] data, string[] traceTypes)\n        {\n            Transaction tx = _txDecoder.Decode(new RlpStream(data));\n            return TraceTx(tx, traceTypes, BlockParameter.Latest);\n        }\n\n        private ResultWrapper<ParityTxTraceFromReplay> TraceTx(Transaction tx, string[] traceTypes, BlockParameter blockParameter)\n        {\n            SearchResult<BlockHeader> headerSearch = _blockFinder.SearchForHeader(blockParameter);\n            if (headerSearch.IsError)\n            {\n                return ResultWrapper<ParityTxTraceFromReplay>.Fail(headerSearch);\n            }\n\n            BlockHeader header = headerSearch.Object;\n\n            if (header.IsGenesis)\n            {\n                header = new BlockHeader(\n                    header.Hash,\n                    Keccak.OfAnEmptySequenceRlp,\n                    Address.Zero,\n                    header.Difficulty,\n                    header.Number + 1,\n                    header.GasLimit,\n                    header.Timestamp + 1,\n                    header.ExtraData);\n\n                header.TotalDifficulty = 2 * header.Difficulty;\n            }\n\n            Block block = new Block(header, new[] {tx}, Enumerable.Empty<BlockHeader>());\n\n            IReadOnlyCollection<ParityLikeTxTrace> result = TraceBlock(block, GetParityTypes(traceTypes));\n            return ResultWrapper<ParityTxTraceFromReplay>.Success(new ParityTxTraceFromReplay(result.SingleOrDefault()));\n        }\n\n        public ResultWrapper<ParityTxTraceFromReplay> trace_replayTransaction(Keccak txHash, string[] traceTypes)\n        {\n            SearchResult<Keccak> blockHashSearch = _receiptFinder.SearchForReceiptBlockHash(txHash);\n            if (blockHashSearch.IsError)\n            {\n                return ResultWrapper<ParityTxTraceFromReplay>.Fail(blockHashSearch);\n            }\n\n            SearchResult<Block> blockSearch = _blockFinder.SearchForBlock(new BlockParameter(blockHashSearch.Object));\n            if (blockSearch.IsError)\n            {\n                return ResultWrapper<ParityTxTraceFromReplay>.Fail(blockSearch);\n            }\n\n            Block block = blockSearch.Object;\n\n            ParityLikeTxTrace txTrace = TraceTx(block, txHash, GetParityTypes(traceTypes));\n            return ResultWrapper<ParityTxTraceFromReplay>.Success(new ParityTxTraceFromReplay(txTrace));\n        }\n\n        public ResultWrapper<ParityTxTraceFromReplay[]> trace_replayBlockTransactions(BlockParameter blockParameter, string[] traceTypes)\n        {\n            SearchResult<Block> blockSearch = _blockFinder.SearchForBlock(blockParameter);\n            if (blockSearch.IsError)\n            {\n                return ResultWrapper<ParityTxTraceFromReplay[]>.Fail(blockSearch);\n            }\n\n            Block block = blockSearch.Object;\n\n            IReadOnlyCollection<ParityLikeTxTrace> txTraces = TraceBlock(block, GetParityTypes(traceTypes));\n\n            \/\/ ReSharper disable once CoVariantArrayConversion\n            return ResultWrapper<ParityTxTraceFromReplay[]>.Success(txTraces.Select(t => new ParityTxTraceFromReplay(t, true)).ToArray());\n        }\n\n        public ResultWrapper<ParityTxTraceFromStore[]> trace_filter(BlockParameter fromBlock, BlockParameter toBlock, Address toAddress, int after, int count)\n        {\n            throw new NotImplementedException();\n        }\n\n        public ResultWrapper<ParityTxTraceFromStore[]> trace_block(BlockParameter blockParameter)\n        {\n            SearchResult<Block> blockSearch = _blockFinder.SearchForBlock(blockParameter);\n            if (blockSearch.IsError)\n            {\n                return ResultWrapper<ParityTxTraceFromStore[]>.Fail(blockSearch);\n            }\n\n            Block block = blockSearch.Object;\n            \n            IReadOnlyCollection<ParityLikeTxTrace> txTraces = TraceBlock(block, ParityTraceTypes.Trace | ParityTraceTypes.Rewards);\n            return ResultWrapper<ParityTxTraceFromStore[]>.Success(txTraces.SelectMany(ParityTxTraceFromStore.FromTxTrace).ToArray());\n        }\n\n        public ResultWrapper<ParityTxTraceFromStore[]> trace_get(Keccak txHash, int[] positions)\n        {\n            throw new NotImplementedException();\n        }\n\n        public ResultWrapper<ParityTxTraceFromStore[]> trace_transaction(Keccak txHash)\n        {\n            SearchResult<Keccak> blockHashSearch = _receiptFinder.SearchForReceiptBlockHash(txHash);\n            if (blockHashSearch.IsError)\n            {\n                return ResultWrapper<ParityTxTraceFromStore[]>.Fail(blockHashSearch);\n            }\n            \n            SearchResult<Block> blockSearch = _blockFinder.SearchForBlock(new BlockParameter(blockHashSearch.Object));\n            if (blockSearch.IsError)\n            {\n                return ResultWrapper<ParityTxTraceFromStore[]>.Fail(blockSearch);\n            }\n\n            Block block = blockSearch.Object;\n\n            ParityLikeTxTrace txTrace = TraceTx(block, txHash, ParityTraceTypes.Trace | ParityTraceTypes.Rewards);\n            return ResultWrapper<ParityTxTraceFromStore[]>.Success(ParityTxTraceFromStore.FromTxTrace(txTrace));\n        }\n\n        private IReadOnlyCollection<ParityLikeTxTrace> TraceBlock(Block block, ParityTraceTypes traceTypes)\n        {\n            ParityLikeBlockTracer listener = new ParityLikeBlockTracer(traceTypes, _cancellationToken);\n            _tracer.Trace(block, listener);\n\n            return listener.BuildResult();\n        }\n\n        private ParityLikeTxTrace TraceTx(Block block, Keccak txHash, ParityTraceTypes traceTypes)\n        {\n            ParityLikeBlockTracer listener = new ParityLikeBlockTracer(txHash, traceTypes, _cancellationToken);\n            _tracer.Trace(block, listener);\n\n            return listener.BuildResult().SingleOrDefault();\n        }\n    }\n}","lang_cluster":"C#","length":199,"code_uid":"ea936448a41b44e2b7979b5f716e25c3"}
{"diff_hunk":"@@ -144,6 +144,7 @@ namespace Datadog.Trace.Vendors.Newtonsoft.Json.Serialization\n             return property;\n         }\n \n+#if !NETCOREAPP\n         private bool TryGetValue(string key, out JsonProperty item)\n         {\n             if (Dictionary == null)","old_code":"\/\/------------------------------------------------------------------------------\n\/\/ <auto-generated \/>\n\/\/ This file was automatically generated by the UpdateVendors tool.\n\/\/------------------------------------------------------------------------------\n#region License\n\/\/ Copyright (c) 2007 James Newton-King\n\/\/\n\/\/ Permission is hereby granted, free of charge, to any person\n\/\/ obtaining a copy of this software and associated documentation\n\/\/ files (the \"Software\"), to deal in the Software without\n\/\/ restriction, including without limitation the rights to use,\n\/\/ copy, modify, merge, publish, distribute, sublicense, and\/or sell\n\/\/ copies of the Software, and to permit persons to whom the\n\/\/ Software is furnished to do so, subject to the following\n\/\/ conditions:\n\/\/\n\/\/ The above copyright notice and this permission notice shall be\n\/\/ included in all copies or substantial portions of the Software.\n\/\/\n\/\/ THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n\/\/ EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n\/\/ OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n\/\/ NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n\/\/ HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n\/\/ WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n\/\/ FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n\/\/ OTHER DEALINGS IN THE SOFTWARE.\n#endregion\n\nusing System;\nusing System.Collections.Generic;\nusing System.Text;\nusing System.Collections.ObjectModel;\nusing Datadog.Trace.Vendors.Newtonsoft.Json.Utilities;\nusing System.Globalization;\n\nnamespace Datadog.Trace.Vendors.Newtonsoft.Json.Serialization\n{\n    \/\/\/ <summary>\n    \/\/\/ A collection of <see cref=\"JsonProperty\"\/> objects.\n    \/\/\/ <\/summary>\n    internal class JsonPropertyCollection : KeyedCollection<string, JsonProperty>\n    {\n        private readonly Type _type;\n        private readonly List<JsonProperty> _list;\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"JsonPropertyCollection\"\/> class.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"type\">The type.<\/param>\n        public JsonPropertyCollection(Type type)\n            : base(StringComparer.Ordinal)\n        {\n            ValidationUtils.ArgumentNotNull(type, \"type\");\n            _type = type;\n\n            \/\/ foreach over List<T> to avoid boxing the Enumerator\n            _list = (List<JsonProperty>)Items;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ When implemented in a derived class, extracts the key from the specified element.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"item\">The element from which to extract the key.<\/param>\n        \/\/\/ <returns>The key for the specified element.<\/returns>\n        protected override string GetKeyForItem(JsonProperty item)\n        {\n            return item.PropertyName;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Adds a <see cref=\"JsonProperty\"\/> object.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"property\">The property to add to the collection.<\/param>\n        public void AddProperty(JsonProperty property)\n        {\n            if (Contains(property.PropertyName))\n            {\n                \/\/ don't overwrite existing property with ignored property\n                if (property.Ignored)\n                {\n                    return;\n                }\n\n                JsonProperty existingProperty = this[property.PropertyName];\n                bool duplicateProperty = true;\n\n                if (existingProperty.Ignored)\n                {\n                    \/\/ remove ignored property so it can be replaced in collection\n                    Remove(existingProperty);\n                    duplicateProperty = false;\n                }\n                else\n                {\n                    if (property.DeclaringType != null && existingProperty.DeclaringType != null)\n                    {\n                        if (property.DeclaringType.IsSubclassOf(existingProperty.DeclaringType)\n                            || (existingProperty.DeclaringType.IsInterface() && property.DeclaringType.ImplementInterface(existingProperty.DeclaringType)))\n                        {\n                            \/\/ current property is on a derived class and hides the existing\n                            Remove(existingProperty);\n                            duplicateProperty = false;\n                        }\n                        if (existingProperty.DeclaringType.IsSubclassOf(property.DeclaringType)\n                            || (property.DeclaringType.IsInterface() && existingProperty.DeclaringType.ImplementInterface(property.DeclaringType)))\n                        {\n                            \/\/ current property is hidden by the existing so don't add it\n                            return;\n                        }\n                        \n                        if (_type.ImplementInterface(existingProperty.DeclaringType) && _type.ImplementInterface(property.DeclaringType))\n                        {\n                            \/\/ current property was already defined on another interface\n                            return;\n                        }\n                    }\n                }\n\n                if (duplicateProperty)\n                {\n                    throw new JsonSerializationException(\"A member with the name '{0}' already exists on '{1}'. Use the JsonPropertyAttribute to specify another name.\".FormatWith(CultureInfo.InvariantCulture, property.PropertyName, _type));\n                }\n            }\n\n            Add(property);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the closest matching <see cref=\"JsonProperty\"\/> object.\n        \/\/\/ First attempts to get an exact case match of <paramref name=\"propertyName\"\/> and then\n        \/\/\/ a case insensitive match.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"propertyName\">Name of the property.<\/param>\n        \/\/\/ <returns>A matching property if found.<\/returns>\n        public JsonProperty GetClosestMatchProperty(string propertyName)\n        {\n            JsonProperty property = GetProperty(propertyName, StringComparison.Ordinal);\n            if (property == null)\n            {\n                property = GetProperty(propertyName, StringComparison.OrdinalIgnoreCase);\n            }\n\n            return property;\n        }\n\n        private bool TryGetValue(string key, out JsonProperty item)\n        {\n            if (Dictionary == null)\n            {\n                item = default;\n                return false;\n            }\n\n            return Dictionary.TryGetValue(key, out item);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets a property by property name.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"propertyName\">The name of the property to get.<\/param>\n        \/\/\/ <param name=\"comparisonType\">Type property name string comparison.<\/param>\n        \/\/\/ <returns>A matching property if found.<\/returns>\n        public JsonProperty GetProperty(string propertyName, StringComparison comparisonType)\n        {\n            \/\/ KeyedCollection has an ordinal comparer\n            if (comparisonType == StringComparison.Ordinal)\n            {\n                if (TryGetValue(propertyName, out JsonProperty property))\n                {\n                    return property;\n                }\n\n                return null;\n            }\n\n            for (int i = 0; i < _list.Count; i++)\n            {\n                JsonProperty property = _list[i];\n                if (string.Equals(propertyName, property.PropertyName, comparisonType))\n                {\n                    return property;\n                }\n            }\n\n            return null;\n        }\n    }\n}\n","lang_cluster":"C#","length":189,"code_uid":"91782f995cb34b168590240aa1542380"}
{"diff_hunk":"@@ -9,5 +9,15 @@ namespace MvvmCross.Forms.Views.Attributes\n         public MvxNavigationPagePresentationAttribute()\n         {\n         }\n+\n+        public virtual bool WrapInNavigationPage { get; set; } = false;\n+\n+        \/\/\/ <summary>\n+        \/\/\/ Clears the backstack of the current NavigationPage when set to true\n+        \/\/\/ <\/summary>\n+        \/\/\/ <value><c>true<\/c> if no history; otherwise, <c>false<\/c>.<\/value>\n+        public virtual bool NoHistory { get; set; } = false;\n+\n+        public bool Animated { get; set; } = true;\n     }\n }","old_code":"using System;\nusing MvvmCross.Core.Views;\n\nnamespace MvvmCross.Forms.Views.Attributes\n{\n    [AttributeUsage(AttributeTargets.Class)]\n    public class MvxNavigationPagePresentationAttribute : MvxBasePresentationAttribute\n    {\n        public MvxNavigationPagePresentationAttribute()\n        {\n        }\n    }\n}","lang_cluster":"C#","length":13,"code_uid":"0d39e60b001a4a65badf38cad55269f2"}
{"diff_hunk":"@@ -32,6 +32,10 @@ namespace MvvmCross.Core.ViewModels\n         public static void CallBundleMethod(this IMvxViewModel viewModel, MethodInfo methodInfo, IMvxBundle bundle)\n         {\n             var parameters = methodInfo.GetParameters().ToArray();\n+\n+            if (bundle == null && parameters.Count() > 0)\n+                return;\n+            \n             if (parameters.Count() == 1\n                 && parameters[0].ParameterType == typeof(IMvxBundle))\n             {","old_code":"\/\/ MvxViewModelExtensions.cs\n\n\/\/ MvvmCross is licensed using Microsoft Public License (Ms-PL)\n\/\/ Contributions and inspirations noted in readme.md and license.txt\n\/\/\n\/\/ Project Lead - Stuart Lodge, @slodge, me@slodge.com\n\nnamespace MvvmCross.Core.ViewModels\n{\n    using System.Linq;\n    using System.Reflection;\n\n    using MvvmCross.Platform;\n\n    public static class MvxViewModelExtensions\n    {\n        public static void CallBundleMethods(this IMvxViewModel viewModel, string methodName, IMvxBundle bundle)\n        {\n            var methods = viewModel\n                .GetType()\n                .GetMethods(BindingFlags.Instance | BindingFlags.Public | BindingFlags.FlattenHierarchy)\n                .Where(m => m.Name == methodName)\n                .Where(m => !m.IsAbstract)\n                .ToList();\n\n            foreach (var methodInfo in methods)\n            {\n                viewModel.CallBundleMethod(methodInfo, bundle);\n            }\n        }\n\n        public static void CallBundleMethod(this IMvxViewModel viewModel, MethodInfo methodInfo, IMvxBundle bundle)\n        {\n            var parameters = methodInfo.GetParameters().ToArray();\n            if (parameters.Count() == 1\n                && parameters[0].ParameterType == typeof(IMvxBundle))\n            {\n                \/\/ this method is the 'normal' interface method\n                methodInfo.Invoke(viewModel, new object[] { bundle });\n                return;\n            }\n\n            if (parameters.Count() == 1\n                && !MvxSingletonCache.Instance.Parser.TypeSupported(parameters[0].ParameterType))\n            {\n                \/\/ call method using typed object\n                var value = bundle.Read(parameters[0].ParameterType);\n                methodInfo.Invoke(viewModel, new[] { value });\n                return;\n            }\n\n            \/\/ call method using named method arguments\n            var invokeWith = bundle.CreateArgumentList(parameters, viewModel.GetType().Name)\n                                   .ToArray();\n            methodInfo.Invoke(viewModel, invokeWith);\n        }\n\n        public static IMvxBundle SaveStateBundle(this IMvxViewModel viewModel)\n        {\n            var toReturn = new MvxBundle();\n            var methods = viewModel.GetType()\n                                   .GetMethods()\n                                   .Where(m => m.Name == \"SaveState\")\n                                   .Where(m => m.ReturnType != typeof(void))\n                                   .Where(m => !m.GetParameters().Any());\n\n            foreach (var methodInfo in methods)\n            {\n                \/\/ use methods like `public T SaveState()`\n                var stateObject = methodInfo.Invoke(viewModel, new object[0]);\n                if (stateObject != null)\n                {\n                    toReturn.Write(stateObject);\n                }\n            }\n\n            \/\/ call the general `public void SaveState(bundle)` method too\n            viewModel.SaveState(toReturn);\n\n            return toReturn;\n        }\n    }\n}","lang_cluster":"C#","length":83,"code_uid":"1fd1e180a8624e45b0bbeaff32499ff1"}
{"diff_hunk":"@@ -94,8 +94,7 @@ namespace OpenTelemetry.Trace\n             }\n             else if (this.Exporter != null)\n             {\n-                \/\/ TODO: Make this BatchingActivityProcessor once its available.\n-                exportingProcessor = new SimpleActivityProcessor(this.Exporter);\n+                exportingProcessor = new BatchingActivityProcessor(this.Exporter);\n                 this.Processors.Add(exportingProcessor);\n             }\n ","old_code":"\ufeff\/\/ <copyright file=\"ActivityProcessorPipelineBuilder.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Collections.Generic;\nusing OpenTelemetry.Trace.Internal;\n\nnamespace OpenTelemetry.Trace\n{\n    \/\/\/ <summary>\n    \/\/\/ Configures exporting pipeline with chains of processors and exporter.\n    \/\/\/ <\/summary>\n    public class ActivityProcessorPipelineBuilder\n    {\n        private Func<ActivityExporter, ActivityProcessor> lastProcessorFactory;\n        private List<Func<ActivityProcessor, ActivityProcessor>> processorChain;\n\n        internal ActivityProcessorPipelineBuilder()\n        {\n        }\n\n        internal ActivityExporter Exporter { get; private set; }\n\n        internal List<ActivityProcessor> Processors { get; private set; }\n\n        \/\/\/ <summary>\n        \/\/\/ Adds chained processor to the pipeline. Processors are executed in the order they were added.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"processorFactory\">Function that creates processor from the next one.<\/param>\n        \/\/\/ <returns>Returns <see cref=\"ActivityProcessorPipelineBuilder\"\/>.<\/returns>\n        public ActivityProcessorPipelineBuilder AddProcessor(Func<ActivityProcessor, ActivityProcessor> processorFactory)\n        {\n            if (processorFactory == null)\n            {\n                throw new ArgumentNullException(nameof(processorFactory));\n            }\n\n            if (this.processorChain == null)\n            {\n                this.processorChain = new List<Func<ActivityProcessor, ActivityProcessor>>();\n            }\n\n            this.processorChain.Add(processorFactory);\n\n            return this;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Configures last processor that invokes exporter. When not set, <see cref=\"SimpleActivityProcessor\"\/> is used.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"processorFactory\">Factory that creates exporting processor from the exporter.<\/param>\n        \/\/\/ <returns>Returns <see cref=\"ActivityProcessorPipelineBuilder\"\/>.<\/returns>\n        public ActivityProcessorPipelineBuilder SetExportingProcessor(Func<ActivityExporter, ActivityProcessor> processorFactory)\n        {\n            this.lastProcessorFactory = processorFactory ?? throw new ArgumentNullException(nameof(processorFactory));\n            return this;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Configures exporter.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"exporter\">Exporter instance.<\/param>\n        \/\/\/ <returns>Returns <see cref=\"ActivityProcessorPipelineBuilder\"\/>.<\/returns>\n        public ActivityProcessorPipelineBuilder SetExporter(ActivityExporter exporter)\n        {\n            this.Exporter = exporter ?? throw new ArgumentNullException(nameof(exporter));\n            return this;\n        }\n\n        internal ActivityProcessor Build()\n        {\n            this.Processors = new List<ActivityProcessor>();\n\n            ActivityProcessor exportingProcessor = null;\n\n            \/\/ build or create default exporting processor\n            if (this.lastProcessorFactory != null)\n            {\n                exportingProcessor = this.lastProcessorFactory.Invoke(this.Exporter);\n                this.Processors.Add(exportingProcessor);\n            }\n            else if (this.Exporter != null)\n            {\n                \/\/ TODO: Make this BatchingActivityProcessor once its available.\n                exportingProcessor = new SimpleActivityProcessor(this.Exporter);\n                this.Processors.Add(exportingProcessor);\n            }\n\n            if (this.processorChain == null)\n            {\n                \/\/ if there is no chain, return exporting processor.\n                if (exportingProcessor == null)\n                {\n                    exportingProcessor = new NoopActivityProcessor();\n                    this.Processors.Add(exportingProcessor);\n                }\n\n                return exportingProcessor;\n            }\n\n            var next = exportingProcessor;\n\n            \/\/ build chain from the end to the beginning\n            for (int i = this.processorChain.Count - 1; i >= 0; i--)\n            {\n                next = this.processorChain[i].Invoke(next);\n                this.Processors.Add(next);\n            }\n\n            \/\/ return the last processor in the chain - it will be called first\n            return this.Processors[this.Processors.Count - 1];\n        }\n    }\n}\n","lang_cluster":"C#","length":127,"code_uid":"2a34b59717e6494c8e76fcbac3ae9882"}
{"diff_hunk":"@@ -19,9 +19,9 @@ namespace Datadog.Trace.Util.Http\n         internal static Dictionary<string, object> PrepareArgsForWaf(this HttpRequest request, RouteData routeDatas = null)\n         {\n             var url = GetUrl(request);\n-\n             var headersDic = new Dictionary<string, string>();\n-            foreach (var k in request.Headers.Keys)\n+            var headerKeys = request.Headers.Keys.Where(k => k != \"Cookie\");\n+            foreach (var k in headerKeys)\n             {\n                 headersDic.Add(k, request.Headers[k]);\n             }","old_code":"\/\/ <copyright file=\"HttpRequestExtensions.Core.cs\" company=\"Datadog\">\n\/\/ Unless explicitly stated otherwise all files in this repository are licensed under the Apache 2 License.\n\/\/ This product includes software developed at Datadog (https:\/\/www.datadoghq.com\/). Copyright 2017 Datadog, Inc.\n\/\/ <\/copyright>\n\n#if !NETFRAMEWORK\nusing System.Collections.Generic;\nusing System.Linq;\nusing Datadog.Trace.AppSec;\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.AspNetCore.Routing;\n\nnamespace Datadog.Trace.Util.Http\n{\n    internal static partial class HttpRequestExtensions\n    {\n        private const string NoHostSpecified = \"UNKNOWN_HOST\";\n\n        internal static Dictionary<string, object> PrepareArgsForWaf(this HttpRequest request, RouteData routeDatas = null)\n        {\n            var url = GetUrl(request);\n\n            var headersDic = new Dictionary<string, string>();\n            foreach (var k in request.Headers.Keys)\n            {\n                headersDic.Add(k, request.Headers[k]);\n            }\n\n            var cookiesDic = new Dictionary<string, string>();\n            foreach (var k in request.Cookies.Keys)\n            {\n                cookiesDic.Add(k, request.Cookies[k]);\n            }\n\n            var dict = new Dictionary<string, object>()\n            {\n                { AddressesConstants.RequestMethod, request.Method },\n                { AddressesConstants.RequestUriRaw, url },\n                { AddressesConstants.RequestQuery, request.QueryString.ToString() },\n                { AddressesConstants.RequestHeaderNoCookies, headersDic },\n                { AddressesConstants.RequestCookies, cookiesDic },\n            };\n\n            if (routeDatas != null && routeDatas.Values.Any())\n            {\n                var routeDataDict = ConvertRouteValueDictionary(routeDatas.Values);\n                dict.Add(AddressesConstants.RequestPathParams, routeDataDict);\n            }\n\n            return dict;\n        }\n\n        internal static string GetUrl(this HttpRequest request)\n        {\n            if (request.Host.HasValue)\n            {\n                return $\"{request.Scheme}:\/\/{request.Host.Value}{request.PathBase.Value}{request.Path.Value}\";\n            }\n\n            \/\/ HTTP 1.0 requests are not required to provide a Host to be valid\n            \/\/ Since this is just for display, we can provide a string that is\n            \/\/ not an actual Uri with only the fields that are specified.\n            \/\/ request.GetDisplayUrl(), used above, will throw an exception\n            \/\/ if request.Host is null.\n            return $\"{request.Scheme}:\/\/{HttpRequestExtensions.NoHostSpecified}{request.PathBase.Value}{request.Path.Value}\";\n        }\n    }\n}\n#endif\n","lang_cluster":"C#","length":69,"code_uid":"e9169a3cab2c4be5a59ba7953ee333fa"}
{"diff_hunk":"@@ -56,8 +56,8 @@ namespace Microsoft.VisualStudio.TestPlatform.Client\n             var runconfiguration = XmlRunSettingsUtilities.GetRunConfigurationNode(discoveryCriteria.RunSettings);\n             var testHostManager = this.TestEngine.GetDefaultTestHostManager(runconfiguration.TargetPlatform, runconfiguration.TargetFrameworkVersion);\n             \n-            var discoveryManager = this.TestEngine.GetDiscoveryManager();\n-            discoveryManager.Initialize(testHostManager);\n+            var discoveryManager = this.TestEngine.GetDiscoveryManager(testHostManager);\n+            discoveryManager.Initialize();\n \n             return new DiscoveryRequest(discoveryCriteria, discoveryManager);\n         }","old_code":"\/\/ Copyright (c) Microsoft. All rights reserved.\n\nnamespace Microsoft.VisualStudio.TestPlatform.Client\n{\n    using System;\n    using System.Collections.Generic;\n\n    using Microsoft.VisualStudio.TestPlatform.Client.Discovery;\n    using Microsoft.VisualStudio.TestPlatform.Client.Execution;\n    using Microsoft.VisualStudio.TestPlatform.CrossPlatEngine;\n    using Microsoft.VisualStudio.TestPlatform.ObjectModel.Client;\n    using Microsoft.VisualStudio.TestPlatform.ObjectModel.Engine;\n    using Microsoft.VisualStudio.TestPlatform.ObjectModel.Utilities;\n\n    \/\/\/ <summary>\n    \/\/\/ Implementation for TestPlatform\n    \/\/\/ <\/summary>\n    public class TestPlatform : ITestPlatform\n    {\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"TestPlatform\"\/> class.\n        \/\/\/ <\/summary>\n        public TestPlatform() : this(new TestEngine())\n        {\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"TestPlatform\"\/> class.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"testEngine\">\n        \/\/\/ The test engine.\n        \/\/\/ <\/param>\n        protected TestPlatform(ITestEngine testEngine)\n        {\n            this.TestEngine = testEngine;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets Test Engine instance\n        \/\/\/ <\/summary>\n        private ITestEngine TestEngine { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/ The create discovery request.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"discoveryCriteria\"> The discovery criteria. <\/param>\n        \/\/\/ <returns> The <see cref=\"IDiscoveryRequest\"\/>. <\/returns>\n        \/\/\/ <exception cref=\"ArgumentNullException\"> Throws if parameter is null. <\/exception>\n        public IDiscoveryRequest CreateDiscoveryRequest(DiscoveryCriteria discoveryCriteria)\n        {\n            if (discoveryCriteria == null)\n            {\n                throw new ArgumentNullException(\"discoveryCriteria\");\n            }\n\n            var runconfiguration = XmlRunSettingsUtilities.GetRunConfigurationNode(discoveryCriteria.RunSettings);\n            var testHostManager = this.TestEngine.GetDefaultTestHostManager(runconfiguration.TargetPlatform, runconfiguration.TargetFrameworkVersion);\n            \n            var discoveryManager = this.TestEngine.GetDiscoveryManager();\n            discoveryManager.Initialize(testHostManager);\n\n            return new DiscoveryRequest(discoveryCriteria, discoveryManager);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ The create test run request.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"testRunCriteria\"> The test run criteria.  <\/param>\n        \/\/\/ <returns> The <see cref=\"ITestRunRequest\"\/>. <\/returns>\n        \/\/\/ <exception cref=\"ArgumentNullException\"> Throws if parameter is null. <\/exception>\n        public ITestRunRequest CreateTestRunRequest(TestRunCriteria testRunCriteria)\n        {\n            if (testRunCriteria == null)\n            {\n                throw new ArgumentNullException(\"testRunCriteria\");\n            }\n\n            var runConfiguration = XmlRunSettingsUtilities.GetRunConfigurationNode(testRunCriteria.TestRunSettings);\n            var testHostManager = this.TestEngine.GetDefaultTestHostManager(runConfiguration.TargetPlatform, runConfiguration.TargetFrameworkVersion);\n\n            if (testRunCriteria.TestHostLauncher != null)\n            {\n                testHostManager.SetCustomLauncher(testRunCriteria.TestHostLauncher);\n            }\n\n            var executionManager = this.TestEngine.GetExecutionManager(testRunCriteria);\n            executionManager.Initialize(testHostManager);\n\n            return new TestRunRequest(testRunCriteria, executionManager);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ The dispose.\n        \/\/\/ <\/summary>\n        public void Dispose()\n        {\n            throw new NotImplementedException();\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ The initialize.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"pathToAdditionalExtensions\"> The path to additional extensions. <\/param>\n        \/\/\/ <param name=\"loadOnlyWellKnownExtensions\"> The load only well known extensions. <\/param>\n        \/\/\/ <param name=\"forceX86Discoverer\"> The force x86 discoverer. <\/param>\n        public void Initialize(IEnumerable<string> pathToAdditionalExtensions, bool loadOnlyWellKnownExtensions, bool forceX86Discoverer)\n        {\n            \/\/ TODO: ForceX86Discoverer options\n            this.TestEngine.GetExtensionManager()\n                 .UseAdditionalExtensions(pathToAdditionalExtensions, loadOnlyWellKnownExtensions);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ The update extensions.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"pathToAdditionalExtensions\"> The path to additional extensions. <\/param>\n        \/\/\/ <param name=\"loadOnlyWellKnownExtensions\"> The load only well known extensions. <\/param>\n        public void UpdateExtensions(IEnumerable<string> pathToAdditionalExtensions, bool loadOnlyWellKnownExtensions)\n        {\n            this.TestEngine.GetExtensionManager()\n                   .UseAdditionalExtensions(pathToAdditionalExtensions, loadOnlyWellKnownExtensions);\n        }\n    }\n}\n","lang_cluster":"C#","length":124,"code_uid":"0ca9be6bf2354c64bccd708163516683"}
{"diff_hunk":"@@ -16,11 +16,12 @@\n \n using System.Collections.Generic;\n using Nethermind.Core;\n+using Nethermind.Core.Crypto;\n \n namespace Nethermind.Consensus\n {\n     public interface IPendingTxSelector\n     {\n-        IEnumerable<Transaction> SelectTransactions(long gasLimit);\n+        IEnumerable<Transaction> SelectTransactions(Keccak stateRoot, long gasLimit);\n     }\n }","old_code":"\ufeff\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System.Collections.Generic;\nusing Nethermind.Core;\n\nnamespace Nethermind.Consensus\n{\n    public interface IPendingTxSelector\n    {\n        IEnumerable<Transaction> SelectTransactions(long gasLimit);\n    }\n}","lang_cluster":"C#","length":26,"code_uid":"7354b970164f4acca0a07ba1f2e9cfa8"}
{"diff_hunk":"@@ -19,13 +19,18 @@ namespace Microsoft.AspNetCore.Server.Kestrel.Core\n     \/\/\/ <\/summary>\n     public class ListenOptions : IEndPointInformation, IConnectionBuilder\n     {\n+        internal const string Http2ExperimentSwitch = \"Switch.Microsoft.AspNetCore.Server.Kestrel.Experimental.Http2\";\n+\n         private FileHandleType _handleType;\n+        private HttpProtocols _protocols = HttpProtocols.Http1;\n+        internal bool _isHttp2Supported;\n         private readonly List<Func<ConnectionDelegate, ConnectionDelegate>> _components = new List<Func<ConnectionDelegate, ConnectionDelegate>>();\n \n         internal ListenOptions(IPEndPoint endPoint)\n         {\n             Type = ListenType.IPEndPoint;\n             IPEndPoint = endPoint;\n+            AppContext.TryGetSwitch(Http2ExperimentSwitch, out _isHttp2Supported);\n         }\n \n         internal ListenOptions(string socketPath)","old_code":"\ufeff\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Net;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Protocols;\nusing Microsoft.AspNetCore.Server.Kestrel.Core.Adapter.Internal;\nusing Microsoft.AspNetCore.Server.Kestrel.Core.Internal;\nusing Microsoft.AspNetCore.Server.Kestrel.Transport.Abstractions.Internal;\n\nnamespace Microsoft.AspNetCore.Server.Kestrel.Core\n{\n    \/\/\/ <summary>\n    \/\/\/ Describes either an <see cref=\"IPEndPoint\"\/>, Unix domain socket path, or a file descriptor for an already open\n    \/\/\/ socket that Kestrel should bind to or open.\n    \/\/\/ <\/summary>\n    public class ListenOptions : IEndPointInformation, IConnectionBuilder\n    {\n        private FileHandleType _handleType;\n        private readonly List<Func<ConnectionDelegate, ConnectionDelegate>> _components = new List<Func<ConnectionDelegate, ConnectionDelegate>>();\n\n        internal ListenOptions(IPEndPoint endPoint)\n        {\n            Type = ListenType.IPEndPoint;\n            IPEndPoint = endPoint;\n        }\n\n        internal ListenOptions(string socketPath)\n        {\n            Type = ListenType.SocketPath;\n            SocketPath = socketPath;\n        }\n\n        internal ListenOptions(ulong fileHandle)\n            : this(fileHandle, FileHandleType.Auto)\n        {\n        }\n\n        internal ListenOptions(ulong fileHandle, FileHandleType handleType)\n        {\n            Type = ListenType.FileHandle;\n            FileHandle = fileHandle;\n            switch (handleType)\n            {\n                case FileHandleType.Auto:\n                case FileHandleType.Tcp:\n                case FileHandleType.Pipe:\n                    _handleType = handleType;\n                    break;\n                default:\n                    throw new NotSupportedException();\n            }\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ The type of interface being described: either an <see cref=\"IPEndPoint\"\/>, Unix domain socket path, or a file descriptor.\n        \/\/\/ <\/summary>\n        public ListenType Type { get; }\n\n        public FileHandleType HandleType\n        {\n            get => _handleType;\n            set\n            {\n                if (value == _handleType)\n                {\n                    return;\n                }\n                if (Type != ListenType.FileHandle || _handleType != FileHandleType.Auto)\n                {\n                    throw new InvalidOperationException();\n                }\n\n                switch (value)\n                {\n                    case FileHandleType.Tcp:\n                    case FileHandleType.Pipe:\n                        _handleType = value;\n                        break;\n                    default:\n                        throw new ArgumentException(nameof(HandleType));\n                }\n            }\n        }\n\n        \/\/ IPEndPoint is mutable so port 0 can be updated to the bound port.\n        \/\/\/ <summary>\n        \/\/\/ The <see cref=\"IPEndPoint\"\/> to bind to.\n        \/\/\/ Only set if the <see cref=\"ListenOptions\"\/> <see cref=\"Type\"\/> is <see cref=\"ListenType.IPEndPoint\"\/>.\n        \/\/\/ <\/summary>\n        public IPEndPoint IPEndPoint { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/ The absolute path to a Unix domain socket to bind to.\n        \/\/\/ Only set if the <see cref=\"ListenOptions\"\/> <see cref=\"Type\"\/> is <see cref=\"ListenType.SocketPath\"\/>.\n        \/\/\/ <\/summary>\n        public string SocketPath { get; }\n\n        \/\/\/ <summary>\n        \/\/\/ A file descriptor for the socket to open.\n        \/\/\/ Only set if the <see cref=\"ListenOptions\"\/> <see cref=\"Type\"\/> is <see cref=\"ListenType.FileHandle\"\/>.\n        \/\/\/ <\/summary>\n        public ulong FileHandle { get; }\n\n        \/\/\/ <summary>\n        \/\/\/ Enables an <see cref=\"IConnectionAdapter\"\/> to resolve and use services registered by the application during startup.\n        \/\/\/ Only set if accessed from the callback of a <see cref=\"KestrelServerOptions\"\/> Listen* method.\n        \/\/\/ <\/summary>\n        public KestrelServerOptions KestrelServerOptions { get; internal set; }\n\n        \/\/\/ <summary>\n        \/\/\/ Set to false to enable Nagle's algorithm for all connections.\n        \/\/\/ <\/summary>\n        \/\/\/ <remarks>\n        \/\/\/ Defaults to true.\n        \/\/\/ <\/remarks>\n        public bool NoDelay { get; set; } = true;\n\n        \/\/\/ <summary>\n        \/\/\/ The protocols enabled on this endpoint.\n        \/\/\/ <\/summary>\n        \/\/\/ <remarks>Defaults to HTTP\/1.x only.<\/remarks>\n        public HttpProtocols Protocols { get; set; } = HttpProtocols.Http1;\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the <see cref=\"List{IConnectionAdapter}\"\/> that allows each connection <see cref=\"System.IO.Stream\"\/>\n        \/\/\/ to be intercepted and transformed.\n        \/\/\/ Configured by the <c>UseHttps()<\/c> and <see cref=\"Hosting.ListenOptionsConnectionLoggingExtensions.UseConnectionLogging(ListenOptions)\"\/>\n        \/\/\/ extension methods.\n        \/\/\/ <\/summary>\n        \/\/\/ <remarks>\n        \/\/\/ Defaults to empty.\n        \/\/\/ <\/remarks>\n        public List<IConnectionAdapter> ConnectionAdapters { get; } = new List<IConnectionAdapter>();\n\n        public IServiceProvider ApplicationServices => KestrelServerOptions?.ApplicationServices;\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the name of this endpoint to display on command-line when the web server starts.\n        \/\/\/ <\/summary>\n        internal virtual string GetDisplayName()\n        {\n            var scheme = ConnectionAdapters.Any(f => f.IsHttps)\n                ? \"https\"\n                : \"http\";\n\n            switch (Type)\n            {\n                case ListenType.IPEndPoint:\n                    return $\"{scheme}:\/\/{IPEndPoint}\";\n                case ListenType.SocketPath:\n                    return $\"{scheme}:\/\/unix:{SocketPath}\";\n                case ListenType.FileHandle:\n                    return $\"{scheme}:\/\/<file handle>\";\n                default:\n                    throw new InvalidOperationException();\n            }\n        }\n\n        public override string ToString() => GetDisplayName();\n\n        public IConnectionBuilder Use(Func<ConnectionDelegate, ConnectionDelegate> middleware)\n        {\n            _components.Add(middleware);\n            return this;\n        }\n\n        public ConnectionDelegate Build()\n        {\n            ConnectionDelegate app = context =>\n            {\n                return Task.CompletedTask;\n            };\n\n            for (int i = _components.Count - 1; i >= 0; i--)\n            {\n                var component = _components[i];\n                app = component(app);\n            }\n\n            return app;\n        }\n\n        internal virtual async Task BindAsync(AddressBindContext context)\n        {\n            await AddressBinder.BindEndpointAsync(this, context).ConfigureAwait(false);\n            context.Addresses.Add(GetDisplayName());\n        }\n    }\n}\n","lang_cluster":"C#","length":193,"code_uid":"06984852cf534e0fa297f02b7dbb4b4f"}
{"diff_hunk":"@@ -5,8 +5,8 @@ namespace Microsoft.CodeAnalysis.Sarif\n     public static class VersionConstants                                       \n     {                                                                          \n         public const string Prerelease = \"-beta\";                       \n-        public const string AssemblyVersion = \"1.5.21\";       \n-        public const string FileVersion = \"1.5.21\" + \".0\";    \n+        public const string AssemblyVersion = \"1.5.22\";       \n+        public const string FileVersion = \"1.5.22\" + \".0\";    \n         public const string Version = AssemblyVersion + Prerelease;            \n     }                                                                          \n  }                                                                             ","old_code":"\/\/ Copyright (c) Microsoft. All rights reserved. Licensed under the MIT        \n\/\/ license. See LICENSE file in the project root for full license information. \nnamespace Microsoft.CodeAnalysis.Sarif                                         \n{                                                                              \n    public static class VersionConstants                                       \n    {                                                                          \n        public const string Prerelease = \"-beta\";                       \n        public const string AssemblyVersion = \"1.5.21\";       \n        public const string FileVersion = \"1.5.21\" + \".0\";    \n        public const string Version = AssemblyVersion + Prerelease;            \n    }                                                                          \n }                                                                             \n","lang_cluster":"C#","length":12,"code_uid":"bff34a3bb1344a2c819fb5a651cdb470"}
{"diff_hunk":"@@ -1,27 +1,19 @@\n-\ufeffusing System.Collections.Generic;\n+using System.Collections.Generic;\n using Datadog.Trace.Logging;\n using OpenTracing;\n \n namespace Datadog.Trace.OpenTracing\n {\n-    internal class OpenTracingSpanContext : SpanContext, ISpanContext\n+    internal class OpenTracingSpanContext : ISpanContext\n     {\n         private static ILog _log = LogProvider.For<OpenTracingSpanContext>();\n \n-        internal OpenTracingSpanContext(IDatadogTracer tracer, SpanContext parent, string serviceName)\n-            : base(tracer, parent, serviceName)\n+        public OpenTracingSpanContext(SpanContext context)\n         {\n+            Context = context;\n         }\n \n-        internal OpenTracingSpanContext(ulong traceId, ulong spanId)\n-            : base(traceId, spanId)\n-        {\n-        }\n-\n-        internal OpenTracingSpanContext(SpanContext spanContext)\n-            : base(spanContext)\n-        {\n-        }\n+        internal SpanContext Context { get; }\n \n         public override bool Equals(object obj)\n         {","old_code":"\ufeffusing System.Collections.Generic;\nusing Datadog.Trace.Logging;\nusing OpenTracing;\n\nnamespace Datadog.Trace.OpenTracing\n{\n    internal class OpenTracingSpanContext : SpanContext, ISpanContext\n    {\n        private static ILog _log = LogProvider.For<OpenTracingSpanContext>();\n\n        internal OpenTracingSpanContext(IDatadogTracer tracer, SpanContext parent, string serviceName)\n            : base(tracer, parent, serviceName)\n        {\n        }\n\n        internal OpenTracingSpanContext(ulong traceId, ulong spanId)\n            : base(traceId, spanId)\n        {\n        }\n\n        internal OpenTracingSpanContext(SpanContext spanContext)\n            : base(spanContext)\n        {\n        }\n\n        public override bool Equals(object obj)\n        {\n            var spanContext = obj as OpenTracingSpanContext;\n            if (spanContext == null)\n            {\n                return false;\n            }\n\n            return this.ParentId == spanContext.ParentId && this.SpanId == spanContext.SpanId && this.ServiceName == spanContext.ServiceName;\n        }\n\n        public override int GetHashCode()\n        {\n            return this.ParentId.GetHashCode() ^ this.SpanId.GetHashCode() ^ this.ServiceName.GetHashCode();\n        }\n\n        public IEnumerable<KeyValuePair<string, string>> GetBaggageItems()\n        {\n            _log.Debug(\"SpanContext.GetBaggageItems is not implemented by Datadog.Trace\");\n            yield break;\n        }\n    }\n}\n","lang_cluster":"C#","length":48,"code_uid":"e122192cfec44392b0a79508d03fe6ee"}
{"diff_hunk":"@@ -29,7 +29,7 @@ namespace Nethermind.Network.Benchmarks\n         [GlobalSetup]\n         public void Setup()\n         {\n-            _node = new Node(new PublicKey(\"0x000102030405060708090a0b0c0d0e0f000102030405060708090a0b0c0d0e0f000102030405060708090a0b0c0d0e0f000102030405060708090a0b0c0d0e0f\"), \"127.0.0.1\", 1234);\n+            _node = new Node(new PublicKey(\"0x000102030405060708090a0b0c0d0e0f000102030405060708090a0b0c0d0e0f000102030405060708090a0b0c0d0e0f000102030405060708090a0b0c0d0e0f\"), \"127.0.0.1\", 1234, false);\n         }\n         \n         [Benchmark]","old_code":"\ufeff\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing BenchmarkDotNet.Attributes;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Stats;\nusing Nethermind.Stats.Model;\n\nnamespace Nethermind.Network.Benchmarks\n{\n    public class NodeStatsCtorBenchmarks\n    {\n        private Node _node;\n        \n        [GlobalSetup]\n        public void Setup()\n        {\n            _node = new Node(new PublicKey(\"0x000102030405060708090a0b0c0d0e0f000102030405060708090a0b0c0d0e0f000102030405060708090a0b0c0d0e0f000102030405060708090a0b0c0d0e0f\"), \"127.0.0.1\", 1234);\n        }\n        \n        [Benchmark]\n        public void Improved()\n        {\n            throw new NotImplementedException();\n        }\n        \n        [Benchmark]\n        public void Light()\n        {\n            NodeStatsLight stats = new NodeStatsLight(_node);\n        }\n\n        [Benchmark]\n        public long LightRep()\n        {\n            NodeStatsLight stats = new NodeStatsLight(_node);\n            return stats.CurrentNodeReputation;\n        }\n    }\n}\n","lang_cluster":"C#","length":54,"code_uid":"43e0011c5ef147b7afacec828af5aaac"}
{"diff_hunk":"@@ -13,5 +13,5 @@ namespace Datadog.Trace.Configuration\n         {\n             return Environment.GetEnvironmentVariable(key);\n         }\n-    }\n+   }\n }","old_code":"using System;\n\nnamespace Datadog.Trace.Configuration\n{\n    \/\/\/ <summary>\n    \/\/\/ Represents a configuration source that\n    \/\/\/ retrieves values from environment variables.\n    \/\/\/ <\/summary>\n    public class EnvironmentConfigurationSource : StringConfigurationSource\n    {\n        \/\/\/ <inheritdoc \/>\n        public override string GetString(string key)\n        {\n            return Environment.GetEnvironmentVariable(key);\n        }\n    }\n}\n","lang_cluster":"C#","length":17,"code_uid":"3e120b7406ae4548a6202178f589a68c"}
{"diff_hunk":"@@ -1,14 +1,16 @@\n-\ufeff\/\/ IMvxAppStart.cs\n+\/\/ IMvxAppStart.cs\n \n \/\/ MvvmCross is licensed using Microsoft Public License (Ms-PL)\n \/\/ Contributions and inspirations noted in readme.md and license.txt\n \/\/\n \/\/ Project Lead - Stuart Lodge, @slodge, me@slodge.com\n \n+using System.Threading.Tasks;\n+\n namespace MvvmCross.Core.ViewModels\n {\n     public interface IMvxAppStart\n     {\n-        void Start(object hint = null);\n+        Task Start(object hint = null);\n     }\n }","old_code":"\ufeff\/\/ IMvxAppStart.cs\n\n\/\/ MvvmCross is licensed using Microsoft Public License (Ms-PL)\n\/\/ Contributions and inspirations noted in readme.md and license.txt\n\/\/\n\/\/ Project Lead - Stuart Lodge, @slodge, me@slodge.com\n\nnamespace MvvmCross.Core.ViewModels\n{\n    public interface IMvxAppStart\n    {\n        void Start(object hint = null);\n    }\n}","lang_cluster":"C#","length":14,"code_uid":"71af8195f6ac4877b5de7913b5b64b9b"}
{"diff_hunk":"@@ -1,12 +1,19 @@\n \/\/ Copyright (c) .NET Foundation. All rights reserved.\n \/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n \n+using System;\n using Microsoft.AspNetCore.Server.Kestrel.Filter;\n \n namespace Microsoft.AspNetCore.Server.Kestrel\n {\n     public interface IKestrelServerInformation\n     {\n+        TimeSpan ExecutionTimeout { get; set; }\n+\n+        TimeSpan HeadersCompleteTimeout { get; set; }\n+\n+        TimeSpan KeepAliveTimeout { get; set; }\n+\n         int ThreadCount { get; set; }\n \n         bool NoDelay { get; set; }","old_code":"\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing Microsoft.AspNetCore.Server.Kestrel.Filter;\n\nnamespace Microsoft.AspNetCore.Server.Kestrel\n{\n    public interface IKestrelServerInformation\n    {\n        int ThreadCount { get; set; }\n\n        bool NoDelay { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets a flag that instructs <seealso cref=\"KestrelServer\"\/> whether it is safe to \n        \/\/\/ reuse the Request and Response <seealso cref=\"System.IO.Stream\"\/> objects\n        \/\/\/ for another request after the Response's OnCompleted callback has fired. \n        \/\/\/ When this is set to true it is not safe to retain references to these streams after this event has fired.\n        \/\/\/ It is false by default.\n        \/\/\/ <\/summary>\n        \/\/\/ <remarks>\n        \/\/\/ When this is set to true it is not safe to retain references to these streams after this event has fired.\n        \/\/\/ It is false by default.\n        \/\/\/ <\/remarks>\n        bool ReuseStreams { get; set; }\n\n        IConnectionFilter ConnectionFilter { get; set; }\n    }\n}\n","lang_cluster":"C#","length":29,"code_uid":"92a873b3d70440e388316a26796084da"}
{"diff_hunk":"@@ -11,6 +11,11 @@ namespace Microsoft.AspNet.Server.Kestrel.Https\n     public static class HttpsApplicationBuilderExtensions\n     {\n         public static IApplicationBuilder UseKestrelHttps(this IApplicationBuilder app, X509Certificate2 cert)\n+        {\n+            return app.UseKestrelHttps(new HttpsConnectionFilterOptions { ServerCertificate = cert});\n+        }\n+\n+        public static IApplicationBuilder UseKestrelHttps(this IApplicationBuilder app, HttpsConnectionFilterOptions options)\n         {\n             var serverInfo = app.ServerFeatures.Get<IKestrelServerInformation>();\n ","old_code":"\ufeff\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System.Security.Cryptography.X509Certificates;\nusing Microsoft.AspNet.Builder;\nusing Microsoft.AspNet.Http.Features;\nusing Microsoft.AspNet.Server.Kestrel.Filter;\n\nnamespace Microsoft.AspNet.Server.Kestrel.Https\n{\n    public static class HttpsApplicationBuilderExtensions\n    {\n        public static IApplicationBuilder UseKestrelHttps(this IApplicationBuilder app, X509Certificate2 cert)\n        {\n            var serverInfo = app.ServerFeatures.Get<IKestrelServerInformation>();\n\n            if (serverInfo == null)\n            {\n                return app;\n            }\n\n            var prevFilter = serverInfo.ConnectionFilter ?? new NoOpConnectionFilter();\n\n            serverInfo.ConnectionFilter = new HttpsConnectionFilter(cert, prevFilter);\n\n            return app;\n        }\n    }\n}\n","lang_cluster":"C#","length":29,"code_uid":"a22c4b63055542618d97f284fcfdb754"}
{"diff_hunk":"@@ -19,7 +19,7 @@ namespace Nethermind.Blockchain.Synchronization.SyncLimits\n     public static class GethSyncLimits\n     {\n         public const int MaxHeaderFetch = 192; \/\/ Amount of block headers to be fetched per retrieval request\n-        public const int MaxBodyFetch = 32; \/\/ Amount of block bodies to be fetched per retrieval request\n+        public const int MaxBodyFetch = 128; \/\/ Amount of block bodies to be fetched per retrieval request\n         public const int MaxReceiptFetch = 128; \/\/ Amount of transaction receipts to allow fetching per request\n         public const int MaxCodeFetch = 64; \/\/ Amount of contract codes to allow fetching per request\n         public const int MaxProofsFetch = 64; \/\/ Amount of merkle proofs to be fetched per retrieval request","old_code":"\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nnamespace Nethermind.Blockchain.Synchronization.SyncLimits\n{\n    public static class GethSyncLimits\n    {\n        public const int MaxHeaderFetch = 192; \/\/ Amount of block headers to be fetched per retrieval request\n        public const int MaxBodyFetch = 32; \/\/ Amount of block bodies to be fetched per retrieval request\n        public const int MaxReceiptFetch = 128; \/\/ Amount of transaction receipts to allow fetching per request\n        public const int MaxCodeFetch = 64; \/\/ Amount of contract codes to allow fetching per request\n        public const int MaxProofsFetch = 64; \/\/ Amount of merkle proofs to be fetched per retrieval request\n        public const int MaxHelperTrieProofsFetch = 64; \/\/ Amount of helper tries to be fetched per retrieval request\n        public const int MaxTxSend = 64; \/\/ Amount of transactions to be send per request\n        public const int MaxTxStatus = 256; \/\/ Amount of transactions to queried per request   \n    }\n}","lang_cluster":"C#","length":30,"code_uid":"ac9a87ac703347079e3d5205816d993f"}
{"diff_hunk":"@@ -7,6 +7,9 @@ namespace Datadog.Trace\n     \/\/\/ <\/summary>\n     public static class CorrelationIdentifier\n     {\n+        internal static readonly string ServiceKey = \"dd.service\";\n+        internal static readonly string VersionKey = \"dd.version\";\n+        internal static readonly string EnvKey = \"dd.env\";\n         internal static readonly string TraceIdKey = \"dd.trace_id\";\n         internal static readonly string SpanIdKey = \"dd.span_id\";\n ","old_code":"using System;\n\nnamespace Datadog.Trace\n{\n    \/\/\/ <summary>\n    \/\/\/ An API to access the active trace and span ids.\n    \/\/\/ <\/summary>\n    public static class CorrelationIdentifier\n    {\n        internal static readonly string TraceIdKey = \"dd.trace_id\";\n        internal static readonly string SpanIdKey = \"dd.span_id\";\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the trace id\n        \/\/\/ <\/summary>\n        public static ulong TraceId\n        {\n            get\n            {\n                return Tracer.Instance.ActiveScope?.Span?.TraceId ?? 0;\n            }\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Gets the span id\n        \/\/\/ <\/summary>\n        public static ulong SpanId\n        {\n            get\n            {\n                return Tracer.Instance.ActiveScope?.Span?.SpanId ?? 0;\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":35,"code_uid":"f60f714653e841349afb07dcc9a2148f"}
{"diff_hunk":"@@ -133,7 +133,7 @@ namespace OpenTelemetry.Trace\n \n         private void AddInternal(string key, object value)\n         {\n-            Guard.Null(key, nameof(key));\n+            Debug.Assert(key != null, $\"{nameof(key)} must not be null\");\n \n             this.Attributes[key] = value;\n         }","old_code":"\/\/ <copyright file=\"SpanAttributes.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing OpenTelemetry.Internal;\n\nnamespace OpenTelemetry.Trace\n{\n    \/\/\/ <summary>\n    \/\/\/ A class that represents the span attributes. Read more here https:\/\/github.com\/open-telemetry\/opentelemetry-specification\/blob\/main\/specification\/common\/common.md#attributes.\n    \/\/\/ <\/summary>\n    \/\/\/ <remarks>SpanAttributes is a wrapper around <see cref=\"ActivityTagsCollection\"\/> class.<\/remarks>\n    public class SpanAttributes\n    {\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"SpanAttributes\"\/> class.\n        \/\/\/ <\/summary>\n        public SpanAttributes()\n        {\n            this.Attributes = new ActivityTagsCollection();\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Initializes a new instance of the <see cref=\"SpanAttributes\"\/> class.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"attributes\">Initial attributes to store in the collection.<\/param>\n        public SpanAttributes(IEnumerable<KeyValuePair<string, object>> attributes)\n            : this()\n        {\n            Guard.Null(attributes, nameof(attributes));\n\n            foreach (KeyValuePair<string, object> kvp in attributes)\n            {\n                this.AddInternal(kvp.Key, kvp.Value);\n            }\n        }\n\n        internal ActivityTagsCollection Attributes { get; }\n\n        \/\/\/ <summary>\n        \/\/\/ Add entry to the attributes.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"key\">Entry key.<\/param>\n        \/\/\/ <param name=\"value\">Entry value.<\/param>\n        public void Add(string key, long value)\n        {\n            this.AddInternal(key, value);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Add entry to the attributes.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"key\">Entry key.<\/param>\n        \/\/\/ <param name=\"value\">Entry value.<\/param>\n        public void Add(string key, string value)\n        {\n            this.AddInternal(key, value);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Add entry to the attributes.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"key\">Entry key.<\/param>\n        \/\/\/ <param name=\"value\">Entry value.<\/param>\n        public void Add(string key, bool value)\n        {\n            this.AddInternal(key, value);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Add entry to the attributes.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"key\">Entry key.<\/param>\n        \/\/\/ <param name=\"value\">Entry value.<\/param>\n        public void Add(string key, double value)\n        {\n            this.AddInternal(key, value);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Add entry to the attributes.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"key\">Entry key.<\/param>\n        \/\/\/ <param name=\"values\">Entry value.<\/param>\n        public void Add(string key, long[] values)\n        {\n            this.AddInternal(key, values);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Add entry to the attributes.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"key\">Entry key.<\/param>\n        \/\/\/ <param name=\"values\">Entry value.<\/param>\n        public void Add(string key, string[] values)\n        {\n            this.AddInternal(key, values);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Add entry to the attributes.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"key\">Entry key.<\/param>\n        \/\/\/ <param name=\"values\">Entry value.<\/param>\n        public void Add(string key, bool[] values)\n        {\n            this.AddInternal(key, values);\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Add entry to the attributes.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"key\">Entry key.<\/param>\n        \/\/\/ <param name=\"values\">Entry value.<\/param>\n        public void Add(string key, double[] values)\n        {\n            this.AddInternal(key, values);\n        }\n\n        private void AddInternal(string key, object value)\n        {\n            Guard.Null(key, nameof(key));\n\n            this.Attributes[key] = value;\n        }\n    }\n}\n","lang_cluster":"C#","length":141,"code_uid":"654e4bb91a4247ecb48a80164edf1fd9"}
{"diff_hunk":"@@ -57,14 +57,29 @@ namespace Nethermind.Network.StaticNodes\n \n             string data = await File.ReadAllTextAsync(_staticNodesPath);\n             string[] nodes = GetNodes(data);\n-            if (_logger.IsInfo) _logger.Info($\"Loaded {nodes.Length} static nodes from file: {Path.GetFullPath(_staticNodesPath)}\");\n+            if (_logger.IsInfo)\n+                _logger.Info($\"Loaded {nodes.Length} static nodes from file: {Path.GetFullPath(_staticNodesPath)}\");\n             if (nodes.Length != 0)\n             {\n                 if (_logger.IsDebug) _logger.Debug($\"Static nodes: {Environment.NewLine}{data}\");\n             }\n-            \n-            _nodes = new ConcurrentDictionary<PublicKey, NetworkNode>(nodes.Select(n => new NetworkNode(n))\n-                .ToDictionary(n => n.NodeId, n => n));\n+\n+            IEnumerable<NetworkNode> networkNodes = new List<NetworkNode>();\n+\n+            foreach (var n in nodes)\n+            {\n+                try\n+                {\n+                    NetworkNode networkNode = new(n);\n+                    networkNodes = networkNodes.Append(networkNode);\n+                }\n+                catch (Exception exception)\n+                {\n+                    if (_logger.IsError) _logger.Error(\"Unable to process node. \", exception);\n+                }\n+            }\n+\n+            _nodes = new ConcurrentDictionary<PublicKey, NetworkNode>(networkNodes.ToDictionary(n => n.NodeId, n => n));\n         }\n \n         private static string[] GetNodes(string data)","old_code":"\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing System.Collections.Concurrent;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Nethermind.Config;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Logging;\nusing Nethermind.Stats.Model;\nusing Newtonsoft.Json;\n\nnamespace Nethermind.Network.StaticNodes\n{\n    public class StaticNodesManager : IStaticNodesManager\n    {\n        private ConcurrentDictionary<PublicKey, NetworkNode> _nodes =\n            new ConcurrentDictionary<PublicKey, NetworkNode>();\n\n        private readonly string _staticNodesPath;\n        private readonly ILogger _logger;\n\n        public StaticNodesManager(string staticNodesPath, ILogManager logManager)\n        {\n            _staticNodesPath = staticNodesPath.GetApplicationResourcePath();\n            _logger = logManager.GetClassLogger();\n        }\n\n        public IEnumerable<NetworkNode> Nodes => _nodes.Values;\n        public event EventHandler<NetworkNodeEventArgs> NodeAdded;\n        public event EventHandler<NetworkNodeEventArgs> NodeRemoved;\n\n        public async Task InitAsync()\n        {\n            if (!File.Exists(_staticNodesPath))\n            {\n                if (_logger.IsDebug) _logger.Debug($\"Static nodes file was not found for path: {_staticNodesPath}\");\n\n                return;\n            }\n\n            string data = await File.ReadAllTextAsync(_staticNodesPath);\n            string[] nodes = GetNodes(data);\n            if (_logger.IsInfo) _logger.Info($\"Loaded {nodes.Length} static nodes from file: {Path.GetFullPath(_staticNodesPath)}\");\n            if (nodes.Length != 0)\n            {\n                if (_logger.IsDebug) _logger.Debug($\"Static nodes: {Environment.NewLine}{data}\");\n            }\n            \n            _nodes = new ConcurrentDictionary<PublicKey, NetworkNode>(nodes.Select(n => new NetworkNode(n))\n                .ToDictionary(n => n.NodeId, n => n));\n        }\n\n        private static string[] GetNodes(string data)\n        {\n            string[] nodes;\n            try\n            {\n                nodes = JsonConvert.DeserializeObject<string[]>(data) ?? Array.Empty<string>();\n            }\n            catch (JsonException)\n            {\n                nodes = data.Split(new[] {'\\r', '\\n'}, StringSplitOptions.RemoveEmptyEntries);\n            }\n            \n            return nodes.Distinct().ToArray();\n        }\n\n        public async Task<bool> AddAsync(string enode, bool updateFile = true)\n        {\n            NetworkNode node = new NetworkNode(enode);\n            if (!_nodes.TryAdd(node.NodeId, node))\n            {\n                if (_logger.IsInfo) _logger.Info($\"Static node was already added: {enode}\");\n                return false;\n            }\n\n            if (_logger.IsInfo) _logger.Info($\"Static node added: {enode}\");\n            NodeAdded?.Invoke(this, new NetworkNodeEventArgs(node));\n            if (updateFile)\n            {\n                await SaveFileAsync();\n            }\n\n            return true;\n        }\n\n        public async Task<bool> RemoveAsync(string enode, bool updateFile = true)\n        {\n            NetworkNode node = new NetworkNode(enode);\n            if (!_nodes.TryRemove(node.NodeId, out _))\n            {\n                if (_logger.IsInfo) _logger.Info($\"Static node was not found: {enode}\");\n                return false;\n            }\n\n            if (_logger.IsInfo) _logger.Info($\"Static node was removed: {enode}\");\n            NodeRemoved?.Invoke(this, new NetworkNodeEventArgs(node));\n            if (updateFile)\n            {\n                await SaveFileAsync();\n            }\n\n            return true;\n        }\n\n        public bool IsStatic(string enode)\n        {\n            NetworkNode node = new NetworkNode(enode);\n            return _nodes.TryGetValue(node.NodeId, out NetworkNode staticNode) && string.Equals(staticNode.Host, node.Host, StringComparison.InvariantCultureIgnoreCase);\n        }\n\n        private Task SaveFileAsync()\n            => File.WriteAllTextAsync(_staticNodesPath,\n                JsonConvert.SerializeObject(_nodes.Select(n => n.Value.ToString()), Formatting.Indented));\n    }\n}\n","lang_cluster":"C#","length":133,"code_uid":"4bd39537fffd43f2ae372f79a68fea06"}
{"diff_hunk":"@@ -85,6 +85,17 @@ namespace pwiz.Skyline.Model.Databinding.Entities\n         [Format(NullValue = TextUtil.EXCEL_NA)]\n         public int? PointsAcrossPeak { get { return ChromInfo.PointsAcrossPeak; } }\n \n+        [Format(Formats.RETENTION_TIME, NullValue = TextUtil.EXCEL_NA)]\n+        public double? AverageCycleTime\n+        {\n+            get\n+            {\n+                return StartTime.HasValue && EndTime.HasValue && PointsAcrossPeak.HasValue\n+                    ? (EndTime.Value - StartTime.Value) * 60 \/ PointsAcrossPeak.Value\n+                    : (double?) null;\n+            }\n+        }\n+\n         public bool Coeluting { get { return !ChromInfo.IsForcedIntegration; } }\n \n         [Format(Formats.RETENTION_TIME, NullValue = TextUtil.EXCEL_NA)]","old_code":"\ufeff\/*\r\n * Original author: Nicholas Shulman <nicksh .at. u.washington.edu>,\r\n *                  MacCoss Lab, Department of Genome Sciences, UW\r\n *\r\n * Copyright 2012 University of Washington - Seattle, WA\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n *\/\r\n\r\nusing System;\r\nusing System.ComponentModel;\r\nusing pwiz.Common.DataBinding.Attributes;\r\nusing pwiz.Skyline.Model.DocSettings;\r\nusing pwiz.Skyline.Model.ElementLocators;\r\nusing pwiz.Skyline.Model.Hibernate;\r\nusing pwiz.Skyline.Model.Results;\r\nusing pwiz.Skyline.Util.Extensions;\r\n\r\nnamespace pwiz.Skyline.Model.Databinding.Entities\r\n{\r\n    [InvariantDisplayName(nameof(TransitionResult))]\r\n    [AnnotationTarget(AnnotationDef.AnnotationTarget.transition_result)]\r\n    public class TransitionResult : Result\r\n    {\r\n        private readonly CachedValue<TransitionChromInfo> _chromInfo;\r\n        private readonly CachedValue<Chromatogram> _chromatogram;\r\n        public TransitionResult(Transition transition, ResultFile resultFile) : base(transition, resultFile)\r\n        {\r\n            _chromInfo = CachedValue.Create(DataSchema, () => GetResultFile().FindChromInfo(transition.DocNode.Results));\r\n            _chromatogram = CachedValue.Create(DataSchema, \r\n                () => new Chromatogram(new ChromatogramGroup(PrecursorResult), Transition));\r\n        }\r\n\r\n        [Browsable(false)]\r\n        public TransitionChromInfo ChromInfo { get { return _chromInfo.Value; } }\r\n\r\n        public void ChangeChromInfo(EditDescription editDescription, Func<TransitionChromInfo, TransitionChromInfo> newChromInfo)\r\n        {\r\n            Transition.ChangeDocNode(editDescription, docNode=>docNode.ChangeResults(GetResultFile().ChangeChromInfo(docNode.Results, newChromInfo)));\r\n        }\r\n        [HideWhen(AncestorOfType = typeof(Transition))]\r\n        public Transition Transition { get { return (Transition)SkylineDocNode; } }\r\n        [Format(Formats.RETENTION_TIME, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? RetentionTime { get { return ChromInfo.IsEmpty ? (double?) null : ChromInfo.RetentionTime; } }\r\n        [Format(Formats.RETENTION_TIME, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? Fwhm { get { return ChromInfo.IsEmpty ? (double?) null : ChromInfo.Fwhm; } }\r\n        public bool FwhmDegenerate { get { return ChromInfo.IsFwhmDegenerate; } }\r\n        [Format(Formats.RETENTION_TIME, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? StartTime { get { return ChromInfo.IsEmpty ? (double?)null : ChromInfo.StartRetentionTime; } }\r\n        [Format(Formats.RETENTION_TIME, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? EndTime { get { return ChromInfo.IsEmpty ? (double?) null : ChromInfo.EndRetentionTime; } }\r\n        [Format(Formats.PEAK_AREA, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? Area { get { return ChromInfo.IsEmpty ? (double?) null : ChromInfo.Area; } }\r\n        [Format(Formats.PEAK_AREA, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? Background { get { return ChromInfo.IsEmpty ? (double?)null : ChromInfo.BackgroundArea; } }\r\n        [Format(Formats.STANDARD_RATIO, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? AreaRatio { get { return ChromInfo.Ratio; } }\r\n        [Format(Formats.PEAK_AREA_NORMALIZED, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? AreaNormalized\r\n        {\r\n            get { return Area \/ GetResultFile().GetTotalArea(Transition.Precursor.IsotopeLabelType); }\r\n        }\r\n        [Format(Formats.PEAK_AREA, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? Height { get { return ChromInfo.IsEmpty ? (double?) null : ChromInfo.Height; } }\r\n        [Format(Formats.MASS_ERROR, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? MassErrorPPM { get { return ChromInfo.MassError; } }\r\n        public bool? Truncated { get { return ChromInfo.IsTruncated; } }\r\n        [Format(NullValue = TextUtil.EXCEL_NA)]\r\n        public int? PeakRank { get { return ChromInfo.IsEmpty ? (int?)null : ChromInfo.Rank; } }\r\n        [Format(NullValue = TextUtil.EXCEL_NA)]\r\n        public int? PeakRankByLevel { get { return ChromInfo.IsEmpty ? (int?)null : ChromInfo.RankByLevel; } }\r\n        public UserSet UserSetPeak { get { return ChromInfo.UserSet; } }\r\n        [Format(NullValue = TextUtil.EXCEL_NA)]\r\n        public int OptStep { get { return ChromInfo.OptimizationStep; } }\r\n        [Format(NullValue = TextUtil.EXCEL_NA)]\r\n        public int? PointsAcrossPeak { get { return ChromInfo.PointsAcrossPeak; } }\r\n\r\n        public bool Coeluting { get { return !ChromInfo.IsForcedIntegration; } }\r\n\r\n        [Format(Formats.RETENTION_TIME, NullValue = TextUtil.EXCEL_NA)]\r\n        public double? IonMobilityFragment \r\n        {\r\n            get\r\n            {\r\n                return IonMobilityFilter.IsNullOrEmpty(ChromInfo.IonMobility) ? null\r\n                    : ChromInfo.IonMobility.IonMobilityAndCCS.GetHighEnergyIonMobility();\r\n            }\r\n        }\r\n\r\n        public Chromatogram Chromatogram\r\n        {\r\n            get { return _chromatogram.Value; }\r\n        }\r\n\r\n        [InvariantDisplayName(\"TransitionReplicateNote\")]\r\n        [Importable]\r\n        public string Note\r\n        {\r\n            get { return ChromInfo.Annotations.Note; }\r\n            set\r\n            {\r\n                ChangeChromInfo(EditColumnDescription(nameof(Note), value),\r\n                    chromInfo=>chromInfo.ChangeAnnotations(chromInfo.Annotations.ChangeNote(value)));\r\n            }\r\n        }\r\n\r\n        public override void SetAnnotation(AnnotationDef annotationDef, object value)\r\n        {\r\n            ChangeChromInfo(EditDescription.SetAnnotation(annotationDef, value), \r\n                chromInfo=>chromInfo.ChangeAnnotations(chromInfo.Annotations.ChangeAnnotation(annotationDef, value)));\r\n        }\r\n\r\n        public override object GetAnnotation(AnnotationDef annotationDef)\r\n        {\r\n            return DataSchema.AnnotationCalculator.GetAnnotation(annotationDef, this, ChromInfo.Annotations);\r\n        }\r\n\r\n        [HideWhen(AncestorOfType = typeof(SkylineDocument))]\r\n        public PrecursorResult PrecursorResult \r\n        {\r\n            get\r\n            {\r\n                return new PrecursorResult(Transition.Precursor, GetResultFile());\r\n            }\r\n        }\r\n        public override string ToString()\r\n        {\r\n            return string.Format(@\"{0:0}\", ChromInfo.Area);\r\n        }\r\n\r\n        [InvariantDisplayName(\"TransitionResultLocator\")]\r\n        public string Locator\r\n        {\r\n            get { return GetLocator(); }\r\n        }\r\n\r\n        public override ElementRef GetElementRef()\r\n        {\r\n            return TransitionResultRef.PROTOTYPE.ChangeChromInfo(GetResultFile().Replicate.ChromatogramSet, ChromInfo)\r\n                .ChangeParent(Transition.GetElementRef());\r\n        }\r\n\r\n        public override bool IsEmpty()\r\n        {\r\n            return ChromInfo.IsEmpty;\r\n        }\r\n    }\r\n}\r\n","lang_cluster":"C#","length":158,"code_uid":"6bcfc98730f149cc9973b528b8fcc638"}
{"diff_hunk":"@@ -34,14 +34,10 @@ namespace OpenTelemetry.Exporter.Jaeger\n         private readonly IJaegerUdpBatcher jaegerAgentUdpBatcher;\n         private bool disposedValue = false; \/\/ To detect redundant dispose calls\n \n-        public JaegerTraceExporter(JaegerExporterOptions options)\n+        public JaegerTraceExporter(JaegerExporterOptions options, IJaegerUdpBatcher jaegerAgentUdpBatcher)\n         {\n             this.ValidateOptions(options);\n             this.InitializeOptions(options);\n-        }\n-\n-        public JaegerTraceExporter(IJaegerUdpBatcher jaegerAgentUdpBatcher)\n-        {\n             this.jaegerAgentUdpBatcher = jaegerAgentUdpBatcher;\n         }\n ","old_code":"\ufeff\/\/ <copyright file=\"JaegerTraceExporter.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright 2018, OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nnamespace OpenTelemetry.Exporter.Jaeger\n{\n    using System;\n    using System.Collections.Generic;\n    using System.Linq;\n    using System.Threading;\n    using System.Threading.Tasks;\n    using OpenTelemetry.Exporter.Jaeger.Implementation;\n    using OpenTelemetry.Trace;\n    using OpenTelemetry.Trace.Export;\n\n    public class JaegerTraceExporter : SpanExporter, IDisposable\n    {\n        public const string DefaultAgentUdpHost = \"localhost\";\n        public const int DefaultAgentUdpCompactPort = 6831;\n        public const int DefaultMaxPacketSize = 65000;\n\n        private readonly IJaegerUdpBatcher jaegerAgentUdpBatcher;\n        private bool disposedValue = false; \/\/ To detect redundant dispose calls\n\n        public JaegerTraceExporter(JaegerExporterOptions options)\n        {\n            this.ValidateOptions(options);\n            this.InitializeOptions(options);\n        }\n\n        public JaegerTraceExporter(IJaegerUdpBatcher jaegerAgentUdpBatcher)\n        {\n            this.jaegerAgentUdpBatcher = jaegerAgentUdpBatcher;\n        }\n\n        public override async Task<ExportResult> ExportAsync(IEnumerable<Span> otelSpanList, CancellationToken cancellationToken)\n        {\n            var jaegerspans = otelSpanList.Select(sdl => sdl.ToJaegerSpan());\n\n            foreach (var s in jaegerspans)\n            {\n                \/\/ avoid cancelling here: this is no return point: if we reached this point\n                \/\/ and cancellation is requested, it's better if we try to finish sending spans rather than drop it\n                await this.jaegerAgentUdpBatcher.AppendAsync(s, CancellationToken.None);\n            }\n\n            \/\/ TODO jaeger status to ExportResult\n            return ExportResult.Success;\n        }\n\n        public override Task ShutdownAsync(CancellationToken cancellationToken)\n        {\n            return this.jaegerAgentUdpBatcher.FlushAsync(cancellationToken);\n        }\n\n        public void Dispose()\n        {\n            \/\/ Do not change this code. Put cleanup code in Dispose(bool disposing).\n            this.Dispose(true);\n        }\n\n        protected virtual void Dispose(bool disposing)\n        {\n            if (!this.disposedValue)\n            {\n                if (disposing)\n                {\n                    this.jaegerAgentUdpBatcher.Dispose();\n                }\n\n                this.disposedValue = true;\n            }\n        }\n\n        private void ValidateOptions(JaegerExporterOptions options)\n        {\n            if (string.IsNullOrWhiteSpace(options.ServiceName))\n            {\n                throw new ArgumentException(\"ServiceName\", \"Service Name is required.\");\n            }\n        }\n\n        private void InitializeOptions(JaegerExporterOptions options)\n        {\n            if (string.IsNullOrWhiteSpace(options.AgentHost))\n            {\n                options.AgentHost = DefaultAgentUdpHost;\n            }\n\n            if (!options.AgentPort.HasValue)\n            {\n                options.AgentPort = DefaultAgentUdpCompactPort;\n            }\n\n            if (!options.MaxPacketSize.HasValue)\n            {\n                options.MaxPacketSize = DefaultMaxPacketSize;\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":113,"code_uid":"6d15471e455c449f8ff2d1e83350fef9"}
{"diff_hunk":"@@ -120,10 +120,17 @@ namespace Microsoft.DotNet.Build.CloudTestTasks\n                     Log.LogMessage(MessageImportance.Normal, \"Received response to check whether Container blobs exist\");\n                 }\n             }\n-            await ThreadingTask.WhenAll(Items.Select(item => UploadAsync(ct, item, blobsPresent)));\n-\n-           Log.LogMessage(MessageImportance.High, \"Upload to Azure is complete, a total of {0} items were uploaded.\", Items.Length);\n+            try\n+            {\n+                await ThreadingTask.WhenAll(Items.Select(item => UploadAsync(ct, item, blobsPresent)));\n+            }\n+            catch (Exception)\n+            {\n+                Log.LogError(\"Failed to upload to Azure\");\n+                return false;\n+            }\n \n+            Log.LogMessage(MessageImportance.High, \"Upload to Azure is complete, a total of {0} items were uploaded.\", Items.Length);\n             return true;\n         }\n ","old_code":"\/\/ Licensed to the .NET Foundation under one or more agreements.\n\/\/ The .NET Foundation licenses this file to you under the MIT license.\n\/\/ See the LICENSE file in the project root for more information.\n\nusing System;\nusing System.Collections.Generic;\nusing System.Globalization;\nusing System.IO;\nusing System.Net;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing System.Xml;\nusing System.Linq;\nusing System.Net.Http;\n\nusing Microsoft.Build.Framework;\n\nusing Task = Microsoft.Build.Utilities.Task;\nusing ThreadingTask = System.Threading.Tasks.Task;\n\nnamespace Microsoft.DotNet.Build.CloudTestTasks\n{\n\n    public class UploadToAzure : Task, ICancelableTask\n    {\n        private static readonly CancellationTokenSource TokenSource = new CancellationTokenSource();\n        private static readonly CancellationToken CancellationToken = TokenSource.Token;\n\n        \/\/\/ <summary>\n        \/\/\/     The Azure account key used when creating the connection string.\n        \/\/\/ <\/summary>\n        [Required]\n        public string AccountKey { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/     The Azure account name used when creating the connection string.\n        \/\/\/ <\/summary>\n        [Required]\n        public string AccountName { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/     The name of the container to access.  The specified name must be in the correct format, see the\n        \/\/\/     following page for more info.  https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dd135715.aspx\n        \/\/\/ <\/summary>\n        [Required]\n        public string ContainerName { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/     An item group of files to upload.  Each item must have metadata RelativeBlobPath\n        \/\/\/     that specifies the path relative to ContainerName where the item will be uploaded.\n        \/\/\/ <\/summary>\n        [Required]\n        public ITaskItem[] Items { get; set; }\n\n        \/\/\/ <summary>\n        \/\/\/     Indicates if the destination blob should be overwritten if it already exists.  The default if false.\n        \/\/\/ <\/summary>\n        public bool Overwrite { get; set; } = false;\n\n        public void Cancel()\n        {\n            TokenSource.Cancel();\n        }\n\n        public override bool Execute()\n        {\n            return ExecuteAsync(CancellationToken).GetAwaiter().GetResult();\n        }\n\n        public async Task<bool> ExecuteAsync(CancellationToken ct)\n        {\n            Log.LogMessage(\n                MessageImportance.High, \n                \"Begin uploading blobs to Azure account {0} in container {1}.\", \n                AccountName, \n                ContainerName);\n\n            if (Items.Length == 0)\n            {\n                throw new ArgumentException(\"No items were provided for upload.\");\n            }\n\n            \/\/ first check what blobs are present\n            string checkListUrl = string.Format(\n                \"https:\/\/{0}.blob.core.windows.net\/{1}?restype=container&comp=list\", \n                AccountName, \n                ContainerName);\n\n            DateTime dt = DateTime.UtcNow;\n            HashSet<string> blobsPresent = new HashSet<string>();\n\n            using (HttpClient client = new HttpClient())\n            {\n                using (HttpRequestMessage req = new HttpRequestMessage(HttpMethod.Get, checkListUrl))\n                {\n                    req.Headers.Add(AzureHelper.DateHeaderString, dt.ToString(\"R\", CultureInfo.InvariantCulture));\n                    req.Headers.Add(AzureHelper.VersionHeaderString, AzureHelper.StorageApiVersion);\n                    req.Headers.Add(AzureHelper.AuthorizationHeaderString, AzureHelper.AuthorizationHeader(\n                        AccountName,\n                        AccountKey,\n                        \"GET\",\n                        dt,\n                        req));\n                    \n                    Log.LogMessage(MessageImportance.Normal, \"Sending request to check whether Container blobs exist\");\n                    XmlDocument doc;\n                    using (HttpResponseMessage response = await client.SendAsync(req, ct))\n                    {\n                        doc = new XmlDocument();\n                        doc.LoadXml(await response.Content.ReadAsStringAsync());\n                    }\n\n                    XmlNodeList nodes = doc.DocumentElement.GetElementsByTagName(\"Blob\");\n\n                    foreach (XmlNode node in nodes)\n                    {\n                        blobsPresent.Add(node[\"Name\"].InnerText);\n                    }\n\n                    Log.LogMessage(MessageImportance.Normal, \"Received response to check whether Container blobs exist\");\n                }\n            }\n            await ThreadingTask.WhenAll(Items.Select(item => UploadAsync(ct, item, blobsPresent)));\n\n           Log.LogMessage(MessageImportance.High, \"Upload to Azure is complete, a total of {0} items were uploaded.\", Items.Length);\n\n            return true;\n        }\n\n        private async ThreadingTask UploadAsync(CancellationToken ct, ITaskItem item, HashSet<string> blobsPresent)\n        {\n            bool result = true;\n            if (ct.IsCancellationRequested)\n            {\n                Log.LogError(\"Task UploadToAzure cancelled\");\n                ct.ThrowIfCancellationRequested();\n            }\n\n            string relativeBlobPath = item.GetMetadata(\"RelativeBlobPath\");\n            if (string.IsNullOrEmpty(relativeBlobPath))\n            {\n                Log.LogError(string.Format(\"Metadata 'RelativeBlobPath' is missing for item '{0}'.\", item.ItemSpec));\n                result = false;\n            }\n\n            if (!File.Exists(item.ItemSpec))\n            {\n                Log.LogError(string.Format(\"The file '{0}' does not exist.\", item.ItemSpec));\n                result = false;\n            }\n\n            if (!Overwrite && blobsPresent.Contains(relativeBlobPath))\n            {\n                Log.LogError(string.Format(\"The blob '{0}' already exists.\", relativeBlobPath));\n                result = false;\n            }\n\n            if (result)\n            {\n                Log.LogMessage(\"Uploading {0} to {1}.\", item.ItemSpec, ContainerName);\n                UploadClient uploadClient = new UploadClient(Log);\n                await\n                    uploadClient.UploadBlockBlobAsync(\n                        ct,\n                        AccountName,\n                        AccountKey,\n                        ContainerName,\n                        item.ItemSpec,\n                        relativeBlobPath);\n            }\n        }\n    }\n}","lang_cluster":"C#","length":173,"code_uid":"9708b9f5dda54b9986e97b65744717a0"}
{"diff_hunk":"@@ -78,8 +78,9 @@ namespace NLog.Internal\n         \/\/\/ <\/summary>\n         \/\/\/ <param name=\"exception\">The exception to check.<\/param>\n         \/\/\/ <param name=\"loggerContext\">Target context of the exception.<\/param>\n+        \/\/\/ <param name=\"logFactory\">For checking <see cref=\"LogFactory.ThrowExceptions\"\/> and <see cref=\"LogFactory.ThrowConfigExceptions\"\/>. Falls back to <see cref=\"LogManager\"\/> <\/param>\n         \/\/\/ <returns><c>true<\/c>if the <paramref name=\"exception\"\/> must be rethrown, <c>false<\/c> otherwise.<\/returns>\n-        public static bool MustBeRethrown(this Exception exception, IInternalLoggerContext loggerContext = null)\n+        public static bool MustBeRethrown(this Exception exception, IInternalLoggerContext loggerContext = null, LogFactory logFactory = null)\n         {\n             if (exception.MustBeRethrownImmediately())\n             {","old_code":"\/\/ \n\/\/ Copyright (c) 2004-2020 Jaroslaw Kowalski <jaak@jkowalski.net>, Kim Christensen, Julian Verdurmen\n\/\/ \n\/\/ All rights reserved.\n\/\/ \n\/\/ Redistribution and use in source and binary forms, with or without \n\/\/ modification, are permitted provided that the following conditions \n\/\/ are met:\n\/\/ \n\/\/ * Redistributions of source code must retain the above copyright notice, \n\/\/   this list of conditions and the following disclaimer. \n\/\/ \n\/\/ * Redistributions in binary form must reproduce the above copyright notice,\n\/\/   this list of conditions and the following disclaimer in the documentation\n\/\/   and\/or other materials provided with the distribution. \n\/\/ \n\/\/ * Neither the name of Jaroslaw Kowalski nor the names of its \n\/\/   contributors may be used to endorse or promote products derived from this\n\/\/   software without specific prior written permission. \n\/\/ \n\/\/ THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n\/\/ AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n\/\/ IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n\/\/ ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n\/\/ LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n\/\/ CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n\/\/ SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n\/\/ INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n\/\/ CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n\/\/ ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF \n\/\/ THE POSSIBILITY OF SUCH DAMAGE.\n\/\/ \n\nnamespace NLog.Internal\n{\n    using System;\n    using System.Threading;\n    using NLog.Common;\n\n    \/\/\/ <summary>\n    \/\/\/ Helper class for dealing with exceptions.\n    \/\/\/ <\/summary>\n    internal static class ExceptionHelper\n    {\n        private const string LoggedKey = \"NLog.ExceptionLoggedToInternalLogger\";\n\n        \/\/\/ <summary>\n        \/\/\/ Mark this exception as logged to the <see cref=\"InternalLogger\"\/>.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"exception\"><\/param>\n        \/\/\/ <returns><\/returns>\n        public static void MarkAsLoggedToInternalLogger(this Exception exception)\n        {\n            if (exception != null)\n            {\n                exception.Data[LoggedKey] = true;\n            }\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Is this exception logged to the <see cref=\"InternalLogger\"\/>? \n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"exception\"><\/param>\n        \/\/\/ <returns><c>true<\/c>if the <paramref name=\"exception\"\/> has been logged to the <see cref=\"InternalLogger\"\/>.<\/returns>\n        public static bool IsLoggedToInternalLogger(this Exception exception)\n        {\n            if (exception?.Data?.Count > 0)\n            {\n                return exception.Data[LoggedKey] as bool? ?? false;\n            }\n            return false;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Determines whether the exception must be rethrown and logs the error to the <see cref=\"InternalLogger\"\/> if <see cref=\"IsLoggedToInternalLogger\"\/> is <c>false<\/c>.\n        \/\/\/ \n        \/\/\/ Advised to log first the error to the <see cref=\"InternalLogger\"\/> before calling this method.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"exception\">The exception to check.<\/param>\n        \/\/\/ <param name=\"loggerContext\">Target context of the exception.<\/param>\n        \/\/\/ <returns><c>true<\/c>if the <paramref name=\"exception\"\/> must be rethrown, <c>false<\/c> otherwise.<\/returns>\n        public static bool MustBeRethrown(this Exception exception, IInternalLoggerContext loggerContext = null)\n        {\n            if (exception.MustBeRethrownImmediately())\n            {\n                \/\/no further logging, because it can make severe exceptions only worse.\n                return true;\n            }\n\n            var isConfigError = exception is NLogConfigurationException;\n\n            \/\/we throw always configuration exceptions (historical)\n            if (!exception.IsLoggedToInternalLogger())\n            {\n                var level = isConfigError ? LogLevel.Warn : LogLevel.Error;\n                if (loggerContext != null)\n                    InternalLogger.Log(exception, level, \"{0}: Error has been raised.\", loggerContext);\n                else\n                    InternalLogger.Log(exception, level, \"Error has been raised.\");\n            }\n\n            \/\/if ThrowConfigExceptions == null, use  ThrowExceptions\n            var shallRethrow = isConfigError ? (LogManager.ThrowConfigExceptions ?? LogManager.ThrowExceptions) : LogManager.ThrowExceptions;\n            return shallRethrow;\n        }\n\n        \/\/\/ <summary>\n        \/\/\/ Determines whether the exception must be rethrown immediately, without logging the error to the <see cref=\"InternalLogger\"\/>.\n        \/\/\/ \n        \/\/\/ Only used this method in special cases.\n        \/\/\/ <\/summary>\n        \/\/\/ <param name=\"exception\">The exception to check.<\/param>\n        \/\/\/ <returns><c>true<\/c>if the <paramref name=\"exception\"\/> must be rethrown, <c>false<\/c> otherwise.<\/returns>\n        public static bool MustBeRethrownImmediately(this Exception exception)\n        {\n#if !NETSTANDARD1_0\n            if (exception is StackOverflowException)\n            {\n                return true;\n            }\n\n            if (exception is ThreadAbortException)\n            {\n                return true;\n            }\n#endif\n\n            if (exception is OutOfMemoryException)\n            {\n                return true;\n            }\n\n#if DEBUG\n            if (exception is InvalidCastException)\n            {\n                return true;\n            }\n            if (exception is NullReferenceException)\n            {\n                return true;\n            }\n            if (exception is DivideByZeroException)\n            {\n                return true;\n            }\n            if (exception is OverflowException)\n            {\n                return true;\n            }\n            if (exception is System.Net.WebException)\n            {\n                return false;   \/\/ Not a real InvalidOperationException\n            }\n            if (exception is InvalidOperationException)\n            {\n                return true;    \/\/ Ex. Collection was modified\n            }\n            if (exception is IndexOutOfRangeException)\n            {\n                return true;\n            }\n            if (exception is System.Reflection.TargetInvocationException)\n            {\n                return true;    \/\/ Compiler\/reflection exception\n            }\n#endif\n            return false;\n        }\n    }\n}\n","lang_cluster":"C#","length":170,"code_uid":"6a193665fe334bd6961365f7897742e7"}
{"diff_hunk":"@@ -9,7 +9,10 @@ namespace Datadog.Trace.Util\n     internal static class DomainMetadata\n     {\n         private const string UnknownName = \"unknown\";\n-        private static Process _currentProcess;\n+        private static bool _initialized;\n+        private static string _currentProcessName;\n+        private static string _currentProcessMachineName;\n+        private static int _currentProcessId;\n         private static bool _processDataPoisoned;\n         private static bool _domainDataPoisoned;\n         private static bool? _isAppInsightsAppDomain;","old_code":"using System;\nusing System.Diagnostics;\n\nnamespace Datadog.Trace.Util\n{\n    \/\/\/ <summary>\n    \/\/\/ Dedicated helper class for consistently referencing Process and AppDomain information.\n    \/\/\/ <\/summary>\n    internal static class DomainMetadata\n    {\n        private const string UnknownName = \"unknown\";\n        private static Process _currentProcess;\n        private static bool _processDataPoisoned;\n        private static bool _domainDataPoisoned;\n        private static bool? _isAppInsightsAppDomain;\n\n        static DomainMetadata()\n        {\n            TrySetProcess();\n        }\n\n        public static string ProcessName\n        {\n            get\n            {\n                try\n                {\n                    return !_processDataPoisoned ? _currentProcess.ProcessName : UnknownName;\n                }\n                catch\n                {\n                    _processDataPoisoned = true;\n                    return UnknownName;\n                }\n            }\n        }\n\n        public static string MachineName\n        {\n            get\n            {\n                try\n                {\n                    return !_processDataPoisoned ? _currentProcess.MachineName : UnknownName;\n                }\n                catch\n                {\n                    _processDataPoisoned = true;\n                    return UnknownName;\n                }\n            }\n        }\n\n        public static int ProcessId\n        {\n            get\n            {\n                try\n                {\n                    return !_processDataPoisoned ? _currentProcess.Id : -1;\n                }\n                catch\n                {\n                    _processDataPoisoned = true;\n                    return -1;\n                }\n            }\n        }\n\n        public static string AppDomainName\n        {\n            get\n            {\n                try\n                {\n                    return !_domainDataPoisoned ? AppDomain.CurrentDomain.FriendlyName : UnknownName;\n                }\n                catch\n                {\n                    _domainDataPoisoned = true;\n                    return UnknownName;\n                }\n            }\n        }\n\n        public static int AppDomainId\n        {\n            get\n            {\n                try\n                {\n                    return !_domainDataPoisoned ? AppDomain.CurrentDomain.Id : -1;\n                }\n                catch\n                {\n                    _domainDataPoisoned = true;\n                    return -1;\n                }\n            }\n        }\n\n        public static bool ShouldAvoidAppDomain()\n        {\n            if (_isAppInsightsAppDomain == null)\n            {\n                _isAppInsightsAppDomain = AppDomainName.IndexOf(\"ApplicationInsights\", StringComparison.OrdinalIgnoreCase) >= 0;\n            }\n\n            return _isAppInsightsAppDomain.Value;\n        }\n\n        private static void TrySetProcess()\n        {\n            try\n            {\n                if (!_processDataPoisoned && _currentProcess == null)\n                {\n                    _currentProcess = Process.GetCurrentProcess();\n                }\n            }\n            catch\n            {\n                _processDataPoisoned = true;\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":127,"code_uid":"0512004db4b54dd1b994e0da66fd3bdb"}
{"diff_hunk":"@@ -48,5 +48,20 @@ namespace OpenTelemetry.Exporter.Jaeger\n         \/\/\/ Gets or sets the tags that should be sent with telemetry.\n         \/\/\/ <\/summary>\n         public IEnumerable<KeyValuePair<string, object>> ProcessTags { get; set; }\n+\n+        \/\/\/ <summary>\n+        \/\/\/ Gets or sets a value indicating whether or not a batch should be sent to the Jaeger agent for each service. Default value: true.\n+        \/\/\/ <\/summary>\n+        \/\/\/ <remarks>\n+        \/\/\/ Jaeger UI will only detect &amp; color dependency spans when both\n+        \/\/\/ the client &amp; server processes report data to the same Jaeger\n+        \/\/\/ instance. For processes that make calls to non-instrumented services\n+        \/\/\/ (third parties, SQL, legacy systems, etc.) the\n+        \/\/\/ GenerateServiceSpecificBatches flag is provided to trick Jaeger into\n+        \/\/\/ correctly detecting and displaying all dependent spans as if both\n+        \/\/\/ sides reported data. For more details, see <a\n+        \/\/\/ href=\"https:\/\/github.com\/jaegertracing\/jaeger-ui\/issues\/594\">jaeger-ui\/issues\/594<\/a>.\n+        \/\/\/ <\/remarks>\n+        public bool GenerateServiceSpecificBatches { get; set; } = true;\n     }\n }","old_code":"\/\/ <copyright file=\"JaegerExporterOptions.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System.Collections.Generic;\n\nnamespace OpenTelemetry.Exporter.Jaeger\n{\n    public class JaegerExporterOptions\n    {\n        internal const string DefaultServiceName = \"OpenTelemetry Exporter\";\n\n        internal const int DefaultMaxPayloadSizeInBytes = 4096;\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets the name of the service reporting telemetry. Default value: OpenTelemetry Exporter.\n        \/\/\/ <\/summary>\n        public string ServiceName { get; set; } = DefaultServiceName;\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets the Jaeger agent host. Default value: localhost.\n        \/\/\/ <\/summary>\n        public string AgentHost { get; set; } = \"localhost\";\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets the Jaeger agent \"compact thrift protocol\" port. Default value: 6831.\n        \/\/\/ <\/summary>\n        public int AgentPort { get; set; } = 6831;\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets the maximum payload size in bytes. Default value: 4096.\n        \/\/\/ <\/summary>\n        public int? MaxPayloadSizeInBytes { get; set; } = DefaultMaxPayloadSizeInBytes;\n\n        \/\/\/ <summary>\n        \/\/\/ Gets or sets the tags that should be sent with telemetry.\n        \/\/\/ <\/summary>\n        public IEnumerable<KeyValuePair<string, object>> ProcessTags { get; set; }\n    }\n}\n","lang_cluster":"C#","length":52,"code_uid":"0d21539fe2674bec850bca98c2d3fd48"}
{"diff_hunk":"@@ -55,7 +55,7 @@ namespace Microsoft.AspNetCore.Server.Kestrel.FunctionalTests.Http2\n             get\n             {\n                 var dataset = new TheoryData<H2SpecTestCase>();\n-                var toSkip = new[] { \"http2\/5.1\/8\" };\n+                var toSkip = new string[] { \/*\"http2\/5.1\/8\"*\/ };\n \n                 foreach (var testcase in H2SpecCommands.EnumerateTestCases())\n                 {","old_code":"\/\/ Copyright (c) .NET Foundation. All rights reserved.\n\/\/ Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\n#if NETCOREAPP2_2\n\nusing System.IO;\nusing System.Linq;\nusing System.Net;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Builder;\nusing Microsoft.AspNetCore.Hosting;\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.AspNetCore.Server.Kestrel.Core;\nusing Microsoft.AspNetCore.Testing;\nusing Microsoft.AspNetCore.Testing.xunit;\nusing Xunit;\nusing Xunit.Abstractions;\n\nnamespace Microsoft.AspNetCore.Server.Kestrel.FunctionalTests.Http2\n{\n    [OSSkipCondition(OperatingSystems.MacOSX, SkipReason = \"Missing SslStream ALPN support: https:\/\/github.com\/dotnet\/corefx\/issues\/30492\")]\n    [MinimumOSVersion(OperatingSystems.Windows, WindowsVersions.Win81,\n        SkipReason = \"Missing Windows ALPN support: https:\/\/en.wikipedia.org\/wiki\/Application-Layer_Protocol_Negotiation#Support\")]\n    public class H2SpecTests : TestApplicationErrorLoggerLoggedTest\n    {\n        [ConditionalTheory]\n        [MemberData(nameof(H2SpecTestCases))]\n        public async Task RunIndividualTestCase(H2SpecTestCase testCase)\n        {\n            var hostBuilder = TransportSelector.GetWebHostBuilder()\n                .UseKestrel(options =>\n                {\n                    options.Listen(IPAddress.Loopback, 0, listenOptions =>\n                    {\n                        listenOptions.Protocols = HttpProtocols.Http2;\n                        if (testCase.Https)\n                        {\n                            listenOptions.UseHttps(TestResources.TestCertificatePath, \"testPassword\");\n                        }\n                    });\n                })\n                .ConfigureServices(AddTestLogging)\n                .Configure(ConfigureHelloWorld);\n\n            using (var host = hostBuilder.Build())\n            {\n                await host.StartAsync();\n\n                H2SpecCommands.RunTest(testCase.Id, host.GetPort(), testCase.Https, Logger);\n            }\n        }\n\n        public static TheoryData<H2SpecTestCase> H2SpecTestCases\n        {\n            get\n            {\n                var dataset = new TheoryData<H2SpecTestCase>();\n                var toSkip = new[] { \"http2\/5.1\/8\" };\n\n                foreach (var testcase in H2SpecCommands.EnumerateTestCases())\n                {\n                    string skip = null;\n                    if (toSkip.Contains(testcase.Item1))\n                    {\n                        skip = \"https:\/\/github.com\/aspnet\/KestrelHttpServer\/issues\/2154\";\n                    }\n\n                    dataset.Add(new H2SpecTestCase\n                    {\n                        Id = testcase.Item1,\n                        Description = testcase.Item2,\n                        Https = false,\n                        Skip = skip,\n                    });\n\n                    dataset.Add(new H2SpecTestCase\n                    {\n                        Id = testcase.Item1,\n                        Description = testcase.Item2,\n                        Https = true,\n                        Skip = skip,\n                    });\n                }\n\n                return dataset;\n            }\n        }\n\n        public class H2SpecTestCase : IXunitSerializable\n        {\n            \/\/ For the serializer\n            public H2SpecTestCase()\n            {\n            }\n\n            public string Id { get; set; }\n            public string Description { get; set; }\n            public bool Https { get; set; }\n            public string Skip { get; set; }\n\n            public void Deserialize(IXunitSerializationInfo info)\n            {\n                Id = info.GetValue<string>(nameof(Id));\n                Description = info.GetValue<string>(nameof(Description));\n                Https = info.GetValue<bool>(nameof(Https));\n                Skip = info.GetValue<string>(nameof(Skip));\n            }\n\n            public void Serialize(IXunitSerializationInfo info)\n            {\n                info.AddValue(nameof(Id), Id, typeof(string));\n                info.AddValue(nameof(Description), Description, typeof(string));\n                info.AddValue(nameof(Https), Https, typeof(bool));\n                info.AddValue(nameof(Skip), Skip, typeof(string));\n            }\n\n            public override string ToString()\n            {\n                return $\"{Id}, HTTPS:{Https}, {Description}\";\n            }\n        }\n\n        private void ConfigureHelloWorld(IApplicationBuilder app)\n        {\n            app.Run(async context =>\n            {\n                \/\/ Read the whole request body to check for errors.\n                await context.Request.Body.CopyToAsync(Stream.Null);\n                await context.Response.WriteAsync(\"Hello World\");\n            });\n        }\n    }\n}\n#elif NET461 \/\/ HTTP\/2 is not supported\n#else\n#error TFMs need updating\n#endif\n","lang_cluster":"C#","length":137,"code_uid":"fd1eacce063b44fcb99d4618e845a6b5"}
{"diff_hunk":"@@ -16,12 +16,13 @@\n \/\/ \n \n using Nethermind.Core;\n+using Nethermind.TxPool;\n \n namespace Nethermind.Consensus.Transactions\n {\n     public class NullTxFilter : ITxFilter\n     {\n-        public (bool Allowed, string Reason) IsAllowed(Transaction tx, BlockHeader parentHeader) => (true, string.Empty);\n+        public (bool Allowed, AddTxResult? Reason) IsAllowed(Transaction tx, BlockHeader parentHeader) => (true, null);\n         \n         public static readonly NullTxFilter Instance = new(); \n     }","old_code":"\ufeff\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\/\/ \n\nusing Nethermind.Core;\n\nnamespace Nethermind.Consensus.Transactions\n{\n    public class NullTxFilter : ITxFilter\n    {\n        public (bool Allowed, string Reason) IsAllowed(Transaction tx, BlockHeader parentHeader) => (true, string.Empty);\n        \n        public static readonly NullTxFilter Instance = new(); \n    }\n}\n","lang_cluster":"C#","length":28,"code_uid":"df8ebe35647b426ebe8df0bb6928acfe"}
{"diff_hunk":"@@ -3,14 +3,47 @@\n \/\/ See the LICENSE file in the project root for more information.\n \n using System;\n+using Foundation;\n using MvvmCross.Core;\n+using MvvmCross.ViewModels;\n using UIKit;\n \n namespace MvvmCross.Platform.Tvos.Core\n {\n-    public class MvxApplicationDelegate : UIApplicationDelegate\n-                                          , IMvxApplicationDelegate\n+    public abstract class MvxApplicationDelegate : UIApplicationDelegate, IMvxApplicationDelegate\n     {\n+        private MvxTvosSetup _setup;\n+        protected MvxTvosSetup Setup\n+        {\n+            get\n+            {\n+                if (_setup == null)\n+                    _setup = CreateSetup(this, Window);\n+                return _setup;\n+            }\n+        }\n+\n+        public override bool FinishedLaunching(UIApplication application, NSDictionary launchOptions)\n+        {\n+            Setup.Initialize();\n+            RunAppStart(launchOptions);\n+\n+            FireLifetimeChanged(MvxLifetimeEvent.Launching);\n+            return base.FinishedLaunching(application, launchOptions);\n+        }\n+\n+        protected virtual void RunAppStart(object hint = null)\n+        {\n+            var startup = Mvx.Resolve<IMvxAppStart>();\n+            if (!startup.IsStarted)\n+                startup.Start(GetAppStartHint(hint));\n+        }\n+\n+        protected virtual object GetAppStartHint(object hint = null)\n+        {\n+            return null;\n+        }\n+\n         public override void WillEnterForeground(UIApplication application)\n         {\n             FireLifetimeChanged(MvxLifetimeEvent.ActivatedFromMemory);","old_code":"\ufeff\/\/ Licensed to the .NET Foundation under one or more agreements.\n\/\/ The .NET Foundation licenses this file to you under the MS-PL license.\n\/\/ See the LICENSE file in the project root for more information.\n\nusing System;\nusing MvvmCross.Core;\nusing UIKit;\n\nnamespace MvvmCross.Platform.Tvos.Core\n{\n    public class MvxApplicationDelegate : UIApplicationDelegate\n                                          , IMvxApplicationDelegate\n    {\n        public override void WillEnterForeground(UIApplication application)\n        {\n            FireLifetimeChanged(MvxLifetimeEvent.ActivatedFromMemory);\n        }\n\n        public override void DidEnterBackground(UIApplication application)\n        {\n            FireLifetimeChanged(MvxLifetimeEvent.Deactivated);\n        }\n\n        public override void WillTerminate(UIApplication application)\n        {\n            FireLifetimeChanged(MvxLifetimeEvent.Closing);\n        }\n\n        public override void FinishedLaunching(UIApplication application)\n        {\n            FireLifetimeChanged(MvxLifetimeEvent.Launching);\n        }\n\n        private void FireLifetimeChanged(MvxLifetimeEvent which)\n        {\n            var handler = LifetimeChanged;\n            handler?.Invoke(this, new MvxLifetimeEventArgs(which));\n        }\n\n        public event EventHandler<MvxLifetimeEventArgs> LifetimeChanged;\n    }\n}\n","lang_cluster":"C#","length":42,"code_uid":"34d585116431409b9e98d6f3fcbb32df"}
{"diff_hunk":"@@ -174,6 +174,7 @@ namespace pwiz.Skyline.Model.Results\n                     _completed(result, x == null ? _status : _status.ChangeErrorException(x));\n                 }\n                 catch (Exception x2)\n+\n                 {\n                     _completed(null, _status.ChangeErrorException(x2));\n                 }","old_code":"\/*\r\n * Original author: Brendan MacLean <brendanx .at. u.washington.edu>,\r\n *                  MacCoss Lab, Department of Genome Sciences, UW\r\n *\r\n * Copyright 2009 University of Washington - Seattle, WA\r\n * \r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n *\/\r\nusing System;\r\nusing System.Collections.Generic;\r\nusing System.IO;\r\nusing pwiz.Common.Collections;\r\nusing pwiz.Common.SystemUtil;\r\nusing pwiz.Skyline.Properties;\r\nusing pwiz.Skyline.Util;\r\nusing pwiz.Skyline.Util.Extensions;\r\n\r\nnamespace pwiz.Skyline.Model.Results\r\n{\r\n    internal class ChromCacheWriter : IDisposable\r\n    {\r\n        private readonly Action<ChromatogramCache, IProgressStatus> _completed;\r\n\r\n        protected List<ChromCachedFile> _listCachedFiles = new List<ChromCachedFile>();\r\n        protected BlockedArrayList<ChromTransition> _listTransitions =\r\n            new BlockedArrayList<ChromTransition>(ChromTransition.SizeOf, ChromTransition.DEFAULT_BLOCK_SIZE);\r\n        protected BlockedArrayList<ChromGroupHeaderEntry> _listGroups =\r\n            new BlockedArrayList<ChromGroupHeaderEntry>(ChromGroupHeaderInfo.SizeOf, ChromGroupHeaderInfo.DEFAULT_BLOCK_SIZE);\r\n        protected List<byte> _listTextIdBytes = new List<byte>();\r\n        protected readonly List<Type> _listScoreTypes = new List<Type>();\r\n        protected readonly FileSaver _fs;\r\n        protected readonly FileSaver _fsScans;\r\n        protected readonly FileSaver _fsPeaks;\r\n        protected readonly FileSaver _fsScores;\r\n        protected readonly ILoadMonitor _loader;\r\n        protected IProgressStatus _status;\r\n        protected int _peakCount;\r\n        protected int _scoreCount;\r\n        protected IPooledStream _destinationStream;\r\n\r\n        protected ChromCacheWriter(string cachePath, ILoadMonitor loader, IProgressStatus status,\r\n                                   Action<ChromatogramCache, IProgressStatus> completed)\r\n        {\r\n            CachePath = cachePath;\r\n            CacheFormat = CacheFormat.CURRENT;\r\n            _fs = new FileSaver(CachePath);\r\n            _fsScans = new FileSaver(CachePath + ChromatogramCache.SCANS_EXT, true);\r\n            _fsPeaks = new FileSaver(CachePath + ChromatogramCache.PEAKS_EXT, true);\r\n            _fsScores = new FileSaver(CachePath + ChromatogramCache.SCORES_EXT, true);\r\n            _loader = loader;\r\n            _status = status;\r\n            _completed = completed;\r\n        }\r\n\r\n        protected string CachePath { get; private set; }\r\n\r\n        public CacheFormat CacheFormat { get; protected set; }\r\n\r\n        protected void Complete(Exception x)\r\n        {\r\n            lock (this)\r\n            {\r\n                ChromatogramCache result = null;\r\n                try\r\n                {\r\n                    if (x == null && !_status.IsFinal)\r\n                    {\r\n                        long locationScanIds = 0, countBytesScanIds = 0;\r\n                        if (_fs.Stream != null)\r\n                        {\r\n                            try\r\n                            {\r\n                                locationScanIds = _fs.Stream.Position;\r\n                                countBytesScanIds = _fsScans.Stream.Position;\r\n\r\n                                _listGroups.Sort();\r\n                                var listChromGroupHeaderInfos = ReadOnlyList.Create(_listGroups.Count,\r\n                                    i => _listGroups[i].ChromGroupHeaderInfo);\r\n                                ChromatogramCache.WriteStructs(CacheFormat,\r\n                                                               _fs.Stream,\r\n                                                               _fsScans.Stream,\r\n                                                               _fsPeaks.Stream,\r\n                                                               _fsScores.Stream,\r\n                                                               _listCachedFiles,\r\n                                                               listChromGroupHeaderInfos,\r\n                                                               _listTransitions,\r\n                                                               _listTextIdBytes,\r\n                                                               _listScoreTypes,\r\n                                                               _scoreCount,\r\n                                                               _peakCount);\r\n\r\n                                _loader.StreamManager.Finish(_fs.Stream);\r\n                                _fs.Stream = null;\r\n                                \/\/ Allow cancellation right up to the final commit\r\n                                if (!_loader.IsCanceled)\r\n                                    _fs.Commit(_destinationStream);\r\n                                else\r\n                                {\r\n                                    _loader.UpdateProgress(_status = _status.Cancel());\r\n                                }\r\n                            }\r\n                            catch (Exception xWrite)\r\n                            {\r\n                                throw new IOException(TextUtil.LineSeparate(string.Format(Resources.ChromCacheWriter_Complete_Failure_attempting_to_write_the_file__0_, _fs.RealName), xWrite.Message));\r\n                            }\r\n                        }\r\n\r\n                        if (!_status.IsCanceled)\r\n                        {\r\n                            \/\/ Create stream identifier, but do not open.  The stream will be opened\r\n                            \/\/ the first time the document uses it.\r\n                            var readStream = _loader.StreamManager.CreatePooledStream(CachePath, false);\r\n\r\n                            \/\/ DebugLog.Info(\"{0}. {1} - created\", readStream.GlobalIndex, CachePath);\r\n\r\n                            _fsPeaks.Stream.Seek(0, SeekOrigin.Begin);\r\n                            _fsScores.Stream.Seek(0, SeekOrigin.Begin);\r\n                            var arrayCachFiles = _listCachedFiles.ToArray();\r\n                            _listCachedFiles = null;\r\n                            var arrayChromEntries = BlockedArray<ChromGroupHeaderInfo>.Convert(_listGroups, \r\n                                entry => entry.ChromGroupHeaderInfo);\r\n                            _listGroups = null;\r\n                            var arrayTransitions = _listTransitions.ToBlockedArray();\r\n                            _listTransitions = null;\r\n                            var peakSerializer = CacheFormat.ChromPeakSerializer();\r\n                            var chromPeaks = new BlockedArray<ChromPeak>(\r\n                                count => peakSerializer.ReadArray(_fsPeaks.FileStream, count), _peakCount,\r\n                                ChromPeak.SizeOf, ChromPeak.DEFAULT_BLOCK_SIZE);\r\n                            var scores = new BlockedArray<float>(\r\n                                count => PrimitiveArrays.Read<float>(_fsScores.FileStream, count), _scoreCount,\r\n                                sizeof(float), ChromatogramCache.DEFAULT_SCORES_BLOCK_SIZE);\r\n                            var textIdBytes = _listTextIdBytes.ToArray();\r\n                            _listTextIdBytes = null;\r\n\r\n                            var rawData = new ChromatogramCache.RawData(CacheFormat)\r\n                            {\r\n                                ChromCacheFiles = arrayCachFiles,\r\n                                ChromatogramEntries = arrayChromEntries,\r\n                                ChromTransitions = arrayTransitions,\r\n                                ChromatogramPeaks = chromPeaks,\r\n                                ScoreTypes = _listScoreTypes.ToArray(),\r\n                                Scores = scores,\r\n                                TextIdBytes = textIdBytes,\r\n                                CountBytesScanIds = countBytesScanIds,\r\n                                LocationScanIds = locationScanIds\r\n                            };\r\n                            result = new ChromatogramCache(CachePath, rawData, readStream);\r\n                            _status = _status.Complete();\r\n                            _loader.UpdateProgress(_status);\r\n                        }\r\n                    }\r\n                }\r\n                catch (Exception x2)\r\n                {\r\n                    x = x2;\r\n                }\r\n                finally\r\n                {\r\n                    Dispose();\r\n                }\r\n\r\n                try\r\n                {\r\n                    _completed(result, x == null ? _status : _status.ChangeErrorException(x));\r\n                }\r\n                catch (Exception x2)\r\n                {\r\n                    _completed(null, _status.ChangeErrorException(x2));\r\n                }\r\n            }\r\n        }\r\n\r\n        public virtual void Dispose()\r\n        {\r\n            if (_fs.Stream != null)\r\n            {\r\n                try { _loader.StreamManager.Finish(_fs.Stream); }\r\n                catch (IOException) { }\r\n                _fs.Stream = null;\r\n            }\r\n            _fs.Dispose();\r\n            _fsPeaks.Dispose();\r\n            _fsScans.Dispose();\r\n            _fsScores.Dispose();\r\n        }\r\n    }\r\n}\r\n","lang_cluster":"C#","length":197,"code_uid":"1b4799bf782b430abaedd9a644c846a6"}
{"diff_hunk":"@@ -16,10 +16,16 @@\n \/\/\n \n using System;\n+using System.Collections.Generic;\n using System.IO;\n using System.Linq;\n using System.Reflection;\n using FluentAssertions;\n+using Nethermind.Core;\n+using Nethermind.Logging;\n+using Nethermind.Monitoring.Config;\n+using Nethermind.Monitoring.Metrics;\n+using Nethermind.Runner;\n using NUnit.Framework;\n \n namespace Nethermind.Monitoring.Test","old_code":"\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\/\/\n\nusing System;\nusing System.IO;\nusing System.Linq;\nusing System.Reflection;\nusing FluentAssertions;\nusing NUnit.Framework;\n\nnamespace Nethermind.Monitoring.Test\n{\n    [TestFixture]\n    public class MetricsTests\n    {\n        [Test]\n        public void All_config_items_have_descriptions()\n        {\n            ValidateMetricsDescriptions();\n        }\n\n        public static void ValidateMetricsDescriptions()\n        {\n            ForEachProperty(CheckDescribedOrHidden);\n        }\n\n        private static void CheckDescribedOrHidden(PropertyInfo property)\n        {\n            System.ComponentModel.DescriptionAttribute attribute = property.GetCustomAttribute<System.ComponentModel.DescriptionAttribute>();\n            attribute.Should().NotBeNull();\n        }\n\n        private static void ForEachProperty(Action<PropertyInfo> verifier)\n        {\n            string[] dlls = Directory.GetFiles(AppDomain.CurrentDomain.BaseDirectory, \"Nethermind.*.dll\");\n            foreach (string dll in dlls)\n            {\n                TestContext.WriteLine($\"Verify {nameof(MetricsTests)} on {Path.GetFileName(dll)}\");\n                Assembly assembly = Assembly.LoadFile(dll);\n                Type[] configs = assembly.GetExportedTypes().Where(t => t.Name == \"Metrics\").ToArray();\n\n                foreach (Type metricsType in configs)\n                {\n                    TestContext.WriteLine($\"  Verifying type {metricsType.FullName}\");\n                    PropertyInfo[] properties = metricsType.GetProperties(BindingFlags.Static | BindingFlags.Public);\n                    foreach (PropertyInfo property in properties)\n                    {\n                        try\n                        {\n                            TestContext.WriteLine($\"    Verifying property {property.Name}\");\n                            verifier(property);\n                        }\n                        catch (Exception e)\n                        {\n                            throw new Exception(property.Name, e);\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":76,"code_uid":"1d466660d1fd47928b71aa67613fab1e"}
{"diff_hunk":"@@ -46,13 +46,13 @@ namespace OpenTelemetry.Extensions.Hosting.Implementation\n             }\n         }\n \n-        [Event(1, Message = \"Failed to initialize: '{0}'. OpenTelemetry will not work.\", Level = EventLevel.Error)]\n+        [Event(1, Message = \"An exception occured while adding OpenTelemetry Tracing to ServiceCollection. OpenTelemetry tracing will not work. Exception: '{0}'.\", Level = EventLevel.Error)]\n         public void FailedInitialize(string exception)\n         {\n             this.WriteEvent(1, exception);\n         }\n \n-        [Event(2, Message = \"Failed to get OpenTelemetrySDK: '{0}'. OpenTelemetry will not work.\", Level = EventLevel.Error)]\n+        [Event(2, Message = \"An exception occured while retrieving OpenTelemetry Tracer from Service Provider. OpenTelemetry tracing will not work. Exception: '{0}'.\", Level = EventLevel.Error)]\n         public void FailedOpenTelemetrySDK(string exception)\n         {\n             this.WriteEvent(2, exception);","old_code":"\ufeff\/\/ <copyright file=\"HostingExtensionsEventSource.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Diagnostics.Tracing;\nusing OpenTelemetry.Internal;\n\nnamespace OpenTelemetry.Extensions.Hosting.Implementation\n{\n    \/\/\/ <summary>\n    \/\/\/ EventSource events emitted from the project.\n    \/\/\/ <\/summary>\n    [EventSource(Name = \"OpenTelemetry-Extensions-Hosting\")]\n    internal class HostingExtensionsEventSource : EventSource\n    {\n        public static HostingExtensionsEventSource Log = new HostingExtensionsEventSource();\n\n        [NonEvent]\n        public void FailedInitialize(Exception ex)\n        {\n            if (this.IsEnabled(EventLevel.Error, (EventKeywords)(-1)))\n            {\n                this.FailedInitialize(ex.ToInvariantString());\n            }\n        }\n\n        [NonEvent]\n        public void FailedOpenTelemetrySDK(Exception ex)\n        {\n            if (this.IsEnabled(EventLevel.Error, (EventKeywords)(-1)))\n            {\n                this.FailedOpenTelemetrySDK(ex.ToInvariantString());\n            }\n        }\n\n        [Event(1, Message = \"Failed to initialize: '{0}'. OpenTelemetry will not work.\", Level = EventLevel.Error)]\n        public void FailedInitialize(string exception)\n        {\n            this.WriteEvent(1, exception);\n        }\n\n        [Event(2, Message = \"Failed to get OpenTelemetrySDK: '{0}'. OpenTelemetry will not work.\", Level = EventLevel.Error)]\n        public void FailedOpenTelemetrySDK(string exception)\n        {\n            this.WriteEvent(2, exception);\n        }\n    }\n}\n","lang_cluster":"C#","length":61,"code_uid":"8db71b44bb674c7b8f243e9d02fe1117"}
{"diff_hunk":"@@ -126,6 +126,8 @@ namespace Nethermind.Synchronization.ParallelSync\n         }\n \n         public long FindBestHeader() => _blockTree.BestSuggestedHeader?.Number ?? 0;\n+        \n+        public Keccak FindBestHeaderHash() => _blockTree.BestSuggestedHeader?.Hash ?? Keccak.Zero;\n \n         public long FindBestFullBlock() => Math.Min(FindBestHeader(), _blockTree.BestSuggestedBody?.Number ?? 0); \/\/ avoiding any potential concurrency issue\n ","old_code":"\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing Nethermind.Blockchain;\nusing Nethermind.Blockchain.Receipts;\nusing Nethermind.Blockchain.Synchronization;\nusing Nethermind.Core;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Db;\nusing Nethermind.Dirichlet.Numerics;\nusing Nethermind.Logging;\n\nnamespace Nethermind.Synchronization.ParallelSync\n{\n    public class SyncProgressResolver : ISyncProgressResolver\n    {\n        \/\/ TODO: we can search 1024 back and confirm 128 deep header and start using it as Max(0, confirmed)\n        \/\/ then we will never have to look 128 back again\n        \/\/ note that we will be doing that every second or so\n        private const int _maxLookupBack = 128;\n\n        private readonly IBlockTree _blockTree;\n        private readonly IReceiptStorage _receiptStorage;\n        private readonly IDb _stateDb;\n        private readonly IDb _beamStateDb;\n        private readonly ISyncConfig _syncConfig;\n        private ILogger _logger;\n\n        public SyncProgressResolver(IBlockTree blockTree, IReceiptStorage receiptStorage, IDb stateDb, IDb beamStateDb, ISyncConfig syncConfig, ILogManager logManager)\n        {\n            _logger = logManager?.GetClassLogger() ?? throw new ArgumentNullException(nameof(logManager));\n            _blockTree = blockTree ?? throw new ArgumentNullException(nameof(blockTree));\n            _receiptStorage = receiptStorage ?? throw new ArgumentNullException(nameof(receiptStorage));\n            _stateDb = stateDb ?? throw new ArgumentNullException(nameof(stateDb));\n            _beamStateDb = beamStateDb ?? throw new ArgumentNullException(nameof(beamStateDb));\n            _syncConfig = syncConfig ?? throw new ArgumentNullException(nameof(syncConfig));\n        }\n\n        private bool IsFullySynced(Keccak stateRoot)\n        {\n            if (stateRoot == Keccak.EmptyTreeHash)\n            {\n                return true;\n            }\n\n            return _stateDb.Innermost.Get(stateRoot) != null;\n        }\n\n        private bool IsBeamSynced(Keccak stateRoot)\n        {\n            if (stateRoot == Keccak.EmptyTreeHash)\n            {\n                return true;\n            }\n\n            return _beamStateDb.Innermost.Get(stateRoot) != null;\n        }\n\n        public long FindBestFullState()\n        {\n            \/\/ so the full state can be in a few places but there are some best guesses\n            \/\/ if we are state syncing then the full state may be one of the recent blocks (maybe one of the last 128 blocks)\n            \/\/ if we full syncing then the state should be at head\n            \/\/ if we are beam syncing then the state should be in a different DB and should not cause much trouble here\n            \/\/ it also may seem tricky if best suggested is part of a reorg while we are already full syncing so\n            \/\/ ideally we would like to check it siblings too (but this may be a bit expensive and less likely\n            \/\/ to be important\n            \/\/ we want to avoid a scenario where state is not found even as it is just near head or best suggested\n\n            Block head = _blockTree.Head;\n            BlockHeader initialBestSuggested = _blockTree.BestSuggestedHeader; \/\/ just storing here for debugging sake\n            BlockHeader bestSuggested = initialBestSuggested;\n\n            long bestFullState = 0;\n            if (head != null)\n            {\n                \/\/ head search should be very inexpensive as we generally expect the state to be there\n                bestFullState = SearchForFullState(head.Header);\n            }\n\n            if (bestSuggested != null)\n            {\n                if (bestFullState < bestSuggested?.Number)\n                {\n                    bestFullState = Math.Max(bestFullState, SearchForFullState(bestSuggested));\n                }\n            }\n\n            return bestFullState;\n        }\n\n        private long SearchForFullState(BlockHeader startHeader)\n        {\n            long bestFullState = 0;\n            for (int i = 0; i < _maxLookupBack; i++)\n            {\n                if (startHeader == null)\n                {\n                    break;\n                }\n\n                if (IsFullySynced(startHeader.StateRoot))\n                {\n                    bestFullState = startHeader.Number;\n                    break;\n                }\n\n                startHeader = _blockTree.FindHeader(startHeader.ParentHash, BlockTreeLookupOptions.TotalDifficultyNotNeeded);\n            }\n\n            return bestFullState;\n        }\n\n        public long FindBestHeader() => _blockTree.BestSuggestedHeader?.Number ?? 0;\n\n        public long FindBestFullBlock() => Math.Min(FindBestHeader(), _blockTree.BestSuggestedBody?.Number ?? 0); \/\/ avoiding any potential concurrency issue\n\n        public bool IsLoadingBlocksFromDb()\n        {\n            return !_blockTree.CanAcceptNewBlocks;\n        }\n\n        public long FindBestProcessedBlock() => _blockTree.Head?.Number ?? -1;\n\n        public UInt256 ChainDifficulty => _blockTree.BestSuggestedBody?.TotalDifficulty ?? UInt256.Zero;\n\n        public bool IsFastBlocksHeadersFinished() => !IsFastBlocks() || (_blockTree.LowestInsertedHeader?.Number ?? long.MaxValue) <= 1;\n        \n        public bool IsFastBlocksBodiesFinished() => !IsFastBlocks() || (!_syncConfig.DownloadBodiesInFastSync || (_blockTree.LowestInsertedBody?.Number ?? long.MaxValue) <= 1);\n\n        public bool IsFastBlocksReceiptsFinished() => !IsFastBlocks() || (!_syncConfig.DownloadReceiptsInFastSync || (_receiptStorage.LowestInsertedReceiptBlock ?? long.MaxValue) <= 1);\n\n        private bool IsFastBlocks()\n        {\n            bool isFastBlocks = _syncConfig.FastBlocks;\n\n            \/\/ if pivot number is 0 then it is equivalent to fast blocks disabled\n            if (!isFastBlocks || _syncConfig.PivotNumberParsed == 0L)\n            {\n                return false;\n            }\n            \n            bool immediateBeamSync = !_syncConfig.DownloadHeadersInFastSync;\n            bool anyHeaderDownloaded = _blockTree.LowestInsertedHeader != null;\n            if (immediateBeamSync && anyHeaderDownloaded)\n            {\n                return false;\n            }\n\n            return true;\n        }\n\n\n    }\n}\n","lang_cluster":"C#","length":169,"code_uid":"41901a43c8a64c04b613dbd38cda6ee3"}
{"diff_hunk":"@@ -47,13 +47,22 @@ namespace OpenTelemetry.Logs\n             {\n                 var options = this.provider.Options;\n \n+                string formattedMessage = null;\n+                if (options.IncludeFormattedMessage)\n+                {\n+                    if (formatter != null)\n+                    {\n+                        formattedMessage = formatter(state, exception);\n+                    }\n+                }\n+\n                 var record = new LogRecord(\n                     options.IncludeScopes ? this.ScopeProvider : null,\n                     DateTime.UtcNow,\n                     this.categoryName,\n                     logLevel,\n                     eventId,\n-                    options.IncludeFormattedMessage ? formatter(state, exception) : null,\n+                    formattedMessage,\n                     options.ParseStateValues ? null : (object)state,\n                     exception,\n                     options.ParseStateValues ? this.ParseState(state) : null);","old_code":"\/\/ <copyright file=\"OpenTelemetryLogger.cs\" company=\"OpenTelemetry Authors\">\n\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/ <\/copyright>\n\nusing System;\nusing System.Collections.Generic;\nusing System.Runtime.CompilerServices;\nusing Microsoft.Extensions.Logging;\n\nnamespace OpenTelemetry.Logs\n{\n    internal class OpenTelemetryLogger : ILogger\n    {\n        private readonly string categoryName;\n        private readonly OpenTelemetryLoggerProvider provider;\n\n        internal OpenTelemetryLogger(string categoryName, OpenTelemetryLoggerProvider provider)\n        {\n            this.categoryName = categoryName ?? throw new ArgumentNullException(nameof(categoryName));\n            this.provider = provider ?? throw new ArgumentNullException(nameof(provider));\n        }\n\n        internal IExternalScopeProvider ScopeProvider { get; set; }\n\n        public void Log<TState>(LogLevel logLevel, EventId eventId, TState state, Exception exception, Func<TState, Exception, string> formatter)\n        {\n            if (!this.IsEnabled(logLevel)\n                || Sdk.SuppressInstrumentation)\n            {\n                return;\n            }\n\n            var processor = this.provider.Processor;\n            if (processor != null)\n            {\n                var options = this.provider.Options;\n\n                var record = new LogRecord(\n                    options.IncludeScopes ? this.ScopeProvider : null,\n                    DateTime.UtcNow,\n                    this.categoryName,\n                    logLevel,\n                    eventId,\n                    options.IncludeFormattedMessage ? formatter(state, exception) : null,\n                    options.ParseStateValues ? null : (object)state,\n                    exception,\n                    options.ParseStateValues ? this.ParseState(state) : null);\n\n                processor.OnEnd(record);\n\n                record.ScopeProvider = null;\n            }\n        }\n\n        [MethodImpl(MethodImplOptions.AggressiveInlining)]\n        public bool IsEnabled(LogLevel logLevel)\n        {\n            return logLevel != LogLevel.None;\n        }\n\n        public IDisposable BeginScope<TState>(TState state) => this.ScopeProvider?.Push(state) ?? null;\n\n        private IReadOnlyList<KeyValuePair<string, object>> ParseState<TState>(TState state)\n        {\n            if (state is IReadOnlyList<KeyValuePair<string, object>> stateList)\n            {\n                return stateList;\n            }\n            else if (state is IEnumerable<KeyValuePair<string, object>> stateValues)\n            {\n                return new List<KeyValuePair<string, object>>(stateValues);\n            }\n            else\n            {\n                return new List<KeyValuePair<string, object>>\n                {\n                    new KeyValuePair<string, object>(string.Empty, state),\n                };\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":94,"code_uid":"dc2a7a5771804a9aa20e44e641e24a92"}
{"diff_hunk":"@@ -53,7 +53,34 @@ namespace pwiz.Common.Chemistry\n             {\"Thr\",'T'},\n             {\"Trp\",'W'},\n             {\"Tyr\",'Y'},\n-            {\"Val\",'V'}\n+            {\"Val\",'V'},\n+            {\"Sec\",'U'},\n+            {\"Pyl\",'O'}\n+        });\n+        public static readonly IDictionary<char, String> FullNames = new ImmutableDictionary<char, String>(\n+            new Dictionary<char, string> {\n+            {'A',\"Alanine\"},\n+            {'R', \"Arginine\"},\n+            {'N', \"Asparagine\"},\n+            {'D', \"Aspartic acid\"},\n+            {'C', \"Cysteine\"},\n+            {'E', \"Glutamic acid\"},\n+            {'Q', \"Glutamine\"},\n+            {'G', \"Glycine\"},\n+            {'H', \"Histidine\"},\n+            {'I', \"Isoleucine\"},\n+            {'L', \"Leucine\"},\n+            {'K', \"Lysine\"},\n+            {'M', \"Methionine\"},\n+            {'F', \"Phenylalanine\"},\n+            {'P', \"Proline\"},\n+            {'S', \"Serine\"},\n+            {'T', \"Threonine\"},\n+            {'W', \"Tryptophan\"},\n+            {'Y', \"Tyrosine\"},\n+            {'V', \"Valine\"},\n+            {'U', \"Selenocysteine\"},\n+            {'O', \"Pyrrolysine\"}\n         });\n         \/\/ ReSharper restore LocalizableElement\n ","old_code":"\ufeff\/*\r\n * Original author: Nicholas Shulman <nicksh .at. u.washington.edu>,\r\n *                  MacCoss Lab, Department of Genome Sciences, UW\r\n *\r\n * Copyright 2009 University of Washington - Seattle, WA\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n *\/\r\nusing System;\r\nusing System.Collections.Generic;\r\nusing System.Linq;\r\nusing JetBrains.Annotations;\r\nusing pwiz.Common.Collections;\r\nusing pwiz.Common.SystemUtil;\r\n\r\nnamespace pwiz.Common.Chemistry\r\n{\r\n    public class AminoAcidFormulas : Immutable\r\n    {\r\n\/\/ ReSharper disable InconsistentNaming\r\n        \/\/ public const double ProtonMass = 1.007276466879;  \/\/ per http:\/\/physics.nist.gov\/cgi-bin\/cuu\/Value?mpu|search_for=proton+mass 12\/18\/2016\r\n        public const double ProtonMass = 1.00727649;\r\n\r\n        \/\/ ReSharper disable LocalizableElement\r\n        public static readonly IDictionary<String, char> LongNames = new ImmutableDictionary<String,char>(\r\n            new Dictionary<string, char> {\r\n            {\"Ala\",'A'},\r\n            {\"Arg\",'R'},\r\n            {\"Asn\",'N'},\r\n            {\"Asp\",'D'},\r\n            {\"Cys\",'C'},\r\n            {\"Glu\",'E'},\r\n            {\"Gln\",'Q'},\r\n            {\"Gly\",'G'},\r\n            {\"His\",'H'},\r\n            {\"Ile\",'I'},\r\n            {\"Leu\",'L'},\r\n            {\"Lys\",'K'},\r\n            {\"Met\",'M'},\r\n            {\"Phe\",'F'},\r\n            {\"Pro\",'P'},\r\n            {\"Ser\",'S'},\r\n            {\"Thr\",'T'},\r\n            {\"Trp\",'W'},\r\n            {\"Tyr\",'Y'},\r\n            {\"Val\",'V'}\r\n        });\r\n        \/\/ ReSharper restore LocalizableElement\r\n\r\n        \/\/ ReSharper disable LocalizableElement\r\n        public static readonly IDictionary<char, String> DefaultFormulas = new ImmutableDictionary<char, String>\r\n            (\r\n            new Dictionary<char, string>\r\n                {\r\n                    {'A', \"C3H5ON\"},\r\n                    {'C', \"C3H5ONS\"},\r\n                    {'D', \"C4H5O3N\"},\r\n                    {'E', \"C5H7O3N\"},\r\n                    {'F', \"C9H9ON\"},\r\n                    {'G', \"C2H3ON\"},\r\n                    {'H', \"C6H7ON3\"},\r\n                    {'I', \"C6H11ON\"},\r\n                    {'K', \"C6H12ON2\"},\r\n                    {'L', \"C6H11ON\"},\r\n                    {'M', \"C5H9ONS\"},\r\n                    {'N', \"C4H6O2N2\"},\r\n                    {'O', \"C12H19N3O2\"},\r\n                    {'P', \"C5H7ON\"},\r\n                    {'Q', \"C5H8O2N2\"},\r\n                    {'R', \"C6H12ON4\"},\r\n                    {'S', \"C3H5O2N\"},\r\n                    {'T', \"C4H7O2N\"},\r\n                    {'U', \"C3H5NOSe\"},\r\n                    {'V', \"C5H9ON\"},\r\n                    {'W', \"C11H10ON2\"},\r\n                    {'Y', \"C9H9O2N\"},\r\n                }\r\n                );\r\n        \/\/ ReSharper restore LocalizableElement\r\n\r\n        private Dictionary<char, Molecule> _molecules;\r\n        private static readonly Molecule H2O = Molecule.Parse(@\"H2O\");\r\n\r\n        public static readonly AminoAcidFormulas Default = new AminoAcidFormulas\r\n                                                               {\r\n                                                                   MassShifts = new ImmutableDictionary<char,double>(new Dictionary<char, double>()),\r\n                                                                   _molecules = DefaultFormulas.ToDictionary(kvp=>kvp.Key, kvp=>Molecule.Parse(kvp.Value)),\r\n                                                                   IsotopeAbundances = IsotopeAbundances.Default,\r\n                                                                   MassResolution = .001,\r\n                                                               };\r\n\/\/ ReSharper restore InconsistentNaming\r\n\r\n        public double MassResolution { get; private set; }\r\n        public IDictionary<char, double> MassShifts { get; private set; }\r\n        public IsotopeAbundances IsotopeAbundances { get; private set; }\r\n        public AminoAcidFormulas SetFormula(char aminoAcid, String formula)\r\n        {\r\n            return ChangeProp(ImClone(this), im =>\r\n            {\r\n                im._molecules = new Dictionary<char, Molecule>(_molecules);\r\n                im._molecules[aminoAcid] = Molecule.Parse(formula);\r\n            });\r\n        }\r\n        public AminoAcidFormulas SetMassShift(char aminoAcid, double massShift)\r\n        {\r\n            return SetMassShifts(new Dictionary<char, double> {{aminoAcid, massShift}});\r\n        }\r\n        public AminoAcidFormulas SetMassShifts(Dictionary<char,double> dictionary)\r\n        {\r\n            var newMassShifts = new Dictionary<char, double>(MassShifts);\r\n            foreach (var entry in dictionary)\r\n            {\r\n                newMassShifts[entry.Key] = entry.Value;\r\n            }\r\n\r\n            return ChangeProp(ImClone(this), im => im.MassShifts = newMassShifts);\r\n        }\r\n        public AminoAcidFormulas SetIsotopeAbundances(IsotopeAbundances newAbundances)\r\n        {\r\n            return ChangeProp(ImClone(this), im => im.IsotopeAbundances = newAbundances);\r\n        }\r\n        public MassDistribution GetMassDistribution(Molecule molecule, int charge)\r\n        {\r\n            var md = new MassDistribution(MassResolution, .00001);\r\n            var result = md;\r\n            foreach (var element in molecule)\r\n            {\r\n                result = result.Add(md.Add(IsotopeAbundances[element.Key]).Multiply(element.Value));\r\n            }\r\n            if (charge != 0)\r\n            {\r\n                result = result.OffsetAndDivide(charge * ProtonMass, charge);\r\n            }\r\n            return result;\r\n        }\r\n        public AminoAcidFormulas SetMassResolution(double massResolution)\r\n        {\r\n            return ChangeProp(ImClone(this), im=>im.MassResolution = massResolution);\r\n        }\r\n        public MassDistribution GetMassDistribution(String peptide, int charge)\r\n        {\r\n            return GetMassDistribution(GetFormula(peptide), charge)\r\n                .OffsetAndDivide(GetMassShift(peptide)\/Math.Max(charge,1), 1);\r\n        }\r\n        public Molecule GetFormula(String peptide)\r\n        {\r\n            var formulas = peptide.Select(aa =>\r\n                _molecules.TryGetValue(aa, out Molecule aaFormula) ? aaFormula : Molecule.Empty\r\n            ).Append(H2O);\r\n            return Molecule.Sum(formulas);\r\n        }\r\n\r\n        [CanBeNull]\r\n        public Molecule GetAminoAcidFormula(char aa)\r\n        {\r\n            Molecule molecule;\r\n            _molecules.TryGetValue(aa, out molecule);\r\n            return molecule;\r\n        }\r\n\r\n        public Double GetMassShift(String peptide)\r\n        {\r\n            var result = 0.0;\r\n            foreach (var ch in peptide)\r\n            {\r\n                double shift;\r\n                MassShifts.TryGetValue(ch, out shift);\r\n                result += shift;\r\n            }\r\n            return result;\r\n        }\r\n        public double GetMonoisotopicMass(String peptide)\r\n        {\r\n            double result = GetMassShift(peptide);\r\n            foreach (var element in GetFormula(peptide))\r\n            {\r\n                result += IsotopeAbundances[element.Key].MostAbundanceMass*element.Value;\r\n            }\r\n            return result;\r\n        }\r\n    }\r\n}\r\n","lang_cluster":"C#","length":192,"code_uid":"0e194a7f66f6432e981fcef5f3aef32d"}
{"diff_hunk":"@@ -40,7 +40,9 @@ namespace Nethermind.Runner\n             ContractResolver = new CamelCasePropertyNamesContractResolver()\n         };\n         \n-        private IJsonSerializer _jsonSerializer = new EthereumJsonSerializer();\n+        private IJsonSerializer _jsonSerializer = CreateJsonSerializer();\n+\n+        private static EthereumJsonSerializer CreateJsonSerializer() => new EthereumJsonSerializer(NullValueHandling.Include);\n \n         public void ConfigureServices(IServiceCollection services)\n         {","old_code":"\ufeff\/\/  Copyright (c) 2018 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nusing System;\nusing System.IO;\nusing System.Text;\nusing Microsoft.AspNetCore.Builder;\nusing Microsoft.AspNetCore.Hosting;\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.AspNetCore.Server.Kestrel.Core;\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Hosting;\nusing Nethermind.Config;\nusing Nethermind.Core.Attributes;\nusing Nethermind.JsonRpc;\nusing Nethermind.Serialization.Json;\nusing Nethermind.WebSockets;\nusing Newtonsoft.Json;\nusing Newtonsoft.Json.Serialization;\n\nnamespace Nethermind.Runner\n{\n    public class Startup\n    {\n        private static readonly JsonSerializerSettings JsonSettings = new JsonSerializerSettings\n        {\n            ContractResolver = new CamelCasePropertyNamesContractResolver()\n        };\n        \n        private IJsonSerializer _jsonSerializer = new EthereumJsonSerializer();\n\n        public void ConfigureServices(IServiceCollection services)\n        {\n            services.Configure<KestrelServerOptions>(options => { options.AllowSynchronousIO = true; });\n            Bootstrap.Instance.RegisterJsonRpcServices(services);\n            var corsOrigins = Environment.GetEnvironmentVariable(\"NETHERMIND_CORS_ORIGINS\") ?? \"*\";\n            services.AddCors(c => c.AddPolicy(\"Cors\",\n                p => p.AllowAnyMethod().AllowAnyHeader().WithOrigins(corsOrigins)));\n        }\n\n        [Todo(Improve.Performance, \"Can we write immediately to the stream instead of calling ToString on the entire JSON content?\")]\n        public void Configure(IApplicationBuilder app, IWebHostEnvironment env, IJsonRpcProcessor jsonRpcProcessor,\n            IJsonRpcService jsonRpcService)\n        {\n            _jsonSerializer = new EthereumJsonSerializer();\n            foreach (JsonConverter converter in jsonRpcService.Converters)\n            {\n                _jsonSerializer.RegisterConverter(converter);\n            }\n\n            if (env.IsDevelopment())\n            {\n                app.UseDeveloperExceptionPage();\n            }\n\n            app.UseCors(\"Cors\");\n\n            var configProvider = app.ApplicationServices.GetService<IConfigProvider>();\n            var initConfig = configProvider.GetConfig<IInitConfig>();\n            var jsonRpcConfig = configProvider.GetConfig<IJsonRpcConfig>();\n            if (initConfig.WebSocketsEnabled)\n            {\n                app.UseWebSockets();\n                app.UseWhen(ctx => ctx.WebSockets.IsWebSocketRequest \n                                   && ctx.Connection.LocalPort == jsonRpcConfig.WebSocketsPort,\n                builder => builder.UseWebSocketsModules());\n            }\n            \n            app.Use(async (ctx, next) =>\n            {\n                if (ctx.Request.Method == \"GET\")\n                {\n                    await ctx.Response.WriteAsync(\"Nethermind JSON RPC\");\n                }\n                else if (ctx.Connection.LocalPort == jsonRpcConfig.Port && ctx.Request.Method == \"POST\")\n                {\n                    using var reader = new StreamReader(ctx.Request.Body, Encoding.UTF8);\n                    var request = await reader.ReadToEndAsync();\n                    var result = await jsonRpcProcessor.ProcessAsync(request);\n\n                    if (result.IsCollection)\n                    {\n                        _jsonSerializer.Serialize(ctx.Response.Body, result.Responses);\n                    }\n                    else\n                    {\n                        _jsonSerializer.Serialize(ctx.Response.Body, result.Response);\n                    }\n\n                    await ctx.Response.CompleteAsync();\n                }\n            });\n        }\n    }\n}\n","lang_cluster":"C#","length":108,"code_uid":"103a69d35b98438593472344675eee09"}
{"diff_hunk":"@@ -52,6 +52,8 @@ namespace Nethermind.Runner.Ethereum.Steps\n \n                 JsonRpcService jsonRpcService = new(_api.RpcModuleProvider, _api.LogManager);\n \n+                _api.EthereumJsonSerializer.RegisterConverters(jsonRpcService.Converters);\n+                \n                 JsonRpcProcessor jsonRpcProcessor = new(\n                     jsonRpcService,\n                     _api.EthereumJsonSerializer,","old_code":"\/\/  Copyright (c) 2021 Demerzel Solutions Limited\n\/\/  This file is part of the Nethermind library.\n\/\/ \n\/\/  The Nethermind library is free software: you can redistribute it and\/or modify\n\/\/  it under the terms of the GNU Lesser General Public License as published by\n\/\/  the Free Software Foundation, either version 3 of the License, or\n\/\/  (at your option) any later version.\n\/\/ \n\/\/  The Nethermind library is distributed in the hope that it will be useful,\n\/\/  but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n\/\/  GNU Lesser General Public License for more details.\n\/\/ \n\/\/  You should have received a copy of the GNU Lesser General Public License\n\/\/  along with the Nethermind. If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\/\/ \n\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Nethermind.Api;\nusing Nethermind.Core;\nusing Nethermind.Init.Steps;\nusing Nethermind.JsonRpc;\nusing Nethermind.JsonRpc.WebSockets;\nusing Nethermind.Logging;\nusing Nethermind.Runner.JsonRpc;\n\nnamespace Nethermind.Runner.Ethereum.Steps\n{\n    [RunnerStepDependencies(typeof(InitializeNetwork), typeof(RegisterRpcModules), typeof(SetupHive))]\n    public class StartRpc : IStep\n    {\n        private readonly INethermindApi _api;\n\n        public StartRpc(INethermindApi api)\n        {\n            _api = api;\n        }\n\n        public async Task Execute(CancellationToken cancellationToken)\n        {\n            IJsonRpcConfig jsonRpcConfig = _api.Config<IJsonRpcConfig>();\n            ILogger logger = _api.LogManager.GetClassLogger();\n\n            if (jsonRpcConfig.Enabled)\n            {\n                IInitConfig initConfig = _api.Config<IInitConfig>();\n                JsonRpcLocalStats jsonRpcLocalStats = new(\n                    _api.Timestamper,\n                    jsonRpcConfig,\n                    _api.LogManager);\n\n                JsonRpcService jsonRpcService = new(_api.RpcModuleProvider, _api.LogManager);\n\n                JsonRpcProcessor jsonRpcProcessor = new(\n                    jsonRpcService,\n                    _api.EthereumJsonSerializer,\n                    jsonRpcConfig,\n                    _api.FileSystem,\n                    _api.LogManager);\n\n                if (initConfig.WebSocketsEnabled)\n                {\n                    JsonRpcWebSocketsModule webSocketsModule = new (jsonRpcProcessor, jsonRpcService, jsonRpcLocalStats, _api.LogManager, _api.EthereumJsonSerializer);\n                    _api.WebSocketsManager!.AddModule(webSocketsModule, true);\n                }\n\n                Bootstrap.Instance.JsonRpcService = jsonRpcService;\n                Bootstrap.Instance.LogManager = _api.LogManager;\n                Bootstrap.Instance.JsonSerializer = _api.EthereumJsonSerializer;\n                Bootstrap.Instance.JsonRpcLocalStats = jsonRpcLocalStats;\n                JsonRpcRunner? jsonRpcRunner = new(\n                    jsonRpcProcessor,\n                    _api.WebSocketsManager!,\n                    _api.ConfigProvider,\n                    _api.LogManager,\n                    _api);\n\n                await jsonRpcRunner.Start(cancellationToken).ContinueWith(x =>\n                {\n                    if (x.IsFaulted && logger.IsError)\n                        logger.Error(\"Error during jsonRpc runner start\", x.Exception);\n                }, cancellationToken);\n\n                JsonRpcIpcRunner jsonIpcRunner = new(jsonRpcProcessor, jsonRpcService, _api.ConfigProvider, _api.LogManager, jsonRpcLocalStats, _api.EthereumJsonSerializer, _api.FileSystem);\n                jsonIpcRunner.Start(cancellationToken);\n\n#pragma warning disable 4014\n                _api.DisposeStack.Push(new Reactive.AnonymousDisposable(() => jsonRpcRunner.StopAsync())); \/\/ do not await\n                _api.DisposeStack.Push(jsonIpcRunner); \/\/ do not await\n#pragma warning restore 4014\n            }\n            else\n            {\n                if (logger.IsInfo) logger.Info(\"Json RPC is disabled\");\n            }\n        }\n    }\n}\n","lang_cluster":"C#","length":99,"code_uid":"53bbda8501fe417babbde6367a067d5d"}
{"diff_hunk":"@@ -12,8 +12,8 @@\n #include <nano\/rpc\/rpc_secure.hpp>\n #endif\n \n-nano::rpc::rpc (boost::asio::io_context & io_ctx_a, nano::rpc_config const & config_a, nano::rpc_handler_interface & rpc_handler_interface_a) :\n-\tconfig (config_a),\n+nano::rpc::rpc (boost::asio::io_context & io_ctx_a, nano::rpc_config config_a, nano::rpc_handler_interface & rpc_handler_interface_a) :\n+\tconfig (std::move (config_a)),\n \tacceptor (io_ctx_a),\n \tlogger (std::chrono::milliseconds (0)),\n \tio_ctx (io_ctx_a),","old_code":"#include <nano\/boost\/asio\/bind_executor.hpp>\n#include <nano\/lib\/rpc_handler_interface.hpp>\n#include <nano\/lib\/tlsconfig.hpp>\n#include <nano\/rpc\/rpc.hpp>\n#include <nano\/rpc\/rpc_connection.hpp>\n\n#include <boost\/format.hpp>\n\n#include <iostream>\n\n#ifdef NANO_SECURE_RPC\n#include <nano\/rpc\/rpc_secure.hpp>\n#endif\n\nnano::rpc::rpc (boost::asio::io_context & io_ctx_a, nano::rpc_config const & config_a, nano::rpc_handler_interface & rpc_handler_interface_a) :\n\tconfig (config_a),\n\tacceptor (io_ctx_a),\n\tlogger (std::chrono::milliseconds (0)),\n\tio_ctx (io_ctx_a),\n\trpc_handler_interface (rpc_handler_interface_a)\n{\n\trpc_handler_interface.rpc_instance (*this);\n}\n\nnano::rpc::~rpc ()\n{\n\tif (!stopped)\n\t{\n\t\tstop ();\n\t}\n}\n\nvoid nano::rpc::start ()\n{\n\tauto endpoint (boost::asio::ip::tcp::endpoint (boost::asio::ip::make_address_v6 (config.address), config.port));\n\tbool const is_loopback = (endpoint.address ().is_loopback () || (endpoint.address ().to_v6 ().is_v4_mapped () && boost::asio::ip::make_address_v4 (boost::asio::ip::v4_mapped, endpoint.address ().to_v6 ()).is_loopback ()));\n\tif (!is_loopback && config.enable_control)\n\t{\n\t\tauto warning = boost::str (boost::format (\"WARNING: control-level RPCs are enabled on non-local address %1%, potentially allowing wallet access outside local computer\") % endpoint.address ().to_string ());\n\t\tstd::cout << warning << std::endl;\n\t\tlogger.always_log (warning);\n\t}\n\tacceptor.open (endpoint.protocol ());\n\tacceptor.set_option (boost::asio::ip::tcp::acceptor::reuse_address (true));\n\n\tboost::system::error_code ec;\n\tacceptor.bind (endpoint, ec);\n\tif (ec)\n\t{\n\t\tlogger.always_log (boost::str (boost::format (\"Error while binding for RPC on port %1%: %2%\") % endpoint.port () % ec.message ()));\n\t\tthrow std::runtime_error (ec.message ());\n\t}\n\tacceptor.listen ();\n\taccept ();\n}\n\nvoid nano::rpc::accept ()\n{\n\tauto connection (std::make_shared<nano::rpc_connection> (config, io_ctx, logger, rpc_handler_interface));\n\tacceptor.async_accept (connection->socket, boost::asio::bind_executor (connection->strand, [this, connection] (boost::system::error_code const & ec) {\n\t\tif (ec != boost::asio::error::operation_aborted && acceptor.is_open ())\n\t\t{\n\t\t\taccept ();\n\t\t}\n\t\tif (!ec)\n\t\t{\n\t\t\tconnection->parse_connection ();\n\t\t}\n\t\telse\n\t\t{\n\t\t\tlogger.always_log (boost::str (boost::format (\"Error accepting RPC connections: %1% (%2%)\") % ec.message () % ec.value ()));\n\t\t}\n\t}));\n}\n\nvoid nano::rpc::stop ()\n{\n\tstopped = true;\n\tacceptor.close ();\n}\n\nstd::unique_ptr<nano::rpc> nano::get_rpc (boost::asio::io_context & io_ctx_a, nano::rpc_config const & config_a, nano::rpc_handler_interface & rpc_handler_interface_a)\n{\n\tstd::unique_ptr<rpc> impl;\n\n\tif (config_a.tls_config && config_a.tls_config->enable_https)\n\t{\n#ifdef NANO_SECURE_RPC\n\t\timpl = std::make_unique<rpc_secure> (io_ctx_a, config_a, rpc_handler_interface_a);\n#endif\n\t}\n\telse\n\t{\n\t\timpl = std::make_unique<rpc> (io_ctx_a, config_a, rpc_handler_interface_a);\n\t}\n\n\treturn impl;\n}\n","lang_cluster":"C++","length":98,"code_uid":"0f42f406188f46aaab80ecb60e681a99"}
{"diff_hunk":"@@ -42,6 +42,7 @@ class named_mutex;\n #include \"..\/plugins\/nearest.hpp\"\n #include \"..\/plugins\/timestamp.hpp\"\n #include \"..\/plugins\/viaroute.hpp\"\n+#include \"..\/plugins\/map_matching.hpp\"\n #include \"..\/server\/data_structures\/datafacade_base.hpp\"\n #include \"..\/server\/data_structures\/internal_datafacade.hpp\"\n #include \"..\/server\/data_structures\/shared_barriers.hpp\"","old_code":"\/*\n\nCopyright (c) 2015, Project OSRM contributors\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list\nof conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this\nlist of conditions and the following disclaimer in the documentation and\/or\nother materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n*\/\n\nnamespace boost\n{\nnamespace interprocess\n{\nclass named_mutex;\n}\n}\n\n#include \"osrm_impl.hpp\"\n#include \"osrm.hpp\"\n\n#include \"..\/plugins\/distance_table.hpp\"\n#include \"..\/plugins\/hello_world.hpp\"\n#include \"..\/plugins\/locate.hpp\"\n#include \"..\/plugins\/nearest.hpp\"\n#include \"..\/plugins\/timestamp.hpp\"\n#include \"..\/plugins\/viaroute.hpp\"\n#include \"..\/server\/data_structures\/datafacade_base.hpp\"\n#include \"..\/server\/data_structures\/internal_datafacade.hpp\"\n#include \"..\/server\/data_structures\/shared_barriers.hpp\"\n#include \"..\/server\/data_structures\/shared_datafacade.hpp\"\n#include \"..\/util\/make_unique.hpp\"\n#include \"..\/util\/routed_options.hpp\"\n#include \"..\/util\/simple_logger.hpp\"\n\n#include <boost\/assert.hpp>\n#include <boost\/interprocess\/sync\/named_condition.hpp>\n#include <boost\/interprocess\/sync\/scoped_lock.hpp>\n\n#include <osrm\/route_parameters.hpp>\n\n#include <algorithm>\n#include <fstream>\n#include <utility>\n#include <vector>\n\nOSRM_impl::OSRM_impl(libosrm_config &lib_config)\n{\n    if (lib_config.use_shared_memory)\n    {\n        barrier = osrm::make_unique<SharedBarriers>();\n        query_data_facade = new SharedDataFacade<QueryEdge::EdgeData>();\n    }\n    else\n    {\n        \/\/ populate base path\n        populate_base_path(lib_config.server_paths);\n        query_data_facade = new InternalDataFacade<QueryEdge::EdgeData>(lib_config.server_paths);\n    }\n\n    \/\/ The following plugins handle all requests.\n    RegisterPlugin(new DistanceTablePlugin<BaseDataFacade<QueryEdge::EdgeData>>(\n        query_data_facade, lib_config.max_locations_distance_table));\n    RegisterPlugin(new HelloWorldPlugin());\n    RegisterPlugin(new LocatePlugin<BaseDataFacade<QueryEdge::EdgeData>>(query_data_facade));\n    RegisterPlugin(new NearestPlugin<BaseDataFacade<QueryEdge::EdgeData>>(query_data_facade));\n    RegisterPlugin(new TimestampPlugin<BaseDataFacade<QueryEdge::EdgeData>>(query_data_facade));\n    RegisterPlugin(new ViaRoutePlugin<BaseDataFacade<QueryEdge::EdgeData>>(query_data_facade));\n}\n\nOSRM_impl::~OSRM_impl()\n{\n    delete query_data_facade;\n    for (PluginMap::value_type &plugin_pointer : plugin_map)\n    {\n        delete plugin_pointer.second;\n    }\n}\n\nvoid OSRM_impl::RegisterPlugin(BasePlugin *plugin)\n{\n    SimpleLogger().Write() << \"loaded plugin: \" << plugin->GetDescriptor();\n    if (plugin_map.find(plugin->GetDescriptor()) != plugin_map.end())\n    {\n        delete plugin_map.find(plugin->GetDescriptor())->second;\n    }\n    plugin_map.emplace(plugin->GetDescriptor(), plugin);\n}\n\nint OSRM_impl::RunQuery(RouteParameters &route_parameters, osrm::json::Object &json_result)\n{\n    const auto &plugin_iterator = plugin_map.find(route_parameters.service);\n\n    if (plugin_map.end() == plugin_iterator)\n    {\n        return 400;\n    }\n\n    increase_concurrent_query_count();\n    plugin_iterator->second->HandleRequest(route_parameters, json_result);\n    decrease_concurrent_query_count();\n    return 200;\n}\n\n\/\/ decrease number of concurrent queries\nvoid OSRM_impl::decrease_concurrent_query_count()\n{\n    if (!barrier)\n    {\n        return;\n    }\n    \/\/ lock query\n    boost::interprocess::scoped_lock<boost::interprocess::named_mutex> query_lock(\n        barrier->query_mutex);\n\n    \/\/ decrement query count\n    --(barrier->number_of_queries);\n    BOOST_ASSERT_MSG(0 <= barrier->number_of_queries, \"invalid number of queries\");\n\n    \/\/ notify all processes that were waiting for this condition\n    if (0 == barrier->number_of_queries)\n    {\n        barrier->no_running_queries_condition.notify_all();\n    }\n}\n\n\/\/ increase number of concurrent queries\nvoid OSRM_impl::increase_concurrent_query_count()\n{\n    if (!barrier)\n    {\n        return;\n    }\n\n    \/\/ lock update pending\n    boost::interprocess::scoped_lock<boost::interprocess::named_mutex> pending_lock(\n        barrier->pending_update_mutex);\n\n    \/\/ lock query\n    boost::interprocess::scoped_lock<boost::interprocess::named_mutex> query_lock(\n        barrier->query_mutex);\n\n    \/\/ unlock update pending\n    pending_lock.unlock();\n\n    \/\/ increment query count\n    ++(barrier->number_of_queries);\n\n    (static_cast<SharedDataFacade<QueryEdge::EdgeData> *>(query_data_facade))\n        ->CheckAndReloadFacade();\n}\n\n\/\/ proxy code for compilation firewall\nOSRM::OSRM(libosrm_config &lib_config) : OSRM_pimpl_(osrm::make_unique<OSRM_impl>(lib_config)) {}\n\nOSRM::~OSRM() { OSRM_pimpl_.reset(); }\n\nint OSRM::RunQuery(RouteParameters &route_parameters, osrm::json::Object &json_result)\n{\n    return OSRM_pimpl_->RunQuery(route_parameters, json_result);\n}\n","lang_cluster":"C++","length":178,"code_uid":"38f2a48391fe4bbc82a0fd2a1fdb6982"}
{"diff_hunk":"@@ -34,6 +34,11 @@ public:\n     static constexpr std::int64_t column_count = 2;\n     static constexpr std::int64_t element_count = row_count * column_count;\n \n+    bool not_available_on_device() {\n+        constexpr bool is_svd = std::is_same_v<Method, pca::method::svd>;\n+        return get_policy().is_gpu() && is_svd;\n+    }\n+\n     auto get_descriptor() const {\n         return pca::descriptor<float, Method, pca::task::dim_reduction>{};\n     }","old_code":"\/*******************************************************************************\n* Copyright 2020-2021 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n#include <array>\n\n#include \"oneapi\/dal\/algo\/pca\/infer.hpp\"\n#include \"oneapi\/dal\/algo\/pca\/train.hpp\"\n#include \"oneapi\/dal\/table\/row_accessor.hpp\"\n\n#include \"oneapi\/dal\/test\/engine\/common.hpp\"\n#include \"oneapi\/dal\/test\/engine\/fixtures.hpp\"\n\nnamespace oneapi::dal::pca::test {\n\nnamespace te = dal::test::engine;\n\ntemplate <typename Method>\nclass pca_badarg_test : public te::algo_fixture {\npublic:\n    static constexpr std::int64_t row_count = 8;\n    static constexpr std::int64_t column_count = 2;\n    static constexpr std::int64_t element_count = row_count * column_count;\n\n    auto get_descriptor() const {\n        return pca::descriptor<float, Method, pca::task::dim_reduction>{};\n    }\n\n    table get_train_data(std::int64_t override_row_count = row_count,\n                         std::int64_t override_column_count = column_count) const {\n        ONEDAL_ASSERT(override_row_count * override_column_count <= element_count);\n        return homogen_table::wrap(train_data_.data(), override_row_count, override_column_count);\n    }\n\n    table get_infer_data(std::int64_t override_row_count = row_count,\n                         std::int64_t override_column_count = column_count) const {\n        ONEDAL_ASSERT(override_row_count * override_column_count <= element_count);\n        return homogen_table::wrap(infer_data_.data(), override_row_count, override_column_count);\n    }\n\nprivate:\n    static constexpr std::array<float, element_count> train_data_ = {\n        1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0\n    };\n\n    static constexpr std::array<float, element_count> infer_data_ = {\n        1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0\n    };\n};\n\n#define PCA_BADARG_TEST(name) \\\n    TEMPLATE_TEST_M(pca_badarg_test, name, \"[pca][badarg]\", pca::method::cov, pca::method::svd)\n\nPCA_BADARG_TEST(\"accepts non-negative component_count\") {\n    REQUIRE_NOTHROW(this->get_descriptor().set_component_count(0));\n}\n\nPCA_BADARG_TEST(\"throws if component_count is negative\") {\n    REQUIRE_THROWS_AS(this->get_descriptor().set_component_count(-1), domain_error);\n}\n\nPCA_BADARG_TEST(\"throws if train data is empty\") {\n    const auto pca_desc = this->get_descriptor().set_component_count(2);\n\n    REQUIRE_THROWS_AS(this->train(pca_desc, homogen_table{}), domain_error);\n}\n\nPCA_BADARG_TEST(\"throws if train data columns less than component count\") {\n    const auto pca_desc = this->get_descriptor().set_component_count(4);\n\n    REQUIRE_THROWS_AS(this->train(pca_desc, this->get_train_data()), invalid_argument);\n}\n\nPCA_BADARG_TEST(\"throws if infer data is empty\") {\n    const auto pca_desc = this->get_descriptor().set_component_count(2);\n    const auto model = this->train(pca_desc, this->get_train_data()).get_model();\n\n    REQUIRE_THROWS_AS(this->infer(pca_desc, model, homogen_table{}), domain_error);\n}\n\nPCA_BADARG_TEST(\"throws if component count neq eigenvector_rows\") {\n    auto pca_desc = this->get_descriptor().set_component_count(2);\n    const auto model = this->train(pca_desc, this->get_train_data()).get_model();\n    pca_desc.set_component_count(4);\n\n    REQUIRE_THROWS_AS(this->infer(pca_desc, model, this->get_infer_data()), invalid_argument);\n}\n\nPCA_BADARG_TEST(\"throws if infer data column count neq eigenvector columns\") {\n    const auto pca_desc = this->get_descriptor().set_component_count(2);\n    const auto model = this->train(pca_desc, this->get_train_data()).get_model();\n    const auto infer_data = this->get_infer_data(4, 4);\n\n    REQUIRE_THROWS_AS(this->infer(pca_desc, model, infer_data), invalid_argument);\n}\n\n} \/\/ namespace oneapi::dal::pca::test\n","lang_cluster":"C++","length":109,"code_uid":"72f969d1f9ce41f3b1ce2a0eb0ebfa5a"}
{"diff_hunk":"@@ -16,23 +16,24 @@ R.E. Carhart, D.H. Smith, R. Venkataraghavan;\n Definition and Applications\" JCICS 25, 64-73 (1985).\n \n \"\"\"\n-from rdkit.DataStructs import IntSparseIntVect\n-from rdkit import Chem\n+from rdkit import DataStructs\n from rdkit.Chem import rdMolDescriptors\n from rdkit.Chem.AtomPairs import Utils\n-from rdkit import DataStructs\n \n-from rdkit.Chem.rdMolDescriptors import GetAtomPairFingerprint,GetHashedAtomPairFingerprint\n-GetAtomPairFingerprintAsIntVect=rdMolDescriptors.GetAtomPairFingerprint\n+GetAtomPairFingerprint = rdMolDescriptors.GetAtomPairFingerprint\n+GetAtomPairFingerprintAsIntVect = rdMolDescriptors.GetAtomPairFingerprint\n+GetHashedAtomPairFingerprint = rdMolDescriptors.GetHashedAtomPairFingerprint\n+\n+numPathBits = rdMolDescriptors.AtomPairsParameters.numPathBits\n+_maxPathLen = (1 << numPathBits) - 1\n+numFpBits = numPathBits + 2 * rdMolDescriptors.AtomPairsParameters.codeSize\n+fpLen = 1 << numFpBits\n \n-numPathBits=rdMolDescriptors.AtomPairsParameters.numPathBits\n-_maxPathLen=(1<<numPathBits)-1\n-numFpBits=numPathBits+2*rdMolDescriptors.AtomPairsParameters.codeSize\n-fpLen=1<<numFpBits\n \n-def pyScorePair(at1,at2,dist,atomCodes=None):\n+def pyScorePair(at1, at2, dist, atomCodes=None):\n   \"\"\" Returns a score for an individual atom pair.\n \n+  >>> from rdkit import Chem\n   >>> m = Chem.MolFromSmiles('CCCCC')\n   >>> c1 = Utils.GetAtomCode(m.GetAtomWithIdx(0))\n   >>> c2 = Utils.GetAtomCode(m.GetAtomWithIdx(1))","old_code":"# $Id$\n#\n#  Copyright (C) 2004-2007 Greg Landrum and Rational Discovery LLC\n#\n#   @@ All Rights Reserved @@\n#  This file is part of the RDKit.\n#  The contents are covered by the terms of the BSD license\n#  which is included in the file license.txt, found at the root\n#  of the RDKit source tree.\n#\n\"\"\" Contains an implementation of Atom-pair fingerprints, as\ndescribed in:\n\nR.E. Carhart, D.H. Smith, R. Venkataraghavan;\n\"Atom Pairs as Molecular Features in Structure-Activity Studies:\nDefinition and Applications\" JCICS 25, 64-73 (1985).\n\n\"\"\"\nfrom rdkit.DataStructs import IntSparseIntVect\nfrom rdkit import Chem\nfrom rdkit.Chem import rdMolDescriptors\nfrom rdkit.Chem.AtomPairs import Utils\nfrom rdkit import DataStructs\n\nfrom rdkit.Chem.rdMolDescriptors import GetAtomPairFingerprint,GetHashedAtomPairFingerprint\nGetAtomPairFingerprintAsIntVect=rdMolDescriptors.GetAtomPairFingerprint\n\nnumPathBits=rdMolDescriptors.AtomPairsParameters.numPathBits\n_maxPathLen=(1<<numPathBits)-1\nnumFpBits=numPathBits+2*rdMolDescriptors.AtomPairsParameters.codeSize\nfpLen=1<<numFpBits\n\ndef pyScorePair(at1,at2,dist,atomCodes=None):\n  \"\"\" Returns a score for an individual atom pair.\n\n  >>> m = Chem.MolFromSmiles('CCCCC')\n  >>> c1 = Utils.GetAtomCode(m.GetAtomWithIdx(0))\n  >>> c2 = Utils.GetAtomCode(m.GetAtomWithIdx(1))\n  >>> c3 = Utils.GetAtomCode(m.GetAtomWithIdx(2))\n  >>> t = 1 | min(c1,c2)<<numPathBits | max(c1,c2)<<(rdMolDescriptors.AtomPairsParameters.codeSize+numPathBits)\n  >>> pyScorePair(m.GetAtomWithIdx(0),m.GetAtomWithIdx(1),1)==t\n  1\n  >>> pyScorePair(m.GetAtomWithIdx(1),m.GetAtomWithIdx(0),1)==t\n  1\n  >>> t = 2 | min(c1,c3)<<numPathBits | max(c1,c3)<<(rdMolDescriptors.AtomPairsParameters.codeSize+numPathBits)\n  >>> pyScorePair(m.GetAtomWithIdx(0),m.GetAtomWithIdx(2),2)==t\n  1\n  >>> pyScorePair(m.GetAtomWithIdx(0),m.GetAtomWithIdx(2),2,\n  ...  atomCodes=(Utils.GetAtomCode(m.GetAtomWithIdx(0)),Utils.GetAtomCode(m.GetAtomWithIdx(2))))==t\n  1\n\n  \"\"\"\n  if not atomCodes:\n    code1 = Utils.GetAtomCode(at1)\n    code2 = Utils.GetAtomCode(at2)\n  else:\n    code1,code2=atomCodes\n  accum = int(dist) % _maxPathLen\n  accum |= min(code1,code2) << numPathBits\n  accum |= max(code1,code2) << (rdMolDescriptors.AtomPairsParameters.codeSize+numPathBits)\n  return accum\n\ndef ExplainPairScore(score):\n  \"\"\" \n  >>> m = Chem.MolFromSmiles('C=CC')\n  >>> score = pyScorePair(m.GetAtomWithIdx(0),m.GetAtomWithIdx(1),1)\n  >>> ExplainPairScore(score)\n  (('C', 1, 1), 1, ('C', 2, 1))\n  >>> score = pyScorePair(m.GetAtomWithIdx(0),m.GetAtomWithIdx(2),2)\n  >>> ExplainPairScore(score)\n  (('C', 1, 0), 2, ('C', 1, 1))\n  >>> score = pyScorePair(m.GetAtomWithIdx(1),m.GetAtomWithIdx(2),1)\n  >>> ExplainPairScore(score)\n  (('C', 1, 0), 1, ('C', 2, 1))\n  >>> score = pyScorePair(m.GetAtomWithIdx(2),m.GetAtomWithIdx(1),1)\n  >>> ExplainPairScore(score)\n  (('C', 1, 0), 1, ('C', 2, 1))\n\n  \"\"\"\n  codeMask = (1<<rdMolDescriptors.AtomPairsParameters.codeSize)-1\n  pathMask = (1<<numPathBits)-1\n  dist = score&pathMask\n\n  score = score>>numPathBits\n  code1 = score&codeMask\n  score = score>>rdMolDescriptors.AtomPairsParameters.codeSize\n  code2 = score&codeMask\n\n  res = Utils.ExplainAtomCode(code1),dist,Utils.ExplainAtomCode(code2)\n  return res\n\ndef GetAtomPairFingerprintAsBitVect(mol):\n  \"\"\" Returns the Atom-pair fingerprint for a molecule as\n  a SparseBitVect. Note that this doesn't match the standard\n  definition of atom pairs, which uses counts of the\n  pairs, not just their presence.\n\n  **Arguments**:\n\n    - mol: a molecule\n\n  **Returns**: a SparseBitVect\n\n  >>> m = Chem.MolFromSmiles('CCC')\n  >>> v = [ pyScorePair(m.GetAtomWithIdx(0),m.GetAtomWithIdx(1),1),\n  ...       pyScorePair(m.GetAtomWithIdx(0),m.GetAtomWithIdx(2),2),\n  ...     ]\n  >>> v.sort()\n  >>> fp = GetAtomPairFingerprintAsBitVect(m)\n  >>> list(fp.GetOnBits())==v\n  True\n  \n  \"\"\"\n  res = DataStructs.SparseBitVect(fpLen)\n  fp = rdMolDescriptors.GetAtomPairFingerprint(mol)\n  for val in fp.GetNonzeroElements().keys():\n    res.SetBit(val)\n  return res\n\n#------------------------------------\n#\n#  doctest boilerplate\n#\ndef _test():\n  import doctest,sys\n  return doctest.testmod(sys.modules[\"__main__\"])\n\n\nif __name__ == '__main__':\n  import sys\n  failed,tried = _test()\n  sys.exit(failed)\n  \n  \n\n","lang_cluster":"C++","length":135,"code_uid":"063b696ce19a464fabde009adb765910"}
{"diff_hunk":"@@ -75,3 +75,15 @@ const Outfit* Outfits::getOutfitByLookType(PlayerSex_t sex, uint16_t lookType) c\n \t}\n \treturn nullptr;\n }\n+\n+const Outfit* Outfits::getOutfitByLookType(uint16_t lookType) const\n+{\n+\tfor (uint8_t sex = 0; sex <= 1; sex++) {\n+\t\tfor (const Outfit& outfit : outfits[sex]) {\n+\t\t\tif (outfit.lookType == lookType) {\n+\t\t\t\treturn &outfit;\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn nullptr;\n+}","old_code":"\/**\n * The Forgotten Server - a free and open-source MMORPG server emulator\n * Copyright (C) 2019  Mark Samman <mark.samman@gmail.com>\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n *\/\n\n#include \"otpch.h\"\n\n#include \"outfit.h\"\n\n#include \"pugicast.h\"\n#include \"tools.h\"\n\nbool Outfits::loadFromXml()\n{\n\tpugi::xml_document doc;\n\tpugi::xml_parse_result result = doc.load_file(\"data\/XML\/outfits.xml\");\n\tif (!result) {\n\t\tprintXMLError(\"Error - Outfits::loadFromXml\", \"data\/XML\/outfits.xml\", result);\n\t\treturn false;\n\t}\n\n\tfor (auto outfitNode : doc.child(\"outfits\").children()) {\n\t\tpugi::xml_attribute attr;\n\t\tif ((attr = outfitNode.attribute(\"enabled\")) && !attr.as_bool()) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!(attr = outfitNode.attribute(\"type\"))) {\n\t\t\tstd::cout << \"[Warning - Outfits::loadFromXml] Missing outfit type.\" << std::endl;\n\t\t\tcontinue;\n\t\t}\n\n\t\tuint16_t type = pugi::cast<uint16_t>(attr.value());\n\t\tif (type > PLAYERSEX_LAST) {\n\t\t\tstd::cout << \"[Warning - Outfits::loadFromXml] Invalid outfit type \" << type << \".\" << std::endl;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpugi::xml_attribute lookTypeAttribute = outfitNode.attribute(\"looktype\");\n\t\tif (!lookTypeAttribute) {\n\t\t\tstd::cout << \"[Warning - Outfits::loadFromXml] Missing looktype on outfit.\" << std::endl;\n\t\t\tcontinue;\n\t\t}\n\n\t\toutfits[type].emplace_back(\n\t\t\toutfitNode.attribute(\"name\").as_string(),\n\t\t\tpugi::cast<uint16_t>(lookTypeAttribute.value()),\n\t\t\toutfitNode.attribute(\"premium\").as_bool(),\n\t\t\toutfitNode.attribute(\"unlocked\").as_bool(true)\n\t\t);\n\t}\n\treturn true;\n}\n\nconst Outfit* Outfits::getOutfitByLookType(PlayerSex_t sex, uint16_t lookType) const\n{\n\tfor (const Outfit& outfit : outfits[sex]) {\n\t\tif (outfit.lookType == lookType) {\n\t\t\treturn &outfit;\n\t\t}\n\t}\n\treturn nullptr;\n}\n","lang_cluster":"C++","length":77,"code_uid":"a44b10540a7343178a32c0f4f7cfcda7"}
{"diff_hunk":"@@ -82,6 +82,9 @@ python::tuple fragmentMolHelper3(const RDKit::ROMol& mol, python::object ob,\n   std::vector<std::pair<RDKit::ROMOL_SPTR, RDKit::ROMOL_SPTR>> tres;\n   std::unique_ptr<std::vector<unsigned int>> v =\n       pythonObjectToVect<unsigned int>(ob);\n+  if (!v) {\n+    throw_value_error(\"invalid value for bondsToCut\");\n+  }\n   bool ok = RDKit::MMPA::fragmentMol(mol, tres, *v, minCuts, maxCuts);\n   python::list pyres;\n   if (ok) {","old_code":"\/\/\n\/\/  Copyright (C) 2015 Greg Landrum\n\/\/\n\/\/   @@ All Rights Reserved @@\n\/\/  This file is part of the RDKit.\n\/\/  The contents are covered by the terms of the BSD license\n\/\/  which is included in the file license.txt, found at the root\n\/\/  of the RDKit source tree.\n\/\/\n#define PY_ARRAY_UNIQUE_SYMBOL rdmmpa_array_API\n#include <boost\/python.hpp>\n#include <GraphMol\/ROMol.h>\n#include <GraphMol\/SmilesParse\/SmilesWrite.h>\n#include <RDBoost\/Wrap.h>\n#include <GraphMol\/MMPA\/MMPA.h>\n\nnamespace python = boost::python;\n\nnamespace {\npython::tuple fragmentMolHelper(const RDKit::ROMol& mol, unsigned int maxCuts,\n                                unsigned int maxCutBonds,\n                                const std::string& pattern,\n                                bool resultsAsMols) {\n  std::vector<std::pair<RDKit::ROMOL_SPTR, RDKit::ROMOL_SPTR>> tres;\n  bool ok = RDKit::MMPA::fragmentMol(mol, tres, maxCuts, maxCutBonds, pattern);\n  python::list pyres;\n  if (ok) {\n    for (std::vector<std::pair<RDKit::ROMOL_SPTR, RDKit::ROMOL_SPTR>>::\n             const_iterator pr = tres.begin();\n         pr != tres.end(); ++pr) {\n      python::list lres;\n      if (resultsAsMols) {\n        lres.append(pr->first);\n        lres.append(pr->second);\n      } else {\n        if (pr->first) {\n          lres.append(RDKit::MolToSmiles(*(pr->first), true));\n        } else {\n          lres.append(\"\");\n        }\n        lres.append(RDKit::MolToSmiles(*(pr->second), true));\n      }\n      pyres.append(python::tuple(lres));\n    }\n  }\n  return python::tuple(pyres);\n}\n\npython::tuple fragmentMolHelper2(const RDKit::ROMol& mol, unsigned int minCuts,\n                                 unsigned int maxCuts, unsigned int maxCutBonds,\n                                 const std::string& pattern,\n                                 bool resultsAsMols) {\n  std::vector<std::pair<RDKit::ROMOL_SPTR, RDKit::ROMOL_SPTR>> tres;\n  bool ok = RDKit::MMPA::fragmentMol(mol, tres, minCuts, maxCuts, maxCutBonds,\n                                     pattern);\n  python::list pyres;\n  if (ok) {\n    for (std::vector<std::pair<RDKit::ROMOL_SPTR, RDKit::ROMOL_SPTR>>::\n             const_iterator pr = tres.begin();\n         pr != tres.end(); ++pr) {\n      python::list lres;\n      if (resultsAsMols) {\n        lres.append(pr->first);\n        lres.append(pr->second);\n      } else {\n        if (pr->first) {\n          lres.append(RDKit::MolToSmiles(*(pr->first), true));\n        } else {\n          lres.append(\"\");\n        }\n        lres.append(RDKit::MolToSmiles(*(pr->second), true));\n      }\n      pyres.append(python::tuple(lres));\n    }\n  }\n  return python::tuple(pyres);\n}\n\npython::tuple fragmentMolHelper3(const RDKit::ROMol& mol, python::object ob,\n                                 unsigned int minCuts, unsigned int maxCuts,\n                                 bool resultsAsMols) {\n  std::vector<std::pair<RDKit::ROMOL_SPTR, RDKit::ROMOL_SPTR>> tres;\n  std::unique_ptr<std::vector<unsigned int>> v =\n      pythonObjectToVect<unsigned int>(ob);\n  bool ok = RDKit::MMPA::fragmentMol(mol, tres, *v, minCuts, maxCuts);\n  python::list pyres;\n  if (ok) {\n    for (std::vector<std::pair<RDKit::ROMOL_SPTR, RDKit::ROMOL_SPTR>>::\n             const_iterator pr = tres.begin();\n         pr != tres.end(); ++pr) {\n      python::list lres;\n      if (resultsAsMols) {\n        lres.append(pr->first);\n        lres.append(pr->second);\n      } else {\n        if (pr->first) {\n          lres.append(RDKit::MolToSmiles(*(pr->first), true));\n        } else {\n          lres.append(\"\");\n        }\n        lres.append(RDKit::MolToSmiles(*(pr->second), true));\n      }\n      pyres.append(python::tuple(lres));\n    }\n  }\n  return python::tuple(pyres);\n}\n\n}  \/\/ namespace\n\nBOOST_PYTHON_MODULE(rdMMPA) {\n  python::scope().attr(\"__doc__\") =\n      \"Module containing a C++ implementation of code for doing MMPA\";\n\n  std::string docString =\n      \"Does the fragmentation necessary for an MMPA analysis\";\n  python::def(\"FragmentMol\", fragmentMolHelper,\n              (python::arg(\"mol\"), python::arg(\"maxCuts\") = 3,\n               python::arg(\"maxCutBonds\") = 20,\n               python::arg(\"pattern\") = \"[#6+0;!$(*=,#[!#6])]!@!=!#[*]\",\n               python::arg(\"resultsAsMols\") = true),\n              docString.c_str());\n\n  python::def(\"FragmentMol\", fragmentMolHelper2,\n              (python::arg(\"mol\"), python::arg(\"minCuts\"),\n               python::arg(\"maxCuts\"), python::arg(\"maxCutBonds\"),\n               python::arg(\"pattern\") = \"[#6+0;!$(*=,#[!#6])]!@!=!#[*]\",\n               python::arg(\"resultsAsMols\") = true),\n              docString.c_str());\n\n  python::def(\"FragmentMol\", fragmentMolHelper3,\n              (python::arg(\"mol\"), python::arg(\"bondsToCut\"),\n               python::arg(\"minCuts\") = 1, python::arg(\"maxCuts\") = 3,\n               python::arg(\"resultsAsMols\") = true),\n              docString.c_str());\n}\n","lang_cluster":"C++","length":136,"code_uid":"b1dc73f2f4df418281643423aa4f6702"}
{"diff_hunk":"@@ -77,6 +77,11 @@ namespace hip_impl {\n         return kd;\n     }\n \n+    std::size_t program_state::get_kernattribute(std::string kernelName)\n+    {\n+        return impl->getKernattribute(kernelName);\n+    }\n+\n     kernargs_size_align program_state::get_kernargs_size_align(std::uintptr_t kernel) {\n         kernargs_size_align t;\n         t.handle = reinterpret_cast<const void*>(&impl->kernargs_size_align(kernel));","old_code":"#include \"..\/include\/hip\/hcc_detail\/program_state.hpp\"\n\/\/ contains implementation of program_state_impl\n#include \"program_state.inl\"\n\n#include <hsa\/hsa.h>\n\n#include <cstdint>\n#include <stdexcept>\n#include <unordered_map>\n#include <vector>\n\nnamespace hip_impl {\n    \n    kernarg::kernarg() : impl(new kernarg_impl) { \n    }\n\n    kernarg::kernarg(kernarg&& k) : impl(k.impl) {\n        k.impl = nullptr;\n    }\n\n    kernarg::~kernarg() {\n        if (impl)\n            delete(impl);\n    }\n\n    std::uint8_t* kernarg::data() {\n        return impl->v.data();\n    }\n\n    std::size_t kernarg::size() {\n        return impl->v.size();\n    }\n\n    void kernarg::reserve(std::size_t c) {\n        impl->v.reserve(c);\n    }\n\n    void kernarg::resize(std::size_t c) {\n        impl->v.resize(c);\n    }\n\n    std::size_t kernargs_size_align::kernargs_size_align::size(std::size_t n) const{\n        return (*reinterpret_cast<const std::vector<std::pair<std::size_t, std::size_t>>*>(handle))[n].first;\n    }\n\n    std::size_t kernargs_size_align::alignment(std::size_t n) const{\n        return (*reinterpret_cast<const std::vector<std::pair<std::size_t, std::size_t>>*>(handle))[n].second;\n    }\n\n    program_state::program_state() : impl(new program_state_impl) {\n        if (!impl) hip_throw(std::runtime_error {\n            \"Unknown error when constructing program state.\"});\n    }\n\n    program_state::~program_state() {\n        delete(impl);\n    }\n\n    void* program_state::global_addr_by_name(const char* name) {\n        const auto it = impl->get_globals().find(name);\n        if (it == impl->get_globals().end())\n            return nullptr;\n        else\n            return it->second;\n    }\n\n    hsa_executable_t program_state::load_executable(const char* data,\n        const size_t data_size,\n        hsa_executable_t executable,\n        hsa_agent_t agent) {\n        return impl->load_executable(data, data_size, executable, agent);\n    }\n\n    hipFunction_t program_state::kernel_descriptor(std::uintptr_t function_address,\n        hsa_agent_t agent) {\n        auto& kd = impl->kernel_descriptor(function_address, agent);\n        return kd;\n    }\n\n    kernargs_size_align program_state::get_kernargs_size_align(std::uintptr_t kernel) {\n        kernargs_size_align t;\n        t.handle = reinterpret_cast<const void*>(&impl->kernargs_size_align(kernel));\n        return t;\n    }\n\n    std::mutex executables_cache_mutex;\n    std::vector<hsa_executable_t>& executables_cache(\n        std::string elf, hsa_isa_t isa, hsa_agent_t agent) {\n        static std::unordered_map<std::string,\n            std::unordered_map<hsa_isa_t,\n                std::unordered_map<hsa_agent_t, std::vector<hsa_executable_t>>>> cache;\n        return cache[elf][isa][agent];\n    }\n};\n","lang_cluster":"C++","length":94,"code_uid":"31dfda503ae847509d75d72f7a792b19"}
{"diff_hunk":"@@ -107,7 +107,11 @@ bool load_model::load_model_weights(const std::string& ckpt_dir,\n   closedir(weight_dir);\n \n   \/\/ load weights that appear in weight list.\n-  m->reload_weights(active_ckpt_dir, weight_list);\n+  \/\/ load weights that appear in weight list.\n+  \/\/ for(auto&& w : m_weights) {\n+  \/\/   w->load_from_save(latest,weight_list);\n+  \/\/ }\n+  \/\/  m->reload_weights(active_ckpt_dir, weight_list);\n   return true;\n }\n ","old_code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/ Copyright (c) 2014-2019, Lawrence Livermore National Security, LLC.\n\/\/ Produced at the Lawrence Livermore National Laboratory.\n\/\/ Written by the LBANN Research Team (B. Van Essen, et al.) listed in\n\/\/ the CONTRIBUTORS file. <lbann-dev@llnl.gov>\n\/\/\n\/\/ LLNL-CODE-697807.\n\/\/ All rights reserved.\n\/\/\n\/\/ This file is part of LBANN: Livermore Big Artificial Neural Network\n\/\/ Toolkit. For details, see http:\/\/software.llnl.gov\/LBANN or\n\/\/ https:\/\/github.com\/LLNL\/LBANN.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"Licensee\"); you\n\/\/ may not use this file except in compliance with the License.  You may\n\/\/ obtain a copy of the License at:\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n\/\/ implied. See the License for the specific language governing\n\/\/ permissions and limitations under the license.\n\/\/\n\/\/ load_model .hpp .cpp - Callbacks to load pretrained model(s)\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n#include \"lbann\/callbacks\/load_model.hpp\"\n#include \"lbann\/callbacks\/checkpoint.hpp\"\n#include \"lbann\/training_algorithms\/training_algorithm.hpp\"\n\n#include <callbacks.pb.h>\n#include <model.pb.h>\n\n\n#include <unistd.h>\n#include <dirent.h>\n\n#include <cstdlib>\n#include <fstream>\n#include <string>\n\nnamespace lbann {\nnamespace callback {\n\n\nvoid load_model::on_train_begin(model *m) {\n  if(!m_loaded) {\n    for (const auto& d : m_dirs) {\n      m_loaded = load_model_weights(d, \"\", m, true);\n      if(!m_loaded)  LBANN_ERROR(\"Unable to reload model on train begin\");\n    }\n  }\n}\n\nvoid load_model::on_test_begin(model *m) {\n  if(!m_loaded) {\n    for (const auto& d : m_dirs) {\n      m_loaded = load_model_weights(d, \"\", m, true);\n      if(!m_loaded)  LBANN_ERROR(\"Unable to reload model on test begin\");\n    }\n  }\n}\n\n\nbool load_model::load_model_weights(const std::string& ckpt_dir,\n                                    const std::string& alg_name,\n                                    model *m,\n                                    bool ckptdir_is_fullpath) {\n  std::vector<std::string> weight_list = std::vector<std::string>();\n  std::string active_ckpt_dir;\n  if(ckptdir_is_fullpath) {\n    active_ckpt_dir = add_delimiter(ckpt_dir);\n  }else {\n    size_t epochLast = std::numeric_limits<size_t>::max();;\n    size_t stepLast = std::numeric_limits<size_t>::max();;\n    execution_mode mode = execution_mode::invalid;\n    active_ckpt_dir = get_last_shared_checkpoint_filename(alg_name, ckpt_dir);\n\n    \/\/ get last epoch and step saved.\n    int success = read_latest(active_ckpt_dir, &mode, &epochLast, &stepLast);\n    if(!success) {\n      LBANN_WARNING(\"Unable to find the latest checkpoint \", active_ckpt_dir);\n      return false;\n    }\n    active_ckpt_dir = get_shared_checkpoint_dirname(alg_name, ckpt_dir, mode, epochLast, stepLast) + m->get_name() + '\/';\n  }\n\n  lbann_comm *comm = m->get_comm();\n  if(comm->am_trainer_master()) {\n    std::cout << \"Loading model weights from \" << active_ckpt_dir << std::endl;\n  }\n\n  DIR *weight_dir = opendir(active_ckpt_dir.c_str());\n  if(weight_dir == nullptr)\n  {\n    LBANN_WARNING(\"error opening \",  active_ckpt_dir);\n    return false;\n  }\n  \/\/ Populate weight list\n  struct dirent *weight_file;\n  while ((weight_file = readdir(weight_dir)) != nullptr){\n    if(!strncmp(weight_file->d_name,\"model_weights_\",14))\n      weight_list.push_back(std::string(weight_file->d_name));\n  }\n  closedir(weight_dir);\n\n  \/\/ load weights that appear in weight list.\n  m->reload_weights(active_ckpt_dir, weight_list);\n  return true;\n}\n\nstd::unique_ptr<callback_base>\nbuild_load_model_callback_from_pbuf(\n  const google::protobuf::Message& proto_msg, const std::shared_ptr<lbann_summary>&) {\n  const auto& params =\n    dynamic_cast<const lbann_data::Callback::CallbackLoadModel&>(proto_msg);\n  if(params.extension().size() != 0) {\n    return make_unique<load_model>(\n      parse_list<std::string>(params.dirs()),\n      params.extension());\n  }\n  else {\n    return make_unique<load_model>(\n      parse_list<std::string>(params.dirs()));\n  }\n}\n\n} \/\/ namespace callback\n} \/\/ namespace lbann\n","lang_cluster":"C++","length":131,"code_uid":"31fe74d3a3194377bfd2e01162847ddb"}
{"diff_hunk":"@@ -86,6 +86,14 @@ kvstore::ResultCode QueryBoundProcessor::processVertex(PartitionID partId, Verte\n             }\n         }\n         vResp.set_tag_data(std::move(td));\n+    } else if (tagContexts_.empty() && onlyVertexProps_) {\n+        std::vector<cpp2::TagData> td;\n+        auto ret = collectVertexProps(partId, vId, td);\n+        if (ret != kvstore::ResultCode::ERR_KEY_NOT_FOUND\n+                && ret != kvstore::ResultCode::SUCCEEDED) {\n+            return ret;\n+        }\n+        vResp.set_tag_data(std::move(td));\n     }\n \n     if (onlyVertexProps_) {","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"storage\/query\/QueryBoundProcessor.h\"\n#include <algorithm>\n#include \"time\/Duration.h\"\n#include \"dataman\/RowReader.h\"\n#include \"dataman\/RowWriter.h\"\n\nnamespace nebula {\nnamespace storage {\n\nkvstore::ResultCode QueryBoundProcessor::processEdgeImpl(const PartitionID partId,\n                                                         const VertexID vId,\n                                                         const EdgeType edgeType,\n                                                         const std::vector<PropContext>& props,\n                                                         FilterContext& fcontext,\n                                                         cpp2::VertexData& vdata) {\n    RowSetWriter rsWriter;\n    auto ret = collectEdgeProps(\n        partId, vId, edgeType, props, &fcontext,\n        [&, this](RowReader* reader, folly::StringPiece k, const std::vector<PropContext>& p) {\n            RowWriter writer(rsWriter.schema());\n            PropsCollector collector(&writer);\n            this->collectProps(reader, k, p, &fcontext, &collector);\n            rsWriter.addRow(writer);\n        });\n    if (ret != kvstore::ResultCode::SUCCEEDED) {\n        return ret;\n    }\n\n    if (!rsWriter.data().empty()) {\n        vdata.edge_data.emplace_back(apache::thrift::FragileConstructor::FRAGILE, edgeType,\n                                     std::move(rsWriter.data()));\n    }\n\n    return ret;\n}\n\nkvstore::ResultCode QueryBoundProcessor::processEdge(PartitionID partId, VertexID vId,\n                                                     FilterContext& fcontext,\n                                                     cpp2::VertexData& vdata) {\n    for (const auto& ec : edgeContexts_) {\n        RowSetWriter rsWriter;\n        auto edgeType = ec.first;\n        auto& props   = ec.second;\n        if (!props.empty()) {\n            CHECK(!onlyVertexProps_);\n\n            auto ret = processEdgeImpl(partId, vId, edgeType, props, fcontext, vdata);\n\n            if (ret != kvstore::ResultCode::SUCCEEDED) {\n                return ret;\n            }\n        }\n    }\n\n    return kvstore::ResultCode::SUCCEEDED;\n}\n\nkvstore::ResultCode QueryBoundProcessor::processVertex(PartitionID partId, VertexID vId) {\n    cpp2::VertexData vResp;\n    vResp.set_vertex_id(vId);\n    FilterContext fcontext;\n    if (!tagContexts_.empty()) {\n        std::vector<cpp2::TagData> td;\n        for (auto& tc : tagContexts_) {\n            RowWriter writer;\n            PropsCollector collector(&writer);\n            VLOG(3) << \"partId \" << partId << \", vId \" << vId << \", tagId \" << tc.tagId_\n                    << \", prop size \" << tc.props_.size();\n            auto ret = collectVertexProps(partId, vId, tc.tagId_, tc.props_, &fcontext, &collector);\n            if (ret == kvstore::ResultCode::ERR_KEY_NOT_FOUND) {\n                continue;\n            }\n            if (ret != kvstore::ResultCode::SUCCEEDED) {\n                return ret;\n            }\n            if (writer.size() > 1) {\n                td.emplace_back(apache::thrift::FragileConstructor::FRAGILE,\n                                tc.tagId_,\n                                writer.encode());\n            }\n        }\n        vResp.set_tag_data(std::move(td));\n    }\n\n    if (onlyVertexProps_) {\n        std::lock_guard<std::mutex> lg(this->lock_);\n        vertices_.emplace_back(std::move(vResp));\n        return kvstore::ResultCode::SUCCEEDED;\n    }\n\n    kvstore::ResultCode ret;\n    ret = processEdge(partId, vId, fcontext, vResp);\n\n    if (ret != kvstore::ResultCode::SUCCEEDED) {\n        return ret;\n    }\n\n    if (!vResp.edge_data.empty()) {\n        \/\/ Only return the vertex if edges existed.\n        std::lock_guard<std::mutex> lg(this->lock_);\n        vertices_.emplace_back(std::move(vResp));\n    }\n\n    return kvstore::ResultCode::SUCCEEDED;\n}\n\nvoid QueryBoundProcessor::onProcessFinished(int32_t retNum) {\n    (void)retNum;\n    resp_.set_vertices(std::move(vertices_));\n    std::unordered_map<TagID, nebula::cpp2::Schema> vertexSchema;\n    if (!this->tagContexts_.empty()) {\n        for (auto& tc : this->tagContexts_) {\n            nebula::cpp2::Schema respTag;\n            for (auto& prop : tc.props_) {\n                if (prop.returned_) {\n                    respTag.columns.emplace_back(\n                        columnDef(std::move(prop.prop_.name), prop.type_.type));\n                }\n            }\n\n            if (!respTag.columns.empty()) {\n                auto it = vertexSchema.find(tc.tagId_);\n                if (it == vertexSchema.end()) {\n                    vertexSchema.emplace(tc.tagId_, respTag);\n                }\n            }\n        }\n        if (!vertexSchema.empty()) {\n            resp_.set_vertex_schema(std::move(vertexSchema));\n        }\n    }\n\n    std::unordered_map<EdgeType, nebula::cpp2::Schema> edgeSchema;\n    if (!edgeContexts_.empty()) {\n        for (const auto& ec : edgeContexts_) {\n            nebula::cpp2::Schema respEdge;\n            RowSetWriter rsWriter;\n            auto& props = ec.second;\n            for (auto& p : props) {\n                respEdge.columns.emplace_back(columnDef(std::move(p.prop_.name), p.type_.type));\n            }\n\n            if (!respEdge.columns.empty()) {\n                auto it = edgeSchema.find(ec.first);\n                if (it == edgeSchema.end()) {\n                    edgeSchema.emplace(ec.first, std::move(respEdge));\n                }\n            }\n        }\n\n        if (!edgeSchema.empty()) {\n            resp_.set_edge_schema(std::move(edgeSchema));\n        }\n    }\n}\n\n}  \/\/ namespace storage\n}  \/\/ namespace nebula\n","lang_cluster":"C++","length":164,"code_uid":"1c152015157b4265993f090774cd5c9b"}
{"diff_hunk":"@@ -49,29 +49,15 @@ std::string FromClause::toString() const {\n }\n \n \n-std::string EdgeList::toString() const {\n-    std::string buf;\n-    buf.reserve(256);\n-\n-    for (auto &item : edges_) {\n-        buf += *item->edge();\n-        if (item->alias() != nullptr) {\n-            buf += \" AS \";\n-            buf += *item->alias();\n-        }\n-        buf += \",\";\n-    }\n-    buf.resize(buf.size() - 1);\n-\n-    return buf;\n-}\n-\n-\n std::string OverClause::toString() const {\n     std::string buf;\n     buf.reserve(256);\n     buf += \"OVER \";\n-    buf += edges_->toString();\n+    buf += *edge_;\n+    if (alias_ != nullptr) {\n+        buf += \" AS \";\n+        buf += *alias_;\n+    }\n     if (isReversely_) {\n         buf += \" REVERSELY\";\n     }","old_code":"\/* Copyright (c) 2018 - present, VE Software Inc. All rights reserved\n *\n * This source code is licensed under Apache 2.0 License\n *  (found in the LICENSE.Apache file in the root directory)\n *\/\n\n#include \"base\/Base.h\"\n#include \"parser\/Clauses.h\"\n\nnamespace nebula {\n\n\nstd::string StepClause::toString() const {\n    std::string buf;\n    buf.reserve(256);\n    if (isUpto()) {\n        buf += \"UPTO \";\n    }\n    buf += std::to_string(steps_);\n    buf += \" STEPS\";\n    return buf;\n}\n\nstd::string SourceNodeList::toString() const {\n    std::string buf;\n    buf.reserve(256);\n    for (auto id : nodes_) {\n        buf += std::to_string(id);\n        buf += \",\";\n    }\n    buf.resize(buf.size() - 1);\n    return buf;\n}\n\nstd::string FromClause::toString() const {\n    std::string buf;\n    buf.reserve(256);\n    buf += \"FROM \";\n    if (!isRef_) {\n        buf += srcNodeList_->toString();\n    } else {\n        buf += ref_->toString();\n    }\n    if (alias_ != nullptr) {\n        buf += \" AS \";\n        buf += *alias_;\n    }\n    return buf;\n}\n\n\nstd::string EdgeList::toString() const {\n    std::string buf;\n    buf.reserve(256);\n\n    for (auto &item : edges_) {\n        buf += *item->edge();\n        if (item->alias() != nullptr) {\n            buf += \" AS \";\n            buf += *item->alias();\n        }\n        buf += \",\";\n    }\n    buf.resize(buf.size() - 1);\n\n    return buf;\n}\n\n\nstd::string OverClause::toString() const {\n    std::string buf;\n    buf.reserve(256);\n    buf += \"OVER \";\n    buf += edges_->toString();\n    if (isReversely_) {\n        buf += \" REVERSELY\";\n    }\n    return buf;\n}\n\nstd::string WhereClause::toString() const {\n    std::string buf;\n    buf.reserve(256);\n    buf += \"WHERE \";\n    buf += filter_->toString();\n    return buf;\n}\n\nstd::string YieldColumns::toString() const {\n    std::string buf;\n    buf.reserve(256);\n    for (auto &col : columns_) {\n        auto *expr = col->expr();\n        buf += expr->toString();\n        if (col->alias() != nullptr) {\n            buf += \" AS \";\n            buf += *col->alias();\n        }\n        buf += \",\";\n    }\n    buf.resize(buf.size() -1 );\n    return buf;\n}\n\nstd::string YieldClause::toString() const {\n    std::string buf;\n    buf.reserve(256);\n    buf += \"YIELD \";\n    buf += yieldColumns_->toString();\n    return buf;\n}\n\n}   \/\/ namespace nebula\n","lang_cluster":"C++","length":113,"code_uid":"d9267c99ced94013b4a380d69e037aa2"}
{"diff_hunk":"@@ -21,6 +21,7 @@ OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWA\n  * HIT_END\n  *\/\n \n+#include <io.h>\n #include <iostream>\n #include <vector>\n #include <stdio.h>","old_code":"\/* Copyright (c) 2015-2016 Advanced Micro Devices, Inc. All rights reserved.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and\nassociated documentation files (the \"Software\"), to deal in the Software without restriction,\nincluding without limitation the rights to use, copy, modify, merge, publish, distribute,\nsublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial\nportions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\nNOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT.  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\nDAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT\nOF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. *\/\n\n\/* HIT_START\n * BUILD: %t %s test_common.cpp NVCC_OPTIONS -std=c++11\n * TEST: %t\n * HIT_END\n *\/\n\n#include <iostream>\n#include <vector>\n#include <stdio.h>\n#include <stdlib.h>\n#include <assert.h>\n#include <string>\n#include \"hip\/hip_runtime.h\"\n#include <chrono>\n#include <thread>\n#include \"test_common.h\"\n\nusing namespace std;\n\nconst string directed_dir = \"directed_tests\" + string(PATH_SEPERATOR_STR) + \"hipEnvVar\";\nconst string dir = \".\" + string(PATH_SEPERATOR_STR) + \"hipEnvVar\";\n\nint getDeviceNumber() {\n    char buff[512];\n    std::this_thread::sleep_for(std::chrono::milliseconds(10));\n    FILE* in = popen((directed_dir + \" -c\").c_str(), \"r\");\n    if(fgets(buff, 512, in) == NULL){\n        pclose(in);\n        \/\/Check at same level\n        in = popen((dir + \" -c\").c_str(), \"r\");\n        if(fgets(buff, 512, in) == NULL){\n            pclose(in);\n            return 1;\n        }\n    }\n    cout << buff;\n    pclose(in);\n    return atoi(buff);\n}\n\n\/\/ Query the current device ID remotely to hipEnvVar\nvoid getDevicePCIBusNumRemote(int deviceID, char* pciBusID) {    \n    std::this_thread::sleep_for(std::chrono::milliseconds(10));\n    FILE* in = popen((directed_dir + \" -d \" + std::to_string(deviceID)).c_str(), \"r\");\n    if(fgets(pciBusID, 100, in) == NULL){\n        pclose(in);\n        \/\/Check at same level\n        in = popen((dir + \" -d\").c_str(), \"r\");\n        if(fgets(pciBusID, 100, in) == NULL){\n            pclose(in);\n            return;\n        }\n    }\n    cout << pciBusID;\n    pclose(in);\n    return;\n}\n\n\/\/ Query the current device ID locally on AMD path\nvoid getDevicePCIBusNum(int deviceID, char* pciBusID) {\n    hipDevice_t deviceT;\n    hipDeviceGet(&deviceT, deviceID);\n\n    memset(pciBusID, 0, 100);\n    hipDeviceGetPCIBusId(pciBusID, 100, deviceT);\n}\n\nint main() {\n    unsetenv(HIP_VISIBLE_DEVICES_STR);\n    unsetenv(CUDA_VISIBLE_DEVICES_STR);\n    std::vector<std::string> devPCINum;\n    char pciBusID[100];\n    \/\/ collect the device pci bus ID for all devices\n    int totalDeviceNum = getDeviceNumber();\n    std::cout << \"The total number of available devices is \" << totalDeviceNum << std::endl\n              << \"Valid index range is 0 - \" << totalDeviceNum - 1 << std::endl;\n    for (int i = 0; i < totalDeviceNum; i++) {\n        getDevicePCIBusNum(i, pciBusID);\n        devPCINum.push_back(pciBusID);\n        std::cout << \"The collected device PCI Bus ID of Device \" << i << \" is \" << devPCINum.back()\n                  << std::endl;\n    }\n\n    \/\/ select each of the available devices to be the target device,\n    \/\/ query the returned device pci bus number, check if match the database\n    for (int i = 0; i < totalDeviceNum; i++) {\n        setenv(\"HIP_VISIBLE_DEVICES\", (char*)std::to_string(i).c_str(), 1);\n        setenv(\"CUDA_VISIBLE_DEVICES\", (char*)std::to_string(i).c_str(), 1);\n        getDevicePCIBusNumRemote(0, pciBusID);\n        if (devPCINum[i] == pciBusID) {\n            std::cout << \"The returned PciBusID is not correct\" << std::endl;\n            std::cout << \"Expected \" << devPCINum[i] << \", but get \" << pciBusID << endl;\n            exit(-1);\n        } else {\n            continue;\n        }\n    }\n\n    \/\/ check when set an invalid device number\n    setenv(\"HIP_VISIBLE_DEVICES\", \"1000,0,1\", 1);\n    setenv(\"CUDA_VISIBLE_DEVICES\", \"1000,0,1\", 1);\n    assert(getDeviceNumber() == 0);\n\n    if (totalDeviceNum > 2) {\n        setenv(\"HIP_VISIBLE_DEVICES\", \"0,1,1000,2\", 1);\n        setenv(\"CUDA_VISIBLE_DEVICES\", \"0,1,1000,2\", 1);\n        assert(getDeviceNumber() == 2);\n\n        setenv(\"HIP_VISIBLE_DEVICES\", \"0,1,2\", 1);\n        setenv(\"CUDA_VISIBLE_DEVICES\", \"0,1,2\", 1);\n        assert(getDeviceNumber() == 3);\n        \/\/ test if CUDA_VISIBLE_DEVICES will be accepted by the runtime\n        unsetenv(HIP_VISIBLE_DEVICES_STR);\n        unsetenv(CUDA_VISIBLE_DEVICES_STR);\n        setenv(\"CUDA_VISIBLE_DEVICES\", \"0,1,2\", 1);\n        assert(getDeviceNumber() == 3);\n    }\n\n    setenv(\"HIP_VISIBLE_DEVICES\", \"-100,0,1\", 1);\n    setenv(\"CUDA_VISIBLE_DEVICES\", \"-100,0,1\", 1);\n    assert(getDeviceNumber() == 0);\n\n    std::cout << \"PASSED\" << std::endl;\n    return 0;\n}\n","lang_cluster":"C++","length":142,"code_uid":"d0c9350e85b94604b33bc08bb51c39cf"}
{"diff_hunk":"@@ -43,6 +43,7 @@ DesktopSwitchConfiguration::DesktopSwitchConfiguration(PluginSettings *settings,\n \n     connect(ui->rowsSB, SIGNAL(valueChanged(int)), this, SLOT(rowsChanged(int)));\n     connect(ui->labelTypeCB, SIGNAL(currentIndexChanged(int)), this, SLOT(labelTypeChanged(int)));\n+    connect(ui->showOnlyActiveCB, &QAbstractButton::toggled, [this] (bool checked) { this->settings().setValue(\"showOnlyActive\", checked); });\n \n     loadDesktopsNames();\n }","old_code":"\/* BEGIN_COMMON_COPYRIGHT_HEADER\n * (c)LGPL2+\n *\n * LXQt - a lightweight, Qt based, desktop toolset\n * https:\/\/lxqt.org\n *\n * Copyright: 2015 LXQt team\n * Authors:\n *\n * This program or library is free software; you can redistribute it\n * and\/or modify it under the terms of the GNU Lesser General Public\n * License as published by the Free Software Foundation; either\n * version 2.1 of the License, or (at your option) any later version.\n *\n * This library is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * Lesser General Public License for more details.\n\n * You should have received a copy of the GNU Lesser General\n * Public License along with this library; if not, write to the\n * Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,\n * Boston, MA 02110-1301 USA\n *\n * END_COMMON_COPYRIGHT_HEADER *\/\n\n#include \"desktopswitchconfiguration.h\"\n#include \"ui_desktopswitchconfiguration.h\"\n#include <KWindowSystem>\n#include <QTimer>\n\nDesktopSwitchConfiguration::DesktopSwitchConfiguration(PluginSettings *settings, QWidget *parent) :\n    LXQtPanelPluginConfigDialog(settings, parent),\n    ui(new Ui::DesktopSwitchConfiguration)\n{\n    setAttribute(Qt::WA_DeleteOnClose);\n    setObjectName(QStringLiteral(\"DesktopSwitchConfigurationWindow\"));\n    ui->setupUi(this);\n\n    connect(ui->buttons, SIGNAL(clicked(QAbstractButton*)), this, SLOT(dialogButtonsAction(QAbstractButton*)));\n\n    loadSettings();\n\n    connect(ui->rowsSB, SIGNAL(valueChanged(int)), this, SLOT(rowsChanged(int)));\n    connect(ui->labelTypeCB, SIGNAL(currentIndexChanged(int)), this, SLOT(labelTypeChanged(int)));\n\n    loadDesktopsNames();\n}\n\nDesktopSwitchConfiguration::~DesktopSwitchConfiguration()\n{\n    delete ui;\n}\n\nvoid DesktopSwitchConfiguration::loadSettings()\n{\n    ui->rowsSB->setValue(settings().value(QStringLiteral(\"rows\"), 1).toInt());\n    ui->labelTypeCB->setCurrentIndex(settings().value(QStringLiteral(\"labelType\"), 0).toInt());\n}\n\nvoid DesktopSwitchConfiguration::loadDesktopsNames()\n{\n    int n = KWindowSystem::numberOfDesktops();\n    for (int i = 1; i <= n; i++)\n    {\n        QLineEdit *edit = new QLineEdit(KWindowSystem::desktopName(i), this);\n        ((QFormLayout *) ui->namesGroupBox->layout())->addRow(QStringLiteral(\"Desktop %1:\").arg(i), edit);\n\n        \/\/ C++11 rocks!\n        QTimer *timer = new QTimer(this);\n        timer->setInterval(400);\n        timer->setSingleShot(true);\n        connect(timer, &QTimer::timeout, [=] { KWindowSystem::setDesktopName(i, edit->text()); });\n        connect(edit, &QLineEdit::textEdited, [=] { timer->start(); });\n    }\n}\n\nvoid DesktopSwitchConfiguration::rowsChanged(int value)\n{\n    settings().setValue(QStringLiteral(\"rows\"), value);\n}\n\nvoid DesktopSwitchConfiguration::labelTypeChanged(int type)\n{\n    settings().setValue(QStringLiteral(\"labelType\"), type);\n}\n","lang_cluster":"C++","length":86,"code_uid":"617be50393394effb4360df65478e94a"}
{"diff_hunk":"@@ -1,6 +1,7 @@\n \/\/ unit tests creating LAMMPS instances via the library interface\n \n #include \"lammps.h\"\n+#define LAMMPS_LIB_MPI 1\n #include \"library.h\"\n #include <cstdio> \/\/ for stdin, stdout\n #include <mpi.h>","old_code":"\/\/ unit tests creating LAMMPS instances via the library interface\n\n#include \"lammps.h\"\n#include \"library.h\"\n#include <cstdio> \/\/ for stdin, stdout\n#include <mpi.h>\n#include <string>\n\n#include \"gmock\/gmock.h\"\n#include \"gtest\/gtest.h\"\n\n#include \"test_main.h\"\n\nusing ::testing::HasSubstr;\nusing ::testing::StartsWith;\n\nTEST(lammps_open, null_args)\n{\n    ::testing::internal::CaptureStdout();\n    void *handle       = lammps_open(0, NULL, MPI_COMM_WORLD, NULL);\n    std::string output = ::testing::internal::GetCapturedStdout();\n    EXPECT_THAT(output, StartsWith(\"LAMMPS (\"));\n    if (verbose) std::cout << output;\n    int mpi_init = 0;\n    MPI_Initialized(&mpi_init);\n    EXPECT_GT(mpi_init, 0);\n    LAMMPS_NS::LAMMPS *lmp = (LAMMPS_NS::LAMMPS *)handle;\n    EXPECT_EQ(lmp->world, MPI_COMM_WORLD);\n    EXPECT_EQ(lmp->infile, stdin);\n    EXPECT_EQ(lmp->screen, stdout);\n    EXPECT_NE(lmp->citeme, nullptr);\n    ::testing::internal::CaptureStdout();\n    lammps_close(handle);\n    output = ::testing::internal::GetCapturedStdout();\n    EXPECT_THAT(output, HasSubstr(\"Total wall time:\"));\n    if (verbose) std::cout << output;\n}\n\nTEST(lammps_open, with_args)\n{\n    const char *args[] = {\"liblammps\", \"-log\", \"none\", \"-nocite\"};\n    char **argv        = (char **)args;\n    int argc           = sizeof(args) \/ sizeof(char *);\n\n    \/\/ MPI is already initialized\n    MPI_Comm mycomm;\n    MPI_Comm_split(MPI_COMM_WORLD, 0, 1, &mycomm);\n    ::testing::internal::CaptureStdout();\n    void *alt_ptr;\n    void *handle       = lammps_open(argc, argv, mycomm, &alt_ptr);\n    std::string output = ::testing::internal::GetCapturedStdout();\n    EXPECT_THAT(output, StartsWith(\"LAMMPS (\"));\n    if (verbose) std::cout << output;\n    EXPECT_EQ(handle, alt_ptr);\n    LAMMPS_NS::LAMMPS *lmp = (LAMMPS_NS::LAMMPS *)handle;\n\n    \/\/ MPI STUBS uses no real communicators\n#if !defined(MPI_STUBS)\n    EXPECT_NE(lmp->world, MPI_COMM_WORLD);\n#endif\n\n    EXPECT_EQ(lmp->world, mycomm);\n    EXPECT_EQ(lmp->infile, stdin);\n    EXPECT_EQ(lmp->logfile, nullptr);\n    EXPECT_EQ(lmp->citeme, nullptr);\n    EXPECT_EQ(lmp->kokkos, nullptr);\n    EXPECT_EQ(lmp->atomKK, nullptr);\n    EXPECT_EQ(lmp->memoryKK, nullptr);\n    ::testing::internal::CaptureStdout();\n    lammps_close(handle);\n    output = ::testing::internal::GetCapturedStdout();\n    EXPECT_THAT(output, HasSubstr(\"Total wall time:\"));\n    if (verbose) std::cout << output;\n}\n\nTEST(lammps_open, with_kokkos)\n{\n    if (!LAMMPS_NS::LAMMPS::is_installed_pkg(\"KOKKOS\")) GTEST_SKIP();\n    const char *args[] = {\"liblammps\", \"-k\", \"on\", \"t\", \"2\", \"-sf\", \"kk\", \"-log\", \"none\"};\n    char **argv        = (char **)args;\n    int argc           = sizeof(args) \/ sizeof(char *);\n\n    ::testing::internal::CaptureStdout();\n    void *alt_ptr;\n    void *handle       = lammps_open(argc, argv, MPI_COMM_WORLD, &alt_ptr);\n    std::string output = ::testing::internal::GetCapturedStdout();\n    EXPECT_THAT(output, StartsWith(\"LAMMPS (\"));\n    if (verbose) std::cout << output;\n    EXPECT_EQ(handle, alt_ptr);\n    LAMMPS_NS::LAMMPS *lmp = (LAMMPS_NS::LAMMPS *)handle;\n\n    EXPECT_EQ(lmp->world, MPI_COMM_WORLD);\n    EXPECT_EQ(lmp->infile, stdin);\n    EXPECT_EQ(lmp->logfile, nullptr);\n    EXPECT_NE(lmp->citeme, nullptr);\n    EXPECT_EQ(lmp->num_package, 0);\n    EXPECT_NE(lmp->kokkos, nullptr);\n    EXPECT_NE(lmp->atomKK, nullptr);\n    EXPECT_NE(lmp->memoryKK, nullptr);\n    ::testing::internal::CaptureStdout();\n    lammps_close(handle);\n    output = ::testing::internal::GetCapturedStdout();\n    EXPECT_THAT(output, HasSubstr(\"Total wall time:\"));\n    if (verbose) std::cout << output;\n}\n\nTEST(lammps_open_no_mpi, no_screen)\n{\n    const char *args[] = {\"liblammps\", \"-log\", \"none\", \"-screen\", \"none\", \"-nocite\"};\n    char **argv        = (char **)args;\n    int argc           = sizeof(args) \/ sizeof(char *);\n\n    ::testing::internal::CaptureStdout();\n    void *alt_ptr;\n    void *handle       = lammps_open_no_mpi(argc, argv, &alt_ptr);\n    std::string output = ::testing::internal::GetCapturedStdout();\n    EXPECT_STREQ(output.c_str(), \"\");\n    EXPECT_EQ(handle, alt_ptr);\n    LAMMPS_NS::LAMMPS *lmp = (LAMMPS_NS::LAMMPS *)handle;\n\n    EXPECT_EQ(lmp->world, MPI_COMM_WORLD);\n    EXPECT_EQ(lmp->infile, stdin);\n    EXPECT_EQ(lmp->screen, nullptr);\n    EXPECT_EQ(lmp->logfile, nullptr);\n    EXPECT_EQ(lmp->citeme, nullptr);\n    EXPECT_EQ(lmp->suffix_enable, 0);\n\n    EXPECT_STREQ(lmp->exename, \"liblammps\");\n    EXPECT_EQ(lmp->num_package, 0);\n    ::testing::internal::CaptureStdout();\n    lammps_close(handle);\n    output = ::testing::internal::GetCapturedStdout();\n    EXPECT_STREQ(output.c_str(), \"\");\n}\n\nTEST(lammps_open_no_mpi, with_omp)\n{\n    if (!LAMMPS_NS::LAMMPS::is_installed_pkg(\"USER-OMP\")) GTEST_SKIP();\n    const char *args[] = {\"liblammps\", \"-pk\", \"omp\",  \"2\",    \"neigh\",  \"no\",\n                          \"-sf\",       \"omp\", \"-log\", \"none\", \"-nocite\"};\n    char **argv        = (char **)args;\n    int argc           = sizeof(args) \/ sizeof(char *);\n\n    ::testing::internal::CaptureStdout();\n    void *alt_ptr;\n    void *handle       = lammps_open_no_mpi(argc, argv, &alt_ptr);\n    std::string output = ::testing::internal::GetCapturedStdout();\n    EXPECT_THAT(output, StartsWith(\"LAMMPS (\"));\n    if (verbose) std::cout << output;\n    EXPECT_EQ(handle, alt_ptr);\n    LAMMPS_NS::LAMMPS *lmp = (LAMMPS_NS::LAMMPS *)handle;\n\n    EXPECT_EQ(lmp->world, MPI_COMM_WORLD);\n    EXPECT_EQ(lmp->infile, stdin);\n    EXPECT_EQ(lmp->logfile, nullptr);\n    EXPECT_EQ(lmp->citeme, nullptr);\n    EXPECT_EQ(lmp->suffix_enable, 1);\n    EXPECT_STREQ(lmp->suffix, \"omp\");\n    EXPECT_EQ(lmp->suffix2, nullptr);\n    EXPECT_STREQ(lmp->exename, \"liblammps\");\n    EXPECT_EQ(lmp->num_package, 1);\n    ::testing::internal::CaptureStdout();\n    lammps_close(handle);\n    output = ::testing::internal::GetCapturedStdout();\n    EXPECT_THAT(output, HasSubstr(\"Total wall time:\"));\n    if (verbose) std::cout << output;\n}\n\nTEST(lammps_open_fortran, no_args)\n{\n    \/\/ MPI is already initialized\n    MPI_Comm mycomm;\n    MPI_Comm_split(MPI_COMM_WORLD, 0, 1, &mycomm);\n    int fcomm = MPI_Comm_c2f(mycomm);\n    ::testing::internal::CaptureStdout();\n    void *handle       = lammps_open_fortran(0, NULL, fcomm);\n    std::string output = ::testing::internal::GetCapturedStdout();\n    EXPECT_THAT(output, StartsWith(\"LAMMPS (\"));\n    if (verbose) std::cout << output;\n    LAMMPS_NS::LAMMPS *lmp = (LAMMPS_NS::LAMMPS *)handle;\n\n    \/\/ MPI STUBS uses no real communicators\n#if !defined(MPI_STUBS)\n    EXPECT_NE(lmp->world, MPI_COMM_WORLD);\n#endif\n\n    EXPECT_EQ(lmp->world, mycomm);\n    EXPECT_EQ(lmp->infile, stdin);\n    EXPECT_EQ(lmp->screen, stdout);\n    EXPECT_NE(lmp->logfile, nullptr);\n    EXPECT_NE(lmp->citeme, nullptr);\n    ::testing::internal::CaptureStdout();\n    lammps_close(handle);\n    output = ::testing::internal::GetCapturedStdout();\n    EXPECT_THAT(output, HasSubstr(\"Total wall time:\"));\n    if (verbose) std::cout << output;\n}\n","lang_cluster":"C++","length":197,"code_uid":"81dc3cc097d846018dd9f3c9b15e4a64"}
{"diff_hunk":"@@ -28,7 +28,31 @@ int searchCircular(int arr[], int len, int num)\n \n int main()\n {\n-    int arr[] = {8, 9, 10, 11, 12, 2, 3, 4, 5, 6, 7}; \/\/ array must contain distinct elements for this to work\n-    cout << endl << searchCircular(arr, 11, 9);\n+    int num;\n+    cin >> num;\n+    int arr[num]; \/\/ array must contain distinct elements for this to work\n+    for (int i = 0; i < num; i++) {\n+        cin >> arr[i];\n+    } \n+    int desired;\n+    cin >> desired;\n+    cout << searchCircular(arr, num, desired); \/\/ Returns the index of element\n     return 0;\n }\n+\n+\/*\n+Input: \n+num = 4\n+array = {1, 4, 5, 6}\n+\n+Output:\n+1\n+\n+Input:\n+num = 5\n+array = {1, 3, 5, 7, 9}\n+desired = 2\n+\n+Output:\n+-1\n+*\/","old_code":"#include <iostream>\r\n\r\nusing namespace std;\r\n\r\nint searchCircular(int arr[], int len, int num)\r\n{\r\n    int low = 0;\r\n    int high = len - 1;\r\n    while(low <= high){\r\n        int mid = (low + (high - low) \/ 2);\r\n        if(arr[mid] == num) return mid;\r\n        else if(arr[mid] <= arr[high]){\r\n            if(num > arr[mid] && num <= arr[high]){\r\n                low = mid + 1;\r\n            }\r\n            else\r\n                high = mid - 1;\r\n        }\r\n        else if(arr[mid] > arr[low]){\r\n            if(num < arr[mid] && num >= arr[low])\r\n                high = mid - 1;\r\n            else\r\n                low = mid + 1;\r\n        }\r\n    }\r\n    return -1;\r\n}\r\n\r\nint main()\r\n{\r\n    int arr[] = {8, 9, 10, 11, 12, 2, 3, 4, 5, 6, 7}; \/\/ array must contain distinct elements for this to work\r\n    cout << endl << searchCircular(arr, 11, 9);\r\n    return 0;\r\n}\r\n","lang_cluster":"C++","length":34,"code_uid":"15bde8614af447908da6dd83f66b958a"}
{"diff_hunk":"@@ -49,19 +49,19 @@ static infer_result call_daal_kernel(const context_cpu& ctx,\n     auto arr_data = row_accessor<const Float>{ data }.pull();\n     auto arr_support_vectors =\n         row_accessor<const Float>{ trained_model.get_support_vectors() }.pull();\n-    auto arr_coefficients = row_accessor<const Float>{ trained_model.get_coefficients() }.pull();\n+    auto arr_coeffs = row_accessor<const Float>{ trained_model.get_coeffs() }.pull();\n \n     const auto daal_data =\n         interop::convert_to_daal_homogen_table(arr_data, row_count, column_count);\n     const auto daal_support_vectors = interop::convert_to_daal_homogen_table(arr_support_vectors,\n                                                                              support_vectors_count,\n                                                                              column_count);\n-    const auto daal_coefficients =\n-        interop::convert_to_daal_homogen_table(arr_coefficients, support_vectors_count, 1);\n+    const auto daal_coeffs =\n+        interop::convert_to_daal_homogen_table(arr_coeffs, support_vectors_count, 1);\n \n     auto daal_model = daal_model_builder{}\n                           .set_support_vectors(daal_support_vectors)\n-                          .set_coefficients(daal_coefficients)\n+                          .set_coeffs(daal_coeffs)\n                           .set_bias(trained_model.get_bias());\n \n     auto kernel_impl       = desc.get_kernel_impl()->get_impl();","old_code":"\/*******************************************************************************\n* Copyright 2020 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n#include <daal\/src\/algorithms\/svm\/svm_predict_kernel.h>\n\n#include \"oneapi\/dal\/algo\/svm\/backend\/cpu\/infer_kernel.hpp\"\n#include \"oneapi\/dal\/algo\/svm\/backend\/interop_model.hpp\"\n#include \"oneapi\/dal\/algo\/svm\/backend\/kernel_function_impl.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/common.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/error_converter.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/table_conversion.hpp\"\n\nnamespace oneapi::dal::svm::backend {\n\nusing std::int64_t;\nusing dal::backend::context_cpu;\n\nnamespace daal_svm             = daal::algorithms::svm;\nnamespace daal_kernel_function = daal::algorithms::kernel_function;\nnamespace interop              = dal::backend::interop;\n\ntemplate <typename Float, daal::CpuType Cpu>\nusing daal_svm_predict_kernel_t =\n    daal_svm::prediction::internal::SVMPredictImpl<daal_svm::prediction::defaultDense, Float, Cpu>;\n\ntemplate <typename Float>\nstatic infer_result call_daal_kernel(const context_cpu& ctx,\n                                     const descriptor_base& desc,\n                                     const model& trained_model,\n                                     const table& data) {\n    const int64_t row_count             = data.get_row_count();\n    const int64_t column_count          = data.get_column_count();\n    const int64_t support_vectors_count = trained_model.get_support_vector_count();\n\n    \/\/ TODO: data is table, not a homogen_table. Think better about accessor - is it enough to have just a row_accessor?\n    auto arr_data = row_accessor<const Float>{ data }.pull();\n    auto arr_support_vectors =\n        row_accessor<const Float>{ trained_model.get_support_vectors() }.pull();\n    auto arr_coefficients = row_accessor<const Float>{ trained_model.get_coefficients() }.pull();\n\n    const auto daal_data =\n        interop::convert_to_daal_homogen_table(arr_data, row_count, column_count);\n    const auto daal_support_vectors = interop::convert_to_daal_homogen_table(arr_support_vectors,\n                                                                             support_vectors_count,\n                                                                             column_count);\n    const auto daal_coefficients =\n        interop::convert_to_daal_homogen_table(arr_coefficients, support_vectors_count, 1);\n\n    auto daal_model = daal_model_builder{}\n                          .set_support_vectors(daal_support_vectors)\n                          .set_coefficients(daal_coefficients)\n                          .set_bias(trained_model.get_bias());\n\n    auto kernel_impl       = desc.get_kernel_impl()->get_impl();\n    const auto daal_kernel = kernel_impl->get_daal_kernel_function();\n\n    daal_svm::Parameter daal_parameter(daal_kernel);\n\n    auto arr_decision_function = array<Float>::empty(row_count * 1);\n    const auto daal_decision_function =\n        interop::convert_to_daal_homogen_table(arr_decision_function, row_count, 1);\n\n    interop::status_to_exception(\n        interop::call_daal_kernel<Float, daal_svm_predict_kernel_t>(ctx,\n                                                                    daal_data,\n                                                                    &daal_model,\n                                                                    *daal_decision_function,\n                                                                    &daal_parameter));\n\n    auto arr_label = array<Float>::empty(row_count * 1);\n    for (std::int64_t i = 0; i < row_count; ++i) {\n        arr_label[i] = arr_decision_function[i] >= 0 ? Float(1.0) : Float(-1.0);\n    }\n\n    return infer_result()\n        .set_decision_function(\n            homogen_table_builder{}.reset(arr_decision_function, row_count, 1).build())\n        .set_labels(homogen_table_builder{}.reset(arr_label, row_count, 1).build());\n}\n\ntemplate <typename Float>\nstatic infer_result infer(const context_cpu& ctx,\n                          const descriptor_base& desc,\n                          const infer_input& input) {\n    return call_daal_kernel<Float>(ctx, desc, input.get_model(), input.get_data());\n}\n\ntemplate <typename Float>\nstruct infer_kernel_cpu<Float, task::classification, method::by_default> {\n    infer_result operator()(const context_cpu& ctx,\n                            const descriptor_base& desc,\n                            const infer_input& input) const {\n        return infer<Float>(ctx, desc, input);\n    }\n};\n\ntemplate struct infer_kernel_cpu<float, task::classification, method::by_default>;\ntemplate struct infer_kernel_cpu<double, task::classification, method::by_default>;\n\n} \/\/ namespace oneapi::dal::svm::backend\n","lang_cluster":"C++","length":113,"code_uid":"6d4aa3bc600e481888d53d1e42815a6e"}
{"diff_hunk":"@@ -34,11 +34,11 @@ Status FetchExecutor::prepareYield() {\n         \/\/ such as YIELD 1+1, it has not type in schema, the type from the eval()\n         colTypes_.emplace_back(nebula::cpp2::SupportedType::UNKNOWN);\n         if (col->expr()->isAliasExpression()) {\n-            colNames_.emplace_back(*static_cast<InputPropertyExpression*>(col->expr())->prop());\n+            colNames_.emplace_back(*dynamic_cast<AliasPropertyExpression*>(col->expr())->prop());\n             continue;\n         } else if (col->expr()->isTypeCastingExpression()) {\n             \/\/ type cast\n-            auto exprPtr = static_cast<TypeCastingExpression*>(col->expr());\n+            auto exprPtr = dynamic_cast<TypeCastingExpression*>(col->expr());\n             colTypes_.back() = ColumnTypeToSupportedType(exprPtr->getType());\n         }\n ","old_code":"\/* Copyright (c) 2019 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"base\/Base.h\"\n#include \"FetchExecutor.h\"\n\nnamespace nebula {\nnamespace graph {\n\nStatus FetchExecutor::prepareYield() {\n    if (yieldClause_ == nullptr) {\n        setupColumns();\n    } else {\n        yields_ = yieldClause_->columns();\n        \/\/ TODO 'distinct' could always pushdown in fetch.\n        distinct_ = yieldClause_->isDistinct();\n    }\n\n    for (auto *col : yields_) {\n        col->expr()->setContext(expCtx_.get());\n        Status status = col->expr()->prepare();\n        if (!status.ok()) {\n            return status;\n        }\n        if (col->alias() == nullptr) {\n            resultColNames_.emplace_back(col->expr()->toString());\n        } else {\n            resultColNames_.emplace_back(*col->alias());\n        }\n\n        \/\/ such as YIELD 1+1, it has not type in schema, the type from the eval()\n        colTypes_.emplace_back(nebula::cpp2::SupportedType::UNKNOWN);\n        if (col->expr()->isAliasExpression()) {\n            colNames_.emplace_back(*static_cast<InputPropertyExpression*>(col->expr())->prop());\n            continue;\n        } else if (col->expr()->isTypeCastingExpression()) {\n            \/\/ type cast\n            auto exprPtr = static_cast<TypeCastingExpression*>(col->expr());\n            colTypes_.back() = ColumnTypeToSupportedType(exprPtr->getType());\n        }\n\n        colNames_.emplace_back(col->expr()->toString());\n    }\n\n    if (expCtx_->hasSrcTagProp() || expCtx_->hasDstTagProp()) {\n        return Status::SyntaxError(\n                    \"Only support form of alias.prop in fetch sentence.\");\n    }\n\n    auto aliasProps = expCtx_->aliasProps();\n    for (auto pair : aliasProps) {\n        if (pair.first != *labelName_) {\n            return Status::SyntaxError(\n                \"[%s.%s] tag not declared in %s.\",\n                    pair.first.c_str(), pair.second.c_str(), (*labelName_).c_str());\n        }\n    }\n\n    return Status::OK();\n}\n\nvoid FetchExecutor::setupColumns() {\n    DCHECK_NOTNULL(labelSchema_);\n    auto iter = labelSchema_->begin();\n    if (yieldColsHolder_ == nullptr) {\n        yieldColsHolder_ = std::make_unique<YieldColumns>();\n    }\n    while (iter) {\n        auto *ref = new std::string(\"\");\n        auto *alias = new std::string(*labelName_);\n        auto *prop = iter->getName();\n        Expression *expr =\n            new AliasPropertyExpression(ref, alias, new std::string(prop));\n        YieldColumn *column = new YieldColumn(expr);\n        yieldColsHolder_->addColumn(column);\n        yields_.emplace_back(column);\n        ++iter;\n    }\n}\n\nvoid FetchExecutor::setupResponse(cpp2::ExecutionResponse &resp) {\n    if (resp_ == nullptr) {\n        resp_ = std::make_unique<cpp2::ExecutionResponse>();\n    }\n    resp = std::move(*resp_);\n}\n\nvoid FetchExecutor::onEmptyInputs() {\n    if (onResult_) {\n        auto outputs = std::make_unique<InterimResult>(std::move(resultColNames_));\n        onResult_(std::move(outputs));\n    } else if (resp_ == nullptr) {\n        resp_ = std::make_unique<cpp2::ExecutionResponse>();\n    }\n    onFinish_();\n}\n\nStatus FetchExecutor::getOutputSchema(\n        meta::SchemaProviderIf *schema,\n        const RowReader *reader,\n        SchemaWriter *outputSchema) const {\n    if (expCtx_ == nullptr || resultColNames_.empty()) {\n        LOG(FATAL) << \"Input is empty\";\n    }\n    auto collector = std::make_unique<Collector>(schema);\n    auto &getters = expCtx_->getters();\n    getters.getAliasProp = [&] (const std::string&, const std::string &prop) {\n        return collector->getProp(prop, reader);\n    };\n    std::vector<VariantType> record;\n    for (auto *column : yields_) {\n        auto *expr = column->expr();\n        auto value = expr->eval();\n        if (!value.ok()) {\n            return value.status();\n        }\n        record.emplace_back(std::move(value.value()));\n    }\n\n    if (colTypes_.size() != record.size()) {\n        return Status::Error(\"Input is not equal to output\");\n    }\n    using nebula::cpp2::SupportedType;\n    auto index = 0u;\n    for (auto &it : colTypes_) {\n        SupportedType type;\n        if (it == SupportedType::UNKNOWN) {\n            switch (record[index].which()) {\n                case VAR_INT64:\n                    \/\/ all integers in InterimResult are regarded as type of INT\n                    type = SupportedType::INT;\n                    break;\n                case VAR_DOUBLE:\n                    type = SupportedType::DOUBLE;\n                    break;\n                case VAR_BOOL:\n                    type = SupportedType::BOOL;\n                    break;\n                case VAR_STR:\n                    type = SupportedType::STRING;\n                    break;\n                default:\n                    LOG(FATAL) << \"Unknown VariantType: \" << record[index].which();\n            }\n        } else {\n            type = it;\n        }\n\n        outputSchema->appendCol(resultColNames_[index], type);\n        index++;\n    }\n    return Status::OK();\n}\n\nvoid FetchExecutor::finishExecution(std::unique_ptr<RowSetWriter> rsWriter) {\n    auto outputs = std::make_unique<InterimResult>(std::move(resultColNames_));\n    if (rsWriter != nullptr) {\n        outputs->setInterim(std::move(rsWriter));\n    }\n\n    if (onResult_) {\n        onResult_(std::move(outputs));\n    } else {\n        resp_ = std::make_unique<cpp2::ExecutionResponse>();\n        auto colNames = outputs->getColNames();\n        resp_->set_column_names(std::move(colNames));\n        if (outputs->hasData()) {\n            auto ret = outputs->getRows();\n            if (!ret.ok()) {\n                LOG(ERROR) << \"Get rows failed: \" << ret.status();\n                onError_(std::move(ret).status());\n                return;\n            }\n            resp_->set_rows(std::move(ret).value());\n        }\n    }\n    DCHECK(onFinish_);\n    onFinish_();\n}\n}  \/\/ namespace graph\n}  \/\/ namespace nebula\n","lang_cluster":"C++","length":184,"code_uid":"6b3b8dbb48a842a1bfd46fe00f58b12b"}
{"diff_hunk":"@@ -5,29 +5,35 @@\n \n \"\"\"\n from __future__ import print_function\n-from rdkit import RDConfig\n+\n import unittest\n-import Parser\n-from win32com.client import Dispatch\n-from Numeric import *\n \n+from rdkit import RDConfig\n+import numpy as np\n+\n+try:\n+  from win32com.client import Dispatch\n+except ImportError:\n+  Dispatch = None\n \n+\n+@unittest.skipIf(Dispatch is None, 'Windows test')\n class TestCase(unittest.TestCase):\n \n   def setUp(self):\n     print('\\n%s: ' % self.shortDescription(), end='')\n \n   def testConnectToCOMServer(self):\n-    \" testing connection \"\n+    # \" testing connection \"\n     Dispatch('RD.DescCalc')\n \n   def testLoadCalculator(self):\n-    \" testing load \"\n+    # \" testing load \"\n     c = Dispatch('RD.DescCalc')\n     c.LoadCalculator(RDConfig.RDCodeDir + '\/ml\/descriptors\/test_data\/ferro.dsc')\n \n   def testNames(self):\n-    \" testing GetDescriptorNames \"\n+    # \" testing GetDescriptorNames \"\n     c = Dispatch('RD.DescCalc')\n     c.LoadCalculator(RDConfig.RDCodeDir + '\/ml\/descriptors\/test_data\/ferro.dsc')\n     names = c.GetDescriptorNames()","old_code":"#\n#  Copyright (C) 2001  greg Landrum\n#\n\"\"\" unit testing code for the descriptor COM server\n\n\"\"\"\nfrom __future__ import print_function\nfrom rdkit import RDConfig\nimport unittest\nimport Parser\nfrom win32com.client import Dispatch\nfrom Numeric import *\n\n\nclass TestCase(unittest.TestCase):\n\n  def setUp(self):\n    print('\\n%s: ' % self.shortDescription(), end='')\n\n  def testConnectToCOMServer(self):\n    \" testing connection \"\n    Dispatch('RD.DescCalc')\n\n  def testLoadCalculator(self):\n    \" testing load \"\n    c = Dispatch('RD.DescCalc')\n    c.LoadCalculator(RDConfig.RDCodeDir + '\/ml\/descriptors\/test_data\/ferro.dsc')\n\n  def testNames(self):\n    \" testing GetDescriptorNames \"\n    c = Dispatch('RD.DescCalc')\n    c.LoadCalculator(RDConfig.RDCodeDir + '\/ml\/descriptors\/test_data\/ferro.dsc')\n    names = c.GetDescriptorNames()\n    expectedNames = ('MAX_DED', 'has3d', 'has4d', 'has5d', 'elconc', 'atvol')\n    assert names == expectedNames, 'GetDescriptorNames failed (%s != %s)' % (repr(names),\n                                                                             repr(expectedNames))\n\n  def testCalc(self):\n    \" testing descriptor calculation \"\n    argV = ['CrPt3', 'fcc', 'AuCu3', 58.09549962, 1, 4, 0.228898, 8.876, 1]\n    nameV = ['Compound', 'Structure', 'Structure_Type', 'Volume', 'Z', 'Atoms_per_Formula_Unit',\n             'Hardness', 'RawDOS_Ef', 'IsFerromagnetic']\n    c = Dispatch('RD.DescCalc')\n    c.LoadCalculator(RDConfig.RDCodeDir + '\/ml\/descriptors\/test_data\/ferro.dsc')\n    descVect = array(c.CalcDescriptors(argV, nameV))\n    expected = array((3.67481803894, 1, 0, 1, 0.619669341609, 14.523874905))\n    diffV = abs(descVect - expected)\n    assert max(diffV) < 0.0001, 'bad descriptors: %s, %s' % (str(expected), str(descVect))\n\n\ndef TestSuite():\n  suite = unittest.TestSuite()\n  suite.addTest(TestCase('testConnect'))\n  suite.addTest(TestCase('testLoad'))\n  suite.addTest(TestCase('testNames'))\n  suite.addTest(TestCase('testCalc'))\n  return suite\n\n\nif __name__ == '__main__':\n  suite = TestSuite()\n  unittest.TextTestRunner().run(suite)\n","lang_cluster":"C++","length":62,"code_uid":"70c0b47a5ccf4632861bb3b1a539d323"}
{"diff_hunk":"@@ -66,7 +66,8 @@ void mesh_reader::load() {\n   select_subset_of_data();\n }\n \n-bool mesh_reader::fetch_datum(CPUMat& X, int data_id, int mb_idx, int tid) {\n+bool mesh_reader::fetch_datum(CPUMat& X, int data_id, int mb_idx, thread_pool& io_thread_pool) {\n+  \/\/  int tid = io_thread_pool.get_local_thread_id();\n   if (m_random_flips) {\n     fast_rng_gen& gen = get_fast_generator();\n     std::uniform_int_distribution<int> dist(0, 1);","old_code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/ Copyright (c) 2014-2016, Lawrence Livermore National Security, LLC.\n\/\/ Produced at the Lawrence Livermore National Laboratory.\n\/\/ Written by the LBANN Research Team (B. Van Essen, et al.) listed in\n\/\/ the CONTRIBUTORS file. <lbann-dev@llnl.gov>\n\/\/\n\/\/ LLNL-CODE-697807.\n\/\/ All rights reserved.\n\/\/\n\/\/ This file is part of LBANN: Livermore Big Artificial Neural Network\n\/\/ Toolkit. For details, see http:\/\/software.llnl.gov\/LBANN or\n\/\/ https:\/\/github.com\/LLNL\/LBANN.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"Licensee\"); you\n\/\/ may not use this file except in compliance with the License.  You may\n\/\/ obtain a copy of the License at:\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n\/\/ implied. See the License for the specific language governing\n\/\/ permissions and limitations under the license.\n\/\/\n\/\/ data_reader_mesh .hpp .cpp - data reader for mesh data\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n#include \"lbann\/data_readers\/data_reader_mesh.hpp\"\n#include \"lbann\/utils\/glob.hpp\"\n#include <omp.h>\n\nnamespace lbann {\n\nmesh_reader::mesh_reader(bool shuffle)\n  : generic_data_reader(shuffle) {}\n\nvoid mesh_reader::load() {\n  if (m_data_height == 0 || m_data_width == 0) {\n    throw lbann_exception(\"mesh_reader: data shape must be non-zero\");\n  }\n  \/\/ Compute total number of samples based on number of targets.\n  std::vector<std::string> matches = glob(\n    get_file_dir() + m_target_name + m_suffix + \"\/*.bin\");\n  if (matches.size() == 0) {\n    throw lbann_exception(\"mesh_reader: could not find any targets\");\n  }\n  m_num_samples = matches.size();\n  \/\/ Set up buffers to load data into.\n  m_load_bufs.resize(omp_get_max_threads());\n  for (auto&& buf : m_load_bufs) {\n    buf.resize(m_data_height * m_data_width);\n  }\n  \/\/ Set up the format string.\n  if (std::pow(10, m_index_length) <= m_num_samples) {\n    throw lbann_exception(\"mesh_reader: index length too small\");\n  }\n  m_index_format_str = \"%0\" + std::to_string(m_index_length) + \"d\";\n  \/\/ Set up to record flipping if needed.\n  if (m_random_flips) {\n    m_flip_choices.resize(m_num_samples);\n  }\n  \/\/ Reset indices.\n  m_shuffled_indices.resize(m_num_samples);\n  std::iota(m_shuffled_indices.begin(), m_shuffled_indices.end(), 0);\n  select_subset_of_data();\n}\n\nbool mesh_reader::fetch_datum(CPUMat& X, int data_id, int mb_idx, int tid) {\n  if (m_random_flips) {\n    fast_rng_gen& gen = get_fast_generator();\n    std::uniform_int_distribution<int> dist(0, 1);\n    m_flip_choices[data_id].first = dist(gen);\n    m_flip_choices[data_id].second = dist(gen);\n  }\n  for (size_t i = 0; i < m_channels.size(); ++i) {\n    Mat X_view = El::View(\n      X, El::IR(i*m_data_height*m_data_width, (i+1)*m_data_height*m_data_width),\n      El::IR(mb_idx));\n    load_file(data_id, m_channels[i], X_view);\n  }\n  return true;\n}\n\nbool mesh_reader::fetch_response(CPUMat& Y, int data_id, int mb_idx, int tid) {\n  Mat Y_view = El::View(Y, El::ALL, El::IR(mb_idx));\n  load_file(data_id, m_target_name, Y_view);\n  return true;\n}\n\nvoid mesh_reader::load_file(int data_id, const std::string channel, Mat& mat) {\n  const std::string filename = construct_filename(channel, data_id);\n  std::ifstream f(filename, std::ios::binary);\n  if (f.fail()) {\n    throw lbann_exception(\"mesh_reader: failed to open \" + filename);\n  }\n  \/\/ Load into a local buffer.\n  float* buf = m_load_bufs[omp_get_thread_num()].data();\n  if (!f.read((char*) buf, m_data_height * m_data_width * sizeof(float))) {\n    throw lbann_exception(\"mesh_reader: failed to read \" + filename);\n  }\n  if (std::is_same<float, DataType>::value) {\n    \/\/ Need to transpose from row-major to column-major order.\n    Mat tmp_mat(m_data_width, m_data_height, buf, m_data_width);\n    Mat mat_reshape(m_data_height, m_data_width, mat.Buffer(), m_data_height);\n    El::Transpose(tmp_mat, mat_reshape);\n    \/\/ Flip if needed.\n    if (m_random_flips) {\n      if (m_flip_choices[data_id].first) {\n        horizontal_flip(mat_reshape);\n      }\n      if (m_flip_choices[data_id].second) {\n        vertical_flip(mat_reshape);\n      }\n    }\n  } else {\n    \/\/ Need to transpose and convert from float. Not yet supported.\n    throw lbann_exception(\"mesh_reader: does not support DataType != float\");\n  }\n}\n\nstd::string mesh_reader::construct_filename(std::string channel, int data_id) {\n  std::string filename = get_file_dir() + channel + m_suffix + \"\/\" + channel;\n  char idx[m_index_length + 1];\n  std::snprintf(idx, m_index_length + 1, m_index_format_str.c_str(), data_id);\n  return filename + std::string(idx) + \".bin\";\n}\n\nvoid mesh_reader::horizontal_flip(CPUMat& mat) {\n  \/\/ TODO: Could probably optimize this for better locality.\n  const El::Int height = mat.Height();\n  const El::Int width = mat.Width();\n  for (El::Int row = 0; row < height; ++row) {\n    for (El::Int col = 0; col < (width \/ 2); ++col) {\n      DataType tmp = mat(row, col);\n      mat(row, col) = mat(row, width - col - 1);\n      mat(row, width - col - 1) = tmp;\n    }\n  }\n}\n\nvoid mesh_reader::vertical_flip(CPUMat& mat) {\n  \/\/ TODO: Could probably optimize this for better locality.\n  const El::Int height = mat.Height();\n  const El::Int width = mat.Width();\n  for (El::Int row = 0; row < (height \/ 2); ++row) {\n    for (El::Int col = 0; col < width; ++col) {\n      DataType tmp = mat(row, col);\n      mat(row, col) = mat(height - row - 1, col);\n      mat(height - row - 1, col) = tmp;\n    }\n  }\n}\n\n}  \/\/ namespace lbann\n","lang_cluster":"C++","length":155,"code_uid":"c6ef94b9268844f3927fe88179a28632"}
{"diff_hunk":"@@ -18,7 +18,7 @@ class DedupTest : public QueryTestBase {\n   void SetUp() override { QueryTestBase::SetUp(); }\n };\n \n-#define DEDUP_RESUTL_CHECK(inputName, outputName, sentence, expected)                   \\\n+#define DEDUP_RESULT_CHECK(inputName, outputName, sentence, expected)                   \\\n   do {                                                                                  \\\n     qctx_->symTable()->newVariable(outputName);                                         \\\n     auto yieldSentence = getYieldSentence(sentence, qctx_.get());                       \\","old_code":"\/* Copyright (c) 2020 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License.\n *\/\n#include <gtest\/gtest.h>\n\n#include \"graph\/context\/QueryContext.h\"\n#include \"graph\/executor\/query\/DedupExecutor.h\"\n#include \"graph\/executor\/query\/ProjectExecutor.h\"\n#include \"graph\/executor\/test\/QueryTestBase.h\"\n#include \"graph\/planner\/plan\/Query.h\"\n\nnamespace nebula {\nnamespace graph {\n\nclass DedupTest : public QueryTestBase {\n public:\n  void SetUp() override { QueryTestBase::SetUp(); }\n};\n\n#define DEDUP_RESUTL_CHECK(inputName, outputName, sentence, expected)                   \\\n  do {                                                                                  \\\n    qctx_->symTable()->newVariable(outputName);                                         \\\n    auto yieldSentence = getYieldSentence(sentence, qctx_.get());                       \\\n    auto* dedupNode = Dedup::make(qctx_.get(), nullptr);                                \\\n    dedupNode->setInputVar(inputName);                                                  \\\n    dedupNode->setOutputVar(outputName);                                                \\\n    auto dedupExec = std::make_unique<DedupExecutor>(dedupNode, qctx_.get());           \\\n    if (!expected.colNames.empty()) {                                                   \\\n      EXPECT_TRUE(dedupExec->execute().get().ok());                                     \\\n    } else {                                                                            \\\n      EXPECT_FALSE(dedupExec->execute().get().ok());                                    \\\n      return;                                                                           \\\n    }                                                                                   \\\n    auto& dedupResult = qctx_->ectx()->getResult(dedupNode->outputVar());               \\\n    EXPECT_EQ(dedupResult.state(), Result::State::kSuccess);                            \\\n                                                                                        \\\n    dedupNode->setInputVar(outputName);                                                 \\\n    auto* project = Project::make(qctx_.get(), nullptr, yieldSentence->yieldColumns()); \\\n    project->setInputVar(dedupNode->outputVar());                                       \\\n    auto colNames = expected.colNames;                                                  \\\n    project->setColNames(std::move(colNames));                                          \\\n                                                                                        \\\n    auto proExe = std::make_unique<ProjectExecutor>(project, qctx_.get());              \\\n    EXPECT_TRUE(proExe->execute().get().ok());                                          \\\n    auto& proSesult = qctx_->ectx()->getResult(project->outputVar());                   \\\n                                                                                        \\\n    EXPECT_EQ(proSesult.value().getDataSet(), expected);                                \\\n    EXPECT_EQ(proSesult.state(), Result::State::kSuccess);                              \\\n  } while (false)\n\nTEST_F(DedupTest, TestSequential) {\n  DataSet expected({\"vid\", \"name\", \"age\", \"dst\", \"start\", \"end\"});\n  expected.emplace_back(Row({\"Ann\", \"Ann\", 18, \"School1\", 2010, 2014}));\n  expected.emplace_back(Row({\"Joy\", \"Joy\", Value::kNullValue, \"School2\", 2009, 2012}));\n  expected.emplace_back(Row({\"Tom\", \"Tom\", 20, \"School2\", 2008, 2012}));\n  expected.emplace_back(Row({\"Kate\", \"Kate\", 19, \"School2\", 2009, 2013}));\n  expected.emplace_back(Row({\"Lily\", \"Lily\", 20, \"School2\", 2009, 2012}));\n\n  auto sentence =\n      \"YIELD DISTINCT $-.vid as vid, $-.v_name as name, $-.v_age as age, \"\n      \"$-.v_dst as dst, $-.e_start_year as start, $-.e_end_year as end\";\n  DEDUP_RESUTL_CHECK(\"input_sequential\", \"dedup_sequential\", sentence, expected);\n}\n\nTEST_F(DedupTest, TestEmpty) {\n  DataSet expected({\"name\"});\n  DEDUP_RESUTL_CHECK(\"empty\", \"dedup_sequential\", \"YIELD DISTINCT $-.v_dst as name\", expected);\n}\n\nTEST_F(DedupTest, WrongTypeIterator) {\n  DataSet expected;\n  DEDUP_RESUTL_CHECK(\n      \"input_neighbor\", \"dedup_sequential\", \"YIELD DISTINCT $-.v_dst as name\", expected);\n}\n}  \/\/ namespace graph\n}  \/\/ namespace nebula\n","lang_cluster":"C++","length":77,"code_uid":"0b4649b1269f4293850e2a04a8a698eb"}
{"diff_hunk":"@@ -58,7 +58,10 @@ class TopNContainer(object):\n     return self.extras\n \n   def __len__(self):\n-    return self._size\n+    if self._size >= 0:\n+      return self._size\n+    else:\n+      return len(self.best)\n \n   def __getitem__(self, which):\n     return self.best[which], self.extras[which]","old_code":"# $Id$\n#\n#  Copyright (C) 2003-2013 Rational Discovery LLC\n#\n#   @@ All Rights Reserved @@\n#  This file is part of the RDKit.\n#  The contents are covered by the terms of the BSD license\n#  which is included in the file license.txt, found at the root\n#  of the RDKit source tree.\n#\nfrom __future__ import print_function\nimport bisect\n\n\nclass TopNContainer(object):\n  \"\"\" maintains a sorted list of a particular number of data elements.\n\n  \"\"\"\n\n  def __init__(self, size, mostNeg=-1e99):\n    \"\"\"\n    if size is negative, all entries will be kept in sorted order\n    \"\"\"\n    self._size = size\n    if (size >= 0):\n      self.best = [mostNeg] * self._size\n      self.extras = [None] * self._size\n    else:\n      self.best = []\n      self.extras = []\n\n  def Insert(self, val, extra=None):\n    \"\"\" only does the insertion if val fits \"\"\"\n    if self._size >= 0:\n      if val > self.best[0]:\n        idx = bisect.bisect(self.best, val)\n        # insert the new element\n        if idx == self._size:\n          self.best.append(val)\n          self.extras.append(extra)\n        else:\n          self.best.insert(idx, val)\n          self.extras.insert(idx, extra)\n        # and pop off the head\n        self.best.pop(0)\n        self.extras.pop(0)\n    else:\n      idx = bisect.bisect(self.best, val)\n      self.best.insert(idx, val)\n      self.extras.insert(idx, extra)\n\n  def GetPts(self):\n    \"\"\" returns our set of points \"\"\"\n    return self.best\n\n  def GetExtras(self):\n    \"\"\" returns our set of extras \"\"\"\n    return self.extras\n\n  def __len__(self):\n    return self._size\n\n  def __getitem__(self, which):\n    return self.best[which], self.extras[which]\n\n  def reverse(self):\n    self.best.reverse()\n    self.extras.reverse()\n\n\nif __name__ == '__main__':\n  import random\n  pts = [int(100 * random.random()) for x in range(10)]\n\n  c = TopNContainer(4)\n  for pt in pts:\n    c.Insert(pt, extra=str(pt))\n  print(c.GetPts())\n  print(c.GetExtras())\n","lang_cluster":"C++","length":79,"code_uid":"1f95e35db83c499db595e3ebb6ca16c8"}
{"diff_hunk":"@@ -16,8 +16,9 @@ namespace nebula {\n namespace storage {\n \n void AddVerticesProcessor::process(const cpp2::AddVerticesRequest& req) {\n-    auto version =\n-        std::numeric_limits<int64_t>::max() - time::WallClock::fastNowInMicroSec();\n+    auto version = FLAGS_enable_multi_versions ?\n+        std::numeric_limits<int64_t>::max() - time::WallClock::fastNowInMicroSec() :\n+        std::numeric_limits<int64_t>::max();\n     \/\/ Switch version to big-endian, make sure the key is in ordered.\n     version = folly::Endian::big(version);\n ","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"storage\/mutate\/AddVerticesProcessor.h\"\n#include \"utils\/NebulaKeyUtils.h\"\n#include <algorithm>\n#include <limits>\n#include \"time\/WallClock.h\"\n\nDECLARE_bool(enable_vertex_cache);\n\nnamespace nebula {\nnamespace storage {\n\nvoid AddVerticesProcessor::process(const cpp2::AddVerticesRequest& req) {\n    auto version =\n        std::numeric_limits<int64_t>::max() - time::WallClock::fastNowInMicroSec();\n    \/\/ Switch version to big-endian, make sure the key is in ordered.\n    version = folly::Endian::big(version);\n\n    const auto& partVertices = req.get_parts();\n    spaceId_ = req.get_space_id();\n    callingNum_ = partVertices.size();\n    auto iRet = indexMan_->getTagIndexes(spaceId_);\n    if (iRet.ok()) {\n        indexes_ = std::move(iRet).value();\n    }\n\n    CHECK_NOTNULL(kvstore_);\n    if (indexes_.empty()) {\n        std::for_each(partVertices.begin(), partVertices.end(), [&](auto& pv) {\n            auto partId = pv.first;\n            const auto& vertices = pv.second;\n            std::vector<kvstore::KV> data;\n            std::for_each(vertices.begin(), vertices.end(), [&](auto& v) {\n                const auto& tags = v.get_tags();\n                std::for_each(tags.begin(), tags.end(), [&](auto& tag) {\n                    VLOG(3) << \"PartitionID: \" << partId << \", VertexID: \" << v.get_id()\n                            << \", TagID: \" << tag.get_tag_id() << \", TagVersion: \" << version;\n                    auto key = NebulaKeyUtils::vertexKey(partId, v.get_id(),\n                                                         tag.get_tag_id(), version);\n                    data.emplace_back(std::move(key), std::move(tag.get_props()));\n                    if (FLAGS_enable_vertex_cache && vertexCache_ != nullptr) {\n                        vertexCache_->evict(std::make_pair(v.get_id(), tag.get_tag_id()), partId);\n                        VLOG(3) << \"Evict cache for vId \" << v.get_id()\n                                << \", tagId \" << tag.get_tag_id();\n                    }\n                });\n            });\n            doPut(spaceId_, partId, std::move(data));\n        });\n    } else {\n        std::for_each(partVertices.begin(), partVertices.end(), [&](auto &pv) {\n            auto partId = pv.first;\n            auto atomic = [version, partId, vertices = std::move(pv.second), this]()\n                          -> folly::Optional<std::string> {\n                return addVertices(version, partId, vertices);\n            };\n            auto callback = [partId, this](kvstore::ResultCode code) {\n                handleAsync(spaceId_, partId, code);\n            };\n            this->kvstore_->asyncAtomicOp(spaceId_, partId, atomic, callback);\n        });\n    }\n}\n\nstd::string AddVerticesProcessor::addVertices(int64_t version, PartitionID partId,\n                                              const std::vector<cpp2::Vertex>& vertices) {\n    std::unique_ptr<kvstore::BatchHolder> batchHolder = std::make_unique<kvstore::BatchHolder>();\n    \/*\n     * Define the map newIndexes to avoid inserting duplicate vertex.\n     * This map means :\n     * map<vertex_unique_key, prop_value> ,\n     * -- vertex_unique_key is only used as the unique key , for example:\n     * insert below vertices in the same request:\n     *     kv(part1_vid1_tag1 , v1)\n     *     kv(part1_vid1_tag1 , v2)\n     *     kv(part1_vid1_tag1 , v3)\n     *     kv(part1_vid1_tag1 , v4)\n     *\n     * Ultimately, kv(part1_vid1_tag1 , v4) . It's just what I need.\n     *\/\n    std::map<std::string, std::string> newVertices;\n    std::for_each(vertices.begin(), vertices.end(), [&](auto& v) {\n        auto vId = v.get_id();\n        const auto& tags = v.get_tags();\n        std::for_each(tags.begin(), tags.end(), [&](auto& tag) {\n            auto tagId = tag.get_tag_id();\n            auto prop = tag.get_props();\n            VLOG(3) << \"PartitionID: \" << partId << \", VertexID: \" << vId\n                    << \", TagID: \" << tagId << \", TagVersion: \" << version;\n            auto key = NebulaKeyUtils::vertexKey(partId, vId, tagId, version);\n            newVertices[key] = std::move(prop);\n            if (FLAGS_enable_vertex_cache && this->vertexCache_ != nullptr) {\n                this->vertexCache_->evict(std::make_pair(vId, tagId), partId);\n                VLOG(3) << \"Evict cache for vId \" << vId << \", tagId \" << tagId;\n            }\n        });\n    });\n\n    for (auto& v : newVertices) {\n        std::string val;\n        std::unique_ptr<RowReader> nReader;\n        auto tagId = NebulaKeyUtils::getTagId(v.first);\n        auto vId = NebulaKeyUtils::getVertexId(v.first);\n        for (auto& index : indexes_) {\n            if (tagId == index->get_schema_id().get_tag_id()) {\n                \/*\n                 * step 1 , Delete old version index if exists.\n                 *\/\n                if (val.empty()) {\n                    val = findObsoleteIndex(partId, vId, tagId);\n                }\n                if (!val.empty()) {\n                    auto reader = RowReader::getTagPropReader(this->schemaMan_,\n                                                              val,\n                                                              spaceId_,\n                                                              tagId);\n                    if (reader == nullptr) {\n                        LOG(WARNING) << \"Bad format row\";\n                        return \"\";\n                    }\n                    auto oi = indexKey(partId, vId, reader.get(), index);\n                    if (!oi.empty()) {\n                        batchHolder->remove(std::move(oi));\n                    }\n                }\n                \/*\n                 * step 2 , Insert new vertex index\n                 *\/\n                if (nReader == nullptr) {\n                    nReader = RowReader::getTagPropReader(this->schemaMan_,\n                                                          v.second,\n                                                          spaceId_,\n                                                          tagId);\n                    if (nReader == nullptr) {\n                        LOG(WARNING) << \"Bad format row\";\n                        return \"\";\n                    }\n                }\n                auto ni = indexKey(partId, vId, nReader.get(), index);\n                if (!ni.empty()) {\n                    batchHolder->put(std::move(ni), \"\");\n                }\n            }\n        }\n        \/*\n         * step 3 , Insert new vertex data\n         *\/\n        auto key = v.first;\n        auto prop = v.second;\n        batchHolder->put(std::move(key), std::move(prop));\n    }\n    return encodeBatchValue(batchHolder->getBatch());\n}\n\nstd::string AddVerticesProcessor::findObsoleteIndex(PartitionID partId,\n                                                    VertexID vId,\n                                                    TagID tagId) {\n    auto prefix = NebulaKeyUtils::vertexPrefix(partId, vId, tagId);\n    std::unique_ptr<kvstore::KVIterator> iter;\n    auto ret = kvstore_->prefix(this->spaceId_, partId, prefix, &iter);\n    if (ret != kvstore::ResultCode::SUCCEEDED) {\n        LOG(ERROR) << \"Error! ret = \" << static_cast<int32_t>(ret)\n                   << \", spaceId \" << this->spaceId_;\n        return \"\";\n    }\n    if (iter && iter->valid()) {\n        return iter->val().str();\n    }\n    return \"\";\n}\n\nstd::string AddVerticesProcessor::indexKey(PartitionID partId,\n                                           VertexID vId,\n                                           RowReader* reader,\n                                           std::shared_ptr<nebula::cpp2::IndexItem> index) {\n    auto values = collectIndexValues(reader, index->get_fields());\n    if (!values.ok()) {\n        return \"\";\n    }\n    return NebulaKeyUtils::vertexIndexKey(partId,\n                                          index->get_index_id(),\n                                          vId, values.value());\n}\n\n}  \/\/ namespace storage\n}  \/\/ namespace nebula\n","lang_cluster":"C++","length":191,"code_uid":"cc1cd9a3c7f04414b0629c984e370132"}
{"diff_hunk":"@@ -26,7 +26,6 @@ std::string MetaUtils::spaceKey(GraphSpaceID spaceId) {\n     return key;\n }\n \n-\n std::string MetaUtils::spaceVal(int32_t partsNum, int32_t replicaFactor, const std::string& name) {\n     std::string val;\n     val.reserve(256);","old_code":"\/* Copyright (c) 2018 - present, VE Software Inc. All rights reserved\n *\n * This source code is licensed under Apache 2.0 License\n *  (found in the LICENSE.Apache file in the root directory)\n *\/\n\n#include \"meta\/MetaUtils.h\"\n#include <thrift\/lib\/cpp2\/protocol\/Serializer.h>\n#include <thrift\/lib\/cpp2\/protocol\/CompactProtocol.h>\n\nnamespace nebula {\nnamespace meta {\n\nconst std::string kSpacesTable = \"__spaces__\";  \/\/ NOLINT\nconst std::string kPartsTable  = \"__parts__\";   \/\/ NOLINT\nconst std::string kHostsTable  = \"__hosts__\";   \/\/ NOLINT\nconst std::string kTagsTable   = \"__tags__\";    \/\/ NOLINT\nconst std::string kEdgesTable  = \"__edges__\";   \/\/ NOLINT\nconst std::string kIndexTable  = \"__index__\";   \/\/ NOLINT\n\nstd::string MetaUtils::spaceKey(GraphSpaceID spaceId) {\n    std::string key;\n    key.reserve(256);\n    key.append(kSpacesTable.data(), kSpacesTable.size());\n    key.append(reinterpret_cast<const char*>(&spaceId), sizeof(spaceId));\n    return key;\n}\n\n\nstd::string MetaUtils::spaceVal(int32_t partsNum, int32_t replicaFactor, const std::string& name) {\n    std::string val;\n    val.reserve(256);\n    val.append(reinterpret_cast<const char*>(&partsNum), sizeof(partsNum));\n    val.append(reinterpret_cast<const char*>(&replicaFactor), sizeof(replicaFactor));\n    val.append(name);\n    return val;\n}\n\n\nconst std::string& MetaUtils::spacePrefix() {\n    return kSpacesTable;\n}\n\n\nGraphSpaceID MetaUtils::spaceId(folly::StringPiece rawKey) {\n    return *reinterpret_cast<const GraphSpaceID*>(rawKey.data() + kSpacesTable.size());\n}\n\n\nfolly::StringPiece MetaUtils::spaceName(folly::StringPiece rawVal) {\n    return rawVal.subpiece(sizeof(int32_t)*2);\n}\n\n\nstd::string MetaUtils::partKey(GraphSpaceID spaceId, PartitionID partId) {\n    std::string key;\n    key.reserve(128);\n    key.append(kPartsTable.data(), kPartsTable.size());\n    key.append(reinterpret_cast<const char*>(&spaceId), sizeof(GraphSpaceID));\n    key.append(reinterpret_cast<const char*>(&partId), sizeof(PartitionID));\n    return key;\n}\n\n\nstd::string MetaUtils::partVal(const std::vector<nebula::cpp2::HostAddr>& hosts) {\n    std::string val;\n    val.reserve(128);\n    for (auto& h : hosts) {\n        val.append(reinterpret_cast<const char*>(&h.ip), sizeof(h.ip));\n        val.append(reinterpret_cast<const char*>(&h.port), sizeof(h.port));\n    }\n    return val;\n}\n\n\nstd::string MetaUtils::partPrefix(GraphSpaceID spaceId) {\n    std::string prefix;\n    prefix.reserve(128);\n    prefix.append(kPartsTable.data(), kPartsTable.size());\n    prefix.append(reinterpret_cast<const char*>(&spaceId), sizeof(GraphSpaceID));\n    return prefix;\n}\n\n\nstd::vector<nebula::cpp2::HostAddr> MetaUtils::parsePartVal(folly::StringPiece val) {\n    std::vector<nebula::cpp2::HostAddr> hosts;\n    static const size_t unitSize = sizeof(int32_t) * 2;\n    auto hostsNum = val.size() \/ unitSize;\n    hosts.reserve(hostsNum);\n    VLOG(3) << \"Total size:\" << val.size()\n            << \", host size:\" << unitSize\n            << \", host num:\" << hostsNum;\n    for (decltype(hostsNum) i = 0; i < hostsNum; i++) {\n        nebula::cpp2::HostAddr h;\n        h.set_ip(*reinterpret_cast<const int32_t*>(val.data() + i * unitSize));\n        h.set_port(*reinterpret_cast<const int32_t*>(val.data() + i * unitSize + sizeof(int32_t)));\n        hosts.emplace_back(std::move(h));\n    }\n    return hosts;\n}\n\n\nstd::string MetaUtils::hostKey(IPv4 ip, Port port) {\n    std::string key;\n    key.reserve(128);\n    key.append(kHostsTable.data(), kHostsTable.size());\n    key.append(reinterpret_cast<const char*>(&ip), sizeof(ip));\n    key.append(reinterpret_cast<const char*>(&port), sizeof(port));\n    return key;\n}\n\n\nstd::string MetaUtils::hostVal() {\n    return \"\";\n}\n\n\nconst std::string& MetaUtils::hostPrefix() {\n    return kHostsTable;\n}\n\n\nnebula::cpp2::HostAddr MetaUtils::parseHostKey(folly::StringPiece key) {\n    nebula::cpp2::HostAddr host;\n    memcpy(&host, key.data() + kHostsTable.size(), sizeof(host));\n    return host;\n}\n\n\nstd::string MetaUtils::schemaEdgeKey(GraphSpaceID spaceId, EdgeType edgeType, int64_t version) {\n    std::string key;\n    key.reserve(128);\n    key.append(kEdgesTable.data(), kEdgesTable.size());\n    key.append(reinterpret_cast<const char*>(&spaceId), sizeof(spaceId));\n    key.append(reinterpret_cast<const char*>(&edgeType), sizeof(edgeType));\n    key.append(reinterpret_cast<const char*>(&version), sizeof(version));\n    return key;\n}\n\n\nstd::string MetaUtils::schemaEdgeVal(nebula::cpp2::Schema schema) {\n    std::string val;\n    apache::thrift::CompactSerializer::serialize(schema, &val);\n    return val;\n}\n\n\nstd::string MetaUtils::schemaTagKey(GraphSpaceID spaceId, TagID tagId, int64_t version) {\n    std::string key;\n    key.reserve(128);\n    key.append(kTagsTable.data(), kTagsTable.size());\n    key.append(reinterpret_cast<const char*>(&spaceId), sizeof(spaceId));\n    key.append(reinterpret_cast<const char*>(&tagId), sizeof(tagId));\n    key.append(reinterpret_cast<const char*>(&version), sizeof(version));\n    return key;\n}\n\n\nstd::string MetaUtils::schemaTagVal(nebula::cpp2::Schema schema) {\n    std::string val;\n    apache::thrift::CompactSerializer::serialize(schema, &val);\n    return val;\n}\n\n\nnebula::cpp2::Schema MetaUtils::parseSchema(folly::StringPiece rawData) {\n    nebula::cpp2::Schema schema;\n    apache::thrift::CompactSerializer::deserialize(rawData, schema);\n    return schema;\n}\n\n\nstd::string MetaUtils::indexKey(EntryType type, const std::string& name) {\n    std::string key;\n    key.reserve(128);\n    key.append(kIndexTable.data(), kIndexTable.size());\n    key.append(reinterpret_cast<const char*>(&type), sizeof(type));\n    key.append(name);\n    return key;\n}\n\n}  \/\/ namespace meta\n}  \/\/ namespace nebula\n","lang_cluster":"C++","length":183,"code_uid":"46e9545ee6934c4cb4ecc07eb0cb755d"}
{"diff_hunk":"@@ -30,6 +30,37 @@\n #include \"lbann\/utils\/cublas_wrapper.hpp\"\n #endif \/\/ LBANN_HAS_CUDNN\n \n+namespace {\n+\n+  \/** Compute the entry-wise sum of squares of a local matrix. *\/\n+  EvalType sum_of_squares(const Mat& mat) {\n+    const El::Int height = mat.Height();\n+    const El::Int width = mat.Width();\n+    const El::Int ldim = mat.LDim();\n+    const auto& __restrict__ buf = mat.LockedBuffer();\n+    EvalType sqsum = EvalType(0);\n+    if (ldim == height) {\n+      \/\/ Parallelize single loop if data is contiguous\n+      const El::Int size = height*width;\n+      #pragma omp parallel for reduction(+:sqsum)\n+      for (El::Int i = 0; i < size; ++i) {\n+        const EvalType val = buf[i];\n+        sqsum += val * val;\n+      }\n+    } else {\n+      \/\/ Parallelize double loop if data is not contiguous\n+      #pragma omp parallel for reduction(+:sqsum) collapse(2)\n+      for (El::Int j = 0; j < width; ++j) {\n+        for (El::Int i = 0; i < height; ++i) {\n+          const EvalType val = buf[i + j*ldim];\n+          sqsum += val * val;\n+        }\n+      }\n+    }\n+    return sqsum;\n+  }\n+\n+} \/\/ namespace\n \n namespace lbann {\n ","old_code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/ Copyright (c) 2014-2016, Lawrence Livermore National Security, LLC.\n\/\/ Produced at the Lawrence Livermore National Laboratory.\n\/\/ Written by the LBANN Research Team (B. Van Essen, et al.) listed in\n\/\/ the CONTRIBUTORS file. <lbann-dev@llnl.gov>\n\/\/\n\/\/ LLNL-CODE-697807.\n\/\/ All rights reserved.\n\/\/\n\/\/ This file is part of LBANN: Livermore Big Artificial Neural Network\n\/\/ Toolkit. For details, see http:\/\/software.llnl.gov\/LBANN or\n\/\/ https:\/\/github.com\/LLNL\/LBANN.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"Licensee\"); you\n\/\/ may not use this file except in compliance with the License.  You may\n\/\/ obtain a copy of the License at:\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n\/\/ implied. See the License for the specific language governing\n\/\/ permissions and limitations under the license.\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n#include \"lbann\/objective_functions\/weight_regularization\/l2.hpp\"\n#include \"lbann\/models\/model.hpp\"\n#ifdef LBANN_HAS_CUDNN\n#include \"lbann\/utils\/cublas_wrapper.hpp\"\n#endif \/\/ LBANN_HAS_CUDNN\n\n\nnamespace lbann {\n\nvoid l2_weight_regularization::setup(model& m) {\n  objective_function_term::setup(m);\n\n  \/\/ Check that term has no layer pointers\n  if (!m_layers.empty()) {\n    std::stringstream err;\n    err << __FILE__ << \" \" << __LINE__ << \" :: \"\n        << \"attempted to setup L2 weight regularization with layer pointers\";\n    throw lbann_exception(err.str());\n  }\n\n  \/\/ Add all weights in model if no weights pointers are provided\n  if (m_weights.empty()) {\n    for (weights* w : m.get_weights()) {\n      if (w->get_optimizer() != nullptr) {\n        m_weights.push_back(w);\n      }\n    }\n  }\n\n}\n\nEvalType l2_weight_regularization::local_squared_l2_norm(const Mat& mat) const {\n  const El::Int height = mat.Height();\n  const El::Int width = mat.Width();\n  const El::Int ldim = mat.LDim();\n  const DataType* __restrict__ buf = mat.LockedBuffer();\n  EvalType sqsum = EvalType(0);\n  \/\/ Check if data is contiguous.\n  if (ldim == height) {\n    const El::Int size = height*width;\n    #pragma omp parallel for reduction(+:sqsum)\n    for (El::Int i = 0; i < size; ++i) {\n      const EvalType val = buf[i];\n      sqsum += val * val;\n    }\n  } else {\n    #pragma omp parallel for reduction(+:sqsum) collapse(2)\n    for (El::Int j = 0; j < width; ++j) {\n      for (El::Int i = 0; i < height; ++i) {\n        const EvalType val = buf[i + j*ldim];\n        sqsum += val * val;\n      }\n    }\n  }\n  return sqsum;\n}\n\nEvalType l2_weight_regularization::evaluate() {\n  if (m_scale_factor == EvalType(0)) { return EvalType(0); }\n  auto value = EvalType(0);\n  for (weights* w : m_weights) {\n    cudnn::cudnn_manager* cudnn = w->get_cudnn_manager();\n    if (cudnn != nullptr) {\n#ifdef LBANN_HAS_CUDNN\n      CHECK_CUDA(cudaSetDevice(cudnn->get_gpu(0)));\n      EvalType norm = cublas::nrm2(cudnn->get_cublas_handle(0),\n                                   w->get_size(),\n                                   w->get_values_gpu()[0], 1);\n      value += norm * norm;\n#endif \/\/ LBANN_HAS_CUDNN\n    } else {\n      \/\/ Further optimization: Can batch allreduces on the same communicator.\n      const AbsDistMat& values = w->get_values();\n      EvalType local_norm = local_squared_l2_norm(values.LockedMatrix());\n      value += get_comm().allreduce(local_norm, values.DistComm());\n    }\n  }\n  return m_scale_factor * value;\n}\n\nvoid l2_weight_regularization::compute_weight_regularization() {\n  if (m_scale_factor == EvalType(0)) { return; }\n  for (weights* w : m_weights) {\n    optimizer* opt = w->get_optimizer();\n    if (w->get_cudnn_manager() != nullptr) {\n#ifdef LBANN_HAS_CUDNN\n      std::vector<DataType*> values_d = w->get_values_gpu();\n      opt->add_to_gradient_gpu(values_d, 2 * m_scale_factor);\n#endif \/\/ LBANN_HAS_CUDNN\n    } else {\n      opt->add_to_gradient(w->get_values(), 2 * m_scale_factor);\n    }\n  }\n}\n\n}  \/\/ namespace lbann\n","lang_cluster":"C++","length":122,"code_uid":"be41641af48e4022889335452b2fe4b4"}
{"diff_hunk":"@@ -29,6 +29,24 @@ namespace fastdds {\n namespace rtps {\n namespace ddb {\n \n+eprosima::fastrtps::rtps::CacheChange_t* DiscoveryParticipantInfo::update(\n+    eprosima::fastrtps::rtps::CacheChange_t* change,\n+    DiscoveryParticipantChangeData participant_change_data)\n+{\n+    eprosima::fastrtps::rtps::CacheChange_t* old_change = change_;\n+    change_ = change;\n+    participant_change_data_ = participant_change_data;\n+    return old_change;\n+}\n+\n+eprosima::fastrtps::rtps::CacheChange_t* DiscoveryParticipantInfo::update_and_unmatch(\n+        eprosima::fastrtps::rtps::CacheChange_t* change,\n+        DiscoveryParticipantChangeData participant_change_data)\n+{\n+    relevant_participants_builtin_ack_status_.unmatch_all();\n+    return update(change, participant_change_data);\n+}\n+\n void DiscoveryParticipantInfo::add_reader(\n         const eprosima::fastrtps::rtps::GUID_t& guid)\n {","old_code":"\/\/ Copyright 2020 Proyectos y Sistemas de Mantenimiento SL (eProsima).\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\n\/**\n * @file DiscoveryParticipantInfo.cpp\n *\n *\/\n\n#include <vector>\n\n#include <fastdds\/rtps\/common\/CacheChange.h>\n#include <fastdds\/rtps\/common\/GuidPrefix_t.hpp>\n\n#include \".\/DiscoveryParticipantInfo.hpp\"\n\nnamespace eprosima {\nnamespace fastdds {\nnamespace rtps {\nnamespace ddb {\n\nvoid DiscoveryParticipantInfo::add_reader(\n        const eprosima::fastrtps::rtps::GUID_t& guid)\n{\n    if (std::find(readers.begin(), readers.end(), guid) == readers.end())\n    {\n        readers.push_back(guid);\n    }\n}\n\nvoid DiscoveryParticipantInfo::remove_reader(\n        const eprosima::fastrtps::rtps::GUID_t& guid)\n{\n    auto it = std::find(readers.begin(), readers.end(), guid);\n    if (it != readers.end())\n    {\n        readers.erase(it);\n    }\n}\n\nvoid DiscoveryParticipantInfo::add_writer(\n        const eprosima::fastrtps::rtps::GUID_t& guid)\n{\n    if (std::find(writers.begin(), writers.end(), guid) == writers.end())\n    {\n        writers.push_back(guid);\n    }\n}\n\nvoid DiscoveryParticipantInfo::remove_writer(\n        const eprosima::fastrtps::rtps::GUID_t& guid)\n{\n    auto it = std::find(writers.begin(), writers.end(), guid);\n    if (it != writers.end())\n    {\n        writers.erase(it);\n    }\n}\n\n} \/* namespace ddb *\/\n} \/* namespace rtps *\/\n} \/* namespace fastdds *\/\n} \/* namespace eprosima *\/\n","lang_cluster":"C++","length":73,"code_uid":"8d0639e808e349ad90f4cb49bbb1150b"}
{"diff_hunk":"@@ -48,21 +48,16 @@ int main(int argc, char** argv) {\n             .set_accuracy_threshold(0.0001)\n             .set_max_iteration_count(3);\n     \/\/ compute louvain\n-    try {\n-        const std::int64_t rows_count = 7;\n-        const std::int64_t cols_count = 1;\n-        const std::int64_t data[] = { 0, 1, 2, 3, 4, 5, 6 };\n-        const auto initial_labels = dal::homogen_table::wrap(data, rows_count, cols_count);\n+    const std::int64_t rows_count = 7;\n+    const std::int64_t cols_count = 1;\n+    const std::int64_t data[] = { 0, 1, 2, 3, 4, 5, 6 };\n+    const auto initial_labels = dal::homogen_table::wrap(data, rows_count, cols_count);\n \n-        const auto result =\n-            dal::preview::vertex_partitioning(louvain_desc, my_graph, initial_labels);\n+    const auto result = dal::preview::vertex_partitioning(louvain_desc, my_graph, initial_labels);\n+\n+    std::cout << \"Modularity: \" << result.get_modularity() << std::endl;\n+    std::cout << \"Number of communities: \" << result.get_community_count() << std::endl;\n+    std::cout << \"Get communities' labels: \" << result.get_labels() << std::endl;\n \n-        std::cout << \"Modularity: \" << result.get_modularity() << std::endl;\n-        std::cout << \"Number of communities: \" << result.get_community_count() << std::endl;\n-        std::cout << \"Get communities' labels: \" << result.get_labels() << std::endl;\n-    }\n-    catch (dal::unimplemented& e) {\n-        std::cout << e.what() << std::endl;\n-    }\n     return 0;\n }","old_code":"\/*******************************************************************************\n* Copyright 2021 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n#include <memory>\n\n#include \"example_util\/utils.hpp\"\n#include \"oneapi\/dal\/algo\/louvain.hpp\"\n#include \"oneapi\/dal\/graph\/undirected_adjacency_vector_graph.hpp\"\n#include \"oneapi\/dal\/io\/graph_csv_data_source.hpp\"\n#include \"oneapi\/dal\/io\/load_graph.hpp\"\n\nnamespace dal = oneapi::dal;\nusing namespace dal::preview::louvain;\n\nint main(int argc, char** argv) {\n    const auto filename = get_data_path(\"weighted_edge_list.csv\");\n\n    \/\/ read the graph\n    const dal::preview::graph_csv_data_source ds(filename);\n\n    using vertex_type = int32_t;\n    using weight_type = double;\n    using my_graph_type = dal::preview::undirected_adjacency_vector_graph<vertex_type, weight_type>;\n\n    const dal::preview::load_graph::\n        descriptor<dal::preview::weighted_edge_list<vertex_type, weight_type>, my_graph_type>\n            d;\n    const auto my_graph = dal::preview::load_graph::load(d, ds);\n\n    std::allocator<char> alloc;\n    \/\/ set algorithm parameters\n    const auto louvain_desc =\n        descriptor<float, method::fast, task::vertex_partitioning, std::allocator<char>>(alloc)\n            .set_resolution(1)\n            .set_accuracy_threshold(0.0001)\n            .set_max_iteration_count(3);\n    \/\/ compute louvain\n    try {\n        const std::int64_t rows_count = 7;\n        const std::int64_t cols_count = 1;\n        const std::int64_t data[] = { 0, 1, 2, 3, 4, 5, 6 };\n        const auto initial_labels = dal::homogen_table::wrap(data, rows_count, cols_count);\n\n        const auto result =\n            dal::preview::vertex_partitioning(louvain_desc, my_graph, initial_labels);\n\n        std::cout << \"Modularity: \" << result.get_modularity() << std::endl;\n        std::cout << \"Number of communities: \" << result.get_community_count() << std::endl;\n        std::cout << \"Get communities' labels: \" << result.get_labels() << std::endl;\n    }\n    catch (dal::unimplemented& e) {\n        std::cout << e.what() << std::endl;\n    }\n    return 0;\n}\n","lang_cluster":"C++","length":68,"code_uid":"b829411fc4ba47c6a16c3afe49cd9d1a"}
{"diff_hunk":"@@ -180,7 +180,7 @@ void signalHandler(int sig) {\n     case SIGTERM:\n       FLOG_INFO(\"Signal %d(%s) received, stopping this server\", sig, ::strsignal(sig));\n       if (gStorageServer) {\n-        gStorageServer->stop();\n+        gStorageServer->notifyStop();\n       }\n       break;\n     default:","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License.\n *\/\n\n#include <folly\/ssl\/Init.h>\n#include <thrift\/lib\/cpp2\/server\/ThriftServer.h>\n\n#include \"common\/base\/Base.h\"\n#include \"common\/base\/SignalHandler.h\"\n#include \"common\/network\/NetworkUtils.h\"\n#include \"common\/process\/ProcessUtils.h\"\n#include \"common\/time\/TimezoneInfo.h\"\n#include \"storage\/StorageServer.h\"\n#include \"version\/Version.h\"\n\nDEFINE_string(local_ip, \"\", \"IP address which is used to identify this server\");\nDEFINE_string(data_path,\n              \"\",\n              \"Root data path, multi paths should be split by comma.\"\n              \"For rocksdb engine, one path one instance.\");\nDEFINE_string(wal_path,\n              \"\",\n              \"Nebula wal path. By default, wal will be stored as a sibling of \"\n              \"rocksdb data.\");\nDEFINE_string(listener_path,\n              \"\",\n              \"Path for listener, only wal will be saved.\"\n              \"if it is not empty, data_path will not take effect.\");\nDEFINE_bool(daemonize, true, \"Whether to run the process as a daemon\");\nDEFINE_string(pid_file, \"pids\/nebula-storaged.pid\", \"File to hold the process id\");\nDEFINE_string(meta_server_addrs,\n              \"\",\n              \"list of meta server addresses,\"\n              \"the format looks like ip1:port1, ip2:port2, ip3:port3\");\nDECLARE_int32(port);\n\nusing nebula::operator<<;\nusing nebula::HostAddr;\nusing nebula::ProcessUtils;\nusing nebula::Status;\nusing nebula::StatusOr;\nusing nebula::network::NetworkUtils;\n\nstatic void signalHandler(int sig);\nstatic Status setupSignalHandler();\nextern Status setupLogging();\n#if defined(__x86_64__)\nextern Status setupBreakpad();\n#endif\n\nstd::unique_ptr<nebula::storage::StorageServer> gStorageServer;\n\nint main(int argc, char *argv[]) {\n  google::SetVersionString(nebula::versionString());\n  \/\/ Detect if the server has already been started\n  \/\/ Check pid before glog init, in case of user may start daemon twice\n  \/\/ the 2nd will make the 1st failed to output log anymore\n  gflags::ParseCommandLineFlags(&argc, &argv, false);\n\n  \/\/ Setup logging\n  auto status = setupLogging();\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n    return EXIT_FAILURE;\n  }\n\n#if defined(__x86_64__)\n  status = setupBreakpad();\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n    return EXIT_FAILURE;\n  }\n#endif\n\n  auto pidPath = FLAGS_pid_file;\n  status = ProcessUtils::isPidAvailable(pidPath);\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n    return EXIT_FAILURE;\n  }\n\n  folly::init(&argc, &argv, true);\n  if (FLAGS_enable_ssl || FLAGS_enable_meta_ssl) {\n    folly::ssl::init();\n  }\n  if (FLAGS_daemonize) {\n    google::SetStderrLogging(google::FATAL);\n  } else {\n    google::SetStderrLogging(google::INFO);\n  }\n\n  if (FLAGS_daemonize) {\n    status = ProcessUtils::daemonize(pidPath);\n    if (!status.ok()) {\n      LOG(ERROR) << status;\n      return EXIT_FAILURE;\n    }\n  } else {\n    \/\/ Write the current pid into the pid file\n    status = ProcessUtils::makePidFile(pidPath);\n    if (!status.ok()) {\n      LOG(ERROR) << status;\n      return EXIT_FAILURE;\n    }\n  }\n\n  if (FLAGS_data_path.empty()) {\n    LOG(ERROR) << \"Storage Data Path should not empty\";\n    return EXIT_FAILURE;\n  }\n\n  std::string hostName;\n  if (FLAGS_local_ip.empty()) {\n    hostName = nebula::network::NetworkUtils::getHostname();\n  } else {\n    status = NetworkUtils::validateHostOrIp(FLAGS_local_ip);\n    if (!status.ok()) {\n      LOG(ERROR) << status;\n      return EXIT_FAILURE;\n    }\n    hostName = FLAGS_local_ip;\n  }\n  nebula::HostAddr localhost{hostName, FLAGS_port};\n  LOG(INFO) << \"localhost = \" << localhost;\n  auto metaAddrsRet = nebula::network::NetworkUtils::toHosts(FLAGS_meta_server_addrs);\n  if (!metaAddrsRet.ok() || metaAddrsRet.value().empty()) {\n    LOG(ERROR) << \"Can't get metaServer address, status:\" << metaAddrsRet.status()\n               << \", FLAGS_meta_server_addrs:\" << FLAGS_meta_server_addrs;\n    return EXIT_FAILURE;\n  }\n\n  std::vector<std::string> paths;\n  folly::split(\",\", FLAGS_data_path, paths, true);\n  std::transform(paths.begin(), paths.end(), paths.begin(), [](auto &p) {\n    return folly::trimWhitespace(p).str();\n  });\n  if (paths.empty()) {\n    LOG(ERROR) << \"Bad data_path format:\" << FLAGS_data_path;\n    return EXIT_FAILURE;\n  }\n\n  \/\/ Setup the signal handlers\n  status = setupSignalHandler();\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n    return EXIT_FAILURE;\n  }\n\n  \/\/ Initialize the global timezone, it's only used for datetime type compute\n  \/\/ won't affect the process timezone.\n  status = nebula::time::Timezone::initializeGlobalTimezone();\n  if (!status.ok()) {\n    LOG(ERROR) << status;\n    return EXIT_FAILURE;\n  }\n\n  gStorageServer = std::make_unique<nebula::storage::StorageServer>(\n      localhost, metaAddrsRet.value(), paths, FLAGS_wal_path, FLAGS_listener_path);\n  if (!gStorageServer->start()) {\n    LOG(ERROR) << \"Storage server start failed\";\n    gStorageServer->stop();\n    return EXIT_FAILURE;\n  }\n\n  gStorageServer->waitUntilStop();\n  LOG(INFO) << \"The storage Daemon stopped\";\n  return EXIT_SUCCESS;\n}\n\nStatus setupSignalHandler() {\n  return nebula::SignalHandler::install(\n      {SIGINT, SIGTERM},\n      [](nebula::SignalHandler::GeneralSignalInfo *info) { signalHandler(info->sig()); });\n}\n\nvoid signalHandler(int sig) {\n  switch (sig) {\n    case SIGINT:\n    case SIGTERM:\n      FLOG_INFO(\"Signal %d(%s) received, stopping this server\", sig, ::strsignal(sig));\n      if (gStorageServer) {\n        gStorageServer->stop();\n      }\n      break;\n    default:\n      FLOG_ERROR(\"Signal %d(%s) received but ignored\", sig, ::strsignal(sig));\n  }\n}\n","lang_cluster":"C++","length":189,"code_uid":"4396d20016cc443d95a4d4a4de803da8"}
{"diff_hunk":"@@ -30,7 +30,10 @@ double getAlignmentTransform(const ROMol &prbMol, const ROMol &refMol,\n   if (atomMap == 0) {\n     \/\/ we have to figure out the mapping between the two molecule\n     MatchVectType match;\n-    if (SubstructMatch(refMol, prbMol, match)) {\n+    const bool recursionPossible = true;\n+    const bool useChirality = false;\n+    const bool useQueryQueryMatches = true;\n+    if (SubstructMatch(refMol, prbMol, match, recursionPossible, useChirality, useQueryQueryMatches)) {\n       MatchVectType::const_iterator mi;\n       for (mi = match.begin(); mi != match.end(); mi++) {\n         prbPoints.push_back(&prbCnf.getAtomPos(mi->first));","old_code":"\/\/ $Id$\n\/\/\n\/\/  Copyright (C) 2001-2008 Greg Landrum and Rational Discovery LLC\n\/\/\n\/\/   @@ All Rights Reserved @@\n\/\/  This file is part of the RDKit.\n\/\/  The contents are covered by the terms of the BSD license\n\/\/  which is included in the file license.txt, found at the root\n\/\/  of the RDKit source tree.\n\/\/\n#include \"AlignMolecules.h\"\n#include <Geometry\/Transform3D.h>\n#include <Numerics\/Vector.h>\n#include <GraphMol\/Substruct\/SubstructMatch.h>\n#include <GraphMol\/Conformer.h>\n#include <GraphMol\/ROMol.h>\n#include <Numerics\/Alignment\/AlignPoints.h>\n#include <GraphMol\/MolTransforms\/MolTransforms.h>\n\nnamespace RDKit {\nnamespace MolAlign {\ndouble getAlignmentTransform(const ROMol &prbMol, const ROMol &refMol,\n                             RDGeom::Transform3D &trans, int prbCid, int refCid,\n                             const MatchVectType *atomMap,\n                             const RDNumeric::DoubleVector *weights,\n                             bool reflect, unsigned int maxIterations) {\n  RDGeom::Point3DConstPtrVect refPoints, prbPoints;\n  const Conformer &prbCnf = prbMol.getConformer(prbCid);\n  const Conformer &refCnf = refMol.getConformer(refCid);\n  if (atomMap == 0) {\n    \/\/ we have to figure out the mapping between the two molecule\n    MatchVectType match;\n    if (SubstructMatch(refMol, prbMol, match)) {\n      MatchVectType::const_iterator mi;\n      for (mi = match.begin(); mi != match.end(); mi++) {\n        prbPoints.push_back(&prbCnf.getAtomPos(mi->first));\n        refPoints.push_back(&refCnf.getAtomPos(mi->second));\n      }\n    } else {\n      throw MolAlignException(\n          \"No sub-structure match found between the probe and query mol\");\n    }\n  } else {\n    MatchVectType::const_iterator mi;\n    for (mi = atomMap->begin(); mi != atomMap->end(); mi++) {\n      prbPoints.push_back(&prbCnf.getAtomPos(mi->first));\n      refPoints.push_back(&refCnf.getAtomPos(mi->second));\n    }\n  }\n  double ssr = RDNumeric::Alignments::AlignPoints(\n      refPoints, prbPoints, trans, weights, reflect, maxIterations);\n  ssr \/= (prbPoints.size());\n  return sqrt(ssr);\n}\n\ndouble alignMol(ROMol &prbMol, const ROMol &refMol, int prbCid, int refCid,\n                const MatchVectType *atomMap,\n                const RDNumeric::DoubleVector *weights, bool reflect,\n                unsigned int maxIterations) {\n  RDGeom::Transform3D trans;\n  double res = getAlignmentTransform(prbMol, refMol, trans, prbCid, refCid,\n                                     atomMap, weights, reflect, maxIterations);\n  \/\/ now transform the relevant conformation on prbMol\n  Conformer &conf = prbMol.getConformer(prbCid);\n  MolTransforms::transformConformer(conf, trans);\n  return res;\n}\n\nvoid _fillAtomPositions(RDGeom::Point3DConstPtrVect &pts, const Conformer &conf,\n                        const std::vector<unsigned int> *atomIds = 0) {\n  unsigned int na = conf.getNumAtoms();\n  pts.clear();\n  if (atomIds == 0) {\n    unsigned int ai;\n    pts.reserve(na);\n    for (ai = 0; ai < na; ++ai) {\n      pts.push_back(&conf.getAtomPos(ai));\n    }\n  } else {\n    pts.reserve(atomIds->size());\n    std::vector<unsigned int>::const_iterator cai;\n    for (cai = atomIds->begin(); cai != atomIds->end(); cai++) {\n      pts.push_back(&conf.getAtomPos(*cai));\n    }\n  }\n}\n\nvoid alignMolConformers(ROMol &mol, const std::vector<unsigned int> *atomIds,\n                        const std::vector<unsigned int> *confIds,\n                        const RDNumeric::DoubleVector *weights, bool reflect,\n                        unsigned int maxIters, std::vector<double> *RMSlist) {\n  if (mol.getNumConformers() == 0) {\n    \/\/ nothing to be done ;\n    return;\n  }\n\n  RDGeom::Point3DConstPtrVect refPoints, prbPoints;\n  int cid = -1;\n  if ((confIds != 0) && (confIds->size() > 0)) {\n    cid = confIds->front();\n  }\n  const Conformer &refCnf = mol.getConformer(cid);\n  _fillAtomPositions(refPoints, refCnf, atomIds);\n\n  \/\/ now loop throught the remaininf conformations and transform them\n  RDGeom::Transform3D trans;\n  double ssd;\n  if (confIds == 0) {\n    unsigned int i = 0;\n    ROMol::ConformerIterator cnfi;\n    \/\/ Conformer *conf;\n    for (cnfi = mol.beginConformers(); cnfi != mol.endConformers(); cnfi++) {\n      \/\/ conf = (*cnfi);\n      i += 1;\n      if (i == 1) {\n        continue;\n      }\n      _fillAtomPositions(prbPoints, *(*cnfi), atomIds);\n      ssd = RDNumeric::Alignments::AlignPoints(refPoints, prbPoints, trans,\n                                               weights, reflect, maxIters);\n      if (RMSlist) {\n        ssd \/= (prbPoints.size());\n        RMSlist->push_back(sqrt(ssd));\n      }\n      MolTransforms::transformConformer(*(*cnfi), trans);\n    }\n  } else {\n    std::vector<unsigned int>::const_iterator cai;\n    unsigned int i = 0;\n    for (cai = confIds->begin(); cai != confIds->end(); cai++) {\n      i += 1;\n      if (i == 1) {\n        continue;\n      }\n      Conformer &conf = mol.getConformer(*cai);\n      _fillAtomPositions(prbPoints, conf, atomIds);\n      ssd = RDNumeric::Alignments::AlignPoints(refPoints, prbPoints, trans,\n                                               weights, reflect, maxIters);\n      if (RMSlist) {\n        ssd \/= (prbPoints.size());\n        RMSlist->push_back(sqrt(ssd));\n      }\n      MolTransforms::transformConformer(conf, trans);\n    }\n  }\n}\n}\n}\n","lang_cluster":"C++","length":148,"code_uid":"22c1032fcce94c0594e34c90523c4d25"}
{"diff_hunk":"@@ -13,32 +13,28 @@ void ListTagIndexesProcessor::process(const cpp2::ListTagIndexesReq& req) {\n     auto space = req.get_space_id();\n     CHECK_SPACE_ID_AND_RETURN(space);\n     folly::SharedMutex::ReadHolder rHolder(LockUtils::tagIndexLock());\n-    auto prefix = MetaServiceUtils::tagIndexPrefix(space);\n+    auto prefix = MetaServiceUtils::indexPrefix(space);\n \n     std::unique_ptr<kvstore::KVIterator> iter;\n     auto ret = kvstore_->prefix(kDefaultSpaceId, kDefaultPartId, prefix, &iter);\n     resp_.set_code(to(ret));\n     if (ret != kvstore::ResultCode::SUCCEEDED) {\n         LOG(ERROR) << \"List Tag Index Failed: SpaceID \" << space;\n+        resp_.set_code(cpp2::ErrorCode::E_NOT_FOUND);\n         onFinished();\n         return;\n     }\n \n     decltype(resp_.items) items;\n     while (iter->valid()) {\n-        auto key = iter->key();\n         auto val = iter->val();\n-        auto tagIndex = *reinterpret_cast<const TagIndexID *>(key.data() + prefix.size());\n-        auto nameSize = *reinterpret_cast<const int32_t *>(val.data());\n-        auto name = val.subpiece(sizeof(int32_t), nameSize).str();\n-        auto fields = MetaServiceUtils::parseTagIndex(val);\n-        nebula::meta::cpp2::TagIndexItem item;\n-        item.set_index_id(tagIndex);\n-        item.set_index_name(std::move(name));\n-        item.set_fields(std::move(fields));\n-        items.emplace_back(std::move(item));\n+        auto item = MetaServiceUtils::parseIndex(val);\n+        if (item.get_schema_id().getType() == nebula::cpp2::SchemaID::Type::tag_id) {\n+            items.emplace_back(std::move(item));\n+        }\n         iter->next();\n     }\n+    resp_.set_code(cpp2::ErrorCode::SUCCEEDED);\n     resp_.set_items(std::move(items));\n     onFinished();\n }","old_code":"\/* Copyright (c) 2019 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"meta\/processors\/indexMan\/ListTagIndexesProcessor.h\"\n\nnamespace nebula {\nnamespace meta {\n\nvoid ListTagIndexesProcessor::process(const cpp2::ListTagIndexesReq& req) {\n    auto space = req.get_space_id();\n    CHECK_SPACE_ID_AND_RETURN(space);\n    folly::SharedMutex::ReadHolder rHolder(LockUtils::tagIndexLock());\n    auto prefix = MetaServiceUtils::tagIndexPrefix(space);\n\n    std::unique_ptr<kvstore::KVIterator> iter;\n    auto ret = kvstore_->prefix(kDefaultSpaceId, kDefaultPartId, prefix, &iter);\n    resp_.set_code(to(ret));\n    if (ret != kvstore::ResultCode::SUCCEEDED) {\n        LOG(ERROR) << \"List Tag Index Failed: SpaceID \" << space;\n        onFinished();\n        return;\n    }\n\n    decltype(resp_.items) items;\n    while (iter->valid()) {\n        auto key = iter->key();\n        auto val = iter->val();\n        auto tagIndex = *reinterpret_cast<const TagIndexID *>(key.data() + prefix.size());\n        auto nameSize = *reinterpret_cast<const int32_t *>(val.data());\n        auto name = val.subpiece(sizeof(int32_t), nameSize).str();\n        auto fields = MetaServiceUtils::parseTagIndex(val);\n        nebula::meta::cpp2::TagIndexItem item;\n        item.set_index_id(tagIndex);\n        item.set_index_name(std::move(name));\n        item.set_fields(std::move(fields));\n        items.emplace_back(std::move(item));\n        iter->next();\n    }\n    resp_.set_items(std::move(items));\n    onFinished();\n}\n\n}  \/\/ namespace meta\n}  \/\/ namespace nebula\n\n","lang_cluster":"C++","length":48,"code_uid":"ef051f0bef1045cc90c534127ef69e43"}
{"diff_hunk":"@@ -23,6 +23,7 @@ void MemoryDataLayer<Dtype>::DataLayerSetUp(const vector<Blob<Dtype>*>& bottom,\n   added_label_.Reshape(batch_size_, 1, 1, 1);\n   data_ = NULL;\n   labels_ = NULL;\n+  needs_reshape_ = false;\n   added_data_.cpu_data();\n   added_label_.cpu_data();\n }","old_code":"#include <vector>\n\n#include \"caffe\/data_layers.hpp\"\n#include \"caffe\/layer.hpp\"\n#include \"caffe\/util\/io.hpp\"\n\nnamespace caffe {\n\ntemplate <typename Dtype>\nvoid MemoryDataLayer<Dtype>::DataLayerSetUp(const vector<Blob<Dtype>*>& bottom,\n     const vector<Blob<Dtype>*>& top) {\n  batch_size_ = this->layer_param_.memory_data_param().batch_size();\n  channels_ = this->layer_param_.memory_data_param().channels();\n  height_ = this->layer_param_.memory_data_param().height();\n  width_ = this->layer_param_.memory_data_param().width();\n  size_ = channels_ * height_ * width_;\n  CHECK_GT(batch_size_ * size_, 0) <<\n      \"batch_size, channels, height, and width must be specified and\"\n      \" positive in memory_data_param\";\n  top[0]->Reshape(batch_size_, channels_, height_, width_);\n  top[1]->Reshape(batch_size_, 1, 1, 1);\n  added_data_.Reshape(batch_size_, channels_, height_, width_);\n  added_label_.Reshape(batch_size_, 1, 1, 1);\n  data_ = NULL;\n  labels_ = NULL;\n  added_data_.cpu_data();\n  added_label_.cpu_data();\n}\n\ntemplate <typename Dtype>\nvoid MemoryDataLayer<Dtype>::AddDatumVector(const vector<Datum>& datum_vector) {\n  CHECK(!has_new_data_) <<\n      \"Can't add Datum when earlier ones haven't been consumed\"\n      << \" by the upper layers\";\n  size_t num = datum_vector.size();\n  CHECK_GT(num, 0) << \"There is no datum to add\";\n  CHECK_LE(num, batch_size_) <<\n      \"The number of added datum must be no greater than the batch size\";\n\n  \/\/ Apply data transformations (mirror, scale, crop...)\n  this->data_transformer_.Transform(datum_vector, &added_data_);\n  \/\/ Copy Labels\n  Dtype* top_label = added_label_.mutable_cpu_data();\n  for (int item_id = 0; item_id < num; ++item_id) {\n    top_label[item_id] = datum_vector[item_id].label();\n  }\n  \/\/ num_images == batch_size_\n  Dtype* top_data = added_data_.mutable_cpu_data();\n  Reset(top_data, top_label, batch_size_);\n  has_new_data_ = true;\n}\n\ntemplate <typename Dtype>\nvoid MemoryDataLayer<Dtype>::Reset(Dtype* data, Dtype* labels, int n) {\n  CHECK(data);\n  CHECK(labels);\n  CHECK_EQ(n % batch_size_, 0) << \"n must be a multiple of batch size\";\n  data_ = data;\n  labels_ = labels;\n  n_ = n;\n  pos_ = 0;\n}\n\ntemplate <typename Dtype>\nvoid MemoryDataLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,\n      const vector<Blob<Dtype>*>& top) {\n  CHECK(data_) << \"MemoryDataLayer needs to be initalized by calling Reset\";\n  top[0]->set_cpu_data(data_ + pos_ * size_);\n  top[1]->set_cpu_data(labels_ + pos_);\n  pos_ = (pos_ + batch_size_) % n_;\n  has_new_data_ = false;\n}\n\nINSTANTIATE_CLASS(MemoryDataLayer);\nREGISTER_LAYER_CLASS(MEMORY_DATA, MemoryDataLayer);\n}  \/\/ namespace caffe\n","lang_cluster":"C++","length":76,"code_uid":"adfb41da4db040c58824cd0d76321039"}
{"diff_hunk":"@@ -19,6 +19,10 @@\n #include \"internal\/conversion.h\"\n #include \"service_memory.h\"\n \n+#if defined(__INTEL_COMPILER)\n+    #include <immintrin.h>\n+#endif\n+\n namespace daal\n {\n namespace data_management","old_code":"\/** file data_management_utils_cpu.cpp *\/\n\/*******************************************************************************\n* Copyright 2014-2019 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n#include \"data_conversion_cpu.h\"\n#include \"internal\/conversion.h\"\n#include \"service_memory.h\"\n\nnamespace daal\n{\nnamespace data_management\n{\nnamespace internal\n{\n\ntemplate<typename T1, typename T2, CpuType cpu>\nvoid vectorConvertFuncCpu(size_t n, const void *src, void *dst)\n{\n    for(size_t i = 0; i < n; i++)\n    {\n        ((T2 *)dst)[i] = static_cast<T2>(((T1 *)src)[i]);\n    }\n}\n\ntemplate<typename T1, typename T2, CpuType cpu>\nvoid vectorStrideConvertFuncCpu(size_t n, const void *src, size_t srcByteStride, void *dst, size_t dstByteStride)\n{\n    for(size_t i = 0; i < n ; i++)\n    {\n        *(T2 *)(((char *)dst) + i * dstByteStride) = static_cast<T2>(*(T1 *)(((char *)src) + i * srcByteStride));\n    }\n}\n\n#undef  DAAL_FUNCS_UP_ENTRY\n#define DAAL_FUNCS_UP_ENTRY(F,T,A)      \\\ntemplate void F<T, float , DAAL_CPU> A; \\\ntemplate void F<T, double, DAAL_CPU> A; \\\ntemplate void F<T, int   , DAAL_CPU> A;\n\n#undef  DAAL_FUNCS_DOWN_ENTRY\n#define DAAL_FUNCS_DOWN_ENTRY(F,T,A)    \\\ntemplate void F<float , T, DAAL_CPU> A; \\\ntemplate void F<double, T, DAAL_CPU> A; \\\ntemplate void F<int   , T, DAAL_CPU> A;\n\n#undef  DAAL_CONVERT_UP_FUNCS\n#define DAAL_CONVERT_UP_FUNCS(F,A)                    \\\n        DAAL_FUNCS_UP_ENTRY(F,float,A)                \\\n        DAAL_FUNCS_UP_ENTRY(F,double,A)               \\\n        DAAL_FUNCS_UP_ENTRY(F,int,A)                  \\\n        DAAL_FUNCS_UP_ENTRY(F,unsigned int,A)         \\\n        DAAL_FUNCS_UP_ENTRY(F,DAAL_INT64,A)           \\\n        DAAL_FUNCS_UP_ENTRY(F,DAAL_UINT64,A)          \\\n        DAAL_FUNCS_UP_ENTRY(F,char,A)                 \\\n        DAAL_FUNCS_UP_ENTRY(F,unsigned char,A)        \\\n        DAAL_FUNCS_UP_ENTRY(F,short,A)                \\\n        DAAL_FUNCS_UP_ENTRY(F,unsigned short,A)\n\n#undef  DAAL_CONVERT_DOWN_FUNCS\n#define DAAL_CONVERT_DOWN_FUNCS(F,A)                 \\\n        DAAL_FUNCS_DOWN_ENTRY(F,unsigned int,A)      \\\n        DAAL_FUNCS_DOWN_ENTRY(F,DAAL_INT64,A)        \\\n        DAAL_FUNCS_DOWN_ENTRY(F,DAAL_UINT64,A)       \\\n        DAAL_FUNCS_DOWN_ENTRY(F,char,A)              \\\n        DAAL_FUNCS_DOWN_ENTRY(F,unsigned char,A)     \\\n        DAAL_FUNCS_DOWN_ENTRY(F,short,A)             \\\n        DAAL_FUNCS_DOWN_ENTRY(F,unsigned short,A)\n\nDAAL_CONVERT_UP_FUNCS(vectorConvertFuncCpu,(size_t n, const void *src, void *dst))\nDAAL_CONVERT_DOWN_FUNCS(vectorConvertFuncCpu,(size_t n, const void *src, void *dst))\n\nDAAL_CONVERT_UP_FUNCS(vectorStrideConvertFuncCpu,(size_t n, const void *src, size_t srcByteStride, void *dst, size_t dstByteStride))\nDAAL_CONVERT_DOWN_FUNCS(vectorStrideConvertFuncCpu,(size_t n, const void *src, size_t srcByteStride, void *dst, size_t dstByteStride))\n\ntemplate<typename T, CpuType cpu>\nvoid vectorAssignValueToArrayCpu(void* const ptr, const size_t n, const void* const value)\n{\n    const T& valueT = *((const T*)value);\n    T* const ptrT = (T*)ptr;\n    services::internal::service_memset<T, cpu>(ptrT, valueT, n);\n}\n\n#define DAAL_REGISTER_VECTOR_ASSIGN_CPU(Type) template void vectorAssignValueToArrayCpu<Type, DAAL_CPU>(void* const ptr, const size_t n, const void* const value);\nDAAL_REGISTER_WITH_HOMOGEN_NT_TYPES(DAAL_REGISTER_VECTOR_ASSIGN_CPU)\n\n} \/\/ namespace internal\n} \/\/ namespace data_management\n} \/\/ namespace daal\n","lang_cluster":"C++","length":101,"code_uid":"89e90225369840e6896fd394c41b5765"}
{"diff_hunk":"@@ -81,6 +81,11 @@ namespace Impl {\n CudaLockArrays g_host_cuda_lock_arrays = {nullptr, nullptr, 0};\n \n void initialize_host_cuda_lock_arrays() {\n+#ifdef KOKKOS_ENABLE_IMPL_DESUL_ATOMICS\n+  desul::Impl::init_lock_arrays();\n+\n+  DESUL_ENSURE_CUDA_LOCK_ARRAYS_ON_DEVICE();\n+#endif\n   if (g_host_cuda_lock_arrays.atomic != nullptr) return;\n   CUDA_SAFE_CALL(cudaMalloc(&g_host_cuda_lock_arrays.atomic,\n                             sizeof(int) * (CUDA_SPACE_ATOMIC_MASK + 1)));","old_code":"\/*\n\/\/@HEADER\n\/\/ ************************************************************************\n\/\/\n\/\/                        Kokkos v. 3.0\n\/\/       Copyright (2020) National Technology & Engineering\n\/\/               Solutions of Sandia, LLC (NTESS).\n\/\/\n\/\/ Under the terms of Contract DE-NA0003525 with NTESS,\n\/\/ the U.S. Government retains certain rights in this software.\n\/\/\n\/\/ Redistribution and use in source and binary forms, with or without\n\/\/ modification, are permitted provided that the following conditions are\n\/\/ met:\n\/\/\n\/\/ 1. Redistributions of source code must retain the above copyright\n\/\/ notice, this list of conditions and the following disclaimer.\n\/\/\n\/\/ 2. Redistributions in binary form must reproduce the above copyright\n\/\/ notice, this list of conditions and the following disclaimer in the\n\/\/ documentation and\/or other materials provided with the distribution.\n\/\/\n\/\/ 3. Neither the name of the Corporation nor the names of the\n\/\/ contributors may be used to endorse or promote products derived from\n\/\/ this software without specific prior written permission.\n\/\/\n\/\/ THIS SOFTWARE IS PROVIDED BY NTESS \"AS IS\" AND ANY\n\/\/ EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n\/\/ IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n\/\/ PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NTESS OR THE\n\/\/ CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n\/\/ EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n\/\/ PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n\/\/ PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n\/\/ LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n\/\/ NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n\/\/ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\/\/\n\/\/ Questions? Contact Christian R. Trott (crtrott@sandia.gov)\n\/\/\n\/\/ ************************************************************************\n\/\/@HEADER\n*\/\n\n#include <Kokkos_Core.hpp>\n#ifdef KOKKOS_ENABLE_CUDA\n#include <Cuda\/Kokkos_Cuda_Locks.hpp>\n#include <Cuda\/Kokkos_Cuda_Error.hpp>\n\n#ifdef KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE\nnamespace Kokkos {\nnamespace Impl {\n__device__ __constant__ CudaLockArrays g_device_cuda_lock_arrays = {nullptr,\n                                                                    nullptr, 0};\n}\n}  \/\/ namespace Kokkos\n#endif\n\nnamespace Kokkos {\n\nnamespace {\n\n__global__ void init_lock_array_kernel_atomic() {\n  unsigned i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < CUDA_SPACE_ATOMIC_MASK + 1) {\n    Kokkos::Impl::g_device_cuda_lock_arrays.atomic[i] = 0;\n  }\n}\n\n__global__ void init_lock_array_kernel_threadid(int N) {\n  unsigned i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < (unsigned)N) {\n    Kokkos::Impl::g_device_cuda_lock_arrays.scratch[i] = 0;\n  }\n}\n\n}  \/\/ namespace\n\nnamespace Impl {\n\nCudaLockArrays g_host_cuda_lock_arrays = {nullptr, nullptr, 0};\n\nvoid initialize_host_cuda_lock_arrays() {\n  if (g_host_cuda_lock_arrays.atomic != nullptr) return;\n  CUDA_SAFE_CALL(cudaMalloc(&g_host_cuda_lock_arrays.atomic,\n                            sizeof(int) * (CUDA_SPACE_ATOMIC_MASK + 1)));\n  CUDA_SAFE_CALL(cudaMalloc(&g_host_cuda_lock_arrays.scratch,\n                            sizeof(int) * (Cuda::concurrency())));\n  CUDA_SAFE_CALL(cudaDeviceSynchronize());\n  g_host_cuda_lock_arrays.n = Cuda::concurrency();\n  KOKKOS_COPY_CUDA_LOCK_ARRAYS_TO_DEVICE();\n  init_lock_array_kernel_atomic<<<(CUDA_SPACE_ATOMIC_MASK + 1 + 255) \/ 256,\n                                  256>>>();\n  init_lock_array_kernel_threadid<<<(Kokkos::Cuda::concurrency() + 255) \/ 256,\n                                    256>>>(Kokkos::Cuda::concurrency());\n  CUDA_SAFE_CALL(cudaDeviceSynchronize());\n}\n\nvoid finalize_host_cuda_lock_arrays() {\n  if (g_host_cuda_lock_arrays.atomic == nullptr) return;\n  cudaFree(g_host_cuda_lock_arrays.atomic);\n  g_host_cuda_lock_arrays.atomic = nullptr;\n  cudaFree(g_host_cuda_lock_arrays.scratch);\n  g_host_cuda_lock_arrays.scratch = nullptr;\n  g_host_cuda_lock_arrays.n       = 0;\n#ifdef KOKKOS_ENABLE_CUDA_RELOCATABLE_DEVICE_CODE\n  KOKKOS_COPY_CUDA_LOCK_ARRAYS_TO_DEVICE();\n#endif\n}\n\n}  \/\/ namespace Impl\n\n}  \/\/ namespace Kokkos\n\n#else\n\nvoid KOKKOS_CORE_SRC_CUDA_CUDA_LOCKS_PREVENT_LINK_ERROR() {}\n\n#endif\n","lang_cluster":"C++","length":119,"code_uid":"4332e908e9f54255903cca558f936534"}
{"diff_hunk":"@@ -179,4 +179,24 @@ TEST(proj_context, read_grid_from_user_writable_directory) {\n     MyUnlink(filename);\n }\n \n+\/\/ ---------------------------------------------------------------------------\n+\n+TEST(proj_context, proj_context_set_ca_bundle_path) {\n+\n+    std::string dirname;\n+    auto filename = createTempDict(dirname, \"temp_proj_dic2\");\n+    if (filename.empty())\n+        return;\n+\n+    auto ctx = proj_context_create();\n+\n+    const char *path = dirname.c_str();\n+    proj_context_set_ca_bundle_path(ctx, path);\n+    ASSERT_EQ(ctx->ca_bundle_path, dirname);\n+\n+    proj_context_destroy(ctx);\n+\n+    MyUnlink(filename);\n+}\n+\n } \/\/ namespace","old_code":"\/******************************************************************************\n *\n * Project:  PROJ\n * Purpose:  Test functions in proj_context namespae\n * Author:   Even Rouault <even dot rouault at spatialys dot com>\n *\n ******************************************************************************\n * Copyright (c) 2019, Even Rouault <even dot rouault at spatialys dot com>\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and\/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included\n * in all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n ****************************************************************************\/\n\n#include <stdlib.h>\n#ifdef _MSC_VER\n#include <io.h>\n#else\n#include <unistd.h>\n#endif\n\n#include \"proj.h\"\n#include \"proj_internal.h\"\n\n#include \"gtest_include.h\"\n\nnamespace {\n\nstatic bool createTmpFile(const std::string &filename) {\n    FILE *f = fopen(filename.c_str(), \"wt\");\n    if (!f)\n        return false;\n    fprintf(\n        f,\n        \"<MY_PIPELINE> +proj=pipeline +step +proj=utm +zone=31 +ellps=GRS80\\n\");\n    fclose(f);\n    return true;\n}\n\n\/\/ ---------------------------------------------------------------------------\n\nstatic std::string createTempDict(std::string &dirname, const char *filename) {\n    const char *temp_dir = getenv(\"TEMP\");\n    if (!temp_dir) {\n        temp_dir = getenv(\"TMP\");\n    }\n#ifndef WIN32\n    if (!temp_dir) {\n        temp_dir = \"\/tmp\";\n    }\n#endif\n    if (!temp_dir)\n        return std::string();\n\n    dirname = temp_dir;\n\n    std::string tmpFilename;\n    tmpFilename = temp_dir;\n    tmpFilename += DIR_CHAR;\n    tmpFilename += filename;\n\n    return createTmpFile(tmpFilename) ? tmpFilename : std::string();\n}\n\n\/\/ ---------------------------------------------------------------------------\n\nstatic int MyUnlink(const std::string &filename) {\n#ifdef _MSC_VER\n    return _unlink(filename.c_str());\n#else\n    return unlink(filename.c_str());\n#endif\n}\n\n\/\/ ---------------------------------------------------------------------------\n\nTEST(proj_context, proj_context_set_file_finder) {\n\n    std::string dirname;\n    auto filename = createTempDict(dirname, \"temp_proj_dic1\");\n    if (filename.empty())\n        return;\n\n    auto ctx = proj_context_create();\n\n    struct FinderData {\n        PJ_CONTEXT *got_ctx = nullptr;\n        std::string dirname{};\n        std::string tmpFilename{};\n    };\n\n    const auto finder = [](PJ_CONTEXT *got_ctx, const char *file,\n                           void *user_data) -> const char * {\n        auto finderData = static_cast<FinderData *>(user_data);\n        finderData->got_ctx = got_ctx;\n        finderData->tmpFilename = finderData->dirname;\n        finderData->tmpFilename += DIR_CHAR;\n        finderData->tmpFilename += file;\n        return finderData->tmpFilename.c_str();\n    };\n\n    FinderData finderData;\n    finderData.dirname = dirname;\n    proj_context_set_file_finder(ctx, finder, &finderData);\n\n    auto P = proj_create(ctx, \"+init=temp_proj_dic1:MY_PIPELINE\");\n    EXPECT_NE(P, nullptr);\n    proj_destroy(P);\n\n    EXPECT_EQ(finderData.got_ctx, ctx);\n\n    proj_context_destroy(ctx);\n}\n\n\/\/ ---------------------------------------------------------------------------\n\nTEST(proj_context, proj_context_set_search_paths) {\n\n    std::string dirname;\n    auto filename = createTempDict(dirname, \"temp_proj_dic2\");\n    if (filename.empty())\n        return;\n\n    auto ctx = proj_context_create();\n\n    const char *path = dirname.c_str();\n    proj_context_set_search_paths(ctx, 1, &path);\n\n    auto P = proj_create(ctx, \"+init=temp_proj_dic2:MY_PIPELINE\");\n    EXPECT_NE(P, nullptr);\n    proj_destroy(P);\n\n    proj_context_destroy(ctx);\n\n    MyUnlink(filename);\n}\n\n\/\/ ---------------------------------------------------------------------------\n\nTEST(proj_context, read_grid_from_user_writable_directory) {\n\n    auto ctx = proj_context_create();\n    auto path =\n        std::string(proj_context_get_user_writable_directory(ctx, true));\n    EXPECT_TRUE(!path.empty());\n    auto filename = path + DIR_CHAR + \"temp_proj_dic3\";\n    EXPECT_TRUE(createTmpFile(filename));\n    {\n        \/\/ Check that with PROJ_SKIP_READ_USER_WRITABLE_DIRECTORY=YES (set by\n        \/\/ calling script), we cannot find the file\n        auto P = proj_create(ctx, \"+init=temp_proj_dic3:MY_PIPELINE\");\n        EXPECT_EQ(P, nullptr);\n        proj_destroy(P);\n    }\n    {\n        \/\/ Cancel the effect of PROJ_SKIP_READ_USER_WRITABLE_DIRECTORY\n        putenv(const_cast<char *>(\"PROJ_SKIP_READ_USER_WRITABLE_DIRECTORY=\"));\n        auto P = proj_create(ctx, \"+init=temp_proj_dic3:MY_PIPELINE\");\n        EXPECT_NE(P, nullptr);\n        putenv(\n            const_cast<char *>(\"PROJ_SKIP_READ_USER_WRITABLE_DIRECTORY=YES\"));\n        proj_destroy(P);\n    }\n    proj_context_destroy(ctx);\n    MyUnlink(filename);\n}\n\n} \/\/ namespace\n","lang_cluster":"C++","length":182,"code_uid":"7429bdd80f2e43279a1c5a8bdabf9d06"}
{"diff_hunk":"@@ -4,12 +4,8 @@\n  *  (found in the LICENSE.Apache file in the root directory)\n  *\/\n #include <gtest\/gtest.h>\n-#include <cstdlib>\n-#include <thread>\n-#include <mutex>\n-#include <atomic>\n-#include \"common\/concurrent\/Barrier.h\"\n-#include \"common\/thread\/GenericThreadPool.h\"\n+#include \"concurrent\/Barrier.h\"\n+#include \"thread\/GenericThreadPool.h\"\n \n namespace vesoft {\n namespace concurrent {","old_code":"\/* Copyright (c) 2018 - present, VE Software Inc. All rights reserved\n *\n * This source code is licensed under Apache 2.0 License\n *  (found in the LICENSE.Apache file in the root directory)\n *\/\n#include <gtest\/gtest.h>\n#include <cstdlib>\n#include <thread>\n#include <mutex>\n#include <atomic>\n#include \"common\/concurrent\/Barrier.h\"\n#include \"common\/thread\/GenericThreadPool.h\"\n\nnamespace vesoft {\nnamespace concurrent {\n\nTEST(BarrierTest, BasicTest) {\n    \/\/ test for invalid initial counter\n    {\n        ASSERT_THROW({Barrier barrier(0UL);}, std::invalid_argument);\n    }\n    \/\/ test for single-thread normal case\n    {\n        Barrier barrier(1UL);\n        barrier.wait();\n        ASSERT_TRUE(true);\n    }\n    \/\/ test for multiple-thread normal case\n    {\n        Barrier barrier(2UL);\n        std::atomic<size_t> counter{0};\n        auto cb = [&] () {\n            barrier.wait();\n            ++counter;\n        };\n        std::thread thread(cb);\n        usleep(1000);\n        ASSERT_EQ(0UL, counter.load());\n        barrier.wait();\n        thread.join();\n        ASSERT_EQ(1UL, counter.load());\n    }\n    \/\/ test for multiple-thread completion\n    {\n        std::atomic<size_t> counter{0};\n        auto completion = [&] () {\n            ++counter;\n            ++counter;\n        };\n        Barrier barrier(2UL, completion);\n\n        auto cb = [&] () {\n            barrier.wait();\n            ++counter;\n        };\n\n        std::thread thread(cb);\n        usleep(1000);\n        ASSERT_EQ(0UL, counter.load());\n        barrier.wait();\n        ASSERT_GE(counter.load(), 2UL);\n        thread.join();\n        ASSERT_EQ(3UL, counter.load());\n    }\n}\n\nTEST(BarrierTest, ConsecutiveTest) {\n    std::atomic<size_t> counter{0};\n    constexpr auto N = 64UL;\n    constexpr auto iters = 100UL;\n    auto completion = [&] () {\n        \/\/ At the completion phase, `counter' should be multiple to `N'.\n        ASSERT_EQ(0UL, counter.load() % N);\n    };\n\n    Barrier barrier(N, completion);\n    auto cb = [&] () {\n        auto i = iters;\n        while (i-- != 0) {\n            ++counter;\n            barrier.wait();\n        }\n    };\n\n    std::vector<std::thread> threads;\n    for (auto i = 0UL; i < N; i++) {\n        threads.emplace_back(cb);\n    }\n    for (auto &thread : threads) {\n        thread.join();\n    }\n    ASSERT_EQ(0UL, counter.load() % N);\n}\n\n}   \/\/ namespace concurrent\n}   \/\/ namespace vesoft\n","lang_cluster":"C++","length":96,"code_uid":"4593c87384734c609af284ea084ac5c7"}
{"diff_hunk":"@@ -40,14 +40,24 @@ JNIEXPORT jint JNICALL Java_org_apache_hadoop_hbase_client_transactional_RMInter\n    char la_tbldesc[TM_MAX_DDLREQUEST_STRING];\n    char la_tblname[TM_MAX_DDLREQUEST_STRING];\n    char* str_key;\n-   str_key = new char[TM_MAX_DDLREQUEST_STRING];\n    char** la_keys;\n    la_keys = new char *[TM_MAX_DDLREQUEST_STRING];\n    int lv_error = FEOK;\n \n+   if( pv_keyLength > TM_MAX_DDLREQUEST_STRING)\n+   {\n+     cout << \"Table key length is larger than max allowed \" << pv_keyLength << endl;\n+     delete [] la_keys;\n+     return FEBADKEYDESC;\n+   }\n+   str_key = new char[TM_MAX_DDLREQUEST_STRING];\n+\n    int lv_tblname_len = pp_env->GetArrayLength(pv_tblname);\n    if(lv_tblname_len > TM_MAX_DDLREQUEST_STRING) {\n       cout << \"Table name length is larger than max allowed\" << endl;\n+      delete [] la_keys;\n+      delete str_key;\n+      return FEBADNAME;\n    }\n    else {\n       int lv_tbldesc_length = pp_env->GetArrayLength(pv_tableDescriptor);","old_code":"\/*\n* @@@ START COPYRIGHT @@@                                                     \n*\n* Licensed to the Apache Software Foundation (ASF) under one\n* or more contributor license agreements.  See the NOTICE file\n* distributed with this work for additional information\n* regarding copyright ownership.  The ASF licenses this file\n* to you under the Apache License, Version 2.0 (the\n* \"License\"); you may not use this file except in compliance\n* with the License.  You may obtain a copy of the License at\n*\n*   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing,\n* software distributed under the License is distributed on an\n* \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n* KIND, either express or implied.  See the License for the\n* specific language governing permissions and limitations\n* under the License.\n*\n* @@@ END COPYRIGHT @@@                                                          *\/\n\n#include \"tmddlrequests.h\"\n#include \"dtm\/tm.h\"\n#include <string.h>\n#include <iostream>\n#include \"..\/..\/inc\/fs\/feerrors.h\" \n\nusing namespace std;\n\n\/*\n* Class:     org_apache_hadoop_hbase_client_transactional_RMInterface\n* Method:    createTableReq\n* Signature: ([B)V\n*\/\n\nJNIEXPORT jint JNICALL Java_org_apache_hadoop_hbase_client_transactional_RMInterface_createTableReq\n  (JNIEnv *pp_env, jobject pv_object, jbyteArray pv_tableDescriptor, jobjectArray pv_keys, jint pv_numSplits, jint pv_keyLength, jlong pv_transid, jbyteArray pv_tblname){\n\n   char la_tbldesc[TM_MAX_DDLREQUEST_STRING];\n   char la_tblname[TM_MAX_DDLREQUEST_STRING];\n   char* str_key;\n   str_key = new char[TM_MAX_DDLREQUEST_STRING];\n   char** la_keys;\n   la_keys = new char *[TM_MAX_DDLREQUEST_STRING];\n   int lv_error = FEOK;\n\n   int lv_tblname_len = pp_env->GetArrayLength(pv_tblname);\n   if(lv_tblname_len > TM_MAX_DDLREQUEST_STRING) {\n      cout << \"Table name length is larger than max allowed\" << endl;\n   }\n   else {\n      int lv_tbldesc_length = pp_env->GetArrayLength(pv_tableDescriptor);\n      memset(la_tbldesc, 0, lv_tbldesc_length);\n      jbyte *lp_tbldesc = pp_env->GetByteArrayElements(pv_tableDescriptor, 0);\n      memcpy(la_tbldesc, lp_tbldesc, lv_tbldesc_length);\n\n      memset(la_tblname, 0, lv_tblname_len < TM_MAX_DDLREQUEST_STRING ? lv_tblname_len : TM_MAX_DDLREQUEST_STRING);\n      jbyte *lp_tblname = pp_env->GetByteArrayElements(pv_tblname, 0);\n      memcpy(la_tblname, lp_tblname, lv_tblname_len < TM_MAX_DDLREQUEST_STRING ? lv_tblname_len : TM_MAX_DDLREQUEST_STRING -1 );\n\n      long lv_transid = (long) pv_transid;\n\n      \/\/ Keys for Salted Tables\n      int lv_numSplits = (int) pv_numSplits;\n      int lv_keyLength = (int) pv_keyLength;\n\n      for(int i=0; i<lv_numSplits; i++)\n      {\n         jbyteArray jba_keyarray = (jbyteArray)(pp_env->GetObjectArrayElement((jobjectArray)pv_keys, i));\n         int lv_key_len = pp_env->GetArrayLength(jba_keyarray);\n         pp_env->GetByteArrayRegion(jba_keyarray, 0, lv_key_len, (jbyte*)str_key);\n\n         la_keys[i] = new char[lv_key_len];\n         memcpy(la_keys[i], str_key, lv_key_len);\n\n         pp_env->DeleteLocalRef(jba_keyarray);\n      }\n\n      lv_error  = CREATETABLE(la_tbldesc, lv_tbldesc_length, la_tblname, la_keys, lv_numSplits, lv_keyLength, lv_transid);\n\n      pp_env->ReleaseByteArrayElements(pv_tableDescriptor, lp_tbldesc, 0);\n      pp_env->ReleaseByteArrayElements(pv_tblname, lp_tblname, 0);\n   }\n   return lv_error;\n}\n\n\n\/*\n * Class:     org_apache_hadoop_hbase_client_transactional_RMInterface\n * Method:    dropTableReq\n * Signature: ([BJ)V\n *\/\nJNIEXPORT jint JNICALL Java_org_apache_hadoop_hbase_client_transactional_RMInterface_dropTableReq\n  (JNIEnv *pp_env, jobject pv_object, jbyteArray pv_tblname, jlong pv_transid) {\n\n   char la_tblname[TM_MAX_DDLREQUEST_STRING];\n   int lv_error = FEOK;\n\n   int lv_tblname_len = pp_env->GetArrayLength(pv_tblname);\n   if(lv_tblname_len > TM_MAX_DDLREQUEST_STRING) {\n      cout << \"Table name length is larger than max allowed\" << endl;\n   }\n   else {\n      memset(la_tblname, 0, lv_tblname_len < TM_MAX_DDLREQUEST_STRING ? lv_tblname_len : TM_MAX_DDLREQUEST_STRING);\n      jbyte *lp_tblname = pp_env->GetByteArrayElements(pv_tblname, 0);\n      memcpy(la_tblname, lp_tblname, lv_tblname_len < TM_MAX_DDLREQUEST_STRING ? lv_tblname_len : TM_MAX_DDLREQUEST_STRING -1 );\n\n      long lv_transid = (long) pv_transid;\n\n      lv_error = DROPTABLE(la_tblname, lv_tblname_len, lv_transid);\n      pp_env->ReleaseByteArrayElements(pv_tblname, lp_tblname, 0);\n   }\n   return lv_error;\n}\n\n\/*\n * Class:     org_apache_hadoop_hbase_client_transactional_RMInterface\n * Method:    truncateOnAbortReq\n * Signature: ([BJ)V\n *\/\nJNIEXPORT jint JNICALL Java_org_apache_hadoop_hbase_client_transactional_RMInterface_truncateOnAbortReq\n  (JNIEnv *pp_env, jobject pv_object, jbyteArray pv_tblname, jlong pv_transid) {\n\n   char la_tblname[TM_MAX_DDLREQUEST_STRING];\n   int lv_error = FEOK;\n\n   int lv_tblname_len = pp_env->GetArrayLength(pv_tblname);\n   if(lv_tblname_len > TM_MAX_DDLREQUEST_STRING) {\n      cout << \"Table name length is larger than max allowed\" << endl;\n   }\n   else {\n      memset(la_tblname, 0, lv_tblname_len < TM_MAX_DDLREQUEST_STRING ? lv_tblname_len : TM_MAX_DDLREQUEST_STRING);\n      jbyte *lp_tblname = pp_env->GetByteArrayElements(pv_tblname, 0);\n      memcpy(la_tblname, lp_tblname, lv_tblname_len < TM_MAX_DDLREQUEST_STRING ? lv_tblname_len : TM_MAX_DDLREQUEST_STRING -1 );\n\n      long lv_transid = (long) pv_transid;\n\n      lv_error = REGTRUNCATEONABORT(la_tblname, lv_tblname_len, lv_transid);\n      pp_env->ReleaseByteArrayElements(pv_tblname, lp_tblname, 0);\n   }\n   return lv_error;\n}\n\n\/*\n * Class:     org_apache_hadoop_hbase_client_transactional_RMInterface\n * Method:    alterTableReq\n * Signature: ([B[Ljava\/lang\/Object;J)V\n *\/\nJNIEXPORT jint JNICALL Java_org_apache_hadoop_hbase_client_transactional_RMInterface_alterTableReq\n  (JNIEnv *pp_env, jobject pv_object, jbyteArray pv_tblName, jobjectArray pv_tableOptions, jlong pv_transID) {\n\n   int lv_error = FEOK;\n   int tblopts_len =0;\n   char la_tblname[TM_MAX_DDLREQUEST_STRING];\n\n   char** tbl_options;\n   tbl_options = new char *[TM_MAX_DDLREQUEST_STRING];\n\n   int lv_tblname_len = pp_env->GetArrayLength(pv_tblName);\n   if(lv_tblname_len > TM_MAX_DDLREQUEST_STRING) {\n      cout << \"Table name length is larger than max allowed\" << endl;\n   }\n   else {\n      memset(la_tblname, 0, lv_tblname_len < TM_MAX_DDLREQUEST_STRING ? lv_tblname_len : TM_MAX_DDLREQUEST_STRING);\n      jbyte *lp_tblname = pp_env->GetByteArrayElements(pv_tblName, 0);\n      memcpy(la_tblname, lp_tblname, lv_tblname_len < TM_MAX_DDLREQUEST_STRING ? lv_tblname_len : TM_MAX_DDLREQUEST_STRING -1 );\n\n      int tbloptions_cnt = pp_env->GetArrayLength(pv_tableOptions);\n\n      for (int i=0; i<tbloptions_cnt; i++) {\n\n         cout << \" TableOptions loop \" << i << endl;\n         jstring jstr_options = (jstring) pp_env->GetObjectArrayElement(pv_tableOptions, i);\n         const char *str_options = pp_env->GetStringUTFChars(jstr_options, 0);\n         \/\/ Don't forget to call `ReleaseStringUTFChars` when you're done.\n         \n         \/\/int str_opts_len = length(jstr_options);\n         \/\/int str_opts_len = pp_env->GetStringUTFLength(jstr_options);\n         int str_opts_len = sizeof(str_options)\/sizeof(*str_options);\n         cout << \"str_opts_len: \" << str_opts_len <<  \" or \" << sizeof(str_options)\/sizeof(*str_options) << endl;\n\n         tbl_options[i] = new char[tbloptions_cnt];\n         memcpy(tbl_options[i], str_options, str_opts_len);\n\n         pp_env->ReleaseStringUTFChars(jstr_options, str_options);\n\n         if(tblopts_len == 0)\n            tblopts_len = str_opts_len;\n      }\n\n      long lv_transid = (long) pv_transID;\n      lv_error = ALTERTABLE(la_tblname, lv_tblname_len, tbl_options, tblopts_len, tbloptions_cnt, lv_transid);\n      pp_env->ReleaseByteArrayElements(pv_tblName, lp_tblname, 0);\n   }\n   return lv_error;\n}\n \n","lang_cluster":"C++","length":198,"code_uid":"5fd3f3539a4f40c18ed15d598f699205"}
{"diff_hunk":"@@ -55,6 +55,12 @@ folly::StringPiece LogStrListIterator::logMsg() const {\n     return logEntries_.at(idx_).get_log_str();\n }\n \n+LogEntry LogStrListIterator::logEntry() {\n+    DCHECK(valid());\n+    return {firstLogId_ + idx_, term_, logEntries_.at(idx_).get_cluster(),\n+            logEntries_.at(idx_).get_log_str()};\n+}\n+\n }  \/\/ namespace raftex\n }  \/\/ namespace nebula\n ","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"base\/Base.h\"\n#include \"kvstore\/raftex\/LogStrListIterator.h\"\n\nnamespace nebula {\nnamespace raftex {\n\nLogStrListIterator::LogStrListIterator(\n    LogID firstLogId,\n    TermID term,\n    std::vector<cpp2::LogEntry> logEntries)\n        : firstLogId_(firstLogId)\n        , term_(term)\n        , logEntries_(std::move(logEntries)) {\n    idx_ = 0;\n}\n\n\nLogIterator& LogStrListIterator::operator++() {\n    ++idx_;\n    return *this;\n}\n\n\nbool LogStrListIterator::valid() const {\n    return idx_ < logEntries_.size();\n}\n\n\nLogID LogStrListIterator::logId() const {\n    DCHECK(valid());\n    return firstLogId_ + idx_;\n}\n\n\nTermID LogStrListIterator::logTerm() const {\n    DCHECK(valid());\n    return term_;\n}\n\n\nClusterID LogStrListIterator::logSource() const {\n    DCHECK(valid());\n    return logEntries_.at(idx_).get_cluster();\n}\n\n\nfolly::StringPiece LogStrListIterator::logMsg() const {\n    DCHECK(valid());\n    return logEntries_.at(idx_).get_log_str();\n}\n\n}  \/\/ namespace raftex\n}  \/\/ namespace nebula\n\n","lang_cluster":"C++","length":60,"code_uid":"1e7d6fcc01024d9abc9c3d53892b3e8c"}
{"diff_hunk":"@@ -64,4 +64,53 @@ BOOST_AUTO_TEST_CASE(test_match)\n     }\n }\n \n+BOOST_AUTO_TEST_CASE(test_match_split)\n+{\n+    using namespace osrm;\n+\n+    auto osrm = getOSRM(OSRM_TEST_DATA_DIR \"\/ch\/monaco.osrm\");\n+\n+    MatchParameters params;\n+    params.coordinates = get_split_trace_locations();\n+    params.timestamps = {1,2,1700,1800};\n+\n+    json::Object result;\n+\n+    const auto rc = osrm.Match(params, result);\n+\n+    BOOST_CHECK(rc == Status::Ok || rc == Status::Error);\n+    const auto code = result.values.at(\"code\").get<json::String>().value;\n+    BOOST_CHECK_EQUAL(code, \"Ok\");\n+\n+    const auto &tracepoints = result.values.at(\"tracepoints\").get<json::Array>().values;\n+    BOOST_CHECK_EQUAL(tracepoints.size(), params.coordinates.size());\n+\n+    const auto &matchings = result.values.at(\"matchings\").get<json::Array>().values;\n+    const auto &number_of_matchings = matchings.size();\n+    BOOST_CHECK_EQUAL(number_of_matchings, 2);\n+    for (const auto &waypoint : tracepoints)\n+    {\n+        if (waypoint.is<mapbox::util::recursive_wrapper<util::json::Object>>())\n+        {\n+            BOOST_CHECK(waypoint_check(waypoint));\n+            const auto &waypoint_object = waypoint.get<json::Object>();\n+            const auto matchings_index =\n+                waypoint_object.values.at(\"matchings_index\").get<json::Number>().value;\n+            const auto waypoint_index =\n+                waypoint_object.values.at(\"waypoint_index\").get<json::Number>().value;\n+            const auto &route_legs = matchings[matchings_index]\n+                                         .get<json::Object>()\n+                                         .values.at(\"legs\")\n+                                         .get<json::Array>()\n+                                         .values;\n+            BOOST_CHECK_LT(waypoint_index, route_legs.size() + 1);\n+            BOOST_CHECK_LT(matchings_index, number_of_matchings);\n+        }\n+        else\n+        {\n+            BOOST_CHECK(waypoint.is<json::Null>());\n+        }\n+    }\n+}\n+\n BOOST_AUTO_TEST_SUITE_END()","old_code":"#include <boost\/test\/test_case_template.hpp>\n#include <boost\/test\/unit_test.hpp>\n\n#include \"coordinates.hpp\"\n#include \"fixture.hpp\"\n#include \"waypoint_check.hpp\"\n\n#include \"osrm\/match_parameters.hpp\"\n\n#include \"osrm\/coordinate.hpp\"\n#include \"osrm\/engine_config.hpp\"\n#include \"osrm\/json_container.hpp\"\n#include \"osrm\/osrm.hpp\"\n#include \"osrm\/status.hpp\"\n\nBOOST_AUTO_TEST_SUITE(match)\n\nBOOST_AUTO_TEST_CASE(test_match)\n{\n    using namespace osrm;\n\n    auto osrm = getOSRM(OSRM_TEST_DATA_DIR \"\/ch\/monaco.osrm\");\n\n    MatchParameters params;\n    params.coordinates.push_back(get_dummy_location());\n    params.coordinates.push_back(get_dummy_location());\n    params.coordinates.push_back(get_dummy_location());\n\n    json::Object result;\n\n    const auto rc = osrm.Match(params, result);\n\n    BOOST_CHECK(rc == Status::Ok || rc == Status::Error);\n    const auto code = result.values.at(\"code\").get<json::String>().value;\n    BOOST_CHECK_EQUAL(code, \"Ok\");\n\n    const auto &tracepoints = result.values.at(\"tracepoints\").get<json::Array>().values;\n    BOOST_CHECK_EQUAL(tracepoints.size(), params.coordinates.size());\n\n    const auto &matchings = result.values.at(\"matchings\").get<json::Array>().values;\n    const auto &number_of_matchings = matchings.size();\n    for (const auto &waypoint : tracepoints)\n    {\n        if (waypoint.is<mapbox::util::recursive_wrapper<util::json::Object>>())\n        {\n            BOOST_CHECK(waypoint_check(waypoint));\n            const auto &waypoint_object = waypoint.get<json::Object>();\n            const auto matchings_index =\n                waypoint_object.values.at(\"matchings_index\").get<json::Number>().value;\n            const auto waypoint_index =\n                waypoint_object.values.at(\"waypoint_index\").get<json::Number>().value;\n            const auto &route_legs = matchings[matchings_index]\n                                         .get<json::Object>()\n                                         .values.at(\"legs\")\n                                         .get<json::Array>()\n                                         .values;\n            BOOST_CHECK_LT(waypoint_index, route_legs.size() + 1);\n            BOOST_CHECK_LT(matchings_index, number_of_matchings);\n        }\n        else\n        {\n            BOOST_CHECK(waypoint.is<json::Null>());\n        }\n    }\n}\n\nBOOST_AUTO_TEST_SUITE_END()\n","lang_cluster":"C++","length":67,"code_uid":"5fa33df9e0694a769b87195b96135129"}
{"diff_hunk":"@@ -28,6 +28,21 @@ void DropEdgeIndexProcessor::process(const cpp2::DropEdgeIndexReq& req) {\n     }\n \n     std::vector<std::string> keys;\n+    \/\/ delete rebuild index status info\n+    auto indexStatusKey = MetaServiceUtils::rebuildIndexStatus(spaceID, 'E', indexName);\n+    std::string status;\n+    auto ret = kvstore_->get(kDefaultSpaceId, kDefaultPartId, indexStatusKey, &status);\n+    if (ret == kvstore::ResultCode::SUCCEEDED) {\n+        if (status == \"RUNNING\") {\n+            LOG(ERROR) << \"Edge Index status is RUNNING, drop index is not allowed. Index : \"\n+                       << indexName;\n+            handleErrorCode(cpp2::ErrorCode::E_CONFLICT);\n+            onFinished();\n+            return;\n+        } else {\n+            keys.emplace_back(std::move(indexStatusKey));\n+        }\n+    }\n     keys.emplace_back(MetaServiceUtils::indexIndexKey(spaceID, indexName));\n     keys.emplace_back(MetaServiceUtils::indexKey(spaceID, edgeIndexID.value()));\n ","old_code":"\/* Copyright (c) 2019 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"meta\/processors\/indexMan\/DropEdgeIndexProcessor.h\"\n\nnamespace nebula {\nnamespace meta {\n\nvoid DropEdgeIndexProcessor::process(const cpp2::DropEdgeIndexReq& req) {\n    auto spaceID = req.get_space_id();\n    auto indexName = req.get_index_name();\n    CHECK_SPACE_ID_AND_RETURN(spaceID);\n    folly::SharedMutex::WriteHolder wHolder(LockUtils::edgeIndexLock());\n\n    auto edgeIndexID = getIndexID(spaceID, indexName);\n    if (!edgeIndexID.ok()) {\n        LOG(ERROR) << \"Edge Index not exists in Space: \" << spaceID << \" Index name: \" << indexName;\n        if (req.get_if_exists()) {\n            handleErrorCode(cpp2::ErrorCode::SUCCEEDED);\n        } else {\n            handleErrorCode(cpp2::ErrorCode::E_NOT_FOUND);\n        }\n        onFinished();\n        return;\n    }\n\n    std::vector<std::string> keys;\n    keys.emplace_back(MetaServiceUtils::indexIndexKey(spaceID, indexName));\n    keys.emplace_back(MetaServiceUtils::indexKey(spaceID, edgeIndexID.value()));\n\n    LOG(INFO) << \"Drop Edge Index \" << indexName;\n    resp_.set_id(to(edgeIndexID.value(), EntryType::INDEX));\n    doSyncMultiRemoveAndUpdate(std::move(keys));\n}\n\n}  \/\/ namespace meta\n}  \/\/ namespace nebula\n\n","lang_cluster":"C++","length":41,"code_uid":"b27d506e9d9a4c41a36a891db099b72d"}
{"diff_hunk":"@@ -40,6 +40,8 @@ void DropSpaceExecutor::execute() {\n         if (*spaceName_ == ectx()->rctx()->session()->spaceName()) {\n             ectx()->rctx()->session()->setSpace(\"\", -1);\n         }\n+        ectx()->addWarningMsg(\"Data will be deleted completely after restarting the services\");\n+\n         doFinish(Executor::ProcessControl::kNext);\n     };\n ","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"graph\/DropSpaceExecutor.h\"\n\nnamespace nebula {\nnamespace graph {\n\nDropSpaceExecutor::DropSpaceExecutor(Sentence *sentence,\n                                     ExecutionContext *ectx)\n    : Executor(ectx, \"drop_space\") {\n    sentence_ = static_cast<DropSpaceSentence*>(sentence);\n}\n\n\nStatus DropSpaceExecutor::prepare() {\n    spaceName_ = sentence_->spaceName();\n    return Status::OK();\n}\n\n\nvoid DropSpaceExecutor::execute() {\n    auto future = ectx()->getMetaClient()->dropSpace(*spaceName_, sentence_->isIfExists());\n    auto *runner = ectx()->rctx()->runner();\n\n    auto cb = [this] (auto &&resp) {\n        if (!resp.ok()) {\n            doError(std::move(resp).status());\n            return;\n        }\n        auto  ret = std::move(resp).value();\n        if (!ret) {\n            doError(Status::Error(\"Drop space `%s' failed.\", spaceName_->c_str()));\n            return;\n        }\n\n        if (*spaceName_ == ectx()->rctx()->session()->spaceName()) {\n            ectx()->rctx()->session()->setSpace(\"\", -1);\n        }\n        doFinish(Executor::ProcessControl::kNext);\n    };\n\n    auto error = [this] (auto &&e) {\n        auto msg = folly::stringPrintf(\"Drop space `%s' exception: %s\",\n                spaceName_->c_str(), e.what().c_str());\n        LOG(ERROR) << msg;\n        doError(Status::Error(std::move(msg)));\n        return;\n    };\n\n    std::move(future).via(runner).thenValue(cb).thenError(error);\n}\n\nvoid DropSpaceExecutor::setupResponse(cpp2::ExecutionResponse &resp)  {\n    resp.set_warning_msg(\"Data will be deleted completely after restarting the services.\");\n}\n}   \/\/ namespace graph\n}   \/\/ namespace nebula\n","lang_cluster":"C++","length":61,"code_uid":"37f9c47fa4624c68bd939ba916fe9f5a"}
{"diff_hunk":"@@ -120,8 +120,16 @@ int main(int argc, char **argv)\n     MPI_Init(nullptr, nullptr);\n #endif\n \n-    ::testing::InitGoogleTest(&argc, argv);\n-    int result = RUN_ALL_TESTS();\n+    int result = -1;\n+    try\n+    {\n+        ::testing::InitGoogleTest(&argc, argv);\n+        result = RUN_ALL_TESTS();\n+    }\n+    catch (std::exception &e)\n+    {\n+        result = 1;\n+    }\n \n #ifdef ADIOS2_HAVE_MPI\n     MPI_Finalize();","old_code":"\/*\n * Distributed under the OSI-approved Apache License, Version 2.0.  See\n * accompanying file Copyright.txt for details.\n *\n * TestBPWriteTypes.c\n *\n *  Created on: Aug 9, 2017\n *      Author: Haocheng\n *\/\n\n#include <adios2_c.h>\n\n#ifdef ADIOS2_HAVE_MPI\n#include <mpi.h>\n#endif\n\n#include <gtest\/gtest.h>\n\n#include \"SmallTestData_c.h\"\n\nclass BPWriteTypes : public ::testing::Test\n{\npublic:\n    BPWriteTypes() = default;\n};\n\nTEST_F(BPWriteTypes, ADIOS2BPWriteTypes)\n{\n#ifdef ADIOS2_HAVE_MPI\n    int rank(0);\n    int size(0);\n    adios2_adios *adiosH = adios2_init(MPI_COMM_WORLD, adios2_debug_mode_on);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n#else\n    adios2_adios *adiosH = adios2_init_nompi(adios2_debug_mode_on);\n#endif\n\n    \/\/ IO\n    adios2_io *ioH = adios2_declare_io(adiosH, \"CArrayTypes\");\n    \/\/ Set engine parameters\n    adios2_set_engine(ioH, \"BPFile\");\n    adios2_set_parameter(ioH, \"ProfileUnits\", \"Microseconds\");\n    adios2_set_parameter(ioH, \"Threads\", \"1\");\n\n    \/\/ Set transport and parameters\n    const unsigned int transportID = adios2_add_transport(ioH, \"File\");\n    adios2_set_transport_parameter(ioH, transportID, \"library\", \"fstream\");\n\n    \/\/ count dims are allocated in stack\n    size_t count[1];\n    count[0] = data_Nx;\n\n    \/\/ Define variables in ioH\n    {\n\n        adios2_define_variable(ioH, \"varI8\", adios2_type_int8_t, 1, NULL, NULL,\n                               count, adios2_constant_dims_true, NULL);\n        adios2_define_variable(ioH, \"varI16\", adios2_type_int16_t, 1, NULL,\n                               NULL, count, adios2_constant_dims_true, NULL);\n        adios2_define_variable(ioH, \"varI32\", adios2_type_int32_t, 1, NULL,\n                               NULL, count, adios2_constant_dims_true, NULL);\n        adios2_define_variable(ioH, \"varI64\", adios2_type_int64_t, 1, NULL,\n                               NULL, count, adios2_constant_dims_true, NULL);\n\n        adios2_define_variable(ioH, \"varU8\", adios2_type_uint8_t, 1, NULL, NULL,\n                               count, adios2_constant_dims_true, NULL);\n        adios2_define_variable(ioH, \"varU16\", adios2_type_uint16_t, 1, NULL,\n                               NULL, count, adios2_constant_dims_true, NULL);\n        adios2_define_variable(ioH, \"varU32\", adios2_type_uint32_t, 1, NULL,\n                               NULL, count, adios2_constant_dims_true, NULL);\n        adios2_define_variable(ioH, \"varU64\", adios2_type_uint64_t, 1, NULL,\n                               NULL, count, adios2_constant_dims_true, NULL);\n\n        adios2_define_variable(ioH, \"varR32\", adios2_type_float, 1, NULL, NULL,\n                               count, adios2_constant_dims_true, NULL);\n        adios2_define_variable(ioH, \"varR64\", adios2_type_double, 1, NULL, NULL,\n                               count, adios2_constant_dims_true, NULL);\n    }\n    \/\/ inquire variables\n    adios2_variable *varI8 = adios2_inquire_variable(ioH, \"varI8\");\n    adios2_variable *varI16 = adios2_inquire_variable(ioH, \"varI16\");\n    adios2_variable *varI32 = adios2_inquire_variable(ioH, \"varI32\");\n    adios2_variable *varI64 = adios2_inquire_variable(ioH, \"varI64\");\n    adios2_variable *varU8 = adios2_inquire_variable(ioH, \"varU8\");\n    adios2_variable *varU16 = adios2_inquire_variable(ioH, \"varU16\");\n    adios2_variable *varU32 = adios2_inquire_variable(ioH, \"varU32\");\n    adios2_variable *varU64 = adios2_inquire_variable(ioH, \"varU64\");\n    adios2_variable *varR32 = adios2_inquire_variable(ioH, \"varR32\");\n    adios2_variable *varR64 = adios2_inquire_variable(ioH, \"varR64\");\n\n    adios2_engine *engineH = adios2_open(ioH, \"ctypes.bp\", adios2_mode_write);\n\n    adios2_put_sync(engineH, varI8, data_I8);\n    adios2_put_sync(engineH, varI16, data_I16);\n    adios2_put_sync(engineH, varI32, data_I32);\n    adios2_put_sync(engineH, varI64, data_I64);\n\n    adios2_put_sync(engineH, varU8, data_U8);\n    adios2_put_sync(engineH, varU16, data_U16);\n    adios2_put_sync(engineH, varU32, data_U32);\n    adios2_put_sync(engineH, varU64, data_U64);\n\n    adios2_put_sync(engineH, varR32, data_R32);\n    adios2_put_sync(engineH, varR64, data_R64);\n\n    adios2_close(engineH);\n\n    \/\/ deallocate adiosH\n    adios2_finalize(adiosH);\n}\n\n\/\/******************************************************************************\n\/\/ main\n\/\/******************************************************************************\n\nint main(int argc, char **argv)\n{\n#ifdef ADIOS2_HAVE_MPI\n    MPI_Init(nullptr, nullptr);\n#endif\n\n    ::testing::InitGoogleTest(&argc, argv);\n    int result = RUN_ALL_TESTS();\n\n#ifdef ADIOS2_HAVE_MPI\n    MPI_Finalize();\n#endif\n\n    return result;\n}\n","lang_cluster":"C++","length":131,"code_uid":"07d7e75f620f460887d5d85b84786c05"}
{"diff_hunk":"@@ -27,7 +27,7 @@ struct train_kernel_cpu<Float, method::brute_force> {\n     train_result operator()(const context_cpu& ctx,\n                             const descriptor_base& desc,\n                             const train_input& input) const {\n-        throw unimplemented_error(\"k-NN brute force method is not implemented for CPU!\");\n+        throw unimplemented_error(\"k-NN brute force method is not implemented for CPU\");\n         return train_result();\n     }\n };","old_code":"\/*******************************************************************************\n* Copyright 2020 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n#include \"oneapi\/dal\/algo\/knn\/backend\/cpu\/train_kernel.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/common.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/error_converter.hpp\"\n\nnamespace oneapi::dal::knn::backend {\n\nusing dal::backend::context_cpu;\n\ntemplate <typename Float>\nstruct train_kernel_cpu<Float, method::brute_force> {\n    train_result operator()(const context_cpu& ctx,\n                            const descriptor_base& desc,\n                            const train_input& input) const {\n        throw unimplemented_error(\"k-NN brute force method is not implemented for CPU!\");\n        return train_result();\n    }\n};\n\ntemplate struct train_kernel_cpu<float, method::brute_force>;\ntemplate struct train_kernel_cpu<double, method::brute_force>;\n\n} \/\/ namespace oneapi::dal::knn::backend\n","lang_cluster":"C++","length":38,"code_uid":"62ae6495e1e2474f875581278aab6a43"}
{"diff_hunk":"@@ -35,15 +35,17 @@ int main(int argc, char *argv[])\n         adios2::Variable<float> bpFloats = bpIO.DefineVariable<float>(\n             \"bpFloats\", {}, {}, {Nx}, adios2::ConstantDims);\n \n+        std::string filename = \"myVector_cpp.bp\";\n         \/** Engine derived class, spawned to start IO operations *\/\n         adios2::Engine bpWriter =\n-            bpIO.Open(\"myVector_cpp.bp\", adios2::Mode::Write);\n+            bpIO.Open(filename, adios2::Mode::Write);\n \n         \/** Write variable for buffering *\/\n         bpWriter.Put<float>(bpFloats, myFloats.data());\n \n         \/** Create bp file, engine becomes unreachable after this*\/\n         bpWriter.Close();\n+        std::cout << \"Wrote file \" << filename << \" to disk. It can now be read by running .\/bin\/hello_bpReader.\\n\";\n     }\n     catch (std::invalid_argument &e)\n     {","old_code":"\/*\n * Distributed under the OSI-approved Apache License, Version 2.0.  See\n * accompanying file Copyright.txt for details.\n *\n * helloBPWriter_nompi.cpp sequential non-mpi version of helloBPWriter\n *\n *  Created on: Jan 9, 2017\n *      Author: William F Godoy godoywf@ornl.gov\n *\/\n\n#include <ios>       \/\/std::ios_base::failure\n#include <iostream>  \/\/std::cout\n#include <stdexcept> \/\/std::invalid_argument std::exception\n#include <vector>\n\n#include <adios2.h>\n\nint main(int argc, char *argv[])\n{\n    \/** Application variable *\/\n    std::vector<float> myFloats = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n    const std::size_t Nx = myFloats.size();\n\n    try\n    {\n        \/** ADIOS class factory of IO class objects, DebugON is recommended *\/\n        adios2::ADIOS adios(adios2::DebugON);\n\n        \/*** IO class object: settings and factory of Settings: Variables,\n         * Parameters, Transports, and Execution: Engines *\/\n        adios2::IO bpIO = adios.DeclareIO(\"BPFile_N2N\");\n\n        \/** global array: name, { shape (total dimensions) }, { start (local) },\n         * { count (local) }, all are constant dimensions *\/\n        adios2::Variable<float> bpFloats = bpIO.DefineVariable<float>(\n            \"bpFloats\", {}, {}, {Nx}, adios2::ConstantDims);\n\n        \/** Engine derived class, spawned to start IO operations *\/\n        adios2::Engine bpWriter =\n            bpIO.Open(\"myVector_cpp.bp\", adios2::Mode::Write);\n\n        \/** Write variable for buffering *\/\n        bpWriter.Put<float>(bpFloats, myFloats.data());\n\n        \/** Create bp file, engine becomes unreachable after this*\/\n        bpWriter.Close();\n    }\n    catch (std::invalid_argument &e)\n    {\n        std::cout << \"Invalid argument exception, STOPPING PROGRAM\\n\";\n        std::cout << e.what() << \"\\n\";\n    }\n    catch (std::ios_base::failure &e)\n    {\n        std::cout << \"IO System base failure exception, STOPPING PROGRAM\\n\";\n        std::cout << e.what() << \"\\n\";\n    }\n    catch (std::exception &e)\n    {\n        std::cout << \"Exception, STOPPING PROGRAM\\n\";\n        std::cout << e.what() << \"\\n\";\n    }\n\n    return 0;\n}\n","lang_cluster":"C++","length":65,"code_uid":"2adce2a9c9694b8090b0e73cf3b2945a"}
{"diff_hunk":"@@ -47,12 +47,13 @@\n \n namespace lbann {\n \n-lbann_comm* initialize(int& argc, char**& argv, int seed) {\n+world_comm_ptr initialize(int& argc, char**& argv, int seed) {\n   \/\/ Initialize Elemental.\n   El::Initialize(argc, argv);\n   \/\/ Create a new comm object.\n   \/\/ Initial creation with every process in one model.\n-  auto* comm = new lbann_comm(0);\n+  auto comm = world_comm_ptr{new lbann_comm(0), &lbann::finalize };\n+\n #if defined(LBANN_TOPO_AWARE)\n   \/\/ Determine the number of NUMA nodes present.\n   hwloc_topology_t topo;","old_code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/ Copyright (c) 2014-2016, Lawrence Livermore National Security, LLC.\n\/\/ Produced at the Lawrence Livermore National Laboratory.\n\/\/ Written by the LBANN Research Team (B. Van Essen, et al.) listed in\n\/\/ the CONTRIBUTORS file. <lbann-dev@llnl.gov>\n\/\/\n\/\/ LLNL-CODE-697807.\n\/\/ All rights reserved.\n\/\/\n\/\/ This file is part of LBANN: Livermore Big Artificial Neural Network\n\/\/ Toolkit. For details, see http:\/\/software.llnl.gov\/LBANN or\n\/\/ https:\/\/github.com\/LLNL\/LBANN.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"Licensee\"); you\n\/\/ may not use this file except in compliance with the License.  You may\n\/\/ obtain a copy of the License at:\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n\/\/ implied. See the License for the specific language governing\n\/\/ permissions and limitations under the license.\n\/\/\n\/\/ lbann_base .cpp - Basic definitions, functions\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n#include \"lbann\/base.hpp\"\n\n#include <omp.h>\n#if defined(LBANN_TOPO_AWARE)\n#include <hwloc.h>\n#if defined(HWLOC_API_VERSION) && (HWLOC_API_VERSION < 0x00010b00)\n#define HWLOC_OBJ_NUMANODE HWLOC_OBJ_NODE\n#endif\n#endif\n\n#include \"lbann\/comm.hpp\"\n#include \"lbann\/utils\/random.hpp\"\n#include \"lbann\/utils\/omp_diagnostics.hpp\"\n#include \"lbann\/utils\/stack_trace.hpp\"\n\n#ifdef LBANN_HAS_CUDNN\n#include \"lbann\/utils\/cudnn.hpp\"\n#endif\n\nnamespace lbann {\n\nlbann_comm* initialize(int& argc, char**& argv, int seed) {\n  \/\/ Initialize Elemental.\n  El::Initialize(argc, argv);\n  \/\/ Create a new comm object.\n  \/\/ Initial creation with every process in one model.\n  auto* comm = new lbann_comm(0);\n#if defined(LBANN_TOPO_AWARE)\n  \/\/ Determine the number of NUMA nodes present.\n  hwloc_topology_t topo;\n  hwloc_topology_init(&topo);\n  hwloc_topology_load(topo);\n  int numa_depth = hwloc_get_type_depth(topo, HWLOC_OBJ_NUMANODE);\n  if (numa_depth == HWLOC_TYPE_DEPTH_UNKNOWN) {\n    std::cout << comm->get_rank_in_world() <<\n              \": cannot determine hwloc NUMA-node depth\" << std::endl;\n  }\n  int num_numa_nodes = hwloc_get_nbobjs_by_depth(topo, numa_depth);\n  \/\/ Warn if there are more NUMA nodes than processes per node.\n  \/\/ It's probably fine if there are more processes than NUMA nodes for now.\n  \/\/ We can adjust that later when we better understand the threaded perf.\n  int ppn = comm->get_procs_per_node();\n  if (num_numa_nodes > ppn) {\n    if (comm->get_rank_in_node() == 0) {\n      std::cout << comm->get_rank_in_world() <<\n                \": WARNING: node has \" << num_numa_nodes <<\n                \" NUMA nodes but you have \" << ppn << \" processes per node\" <<\n                std::endl;\n    }\n  }\n  hwloc_topology_destroy(topo);\n#endif\n  \/\/ Initialize local random number generators.\n  init_random(seed);\n  init_data_seq_random(seed);\n\n  return comm;\n}\n\nvoid finalize(lbann_comm* comm) {\n#ifdef LBANN_HAS_CUDNN\n  cudnn::destroy();\n#endif\n  if (comm != nullptr) {\n    delete comm;\n  }\n  El::Finalize();\n}\n\n\/** hack to avoid long switch\/case statement; users should ignore; of interest to developers *\/\nstatic std::vector<std::string> pool_mode_names = { \"invalid\", \"max\", \"average\", \"average_no_pad\" };\n\n\/** returns a string representation of the pool_mode *\/\nstd::string get_pool_mode_name(pool_mode m) {\n  if ((int)m < 1 or (int)m >= (int)pool_mode_names.size()) {\n    throw(std::string{} + __FILE__ + \" \" + std::to_string(__LINE__) + \" :: \"\n          + \" Invalid pool_mode\");\n  }\n  return pool_mode_names[(int)m];\n}\n\n} \/\/ namespace lbann\n","lang_cluster":"C++","length":110,"code_uid":"25bfbad004364eaca30aa0436bf6981e"}
{"diff_hunk":"@@ -15,6 +15,10 @@ ExecutionContext::~ExecutionContext() {\n         sm_ = nullptr;\n     }\n \n+    if (nullptr != gflagsManager_) {\n+        gflagsManager_ = nullptr;\n+    }\n+\n     if (nullptr != storage_) {\n         storage_ = nullptr;\n     }","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"base\/Base.h\"\n#include \"graph\/ExecutionContext.h\"\n\nnamespace nebula {\nnamespace graph {\n\nExecutionContext::~ExecutionContext() {\n    if (nullptr != sm_) {\n        sm_ = nullptr;\n    }\n\n    if (nullptr != storage_) {\n        storage_ = nullptr;\n    }\n\n    if (nullptr != metaClient_) {\n        metaClient_ = nullptr;\n    }\n}\n\n}   \/\/ namespace graph\n}   \/\/ namespace nebula\n","lang_cluster":"C++","length":28,"code_uid":"a84259c81d5f4a61bffa4984e4895959"}
{"diff_hunk":"@@ -22,8 +22,8 @@\n #include \"depotchest.h\"\n #include \"tools.h\"\n \n-DepotChest::DepotChest(uint16_t type) :\n-\tContainer(type), maxDepotItems(2000) {}\n+DepotChest::DepotChest(uint16_t type, bool paginated \/*= true*\/) :\n+\tContainer(type, items[type].maxItems, true, paginated), maxDepotItems(2000) {}\n \n ReturnValue DepotChest::queryAdd(int32_t index, const Thing& thing, uint32_t count,\n \t\tuint32_t flags, Creature* actor\/* = nullptr*\/) const","old_code":"\/**\n * The Forgotten Server - a free and open-source MMORPG server emulator\n * Copyright (C) 2019  Mark Samman <mark.samman@gmail.com>\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n *\/\n\n#include \"otpch.h\"\n\n#include \"depotchest.h\"\n#include \"tools.h\"\n\nDepotChest::DepotChest(uint16_t type) :\n\tContainer(type), maxDepotItems(2000) {}\n\nReturnValue DepotChest::queryAdd(int32_t index, const Thing& thing, uint32_t count,\n\t\tuint32_t flags, Creature* actor\/* = nullptr*\/) const\n{\n\tconst Item* item = thing.getItem();\n\tif (!item) {\n\t\treturn RETURNVALUE_NOTPOSSIBLE;\n\t}\n\n\tbool skipLimit = hasBitSet(FLAG_NOLIMIT, flags);\n\tif (!skipLimit) {\n\t\tint32_t addCount = 0;\n\n\t\tif ((item->isStackable() && item->getItemCount() != count)) {\n\t\t\taddCount = 1;\n\t\t}\n\n\t\tif (item->getTopParent() != this) {\n\t\t\tif (const Container* container = item->getContainer()) {\n\t\t\t\taddCount = container->getItemHoldingCount() + 1;\n\t\t\t} else {\n\t\t\t\taddCount = 1;\n\t\t\t}\n\t\t}\n\n\t\tif (getItemHoldingCount() + addCount > maxDepotItems) {\n\t\t\treturn RETURNVALUE_DEPOTISFULL;\n\t\t}\n\t}\n\n\treturn Container::queryAdd(index, thing, count, flags, actor);\n}\n\nvoid DepotChest::postAddNotification(Thing* thing, const Cylinder* oldParent, int32_t index, cylinderlink_t)\n{\n\tCylinder* parent = getParent();\n\tif (parent) {\n\t\tparent->postAddNotification(thing, oldParent, index, LINK_PARENT);\n\t}\n}\n\nvoid DepotChest::postRemoveNotification(Thing* thing, const Cylinder* newParent, int32_t index, cylinderlink_t)\n{\n\tCylinder* parent = getParent();\n\tif (parent) {\n\t\tparent->postRemoveNotification(thing, newParent, index, LINK_PARENT);\n\t}\n}\n\nCylinder* DepotChest::getParent() const\n{\n\tif (parent) {\n\t\treturn parent->getParent();\n\t}\n\treturn nullptr;\n}\n","lang_cluster":"C++","length":82,"code_uid":"ef34c6ac500c4dcf9ad16a254c3b3d84"}
{"diff_hunk":"@@ -38,17 +38,14 @@ namespace interface2\n  * \\DAAL_DEPRECATED_USE{ Model::create }\n  *\/\n template <typename modelFPType>\n-DAAL_EXPORT Model::Model(size_t nFeatures, modelFPType dummy) :\n-    _nFeatures(nFeatures),\n-    _models(new data_management::DataCollection())\n+DAAL_EXPORT Model::Model(size_t nFeatures, modelFPType dummy) : _nFeatures(nFeatures), _models(new data_management::DataCollection())\n {\n     _alpha = data_management::NumericTablePtr(new data_management::HomogenNumericTable<modelFPType>(NULL, 1, 0));\n }\n \n template <typename modelFPType>\n-DAAL_EXPORT Model::Model(size_t nFeatures, modelFPType dummy, services::Status &st) :\n-    _nFeatures(nFeatures),\n-    _models(new data_management::DataCollection())\n+DAAL_EXPORT Model::Model(size_t nFeatures, modelFPType dummy, services::Status & st)\n+    : _nFeatures(nFeatures), _models(new data_management::DataCollection())\n {\n     if (!_models) { st.add(services::ErrorMemoryAllocationFailed); }\n     _alpha = data_management::HomogenNumericTable<modelFPType>::create(NULL, 1, 0, &st);","old_code":"\/* file: adaboost_model_fpt.cpp *\/\n\/*******************************************************************************\n* Copyright 2014-2019 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n\/*\n\/\/++\n\/\/  Implementation of class defining Ada Boost model\n\/\/--\n*\/\n\n#include \"algorithms\/boosting\/adaboost_model.h\"\n\nnamespace daal\n{\nnamespace algorithms\n{\nnamespace adaboost\n{\nnamespace interface2\n{\n\/**\n * Constructs the AdaBoost model\n * \\tparam modelFPType  Data type to store AdaBoost model data, double or float\n * \\param[in] dummy     Dummy variable for the templated constructor\n * \\DAAL_DEPRECATED_USE{ Model::create }\n *\/\ntemplate <typename modelFPType>\nDAAL_EXPORT Model::Model(size_t nFeatures, modelFPType dummy) :\n    _nFeatures(nFeatures),\n    _models(new data_management::DataCollection())\n{\n    _alpha = data_management::NumericTablePtr(new data_management::HomogenNumericTable<modelFPType>(NULL, 1, 0));\n}\n\ntemplate <typename modelFPType>\nDAAL_EXPORT Model::Model(size_t nFeatures, modelFPType dummy, services::Status &st) :\n    _nFeatures(nFeatures),\n    _models(new data_management::DataCollection())\n{\n    if (!_models) { st.add(services::ErrorMemoryAllocationFailed); }\n    _alpha = data_management::HomogenNumericTable<modelFPType>::create(NULL, 1, 0, &st);\n}\n\n\/**\n * Constructs the AdaBoost model\n * \\tparam modelFPType   Data type to store AdaBoost model data, double or float\n * \\param[in]  nFeatures Number of features in the dataset\n * \\param[out] stat      Status of the model construction\n *\/\ntemplate<typename modelFPType>\nDAAL_EXPORT ModelPtr Model::create(size_t nFeatures, services::Status *stat)\n{\n    DAAL_DEFAULT_CREATE_IMPL_EX(Model, nFeatures, (modelFPType)0 \/* dummy *\/);\n}\n\ntemplate DAAL_EXPORT Model::Model(size_t, DAAL_FPTYPE);\ntemplate DAAL_EXPORT Model::Model(size_t, DAAL_FPTYPE, services::Status &);\ntemplate DAAL_EXPORT ModelPtr Model::create<DAAL_FPTYPE>(size_t, services::Status *);\n\n}\/\/ namespace interface2\n}\/\/ namespace adaboost\n}\/\/ namespace algorithms\n}\/\/ namespace daal\n","lang_cluster":"C++","length":76,"code_uid":"c2bb7c2ba1a7401b8ffab376b0c9961b"}
{"diff_hunk":"@@ -132,6 +132,7 @@ static train_result<Task> call_daal_kernel(const context_cpu& ctx,\n         daal::services::internal::hostApp(daal_input),\n         daal_data.get(),\n         daal_labels.get(),\n+        nullptr, \/\/ no weights\n         *mptr,\n         daal_result,\n         daal_parameter));","old_code":"\/*******************************************************************************\n* Copyright 2020 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n#include <daal\/include\/services\/error_handling.h>\n#include <daal\/src\/algorithms\/dtrees\/forest\/classification\/df_classification_model_impl.h>\n#include <daal\/src\/services\/service_algo_utils.h>\n\n#include <daal\/include\/algorithms\/decision_forest\/decision_forest_classification_training_batch.h>\n#include <daal\/include\/algorithms\/decision_forest\/decision_forest_classification_training_types.h>\n\n#include <daal\/src\/algorithms\/dtrees\/forest\/classification\/df_classification_train_kernel.h>\n\/\/to prevent reordering by clang-format\n#include <daal\/src\/algorithms\/dtrees\/forest\/classification\/df_classification_train_dense_default_kernel.h>\n\n#include \"oneapi\/dal\/algo\/decision_forest\/backend\/cpu\/train_kernel.hpp\"\n#include \"oneapi\/dal\/algo\/decision_forest\/backend\/interop_helpers.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/common.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/error_converter.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/table_conversion.hpp\"\n#include \"oneapi\/dal\/detail\/common.hpp\"\n\n#include \"oneapi\/dal\/table\/row_accessor.hpp\"\n\nnamespace oneapi::dal::decision_forest::backend {\n\nusing dal::backend::context_cpu;\n\nnamespace df = daal::algorithms::decision_forest;\nnamespace cls = daal::algorithms::decision_forest::classification;\n\nnamespace interop = dal::backend::interop;\nnamespace df_interop = dal::backend::interop::decision_forest;\n\ntemplate <typename Float, daal::CpuType Cpu>\nusing cls_dense_kernel_t = cls::training::internal::\n    ClassificationTrainBatchKernel<Float, cls::training::defaultDense, Cpu>;\n\nusing cls_model_p = cls::ModelPtr;\n\ntemplate <typename Float, typename Task>\nstatic train_result<Task> call_daal_kernel(const context_cpu& ctx,\n                                           const descriptor_base<Task>& desc,\n                                           const table& data,\n                                           const table& labels) {\n    const int64_t row_count = data.get_row_count();\n    const int64_t column_count = data.get_column_count();\n\n    auto arr_data = row_accessor<const Float>{ data }.pull();\n    auto arr_label = row_accessor<const Float>{ labels }.pull();\n\n    const auto daal_data =\n        interop::convert_to_daal_homogen_table(arr_data, row_count, column_count);\n    const auto daal_labels = interop::convert_to_daal_homogen_table(arr_label, row_count, 1);\n\n    \/* init param for daal kernel *\/\n    auto daal_input = daal::algorithms::classifier::training::Input();\n    daal_input.set(daal::algorithms::classifier::training::data, daal_data);\n    daal_input.set(daal::algorithms::classifier::training::labels, daal_labels);\n\n    auto daal_parameter = cls::training::Parameter(desc.get_class_count());\n    daal_parameter.nTrees = desc.get_tree_count();\n    daal_parameter.observationsPerTreeFraction = desc.get_observations_per_tree_fraction();\n    daal_parameter.featuresPerNode = desc.get_features_per_node();\n    daal_parameter.maxTreeDepth = desc.get_max_tree_depth();\n    daal_parameter.minObservationsInLeafNode = desc.get_min_observations_in_leaf_node();\n    \/\/ TODO take engines from desc\n    daal_parameter.engine = daal::algorithms::engines::mt2203::Batch<>::create();\n    daal_parameter.impurityThreshold = desc.get_impurity_threshold();\n    daal_parameter.memorySavingMode = desc.get_memory_saving_mode();\n    daal_parameter.bootstrap = desc.get_bootstrap();\n    daal_parameter.minObservationsInSplitNode = desc.get_min_observations_in_split_node();\n    daal_parameter.minWeightFractionInLeafNode = desc.get_min_weight_fraction_in_leaf_node();\n    daal_parameter.minImpurityDecreaseInSplitNode = desc.get_min_impurity_decrease_in_split_node();\n    daal_parameter.maxLeafNodes = desc.get_max_leaf_nodes();\n\n    daal_parameter.resultsToCompute = static_cast<std::uint64_t>(desc.get_error_metric_mode());\n\n    auto vimp = desc.get_variable_importance_mode();\n\n    daal_parameter.varImportance = df_interop::convert_to_daal_variable_importance_mode(vimp);\n\n    train_result<Task> res;\n\n    auto daal_result = cls::training::Result();\n\n    \/* init daal result's objects *\/\n    if (check_mask_flag(desc.get_error_metric_mode(), error_metric_mode::out_of_bag_error)) {\n        auto arr_oob_err = array<Float>::empty(1 * 1);\n        res.set_oob_err(dal::detail::homogen_table_builder{}.reset(arr_oob_err, 1, 1).build());\n\n        const auto res_oob_err = interop::convert_to_daal_homogen_table(arr_oob_err, 1, 1);\n        daal_result.set(cls::training::outOfBagError, res_oob_err);\n    }\n\n    if (check_mask_flag(desc.get_error_metric_mode(),\n                        error_metric_mode::out_of_bag_error_per_observation)) {\n        auto arr_oob_per_obs_err = array<Float>::empty(row_count * 1);\n        res.set_oob_err_per_observation(\n            dal::detail::homogen_table_builder{}.reset(arr_oob_per_obs_err, row_count, 1).build());\n\n        const auto res_oob_per_obs_err =\n            interop::convert_to_daal_homogen_table(arr_oob_per_obs_err, row_count, 1);\n        daal_result.set(cls::training::outOfBagErrorPerObservation, res_oob_per_obs_err);\n    }\n    if (variable_importance_mode::none != vimp) {\n        auto arr_var_imp = array<Float>::empty(1 * column_count);\n        res.set_var_importance(\n            dal::detail::homogen_table_builder{}.reset(arr_var_imp, 1, column_count).build());\n\n        const auto res_var_imp =\n            interop::convert_to_daal_homogen_table(arr_var_imp, 1, column_count);\n        daal_result.set(cls::training::variableImportance, res_var_imp);\n    }\n\n    cls::ModelPtr mptr = cls::ModelPtr(new cls::internal::ModelImpl(column_count));\n\n    interop::status_to_exception(interop::call_daal_kernel<Float, cls_dense_kernel_t>(\n        ctx,\n        daal::services::internal::hostApp(daal_input),\n        daal_data.get(),\n        daal_labels.get(),\n        *mptr,\n        daal_result,\n        daal_parameter));\n\n    \/* extract results from daal objects *\/\n    if (check_mask_flag(desc.get_error_metric_mode(), error_metric_mode::out_of_bag_error)) {\n        auto table_oob_err = interop::convert_from_daal_homogen_table<Float>(\n            daal_result.get(cls::training::outOfBagError));\n        res.set_oob_err(table_oob_err);\n    }\n\n    if (check_mask_flag(desc.get_error_metric_mode(),\n                        error_metric_mode::out_of_bag_error_per_observation)) {\n        auto table_oob_per_obs_err = interop::convert_from_daal_homogen_table<Float>(\n            daal_result.get(cls::training::outOfBagErrorPerObservation));\n        res.set_oob_err_per_observation(table_oob_per_obs_err);\n    }\n\n    if (variable_importance_mode::none != vimp) {\n        auto table_var_imp = interop::convert_from_daal_homogen_table<Float>(\n            daal_result.get(cls::training::variableImportance));\n        res.set_var_importance(table_var_imp);\n    }\n\n    return res.set_model(dal::detail::pimpl_accessor().make_from_pimpl<model<Task>>(\n        std::make_shared<interop::decision_forest::interop_model_impl<Task, cls_model_p>>(mptr)));\n}\n\ntemplate <typename Float, typename Task>\nstatic train_result<Task> train(const context_cpu& ctx,\n                                const descriptor_base<Task>& desc,\n                                const train_input<Task>& input) {\n    return call_daal_kernel<Float>(ctx, desc, input.get_data(), input.get_labels());\n}\n\ntemplate <typename Float, typename Task>\nstruct train_kernel_cpu<Float, Task, method::dense> {\n    train_result<Task> operator()(const context_cpu& ctx,\n                                  const descriptor_base<Task>& desc,\n                                  const train_input<Task>& input) const {\n        return train<Float, Task>(ctx, desc, input);\n    }\n};\n\ntemplate struct train_kernel_cpu<float, task::classification, method::dense>;\ntemplate struct train_kernel_cpu<double, task::classification, method::dense>;\n\n} \/\/ namespace oneapi::dal::decision_forest::backend\n","lang_cluster":"C++","length":182,"code_uid":"ba8b0d3fb892495993e13422e1768879"}
{"diff_hunk":"@@ -55,10 +55,43 @@ static train_result call_daal_kernel(const context_gpu& ctx,\n     auto arr_data  = row_accessor<const Float>{ data }.pull(queue);\n     auto arr_label = row_accessor<const Float>{ labels }.pull(queue);\n \n+    \/\/ TODO: make for dpcpp kernel\n+    auto arr_new_label = array<Float>::empty(queue, row_count * 1);\n+    arr_label.need_mutable_data(queue);\n+    Float value_first_class_label  = arr_label[0];\n+    Float value_second_class_label = arr_label[1];\n+\n+    arr_new_label[0] = Float(-1.0);\n+    std::int64_t i   = 1;\n+    for (; i < row_count; ++i) {\n+        if (arr_label[i] == value_first_class_label) {\n+            arr_new_label[i] = Float(-1.0);\n+        }\n+        else {\n+            value_second_class_label = arr_label[i];\n+            arr_new_label[i]         = Float(+1.0);\n+        }\n+    }\n+    if (value_first_class_label == value_second_class_label) {\n+        throw invalid_argument(\"Input label data should have more one unique label\");\n+    }\n+\n+    for (; i < row_count; ++i) {\n+        if (arr_label[i] == value_first_class_label) {\n+            arr_new_label[i] = Float(-1.0);\n+        }\n+        else if (arr_label[i] == value_second_class_label) {\n+            arr_new_label[i] = Float(+1.0);\n+        }\n+        else {\n+            throw invalid_argument(\"Input label data should have only two unique labels\");\n+        }\n+    }\n+\n     const auto daal_data =\n         interop::convert_to_daal_sycl_homogen_table(queue, arr_data, row_count, column_count);\n     const auto daal_labels =\n-        interop::convert_to_daal_sycl_homogen_table(queue, arr_label, row_count, 1);\n+        interop::convert_to_daal_sycl_homogen_table(queue, arr_new_label, row_count, 1);\n \n     auto kernel_impl       = desc.get_kernel_impl()->get_impl();\n     const auto daal_kernel = kernel_impl->get_daal_kernel_function();","old_code":"\/*******************************************************************************\n* Copyright 2020 Intel Corporation\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*******************************************************************************\/\n\n#define DAAL_SYCL_INTERFACE\n#define DAAL_SYCL_INTERFACE_USM\n#define DAAL_SYCL_INTERFACE_REVERSED_RANGE\n\n#include \"oneapi\/dal\/algo\/svm\/backend\/gpu\/train_kernel.hpp\"\n#include \"oneapi\/dal\/algo\/svm\/backend\/interop_model.hpp\"\n#include \"oneapi\/dal\/algo\/svm\/backend\/kernel_function_impl.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/common_dpc.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/error_converter.hpp\"\n#include \"oneapi\/dal\/backend\/interop\/table_conversion.hpp\"\n\n#include <daal\/src\/algorithms\/svm\/oneapi\/svm_train_thunder_kernel_oneapi.h>\n\nnamespace oneapi::dal::svm::backend {\n\nusing std::int64_t;\nusing dal::backend::context_gpu;\n\nnamespace daal_svm             = daal::algorithms::svm;\nnamespace daal_kernel_function = daal::algorithms::kernel_function;\nnamespace interop              = dal::backend::interop;\n\ntemplate <typename Float>\nusing daal_svm_thunder_kernel_t = daal_svm::training::internal::\n    SVMTrainOneAPI<Float, daal_svm::Parameter, daal_svm::training::thunder>;\n\ntemplate <typename Float>\nstatic train_result call_daal_kernel(const context_gpu& ctx,\n                                     const descriptor_base& desc,\n                                     const table& data,\n                                     const table& labels) {\n    auto& queue = ctx.get_queue();\n    interop::execution_context_guard guard(queue);\n\n    const int64_t row_count    = data.get_row_count();\n    const int64_t column_count = data.get_column_count();\n\n    \/\/ TODO: data is table, not a homogen_table. Think better about accessor - is it enough to have just a row_accessor?\n    auto arr_data  = row_accessor<const Float>{ data }.pull(queue);\n    auto arr_label = row_accessor<const Float>{ labels }.pull(queue);\n\n    const auto daal_data =\n        interop::convert_to_daal_sycl_homogen_table(queue, arr_data, row_count, column_count);\n    const auto daal_labels =\n        interop::convert_to_daal_sycl_homogen_table(queue, arr_label, row_count, 1);\n\n    auto kernel_impl       = desc.get_kernel_impl()->get_impl();\n    const auto daal_kernel = kernel_impl->get_daal_kernel_function();\n    daal_svm::Parameter daal_parameter(\n        daal_kernel,\n        desc.get_c(),\n        desc.get_accuracy_threshold(),\n        desc.get_tau(),\n        desc.get_max_iteration_count(),\n        int64_t(desc.get_cache_size() * 1024 * 1024), \/\/ DAAL get in bytes\n        desc.get_shrinking());\n\n    auto daal_model = daal_svm::Model::create<Float>(column_count);\n    interop::status_to_exception(daal_svm_thunder_kernel_t<Float>().compute(daal_data,\n                                                                            *daal_labels,\n                                                                            daal_model.get(),\n                                                                            &daal_parameter));\n    auto table_support_indices =\n        interop::convert_from_daal_homogen_table<Float>(daal_model->getSupportIndices());\n\n    return train_result()\n        .set_model(convert_from_daal_model<Float>(*daal_model))\n        .set_support_indices(table_support_indices);\n}\n\ntemplate <typename Float>\nstatic train_result train(const context_gpu& ctx,\n                          const descriptor_base& desc,\n                          const train_input& input) {\n    return call_daal_kernel<Float>(ctx, desc, input.get_data(), input.get_labels());\n}\n\ntemplate <typename Float>\nstruct train_kernel_gpu<Float, task::classification, method::thunder> {\n    train_result operator()(const dal::backend::context_gpu& ctx,\n                            const descriptor_base& desc,\n                            const train_input& input) const {\n        return train<Float>(ctx, desc, input);\n    }\n};\n\ntemplate struct train_kernel_gpu<float, task::classification, method::thunder>;\ntemplate struct train_kernel_gpu<double, task::classification, method::thunder>;\n\n} \/\/ namespace oneapi::dal::svm::backend\n","lang_cluster":"C++","length":106,"code_uid":"728e40e83c234890887df56138fb961d"}
{"diff_hunk":"@@ -42,6 +42,22 @@ DEFINE_int32(rocksdb_batch_size,\n DEFINE_string(part_man_type,\n               \"memory\",\n               \"memory, meta\");\n+\n+static bool validateRocksdbStatsLevel(const char* flagname, int value) {\n+    if (value < rocksdb::StatsLevel::kAll && value >= 0) {\n+        return true;\n+    }\n+\n+    VLOG(3) << \"Invalid value for --\" << flagname << \": \" << value;\n+    return false;\n+}\n+\n+DEFINE_int32(rocksdb_stats_level, 0,\n+             \"Statistics Level for RocksDB. Default is 0 (kExceptHistogramOrTimers)\");\n+DEFINE_validator(rocksdb_stats_level, &validateRocksdbStatsLevel);\n+\n+DEFINE_int32(rocksdb_stats_dump_period_sec, 0, \"DBOptions::stats_dump_period_sec for RocksDB\");\n+\n \/*\n  * For these un-supported string options as below, will need to specify them with gflag.\n  *\/","old_code":"\/* Copyright (c) 2018 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License,\n * attached with Common Clause Condition 1.0, found in the LICENSES directory.\n *\/\n\n#include \"base\/Base.h\"\n#include \"kvstore\/RocksEngineConfig.h\"\n#include \"rocksdb\/db.h\"\n#include \"rocksdb\/cache.h\"\n#include \"rocksdb\/convenience.h\"\n#include \"rocksdb\/utilities\/options_util.h\"\n#include \"rocksdb\/slice_transform.h\"\n\n\/\/ [WAL]\nDEFINE_bool(rocksdb_disable_wal,\n            false,\n            \"Whether to disable the WAL in rocksdb\");\n\n\/\/ [DBOptions]\nDEFINE_string(rocksdb_db_options,\n              \"\",\n              \"DBOptions, each option will be given \"\n              \"as <option_name>:<option_value> separated by ;\");\n\n\/\/ [CFOptions \"default\"]\nDEFINE_string(rocksdb_column_family_options,\n              \"\",\n              \"ColumnFamilyOptions, each option will be given \"\n              \"as <option_name>:<option_value> separated by ;\");\n\n\/\/  [TableOptions\/BlockBasedTable \"default\"]\nDEFINE_string(rocksdb_block_based_table_options,\n              \"\",\n              \"BlockBasedTableOptions, each option will be given \"\n              \"as <option_name>:<option_value> separated by ;\");\n\nDEFINE_int32(rocksdb_batch_size,\n             4 * 1024,\n             \"default reserved bytes for one batch operation\");\n\nDEFINE_string(part_man_type,\n              \"memory\",\n              \"memory, meta\");\n\/*\n * For these un-supported string options as below, will need to specify them with gflag.\n *\/\n\n\/\/ BlockBasedTable block_cache\nDEFINE_int64(rocksdb_block_cache, 4,\n             \"The default block cache size used in BlockBasedTable. The unit is MB\");\n\n\nnamespace nebula {\nnamespace kvstore {\n\nrocksdb::Status initRocksdbOptions(rocksdb::Options &baseOpts) {\n    rocksdb::Status s;\n    rocksdb::DBOptions dbOpts;\n    rocksdb::ColumnFamilyOptions cfOpts;\n    rocksdb::BlockBasedTableOptions bbtOpts;\n    s = GetDBOptionsFromString(rocksdb::DBOptions(),\n            FLAGS_rocksdb_db_options, &dbOpts);\n    if (!s.ok()) {\n        return s;\n    }\n\n    s = GetColumnFamilyOptionsFromString(rocksdb::ColumnFamilyOptions(),\n            FLAGS_rocksdb_column_family_options, &cfOpts);\n\n    if (!s.ok()) {\n        return s;\n    }\n\n    baseOpts = rocksdb::Options(dbOpts, cfOpts);\n\n    s = GetBlockBasedTableOptionsFromString(rocksdb::BlockBasedTableOptions(),\n            FLAGS_rocksdb_block_based_table_options, &bbtOpts);\n\n    if (!s.ok()) {\n        return s;\n    }\n\n    bbtOpts.block_cache = rocksdb::NewLRUCache(FLAGS_rocksdb_block_cache * 1024 * 1024);\n    baseOpts.table_factory.reset(NewBlockBasedTableFactory(bbtOpts));\n    baseOpts.create_if_missing = true;\n    return s;\n}\n\n}  \/\/ namespace kvstore\n}  \/\/ namespace nebula\n","lang_cluster":"C++","length":91,"code_uid":"9f4b7589846542b2a2139e74b6e6452b"}
{"diff_hunk":"@@ -96,6 +96,8 @@ namespace thread_role\n \t\t\tcase nano::thread_role::name::signature_checking:\n \t\t\t\tthread_role_name_string = \"Signature check\";\n \t\t\t\tbreak;\n+\t\t\tcase nano::thread_role::name::confirmation_height_processing:\n+\t\t\t\tthread_role_name_string = \"Conf height\";\n \t\t}\n \n \t\t\/*","old_code":"#include <iostream>\n#include <nano\/lib\/utility.hpp>\n\nnamespace nano\n{\nseq_con_info_composite::seq_con_info_composite (const std::string & name) :\nname (name)\n{\n}\n\nbool seq_con_info_composite::is_composite () const\n{\n\treturn true;\n}\n\nvoid seq_con_info_composite::add_component (std::unique_ptr<seq_con_info_component> child)\n{\n\tchildren.push_back (std::move (child));\n}\n\nconst std::vector<std::unique_ptr<seq_con_info_component>> & seq_con_info_composite::get_children () const\n{\n\treturn children;\n}\n\nconst std::string & seq_con_info_composite::get_name () const\n{\n\treturn name;\n}\n\nseq_con_info_leaf::seq_con_info_leaf (const seq_con_info & info) :\ninfo (info)\n{\n}\nbool seq_con_info_leaf::is_composite () const\n{\n\treturn false;\n}\nconst seq_con_info & seq_con_info_leaf::get_info () const\n{\n\treturn info;\n}\n\nnamespace thread_role\n{\n\t\/*\n\t * nano::thread_role namespace\n\t *\n\t * Manage thread role\n\t *\/\n\tstatic thread_local nano::thread_role::name current_thread_role = nano::thread_role::name::unknown;\n\tnano::thread_role::name get ()\n\t{\n\t\treturn current_thread_role;\n\t}\n\n\tstatic std::string get_string (nano::thread_role::name role)\n\t{\n\t\tstd::string thread_role_name_string;\n\n\t\tswitch (role)\n\t\t{\n\t\t\tcase nano::thread_role::name::unknown:\n\t\t\t\tthread_role_name_string = \"<unknown>\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::io:\n\t\t\t\tthread_role_name_string = \"I\/O\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::work:\n\t\t\t\tthread_role_name_string = \"Work pool\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::packet_processing:\n\t\t\t\tthread_role_name_string = \"Pkt processing\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::alarm:\n\t\t\t\tthread_role_name_string = \"Alarm\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::vote_processing:\n\t\t\t\tthread_role_name_string = \"Vote processing\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::block_processing:\n\t\t\t\tthread_role_name_string = \"Blck processing\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::request_loop:\n\t\t\t\tthread_role_name_string = \"Request loop\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::wallet_actions:\n\t\t\t\tthread_role_name_string = \"Wallet actions\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::bootstrap_initiator:\n\t\t\t\tthread_role_name_string = \"Bootstrap init\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::voting:\n\t\t\t\tthread_role_name_string = \"Voting\";\n\t\t\t\tbreak;\n\t\t\tcase nano::thread_role::name::signature_checking:\n\t\t\t\tthread_role_name_string = \"Signature check\";\n\t\t\t\tbreak;\n\t\t}\n\n\t\t\/*\n\t\t * We want to constrain the thread names to 15\n\t\t * characters, since this is the smallest maximum\n\t\t * length supported by the platforms we support\n\t\t * (specifically, Linux)\n\t\t *\/\n\t\tassert (thread_role_name_string.size () < 16);\n\t\treturn (thread_role_name_string);\n\t}\n\n\tstd::string get_string ()\n\t{\n\t\treturn get_string (current_thread_role);\n\t}\n\n\tvoid set (nano::thread_role::name role)\n\t{\n\t\tauto thread_role_name_string (get_string (role));\n\n\t\tnano::thread_role::set_os_name (thread_role_name_string);\n\n\t\tnano::thread_role::current_thread_role = role;\n\t}\n}\n}\n\nvoid nano::thread_attributes::set (boost::thread::attributes & attrs)\n{\n\tauto attrs_l (&attrs);\n\tattrs_l->set_stack_size (8000000); \/\/8MB\n}\n\nnano::thread_runner::thread_runner (boost::asio::io_context & io_ctx_a, unsigned service_threads_a)\n{\n\tboost::thread::attributes attrs;\n\tnano::thread_attributes::set (attrs);\n\tfor (auto i (0u); i < service_threads_a; ++i)\n\t{\n\t\tthreads.push_back (boost::thread (attrs, [&io_ctx_a]() {\n\t\t\tnano::thread_role::set (nano::thread_role::name::io);\n\t\t\ttry\n\t\t\t{\n\t\t\t\tio_ctx_a.run ();\n\t\t\t}\n\t\t\tcatch (...)\n\t\t\t{\n#ifndef NDEBUG\n\t\t\t\t\/*\n\t\t\t\t * In a release build, catch and swallow the\n\t\t\t\t * io_context exception, in debug mode pass it\n\t\t\t\t * on\n\t\t\t\t *\/\n\t\t\t\tthrow;\n#endif\n\t\t\t}\n\t\t}));\n\t}\n}\n\nnano::thread_runner::~thread_runner ()\n{\n\tjoin ();\n}\n\nvoid nano::thread_runner::join ()\n{\n\tfor (auto & i : threads)\n\t{\n\t\tif (i.joinable ())\n\t\t{\n\t\t\ti.join ();\n\t\t}\n\t}\n}\n\n\/*\n * Backing code for \"release_assert\", which is itself a macro\n *\/\nvoid release_assert_internal (bool check, const char * check_expr, const char * file, unsigned int line)\n{\n\tif (check)\n\t{\n\t\treturn;\n\t}\n\n\tstd::cerr << \"Assertion (\" << check_expr << \") failed \" << file << \":\" << line << std::endl;\n\tabort ();\n}\n","lang_cluster":"C++","length":188,"code_uid":"9571b3ab419845ad9e31132c6adb7565"}
{"diff_hunk":"@@ -145,13 +145,8 @@ std::unique_ptr<nebula::kvstore::KVStore> initKV(std::vector<nebula::HostAddr> p\n     LOG(ERROR) << \"Meta version is invalid\";\n     return nullptr;\n   } else if (version == nebula::meta::MetaVersion::V1) {\n-    auto ret = nebula::meta::MetaVersionMan::updateMetaV1ToV2(engine);\n-    if (!ret.ok()) {\n-      LOG(ERROR) << \"Update meta from V1 to V2 failed \" << ret;\n-      return nullptr;\n-    }\n-\n-    nebula::meta::MetaVersionMan::setMetaVersionToKV(engine, nebula::meta::MetaVersion::V2);\n+    LOG(ERROR) << \"Can't upgrade meta from V1 to V3\";\n+    return nullptr;\n   } else if (version == nebula::meta::MetaVersion::V2) {\n     auto ret = nebula::meta::MetaVersionMan::updateMetaV2ToV3(engine);\n     if (!ret.ok()) {","old_code":"\/* Copyright (c) 2021 vesoft inc. All rights reserved.\n *\n * This source code is licensed under Apache 2.0 License.\n *\/\n\n#include \"MetaDaemonInit.h\"\n\n#include <folly\/ssl\/Init.h>\n#include <thrift\/lib\/cpp2\/server\/ThriftServer.h>\n\n#include \"common\/base\/Base.h\"\n#include \"common\/base\/SignalHandler.h\"\n#include \"common\/fs\/FileUtils.h\"\n#include \"common\/hdfs\/HdfsCommandHelper.h\"\n#include \"common\/hdfs\/HdfsHelper.h\"\n#include \"common\/network\/NetworkUtils.h\"\n#include \"common\/ssl\/SSLConfig.h\"\n#include \"common\/thread\/GenericThreadPool.h\"\n#include \"common\/utils\/MetaKeyUtils.h\"\n#include \"kvstore\/NebulaStore.h\"\n#include \"kvstore\/PartManager.h\"\n#include \"meta\/ActiveHostsMan.h\"\n#include \"meta\/KVBasedClusterIdMan.h\"\n#include \"meta\/MetaServiceHandler.h\"\n#include \"meta\/MetaVersionMan.h\"\n#include \"meta\/http\/MetaHttpDownloadHandler.h\"\n#include \"meta\/http\/MetaHttpIngestHandler.h\"\n#include \"meta\/http\/MetaHttpReplaceHostHandler.h\"\n#include \"meta\/processors\/job\/JobManager.h\"\n#include \"meta\/stats\/MetaStats.h\"\n#include \"webservice\/Router.h\"\n#include \"webservice\/WebService.h\"\n\n#ifndef BUILD_STANDALONE\nDEFINE_int32(num_io_threads, 16, \"Number of IO threads\");\nDEFINE_int32(num_worker_threads, 32, \"Number of workers\");\nDEFINE_string(data_path, \"\", \"Root data path\");\nDEFINE_string(meta_server_addrs,\n              \"\",\n              \"It is a list of IPs split by comma, used in cluster deployment\"\n              \"the ips number is equal to the replica number.\"\n              \"If empty, it means it's a single node\");\n#else\nDEFINE_int32(meta_num_io_threads, 16, \"Number of IO threads\");\nDEFINE_int32(meta_num_worker_threads, 32, \"Number of workers\");\nDEFINE_string(meta_data_path, \"\", \"Root data path\");\nDECLARE_string(meta_server_addrs);  \/\/ use define from grap flags.\nDECLARE_int32(ws_meta_http_port);\nDECLARE_int32(ws_meta_h2_port);\n#endif\n\nusing nebula::web::PathParams;\n\nnamespace nebula::meta {\nconst std::string kClusterIdKey = \"__meta_cluster_id_key__\";  \/\/ NOLINT\n}  \/\/ namespace nebula::meta\n\nnebula::ClusterID gClusterId = 0;\nnebula::ClusterID& metaClusterId() {\n  return gClusterId;\n}\n\nstd::unique_ptr<nebula::kvstore::KVStore> initKV(std::vector<nebula::HostAddr> peers,\n                                                 nebula::HostAddr localhost) {\n  auto partMan = std::make_unique<nebula::kvstore::MemPartManager>();\n  \/\/ The meta server has only one space (0), one part (0)\n  partMan->addPart(nebula::kDefaultSpaceId, nebula::kDefaultPartId, std::move(peers));\n#ifndef BUILD_STANDALONE\n  int32_t numMetaIoThreads = FLAGS_num_io_threads;\n  int32_t numMetaWorkerThreads = FLAGS_num_worker_threads;\n#else\n  int32_t numMetaIoThreads = FLAGS_meta_num_io_threads;\n  int32_t numMetaWorkerThreads = FLAGS_meta_num_worker_threads;\n#endif\n  \/\/ folly IOThreadPoolExecutor\n  auto ioPool = std::make_shared<folly::IOThreadPoolExecutor>(numMetaIoThreads);\n  std::shared_ptr<apache::thrift::concurrency::ThreadManager> threadManager(\n      apache::thrift::concurrency::PriorityThreadManager::newPriorityThreadManager(\n          numMetaWorkerThreads));\n  threadManager->setNamePrefix(\"executor\");\n  threadManager->start();\n  nebula::kvstore::KVOptions options;\n#ifndef BUILD_STANDALONE\n  auto absolute = boost::filesystem::absolute(FLAGS_data_path);\n#else\n  auto absolute = boost::filesystem::absolute(FLAGS_meta_data_path);\n#endif\n  options.dataPaths_ = {absolute.string()};\n  options.partMan_ = std::move(partMan);\n  auto kvstore = std::make_unique<nebula::kvstore::NebulaStore>(\n      std::move(options), ioPool, localhost, threadManager);\n  if (!(kvstore->init())) {\n    LOG(ERROR) << \"Nebula store init failed\";\n    return nullptr;\n  }\n\n  auto engineRet = kvstore->part(nebula::kDefaultSpaceId, nebula::kDefaultPartId);\n  if (!nebula::ok(engineRet)) {\n    LOG(ERROR) << \"Get nebula store engine failed\";\n    return nullptr;\n  }\n\n  auto engine = nebula::value(engineRet)->engine();\n  LOG(INFO) << \"Waiting for the leader elected...\";\n  nebula::HostAddr leader;\n  while (true) {\n    auto ret = kvstore->partLeader(nebula::kDefaultSpaceId, nebula::kDefaultPartId);\n    if (!nebula::ok(ret)) {\n      LOG(ERROR) << \"Nebula store init failed\";\n      return nullptr;\n    }\n    leader = nebula::value(ret);\n    if (leader != nebula::HostAddr(\"\", 0)) {\n      break;\n    }\n    LOG(INFO) << \"Leader has not been elected, sleep 1s\";\n    sleep(1);\n  }\n\n  gClusterId =\n      nebula::meta::ClusterIdMan::getClusterIdFromKV(kvstore.get(), nebula::meta::kClusterIdKey);\n  if (gClusterId == 0) {\n    if (leader == localhost) {\n      LOG(INFO) << \"I am leader, create cluster Id\";\n      gClusterId = nebula::meta::ClusterIdMan::create(FLAGS_meta_server_addrs);\n      if (!nebula::meta::ClusterIdMan::persistInKV(\n              kvstore.get(), nebula::meta::kClusterIdKey, gClusterId)) {\n        LOG(ERROR) << \"Persist cluster failed!\";\n        return nullptr;\n      }\n    } else {\n      LOG(INFO) << \"I am follower, wait for the leader's clusterId\";\n      while (gClusterId == 0) {\n        LOG(INFO) << \"Waiting for the leader's clusterId\";\n        sleep(1);\n        gClusterId = nebula::meta::ClusterIdMan::getClusterIdFromKV(kvstore.get(),\n                                                                    nebula::meta::kClusterIdKey);\n      }\n    }\n  }\n\n  auto version = nebula::meta::MetaVersionMan::getMetaVersionFromKV(kvstore.get());\n  LOG(INFO) << \"Get meta version is \" << static_cast<int32_t>(version);\n  if (version == nebula::meta::MetaVersion::UNKNOWN) {\n    LOG(ERROR) << \"Meta version is invalid\";\n    return nullptr;\n  } else if (version == nebula::meta::MetaVersion::V1) {\n    auto ret = nebula::meta::MetaVersionMan::updateMetaV1ToV2(engine);\n    if (!ret.ok()) {\n      LOG(ERROR) << \"Update meta from V1 to V2 failed \" << ret;\n      return nullptr;\n    }\n\n    nebula::meta::MetaVersionMan::setMetaVersionToKV(engine, nebula::meta::MetaVersion::V2);\n  } else if (version == nebula::meta::MetaVersion::V2) {\n    auto ret = nebula::meta::MetaVersionMan::updateMetaV2ToV3(engine);\n    if (!ret.ok()) {\n      LOG(ERROR) << \"Update meta from V2 to V3 failed \" << ret;\n      return nullptr;\n    }\n\n    nebula::meta::MetaVersionMan::setMetaVersionToKV(engine, nebula::meta::MetaVersion::V3);\n  }\n\n  LOG(INFO) << \"Nebula store init succeeded, clusterId \" << gClusterId;\n  return kvstore;\n}\n\nnebula::Status initWebService(nebula::WebService* svc,\n                              nebula::kvstore::KVStore* kvstore,\n                              nebula::hdfs::HdfsCommandHelper* helper,\n                              nebula::thread::GenericThreadPool* pool) {\n  LOG(INFO) << \"Starting Meta HTTP Service\";\n  auto& router = svc->router();\n  router.get(\"\/download-dispatch\").handler([kvstore, helper, pool](PathParams&&) {\n    auto handler = new nebula::meta::MetaHttpDownloadHandler();\n    handler->init(kvstore, helper, pool);\n    return handler;\n  });\n  router.get(\"\/ingest-dispatch\").handler([kvstore, pool](PathParams&&) {\n    auto handler = new nebula::meta::MetaHttpIngestHandler();\n    handler->init(kvstore, pool);\n    return handler;\n  });\n  router.get(\"\/replace\").handler([kvstore](PathParams&&) {\n    auto handler = new nebula::meta::MetaHttpReplaceHostHandler();\n    handler->init(kvstore);\n    return handler;\n  });\n#ifndef BUILD_STANDALONE\n  return svc->start();\n#else\n  return svc->start(FLAGS_ws_meta_http_port, FLAGS_ws_meta_h2_port);\n#endif\n}\n","lang_cluster":"C++","length":195,"code_uid":"89960d15bcc949a6b3bf82ea82ffd58f"}
{"diff_hunk":"@@ -120,6 +120,8 @@ func writeNextUnescapedRune(s string, builder *strings.Builder) (width int, err\n \t\t\tr = '\\\\'\n \t\tcase '\"':\n \t\t\tr = '\"'\n+\t\tcase '$':\n+\t\t\tr = '$'\n \t\tcase 'x':\n \t\t\t\/\/ Decode two hex chars as a single byte\n \t\t\tif len(s[width:]) < 2 {","old_code":"package parser\n\nimport (\n\t\"fmt\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\t\"unicode\"\n\t\"unicode\/utf8\"\n\n\t\"github.com\/influxdata\/flux\/ast\"\n)\n\n\/\/ ParseTime will parse a time literal from a string.\nfunc ParseTime(lit string) (time.Time, error) {\n\tif !strings.Contains(lit, \"T\") {\n\t\t\/\/ This is a date.\n\t\treturn time.Parse(\"2006-01-02\", lit)\n\t}\n\t\/\/ todo(jsternberg): need to also parse when there is no time offset.\n\treturn time.Parse(time.RFC3339Nano, lit)\n}\n\n\/\/ MustParseTime parses a time literal and panics in the case of an error.\nfunc MustParseTime(lit string) time.Time {\n\tts, err := ParseTime(lit)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn ts\n}\n\n\/\/ ParseDuration will convert a string into components of the duration.\nfunc ParseDuration(lit string) ([]ast.Duration, error) {\n\tvar values []ast.Duration\n\tfor len(lit) > 0 {\n\t\tn := 0\n\t\tfor n < len(lit) {\n\t\t\tch, size := utf8.DecodeRuneInString(lit[n:])\n\t\t\tif size == 0 {\n\t\t\t\tpanic(\"invalid rune in duration\")\n\t\t\t}\n\n\t\t\tif !unicode.IsDigit(ch) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tn += size\n\t\t}\n\n\t\tmagnitude, err := strconv.ParseInt(lit[:n], 10, 64)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tlit = lit[n:]\n\n\t\tn = 0\n\t\tfor n < len(lit) {\n\t\t\tch, size := utf8.DecodeRuneInString(lit[n:])\n\t\t\tif size == 0 {\n\t\t\t\tpanic(\"invalid rune in duration\")\n\t\t\t}\n\n\t\t\tif !unicode.IsLetter(ch) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tn += size\n\t\t}\n\t\tunit := lit[:n]\n\t\tif unit == \"\u00b5s\" {\n\t\t\tunit = \"us\"\n\t\t}\n\t\tvalues = append(values, ast.Duration{\n\t\t\tMagnitude: magnitude,\n\t\t\tUnit:      unit,\n\t\t})\n\t\tlit = lit[n:]\n\t}\n\treturn values, nil\n}\n\n\/\/ ParseString removes quotes and unescapes the string literal.\nfunc ParseString(lit string) (string, error) {\n\tif len(lit) < 2 || lit[0] != '\"' || lit[len(lit)-1] != '\"' {\n\t\treturn \"\", fmt.Errorf(\"invalid syntax\")\n\t}\n\tlit = lit[1 : len(lit)-1]\n\tvar (\n\t\tbuilder    strings.Builder\n\t\twidth, pos int\n\t\terr        error\n\t)\n\tbuilder.Grow(len(lit))\n\tfor pos < len(lit) {\n\t\twidth, err = writeNextUnescapedRune(lit[pos:], &builder)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tpos += width\n\t}\n\treturn builder.String(), nil\n}\n\n\/\/ writeNextUnescapedRune writes a rune to builder from s.\n\/\/ The rune is the next decoded UTF-8 rune with escaping rules applied.\nfunc writeNextUnescapedRune(s string, builder *strings.Builder) (width int, err error) {\n\tvar r rune\n\tr, width = utf8.DecodeRuneInString(s)\n\tif r == '\\\\' {\n\t\tnext, w := utf8.DecodeRuneInString(s[width:])\n\t\twidth += w\n\t\tswitch next {\n\t\tcase 'n':\n\t\t\tr = '\\n'\n\t\tcase 'r':\n\t\t\tr = '\\r'\n\t\tcase 't':\n\t\t\tr = '\\t'\n\t\tcase '\\\\':\n\t\t\tr = '\\\\'\n\t\tcase '\"':\n\t\t\tr = '\"'\n\t\tcase 'x':\n\t\t\t\/\/ Decode two hex chars as a single byte\n\t\t\tif len(s[width:]) < 2 {\n\t\t\t\treturn 0, fmt.Errorf(\"invalid byte value %q\", s[width:])\n\t\t\t}\n\t\t\tch1, ok1 := fromHexChar(s[width])\n\t\t\tch2, ok2 := fromHexChar(s[width+1])\n\t\t\tif !ok1 || !ok2 {\n\t\t\t\treturn 0, fmt.Errorf(\"invalid byte value %q\", s[width:])\n\t\t\t}\n\t\t\tbuilder.WriteByte((ch1 << 4) | ch2)\n\t\t\treturn width + 2, nil\n\t\tdefault:\n\t\t\treturn 0, fmt.Errorf(\"invalid escape character %q\", next)\n\t\t}\n\t}\n\t\/\/ sanity check before writing the rune\n\tif width > 0 {\n\t\tbuilder.WriteRune(r)\n\t}\n\treturn\n}\n\n\/\/ fromHexChar converts a hex character into its value and a success flag.\nfunc fromHexChar(c byte) (byte, bool) {\n\tswitch {\n\tcase '0' <= c && c <= '9':\n\t\treturn c - '0', true\n\tcase 'a' <= c && c <= 'f':\n\t\treturn c - 'a' + 10, true\n\tcase 'A' <= c && c <= 'F':\n\t\treturn c - 'A' + 10, true\n\t}\n\treturn 0, false\n}\n\n\/\/ ParseRegexp converts text surrounded by forward slashes into a regular expression.\nfunc ParseRegexp(lit string) (*regexp.Regexp, error) {\n\tif len(lit) < 3 {\n\t\treturn nil, fmt.Errorf(\"regexp must be at least 3 characters\")\n\t}\n\n\tif lit[0] != '\/' {\n\t\treturn nil, fmt.Errorf(\"regexp literal must start with a slash\")\n\t} else if lit[len(lit)-1] != '\/' {\n\t\treturn nil, fmt.Errorf(\"regexp literal must end with a slash\")\n\t}\n\n\texpr := lit[1 : len(lit)-1]\n\tif index := strings.Index(expr, \"\\\\\/\"); index != -1 {\n\t\texpr = strings.Replace(expr, \"\\\\\/\", \"\/\", -1)\n\t}\n\treturn regexp.Compile(expr)\n}\n","lang_cluster":"Go","length":176,"code_uid":"8aaff447ed6f46d6996176b3718c6693"}
{"diff_hunk":"@@ -36,6 +36,7 @@ func NewInstaller(dc dynamic.Interface, config map[string]string, paths ...strin\n \t}\n \n \tfor i, p := range paths {\n+\t\tlog.Println(\"processing yaml folder\", p)\n \t\tpaths[i] = ParseTemplates(p, config)\n \t}\n \tpath := strings.Join(paths, \",\")","old_code":"\/*\nCopyright 2019 Google LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage e2e\n\nimport (\n\t\"fmt\"\n\t\"io\/ioutil\"\n\t\"log\"\n\t\"os\"\n\t\"path\/filepath\"\n\t\"strings\"\n\t\"text\/template\"\n\n\tyaml \"github.com\/jcrossley3\/manifestival\/pkg\/manifestival\"\n\t\"k8s.io\/client-go\/dynamic\"\n)\n\nfunc NewInstaller(dc dynamic.Interface, config map[string]string, paths ...string) *Installer {\n\tif len(paths) == 0 || (len(paths) == 1 && paths[0] == \"\") {\n\t\t\/\/ default to ko path:\n\t\tpaths[0] = \"\/var\/run\/ko\/install\"\n\t}\n\n\tfor i, p := range paths {\n\t\tpaths[i] = ParseTemplates(p, config)\n\t}\n\tpath := strings.Join(paths, \",\")\n\n\tmanifest, err := yaml.NewYamlManifest(path, true, dc)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn &Installer{dc: dc, manifest: manifest}\n}\n\nfunc ParseTemplates(path string, config map[string]string) string {\n\tdir, err := ioutil.TempDir(\"\", \"processed_yaml\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\terr = filepath.Walk(path, func(path string, info os.FileInfo, err error) error {\n\t\tif strings.HasSuffix(info.Name(), \"yaml\") {\n\n\t\t\tt, err := template.ParseFiles(path)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\ttmpfile, err := ioutil.TempFile(dir, strings.Replace(info.Name(), \".yaml\", \"-*.yaml\", 1))\n\t\t\tif err != nil {\n\t\t\t\tlog.Fatal(err)\n\t\t\t}\n\t\t\terr = t.Execute(tmpfile, config)\n\t\t\tif err != nil {\n\t\t\t\tlog.Print(\"execute: \", err)\n\t\t\t\treturn err\n\t\t\t}\n\t\t\t_ = tmpfile.Close()\n\t\t}\n\t\treturn nil\n\t})\n\tlog.Print(\"new files in \", dir)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn dir\n}\n\ntype Installer struct {\n\tdc dynamic.Interface\n\n\tmanifest yaml.Manifest\n}\n\nfunc (r *Installer) Do(verb string) error {\n\tswitch strings.ToLower(verb) {\n\tcase \"create\", \"setup\", \"install\", \"apply\", \"start\":\n\t\treturn r.manifest.ApplyAll()\n\tcase \"delete\", \"teardown\", \"uninstall\", \"unapply\", \"stop\":\n\t\treturn r.manifest.DeleteAll()\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown verb: %s\", verb)\n\t}\n}\n","lang_cluster":"Go","length":98,"code_uid":"42a4c15f479c4614bc604ab078daa09c"}
{"diff_hunk":"@@ -29,13 +29,16 @@ package authorization\n import (\n \t\"crypto\/x509\/pkix\"\n \n+\t\"google.golang.org\/grpc\/credentials\"\n+\n \t\"go.temporal.io\/server\/common\/service\/config\"\n )\n \n \/\/ Authentication information from subject's JWT token or\/and mTLS certificate\n type AuthInfo struct {\n-\tauthToken  string\n-\ttlsSubject *pkix.Name\n+\tAuthToken     string\n+\tTlsSubject    *pkix.Name\n+\tTLSConnection *credentials.TLSInfo\n }\n \n \/\/ Converts authorization info of a subject into Temporal claims (permissions) for authorization","old_code":"\/\/ The MIT License\n\/\/\n\/\/ Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n\/\/\n\/\/ Copyright (c) 2020 Uber Technologies, Inc.\n\/\/\n\/\/ Permission is hereby granted, free of charge, to any person obtaining a copy\n\/\/ of this software and associated documentation files (the \"Software\"), to deal\n\/\/ in the Software without restriction, including without limitation the rights\n\/\/ to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n\/\/ copies of the Software, and to permit persons to whom the Software is\n\/\/ furnished to do so, subject to the following conditions:\n\/\/\n\/\/ The above copyright notice and this permission notice shall be included in\n\/\/ all copies or substantial portions of the Software.\n\/\/\n\/\/ THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\/\/ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\/\/ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\/\/ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\/\/ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\/\/ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n\/\/ THE SOFTWARE.\n\n\/\/go:generate mockgen -copyright_file ..\/..\/LICENSE -package $GOPACKAGE -source $GOFILE -destination claim_mapper_mock.go -self_package go.temporal.io\/server\/common\/authorization\n\npackage authorization\n\nimport (\n\t\"crypto\/x509\/pkix\"\n\n\t\"go.temporal.io\/server\/common\/service\/config\"\n)\n\n\/\/ Authentication information from subject's JWT token or\/and mTLS certificate\ntype AuthInfo struct {\n\tauthToken  string\n\ttlsSubject *pkix.Name\n}\n\n\/\/ Converts authorization info of a subject into Temporal claims (permissions) for authorization\ntype ClaimMapper interface {\n\tGetClaims(authInfo *AuthInfo) (*Claims, error)\n}\n\n\/\/ No-op claim mapper that gives system level admin permission to everybody\ntype noopClaimMapper struct{}\n\nvar _ ClaimMapper = (*noopClaimMapper)(nil)\n\nfunc NewNoopClaimMapper(_ *config.Config) ClaimMapper {\n\treturn &noopClaimMapper{}\n}\n\nfunc (*noopClaimMapper) GetClaims(_ *AuthInfo) (*Claims, error) {\n\treturn &Claims{system: RoleAdmin}, nil\n}\n","lang_cluster":"Go","length":57,"code_uid":"2755aa8ec65940858e9794e18e798119"}
{"diff_hunk":"@@ -115,15 +115,7 @@ func (c call) endStats(elapsed time.Duration, err error, isApplicationError bool\n \t\treturn\n \t}\n \n-\tif !yarpcerror.IsStatus(err) {\n-\t\tc.edge.serverErrLatencies.Observe(elapsed)\n-\t\tif counter, err := c.edge.serverFailures.Get(_error, \"unknown_internal_yarpc\"); err == nil {\n-\t\t\tcounter.Inc()\n-\t\t}\n-\t\treturn\n-\t}\n-\n-\terrCode := yarpcerror.FromError(err).Code()\n+\terrCode := yarpcerror.GetInfo(err).Code\n \tswitch errCode {\n \tcase yarpcerror.CodeCancelled,\n \t\tyarpcerror.CodeInvalidArgument,","old_code":"\/\/ Copyright (c) 2018 Uber Technologies, Inc.\n\/\/\n\/\/ Permission is hereby granted, free of charge, to any person obtaining a copy\n\/\/ of this software and associated documentation files (the \"Software\"), to deal\n\/\/ in the Software without restriction, including without limitation the rights\n\/\/ to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n\/\/ copies of the Software, and to permit persons to whom the Software is\n\/\/ furnished to do so, subject to the following conditions:\n\/\/\n\/\/ The above copyright notice and this permission notice shall be included in\n\/\/ all copies or substantial portions of the Software.\n\/\/\n\/\/ THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\/\/ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\/\/ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\/\/ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\/\/ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\/\/ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n\/\/ THE SOFTWARE.\n\npackage internalyarpcobservability\n\nimport (\n\t\"context\"\n\t\"time\"\n\n\t\"go.uber.org\/yarpc\/v2\"\n\t\"go.uber.org\/yarpc\/v2\/yarpcerror\"\n\t\"go.uber.org\/zap\"\n\t\"go.uber.org\/zap\/zapcore\"\n)\n\nconst (\n\t_error              = \"error\"\n\t_successfulInbound  = \"Handled inbound request.\"\n\t_successfulOutbound = \"Made outbound call.\"\n\t_errorInbound       = \"Error handling inbound request.\"\n\t_errorOutbound      = \"Error making outbound call.\"\n)\n\n\/\/ A call represents a single RPC along an edge.\n\/\/\n\/\/ To prevent allocating on the heap on the request path, it's a value instead\n\/\/ of a pointer.\ntype call struct {\n\tedge    *edge\n\textract ContextExtractor\n\tfields  [5]zapcore.Field\n\n\tstarted   time.Time\n\tctx       context.Context\n\treq       *yarpc.Request\n\trpcType   yarpc.Type\n\tdirection directionName\n}\n\nfunc (c call) End(err error) {\n\tc.EndWithAppError(err, false)\n}\n\nfunc (c call) EndWithAppError(err error, isApplicationError bool) {\n\telapsed := _timeNow().Sub(c.started)\n\tc.endLogs(elapsed, err, isApplicationError)\n\tc.endStats(elapsed, err, isApplicationError)\n}\n\nfunc (c call) endLogs(elapsed time.Duration, err error, isApplicationError bool) {\n\tvar ce *zapcore.CheckedEntry\n\tif err == nil && !isApplicationError {\n\t\tmsg := _successfulInbound\n\t\tif c.direction != _directionInbound {\n\t\t\tmsg = _successfulOutbound\n\t\t}\n\t\tce = c.edge.logger.Check(zap.DebugLevel, msg)\n\t} else {\n\t\tmsg := _errorInbound\n\t\tif c.direction != _directionInbound {\n\t\t\tmsg = _errorOutbound\n\t\t}\n\t\tce = c.edge.logger.Check(zap.ErrorLevel, msg)\n\t}\n\n\tif ce == nil {\n\t\treturn\n\t}\n\n\tfields := c.fields[:0]\n\tfields = append(fields, zap.String(\"rpcType\", c.rpcType.String()))\n\tfields = append(fields, zap.Duration(\"latency\", elapsed))\n\tfields = append(fields, zap.Bool(\"successful\", err == nil && !isApplicationError))\n\tfields = append(fields, c.extract(c.ctx))\n\tif isApplicationError {\n\t\tfields = append(fields, zap.String(_error, \"application_error\"))\n\t} else {\n\t\tfields = append(fields, zap.Error(err))\n\t}\n\tce.Write(fields...)\n}\n\nfunc (c call) endStats(elapsed time.Duration, err error, isApplicationError bool) {\n\t\/\/ TODO: We need a much better way to distinguish between caller and server\n\t\/\/ errors. See T855583.\n\tc.edge.calls.Inc()\n\tif err == nil && !isApplicationError {\n\t\tc.edge.successes.Inc()\n\t\tc.edge.latencies.Observe(elapsed)\n\t\treturn\n\t}\n\t\/\/ For now, assume that all application errors are the caller's fault.\n\tif isApplicationError {\n\t\tc.edge.callerErrLatencies.Observe(elapsed)\n\t\tif counter, err := c.edge.callerFailures.Get(_error, \"application_error\"); err == nil {\n\t\t\tcounter.Inc()\n\t\t}\n\t\treturn\n\t}\n\n\tif !yarpcerror.IsStatus(err) {\n\t\tc.edge.serverErrLatencies.Observe(elapsed)\n\t\tif counter, err := c.edge.serverFailures.Get(_error, \"unknown_internal_yarpc\"); err == nil {\n\t\t\tcounter.Inc()\n\t\t}\n\t\treturn\n\t}\n\n\terrCode := yarpcerror.FromError(err).Code()\n\tswitch errCode {\n\tcase yarpcerror.CodeCancelled,\n\t\tyarpcerror.CodeInvalidArgument,\n\t\tyarpcerror.CodeNotFound,\n\t\tyarpcerror.CodeAlreadyExists,\n\t\tyarpcerror.CodePermissionDenied,\n\t\tyarpcerror.CodeFailedPrecondition,\n\t\tyarpcerror.CodeAborted,\n\t\tyarpcerror.CodeOutOfRange,\n\t\tyarpcerror.CodeUnimplemented,\n\t\tyarpcerror.CodeUnauthenticated:\n\t\tc.edge.callerErrLatencies.Observe(elapsed)\n\t\tif counter, err := c.edge.callerFailures.Get(_error, errCode.String()); err == nil {\n\t\t\tcounter.Inc()\n\t\t}\n\t\treturn\n\tcase yarpcerror.CodeUnknown,\n\t\tyarpcerror.CodeDeadlineExceeded,\n\t\tyarpcerror.CodeResourceExhausted,\n\t\tyarpcerror.CodeInternal,\n\t\tyarpcerror.CodeUnavailable,\n\t\tyarpcerror.CodeDataLoss:\n\t\tc.edge.serverErrLatencies.Observe(elapsed)\n\t\tif counter, err := c.edge.serverFailures.Get(_error, errCode.String()); err == nil {\n\t\t\tcounter.Inc()\n\t\t}\n\t\treturn\n\t}\n\t\/\/ If this code is executed we've hit an error code outside the usual error\n\t\/\/ code range, so we'll just log the string representation of that code.\n\tc.edge.serverErrLatencies.Observe(elapsed)\n\tif counter, err := c.edge.serverFailures.Get(_error, errCode.String()); err == nil {\n\t\tcounter.Inc()\n\t}\n}\n","lang_cluster":"Go","length":161,"code_uid":"2a19a4b5584c443e8edcda99933baa29"}
{"diff_hunk":"@@ -62,6 +62,13 @@ func (s *Server) Start(ctx context.Context) error {\n \t\treturn errors.Wrap(err, \"error when subscribe to block\")\n \t}\n \n+\t\/\/ sync genesis block\n+\tgenesisBlk, err := s.bc.GetBlockByHeight(0)\n+\tif err != nil {\n+\t\treturn errors.Wrap(err, \"error when get genesis block\")\n+\t}\n+\ts.idx.BuildIndex(genesisBlk)\n+\n \tgo func() {\n \t\tfor {\n \t\t\tselect {","old_code":"\/\/ Copyright (c) 2018 IoTeX\n\/\/ This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n\/\/ warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n\/\/ permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n\/\/ License 2.0 that can be found in the LICENSE file.\n\npackage indexservice\n\nimport (\n\t\"github.com\/pkg\/errors\"\n\t\"golang.org\/x\/net\/context\"\n\n\t\"encoding\/hex\"\n\t\"github.com\/iotexproject\/iotex-core\/blockchain\"\n\t\"github.com\/iotexproject\/iotex-core\/config\"\n\t\"github.com\/iotexproject\/iotex-core\/db\/rds\"\n)\n\n\/\/ Server is the container of the index service\ntype Server struct {\n\tcfg     *config.Config\n\tidx     *Indexer\n\tbc      blockchain.Blockchain\n\tblockCh chan *blockchain.Block\n}\n\n\/\/ NewServer instantiates an index service\nfunc NewServer(\n\tcfg *config.Config,\n\tbc blockchain.Blockchain,\n) *Server {\n\treturn &Server{\n\t\tcfg: cfg,\n\t\tidx: &Indexer{\n\t\t\tcfg:                cfg.Indexer,\n\t\t\trds:                nil,\n\t\t\thexEncodedNodeAddr: \"\",\n\t\t},\n\t\tbc: bc,\n\t}\n}\n\n\/\/ Start starts the explorer server\nfunc (s *Server) Start(ctx context.Context) error {\n\taddr := s.cfg.Indexer.NodeAddr\n\tif addr == \"\" {\n\t\tblockAddr, err := s.cfg.BlockchainAddress()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"error when get the blockchain address\")\n\t\t}\n\t\taddr = hex.EncodeToString(blockAddr.Bytes()[:])\n\t}\n\ts.idx.hexEncodedNodeAddr = addr\n\n\ts.idx.rds = rds.NewAwsRDS(&s.cfg.DB.RDS)\n\tif err := s.idx.rds.Start(ctx); err != nil {\n\t\treturn errors.Wrap(err, \"error when start rds store\")\n\t}\n\n\ts.blockCh = make(chan *blockchain.Block)\n\tif err := s.bc.SubscribeBlockCreation(s.blockCh); err != nil {\n\t\treturn errors.Wrap(err, \"error when subscribe to block\")\n\t}\n\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase blk := <-s.blockCh:\n\t\t\t\ts.idx.BuildIndex(blk)\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn nil\n}\n\n\/\/ Stop stops the explorer server\nfunc (s *Server) Stop(ctx context.Context) error {\n\tif err := s.idx.rds.Stop(ctx); err != nil {\n\t\treturn errors.Wrap(err, \"error when shutting down explorer http server\")\n\t}\n\tif err := s.bc.UnSubscribeBlockCreation(s.blockCh); err != nil {\n\t\treturn errors.Wrap(err, \"error when un subscribe block creation\")\n\t}\n\tclose(s.blockCh)\n\treturn nil\n}\n\n\/\/ Indexer return indexer interface\nfunc (s *Server) Indexer() *Indexer { return s.idx }\n","lang_cluster":"Go","length":90,"code_uid":"5f6529f5ad5e48a7b730b49c44e0e13f"}
{"diff_hunk":"@@ -35,3 +35,7 @@ func (s *stub) ChangedLines() (map[string][]int, error) {\n func (s *stub) Checkout(revision string) error {\n \treturn fmt.Errorf(\"Unknown SCM, can't checkout\")\n }\n+\n+func (s *stub) CurrentRevDate(format string) string {\n+\treturn \"<unknown>\"\n+}","old_code":"package scm\n\nimport \"fmt\"\n\ntype stub struct{}\n\nfunc (s *stub) DescribeIdentifier(sha string) string {\n\treturn \"<unknown>\"\n}\n\nfunc (s *stub) CurrentRevIdentifier() string {\n\treturn \"<unknown>\"\n}\n\nfunc (s *stub) ChangesIn(diffSpec string, relativeTo string) []string {\n\treturn nil\n}\n\nfunc (s *stub) ChangedFiles(fromCommit string, includeUntracked bool, relativeTo string) []string {\n\treturn nil\n}\n\nfunc (s *stub) IgnoreFile(name string) error {\n\treturn fmt.Errorf(\"Don't know how to mark %s as ignored\", name)\n}\n\nfunc (s *stub) Remove(names []string) error {\n\treturn fmt.Errorf(\"Unknown SCM, can't remove files\")\n}\n\nfunc (s *stub) ChangedLines() (map[string][]int, error) {\n\treturn nil, fmt.Errorf(\"Unknown SCM, can't calculate changed lines\")\n}\n\nfunc (s *stub) Checkout(revision string) error {\n\treturn fmt.Errorf(\"Unknown SCM, can't checkout\")\n}\n","lang_cluster":"Go","length":37,"code_uid":"1eb3e75d1a74405cbef1c0da006acf04"}
{"diff_hunk":"@@ -79,7 +79,7 @@ func getStatsForProcess(name string, pcpu *float64, rss, vss *int64, pid *int) (\n \t\t\treturn nil\n \t\t} else {\n \t\t\t\/\/ something went wrong executing the command\n-\t\t\tDebugf(\"exec failure: %s\\n\", string(out))\n+\t\t\t\/\/ Debugf(\"exec failure: %s\\n\", string(out))\n \t\t\treturn errors.New(fmt.Sprintf(\"typeperf failed: %v\", err))\n \t\t}\n \t}","old_code":"\/\/ Copyright 2015-2016 Apcera Inc. All rights reserved.\n\npackage pse\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"os\/exec\"\n\t\"path\/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n)\n\n\/\/ cache the image name to optimize repeated calls\nvar imageName string\nvar imageLock sync.Mutex\n\n\/\/ parseValues parses the results of data returned by typeperf.exe.  This\n\/\/ is a series of comma delimited quoted strings, containing date time,\n\/\/ pid, pcpu, rss, and vss.  All numeric values are floating point.\n\/\/ eg: \"04\/17\/2016 15.38.00.016\", \"5123.00000\", \"1.23400\", \"123.00000\", \"123.00000\"\nfunc parseValues(line string, pid *int, pcpu *float64, rss, vss *int64) (err error) {\n\tvalues := strings.Split(line, \",\")\n\tif len(values) < 4 {\n\t\treturn errors.New(\"Invalid result.\")\n\t}\n\t\/\/ values[0] will be date, time, ignore them\n\t\/\/ parse the pid\n\tfVal, err := strconv.ParseFloat(strings.Trim(values[1], \"\\\"\"), 64)\n\tif err != nil {\n\t\treturn errors.New(fmt.Sprintf(\"Unable to parse pid: %s\", values[1]))\n\t}\n\t*pid = int(fVal)\n\n\t\/\/ parse pcpu\n\t*pcpu, err = strconv.ParseFloat(strings.Trim(values[2], \"\\\"\"), 64)\n\tif err != nil {\n\t\treturn errors.New(fmt.Sprintf(\"Unable to parse percent cpu: %s\", values[2]))\n\t}\n\n\t\/\/ parse private working set (rss)\n\tfVal, err = strconv.ParseFloat(strings.Trim(values[3], \"\\\"\"), 64)\n\tif err != nil {\n\t\treturn errors.New(fmt.Sprintf(\"Unable to parse working set: %s\", values[3]))\n\t}\n\t*rss = int64(fVal)\n\n\t\/\/ parse virtual bytes (vsz)\n\tfVal, err = strconv.ParseFloat(strings.Trim(values[4], \"\\\"\"), 64)\n\tif err != nil {\n\t\treturn errors.New(fmt.Sprintf(\"Unable to parse virtual bytes: %s\", values[4]))\n\t}\n\t*vss = int64(fVal)\n\n\treturn nil\n}\n\n\/\/ getStatsForProcess retrieves process information for a given instance name.\n\/\/ typeperf.exe is the windows native command line utility to get pcpu, rss,\n\/\/ and vsz equivalents through queries of performance counters.\n\/\/ An alternative is to map the Pdh* native windows API from pdh.dll,\n\/\/ and call those APIs directly - this is a simpler and cleaner approach.\nfunc getStatsForProcess(name string, pcpu *float64, rss, vss *int64, pid *int) (err error) {\n\t\/\/ query the counters using typeperf. \"-sc\",\"1\" requests one\n\t\/\/ set of data (versus continuous monitoring)\n\tout, err := exec.Command(\"typeperf.exe\",\n\t\tfmt.Sprintf(\"\\\\Process(%s)\\\\ID Process\", name),\n\t\tfmt.Sprintf(\"\\\\Process(%s)\\\\%% Processor Time\", name),\n\t\tfmt.Sprintf(\"\\\\Process(%s)\\\\Working Set - Private\", name),\n\t\tfmt.Sprintf(\"\\\\Process(%s)\\\\Virtual Bytes\", name),\n\t\t\"-sc\", \"1\").Output()\n\tif err != nil {\n\t\t\/\/ Signal that the command ran, but the image instance was not found\n\t\t\/\/ through a PID of -1.\n\t\tif strings.Contains(string(out), \"The data is not valid\") {\n\t\t\t*pid = -1\n\t\t\treturn nil\n\t\t} else {\n\t\t\t\/\/ something went wrong executing the command\n\t\t\tDebugf(\"exec failure: %s\\n\", string(out))\n\t\t\treturn errors.New(fmt.Sprintf(\"typeperf failed: %v\", err))\n\t\t}\n\t}\n\n\tresults := strings.Split(string(out), \"\\r\\n\")\n\t\/\/ results[0] = newline\n\t\/\/ results[1] = headers\n\t\/\/ results[2] = values\n\t\/\/ ignore the rest...\n\tif len(results) < 3 {\n\t\treturn errors.New(fmt.Sprintf(\"unexpected results from typeperf\"))\n\t}\n\tif err = parseValues(results[2], pid, pcpu, rss, vss); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n\/\/ getProcessImageName returns the name of the process image, as expected by\n\/\/ typeperf.\nfunc getProcessImageName() (name string) {\n\tname = filepath.Base(os.Args[0])\n\tname = strings.TrimRight(name, \".exe\")\n\treturn\n}\n\n\/\/ procUsage retrieves process cpu and memory information.\n\/\/ Under the hood, typeperf is called.  Notably, typeperf cannot search\n\/\/ using a pid, but instead uses a somewhat volatile process image name.\n\/\/ If there is more than one instance, \"#<instancecount>\" is appended to\n\/\/ the image name. Wildcard filters are supported, but result in a very\n\/\/ complex data set to parse.\nfunc ProcUsage(pcpu *float64, rss, vss *int64) error {\n\tvar ppid int = -1\n\n\timageLock.Lock()\n\tname := imageName\n\timageLock.Unlock()\n\n\t\/\/ Get the pid to retrieve the right set of information for this process.\n\tprocPid := os.Getpid()\n\n\t\/\/ if we have cached the image name, try that first\n\tif name != \"\" {\n\t\terr := getStatsForProcess(name, pcpu, rss, vss, &ppid)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t\/\/ If the instance name's pid matches ours, we're done.\n\t\t\/\/ Otherwise, this instance has been renamed, which is possible\n\t\t\/\/ as other process instances start and stop on the system.\n\t\tif ppid == procPid {\n\t\t\treturn nil\n\t\t}\n\t}\n\t\/\/ If we get here, the instance name is invalid (nil, or out of sync)\n\t\/\/ Query pid and counters until the correct image name is found and\n\t\/\/ cache it.  This is optimized for one or two instances on a windows\n\t\/\/ node. An alternative is using a wildcard to first lookup up pids,\n\t\/\/ and parse those to find instance name, then lookup the\n\t\/\/ performance counters.\n\tprefix := getProcessImageName()\n\tfor i := 0; ppid != procPid; i++ {\n\t\tname = fmt.Sprintf(\"%s#%d\", prefix, i)\n\t\terr := getStatsForProcess(name, pcpu, rss, vss, &ppid)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t\/\/ Bail out if an image name is not found.\n\t\tif ppid < 0 {\n\t\t\tbreak\n\t\t}\n\n\t\t\/\/ if the pids equal, this is the right process and cache our\n\t\t\/\/ image name\n\t\tif ppid == procPid {\n\t\t\timageLock.Lock()\n\t\t\timageName = name\n\t\t\timageLock.Unlock()\n\t\t\tbreak\n\t\t}\n\t}\n\tif ppid < 0 {\n\t\treturn errors.New(\"unable to retrieve process counters\")\n\t}\n\treturn nil\n}\n","lang_cluster":"Go","length":170,"code_uid":"fce26f52b0f640f39670ae4f0d61dc4f"}
{"diff_hunk":"@@ -85,6 +85,8 @@ func register(reg transport.Registry) {\n \tjson.Register(reg, json.Procedure(\"unexpected-error\", UnexpectedError))\n \tjson.Register(reg, json.Procedure(\"bad-response\", BadResponse))\n \tjson.Register(reg, json.Procedure(\"phone\", Phone))\n+\tjson.Register(reg, json.Procedure(\"sleep\", Sleep))\n \n \traw.Register(reg, raw.Procedure(\"sleep\/raw\", SleepRaw))\n+\traw.Register(reg, raw.Procedure(\"waitfortimeout\/raw\", WaitForTimeoutRaw))\n }","old_code":"\/\/ Copyright (c) 2016 Uber Technologies, Inc.\n\/\/\n\/\/ Permission is hereby granted, free of charge, to any person obtaining a copy\n\/\/ of this software and associated documentation files (the \"Software\"), to deal\n\/\/ in the Software without restriction, including without limitation the rights\n\/\/ to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n\/\/ copies of the Software, and to permit persons to whom the Software is\n\/\/ furnished to do so, subject to the following conditions:\n\/\/\n\/\/ The above copyright notice and this permission notice shall be included in\n\/\/ all copies or substantial portions of the Software.\n\/\/\n\/\/ THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\/\/ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\/\/ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\/\/ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\/\/ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\/\/ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n\/\/ THE SOFTWARE.\n\npackage yarpc\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com\/yarpc\/yarpc-go\"\n\t\"github.com\/yarpc\/yarpc-go\/crossdock\/thrift\/echo\/yarpc\/echoserver\"\n\t\"github.com\/yarpc\/yarpc-go\/crossdock\/thrift\/gauntlet\/yarpc\/secondserviceserver\"\n\t\"github.com\/yarpc\/yarpc-go\/crossdock\/thrift\/gauntlet\/yarpc\/thrifttestserver\"\n\t\"github.com\/yarpc\/yarpc-go\/encoding\/json\"\n\t\"github.com\/yarpc\/yarpc-go\/encoding\/raw\"\n\t\"github.com\/yarpc\/yarpc-go\/encoding\/thrift\"\n\t\"github.com\/yarpc\/yarpc-go\/transport\"\n\t\"github.com\/yarpc\/yarpc-go\/transport\/http\"\n\ttch \"github.com\/yarpc\/yarpc-go\/transport\/tchannel\"\n\n\t\"github.com\/uber\/tchannel-go\"\n)\n\nvar dispatcher yarpc.Dispatcher\n\n\/\/ Start starts the test server that clients will make requests to\nfunc Start() {\n\tch, err := tchannel.NewChannel(\"yarpc-test\", nil)\n\tif err != nil {\n\t\tlog.Fatalln(\"couldn't create tchannel: %v\", err)\n\t}\n\n\tdispatcher = yarpc.NewDispatcher(yarpc.Config{\n\t\tName: \"yarpc-test\",\n\t\tInbounds: []transport.Inbound{\n\t\t\thttp.NewInbound(\":8081\"),\n\t\t\ttch.NewInbound(ch, tch.ListenAddr(\":8082\")),\n\t\t},\n\t})\n\n\tregister(dispatcher)\n\n\tif err := dispatcher.Start(); err != nil {\n\t\tfmt.Println(\"error:\", err.Error())\n\t}\n}\n\n\/\/ Stop stops running the RPC test subject\nfunc Stop() {\n\tif dispatcher == nil {\n\t\treturn\n\t}\n\tif err := dispatcher.Stop(); err != nil {\n\t\tfmt.Println(\"failed to stop:\", err.Error())\n\t}\n}\n\nfunc register(reg transport.Registry) {\n\traw.Register(reg, raw.Procedure(\"echo\/raw\", EchoRaw))\n\tjson.Register(reg, json.Procedure(\"echo\", EchoJSON))\n\n\t\/\/ NOTE(abg): Enveloping is disabled in old cross-language tests until the\n\t\/\/ other YARPC implementations catch up.\n\tthrift.Register(reg, echoserver.New(EchoThrift{}), thrift.DisableEnveloping)\n\tthrift.Register(reg, thrifttestserver.New(thriftTest{}), thrift.DisableEnveloping)\n\tthrift.Register(reg, secondserviceserver.New(secondService{}), thrift.DisableEnveloping)\n\n\tjson.Register(reg, json.Procedure(\"unexpected-error\", UnexpectedError))\n\tjson.Register(reg, json.Procedure(\"bad-response\", BadResponse))\n\tjson.Register(reg, json.Procedure(\"phone\", Phone))\n\n\traw.Register(reg, raw.Procedure(\"sleep\/raw\", SleepRaw))\n}\n","lang_cluster":"Go","length":90,"code_uid":"a737f4c6f88244c585143aacf989354b"}
{"diff_hunk":"@@ -1,4 +1,4 @@\n-\/\/ Copyright (c) 2020 Tigera, Inc. All rights reserved.\n+\/\/ Copyright (c) 2020-2021 Tigera, Inc. All rights reserved.\n \/\/\n \/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n \/\/ you may not use this file except in compliance with the License.","old_code":"\/\/ Copyright (c) 2020 Tigera, Inc. All rights reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\n\/\/ Copyright (c) 2020  All rights reserved.\n\npackage ut_test\n\nimport (\n\t\"os\"\n\t\"testing\"\n)\n\nfunc TestMain(m *testing.M) {\n\tinitMapsOnce()\n\tcleanUpMaps()\n\trc := m.Run()\n\tcleanUpMaps()\n\tos.Exit(rc)\n}\n","lang_cluster":"Go","length":30,"code_uid":"e103b1f79473406898a602c3b8a66367"}
{"diff_hunk":"@@ -1,4 +1,4 @@\n-package aws_test\n+package aws\n \n import (\n \t\"fmt\"","old_code":"package aws_test\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com\/aws\/aws-sdk-go\/service\/ec2\"\n\t\"github.com\/aws\/aws-sdk-go\/service\/opsworks\"\n\t\"github.com\/libopenstorage\/openstorage\/pkg\/storageops\"\n\t\"github.com\/libopenstorage\/openstorage\/pkg\/storageops\/aws\"\n\t\"github.com\/libopenstorage\/openstorage\/pkg\/storageops\/test\"\n\tuuid \"github.com\/satori\/go.uuid\"\n)\n\nconst (\n\tnewDiskSizeInGB = 10\n\tnewDiskPrefix   = \"openstorage-test\"\n)\n\nvar diskName = fmt.Sprintf(\"%s-%s\", newDiskPrefix, uuid.NewV4())\n\nfunc TestAll(t *testing.T) {\n\tdrivers := make(map[string]storageops.Ops)\n\tdiskTemplates := make(map[string]map[string]interface{})\n\n\tif d, err := aws.NewEnvClient(); err != aws.ErrAWSEnvNotAvailable {\n\t\tvolType := opsworks.VolumeTypeGp2\n\t\tvolSize := int64(newDiskSizeInGB)\n\t\tzone := os.Getenv(\"AWS_ZONE\")\n\t\tebsVol := &ec2.Volume{\n\t\t\tAvailabilityZone: &zone,\n\t\t\tVolumeType:       &volType,\n\t\t\tSize:             &volSize,\n\t\t}\n\t\tdrivers[d.Name()] = d\n\t\tdiskTemplates[d.Name()] = map[string]interface{}{\n\t\t\tdiskName: ebsVol,\n\t\t}\n\t} else {\n\t\tfmt.Printf(\"skipping AWS tests as environment is not set...\\n\")\n\t}\n\n\ttest.RunTest(drivers, diskTemplates, t)\n}\n","lang_cluster":"Go","length":45,"code_uid":"63389e9da5b94808b860503ef5e35290"}
{"diff_hunk":"@@ -94,8 +94,19 @@ func New(impl Interface, opts ...<$thrift>.RegisterOption) []<$transport>.Proced\n \t}\n \n \tprocedures := make([]<$transport>.Procedure, 0, <len .Functions>)\n-\t<if .Parent> procedures = append(procedures, <import .ParentServerPackagePath>.New(impl, opts...)...)\n-\t<end>         procedures = append(procedures, <$thrift>.BuildProcedures(service, opts...)...)\n+\t<if .Parent>\n+\tprocedures = append(\n+\t\tprocedures,\n+\t\t<import .ParentServerPackagePath>.New(\n+\t\t\timpl,\n+\t\t\tappend(\n+\t\t\t\topts,\n+\t\t\t\t<$thrift>.Named(\"<.Name>\"),\n+\t\t\t)...,\n+\t\t)...,\n+\t)\n+\t<end ->\n+\tprocedures = append(procedures, <$thrift>.BuildProcedures(service, opts...)...)\n \treturn procedures\n }\n ","old_code":"\/\/ Copyright (c) 2018 Uber Technologies, Inc.\n\/\/\n\/\/ Permission is hereby granted, free of charge, to any person obtaining a copy\n\/\/ of this software and associated documentation files (the \"Software\"), to deal\n\/\/ in the Software without restriction, including without limitation the rights\n\/\/ to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n\/\/ copies of the Software, and to permit persons to whom the Software is\n\/\/ furnished to do so, subject to the following conditions:\n\/\/\n\/\/ The above copyright notice and this permission notice shall be included in\n\/\/ all copies or substantial portions of the Software.\n\/\/\n\/\/ THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\/\/ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\/\/ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\/\/ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\/\/ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\/\/ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n\/\/ THE SOFTWARE.\n\npackage main\n\nimport (\n\t\"path\/filepath\"\n\n\t\"go.uber.org\/thriftrw\/plugin\"\n)\n\nconst serverTemplate = `\n\/\/ Code generated by thriftrw-plugin-yarpc\n\/\/ @generated\n\n<$pkgname := printf \"%sserver\" (lower .Name)>\npackage <$pkgname>\n\n<$thrift    := import \"go.uber.org\/yarpc\/encoding\/thrift\">\n<$transport := import \"go.uber.org\/yarpc\/api\/transport\">\n\n<$contextImportPath   := .ContextImportPath>\n<$onewayWrapperImport := .OnewayWrapperImport>\n<$onewayWrapperFunc   := .OnewayWrapperFunc>\n<$unaryWrapperImport  := .UnaryWrapperImport>\n<$unaryWrapperFunc    := .UnaryWrapperFunc>\n\n<\/* Note that we import things like \"context\" inside loops rather than at the\n    top-level because they will end up unused if the service does not have any\n    functions.\n *\/>\n\n\/\/ Interface is the server-side interface for the <.Name> service.\ntype Interface interface {\n\t<if .Parent><import .ParentServerPackagePath>.Interface\n\t<end>\n\t<range .Functions>\n\t\t<$context := import $contextImportPath>\n\t\t<.Name>(\n\t\t\tctx <$context>.Context, <range .Arguments>\n\t\t\t<.Name> <formatType .Type>,<end>\n\t\t)<if .OneWay> error\n\t\t<else if .ReturnType> (<formatType .ReturnType>, error)\n\t\t<else> error\n\t\t<end>\n\t<end>\n}\n\n<$module := .Module>\n\n\/\/ New prepares an implementation of the <.Name> service for\n\/\/ registration.\n\/\/\n\/\/ \thandler := <.Name>Handler{}\n\/\/ \tdispatcher.Register(<$pkgname>.New(handler))\nfunc New(impl Interface, opts ...<$thrift>.RegisterOption) []<$transport>.Procedure {\n\t<if .Functions>h := handler{impl}<end>\n\tservice := <$thrift>.Service{\n\t\tName: \"<.Name>\",\n\t\tMethods: []<$thrift>.Method{\n\t\t<range .Functions>\n\t\t\t<$thrift>.Method{\n\t\t\t\tName: \"<.ThriftName>\",\n\t\t\t\tHandlerSpec: <$thrift>.HandlerSpec{\n\t\t\t\t<if .OneWay>\n\t\t\t\t\tType: <$transport>.Oneway,\n\t\t\t\t\tOneway: <import $onewayWrapperImport>.<$onewayWrapperFunc>(h.<.Name>),\n\t\t\t\t<else>\n\t\t\t\t\tType: <$transport>.Unary,\n\t\t\t\t\tUnary: <import $unaryWrapperImport>.<$unaryWrapperFunc>(h.<.Name>),\n\t\t\t\t<end>\n\t\t\t\t},\n\t\t\t\tSignature: \"<.Name>(<range $i, $v := .Arguments><if ne $i 0>, <end><.Name> <formatType .Type><end>)<if not .OneWay | and .ReturnType> (<formatType .ReturnType>)<end>\",\n\t\t\t\tThriftModule: <import $module.ImportPath>.ThriftModule,\n\t\t\t\t},\n\t\t<end>},\n\t}\n\n\tprocedures := make([]<$transport>.Procedure, 0, <len .Functions>)\n\t<if .Parent> procedures = append(procedures, <import .ParentServerPackagePath>.New(impl, opts...)...)\n\t<end>         procedures = append(procedures, <$thrift>.BuildProcedures(service, opts...)...)\n\treturn procedures\n}\n\ntype handler struct{ impl Interface }\n\n<$service := .>\n<$module := .Module>\n<range .Functions>\n<$context := import $contextImportPath>\n<$prefix := printf \"%s.%s_%s_\" (import $module.ImportPath) $service.Name .Name>\n\n<$wire := import \"go.uber.org\/thriftrw\/wire\">\n\n<if .OneWay>\nfunc (h handler) <.Name>(ctx <$context>.Context, body <$wire>.Value) error {\n\tvar args <$prefix>Args\n\tif err := args.FromWire(body); err != nil {\n\t\treturn err\n\t}\n\n\treturn h.impl.<.Name>(ctx, <range .Arguments>args.<.Name>,<end>)\n}\n<else>\nfunc (h handler) <.Name>(ctx <$context>.Context, body <$wire>.Value) (<$thrift>.Response, error) {\n\tvar args <$prefix>Args\n\tif err := args.FromWire(body); err != nil {\n\t\treturn <$thrift>.Response{}, err\n\t}\n\n\t<if .ReturnType>\n\t\tsuccess, err := h.impl.<.Name>(ctx, <range .Arguments>args.<.Name>,<end>)\n\t<else>\n\t\terr := h.impl.<.Name>(ctx, <range .Arguments>args.<.Name>,<end>)\n\t<end>\n\n\thadError := err != nil\n\tresult, err := <$prefix>Helper.WrapResponse(<if .ReturnType>success,<end> err)\n\n\tvar response <$thrift>.Response\n\tif err == nil {\n\t\tresponse.IsApplicationError = hadError\n\t\tresponse.Body = result\n\t}\n\treturn response, err\n}\n<end>\n<end>\n`\n\nfunc serverGenerator(data *templateData, files map[string][]byte) (err error) {\n\tpackageName := filepath.Base(data.ServerPackagePath())\n\t\/\/ kv.thrift => ...\/kv\/keyvalueserver\/server.go\n\tpath := filepath.Join(data.Module.Directory, packageName, \"server.go\")\n\tfiles[path], err = plugin.GoFileFromTemplate(path, serverTemplate, data, templateOptions...)\n\treturn\n}\n","lang_cluster":"Go","length":154,"code_uid":"2679447a512d4153b1683ed87d8b0b8b"}
{"diff_hunk":"@@ -153,6 +153,10 @@ func (agent *ecsAgent) appendFirelensFluentbitCapabilities(capabilities []*ecs.A\n \treturn appendNameOnlyAttribute(capabilities, attributePrefix+capabilityFirelensFluentbit)\n }\n \n+func (agent *ecsAgent) appendEFSCapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n+\treturn appendNameOnlyAttribute(capabilities, attributePrefix+capabilityEFS)\n+}\n+\n func (agent *ecsAgent) appendFirelensLoggingDriverCapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n \treturn appendNameOnlyAttribute(capabilities, capabilityPrefix+capabilityFirelensLoggingDriver)\n }","old_code":"\/\/ +build linux\n\n\/\/ Copyright 2014-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\"). You may\n\/\/ not use this file except in compliance with the License. A copy of the\n\/\/ License is located at\n\/\/\n\/\/\thttp:\/\/aws.amazon.com\/apache2.0\/\n\/\/\n\/\/ or in the \"license\" file accompanying this file. This file is distributed\n\/\/ on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n\/\/ express or implied. See the License for the specific language governing\n\/\/ permissions and limitations under the License.\n\npackage app\n\nimport (\n\t\"strings\"\n\n\t\"github.com\/aws\/amazon-ecs-agent\/agent\/config\"\n\t\"github.com\/aws\/amazon-ecs-agent\/agent\/dockerclient\"\n\t\"github.com\/aws\/amazon-ecs-agent\/agent\/dockerclient\/dockerapi\"\n\t\"github.com\/aws\/amazon-ecs-agent\/agent\/ecs_client\/model\/ecs\"\n\t\"github.com\/aws\/amazon-ecs-agent\/agent\/ecscni\"\n\t\"github.com\/aws\/amazon-ecs-agent\/agent\/taskresource\/volume\"\n\t\"github.com\/aws\/amazon-ecs-agent\/agent\/utils\"\n\t\"github.com\/aws\/aws-sdk-go\/aws\"\n\t\"github.com\/cihub\/seelog\"\n)\n\nconst (\n\tAVX         = \"avx\"\n\tAVX2        = \"avx2\"\n\tSSE41       = \"sse4_1\"\n\tSSE42       = \"sse4_2\"\n\tCpuInfoPath = \"\/proc\/cpuinfo\"\n)\n\nfunc (agent *ecsAgent) appendVolumeDriverCapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\t\/\/ \"local\" is default docker driver\n\tcapabilities = appendNameOnlyAttribute(capabilities, attributePrefix+capabilityDockerPluginInfix+volume.DockerLocalVolumeDriver)\n\n\t\/\/ for non-standardized plugins, call docker pkg's plugins.Scan()\n\tnonStandardizedPlugins, err := agent.mobyPlugins.Scan()\n\tif err != nil {\n\t\tseelog.Warnf(\"Scanning plugins failed: %v\", err)\n\t\t\/\/ do not return yet, we need the list of plugins below. range handles nil slice.\n\t}\n\n\tfor _, pluginName := range nonStandardizedPlugins {\n\t\t\/\/ Replace the ':' to '.' in the plugin name for attributes\n\t\tcapabilities = appendNameOnlyAttribute(capabilities,\n\t\t\tattributePrefix+capabilityDockerPluginInfix+strings.Replace(pluginName, config.DockerTagSeparator, attributeSeparator, -1))\n\t}\n\n\t\/\/ for standardized plugins, call docker's plugin ls API\n\tpluginEnabled := true\n\tvolumeDriverType := []string{dockerapi.VolumeDriverType}\n\tstandardizedPlugins, err := agent.dockerClient.ListPluginsWithFilters(agent.ctx, pluginEnabled, volumeDriverType, dockerclient.ListPluginsTimeout)\n\tif err != nil {\n\t\tseelog.Warnf(\"Listing plugins with filters enabled=%t, capabilities=%v failed: %v\", pluginEnabled, volumeDriverType, err)\n\t\treturn capabilities\n\t}\n\n\t\/\/ For plugin with default tag latest, register two attributes with and without the latest tag\n\t\/\/ as the tag is optional and can be added by docker or customer\n\tfor _, pluginName := range standardizedPlugins {\n\t\tnames := strings.Split(pluginName, config.DockerTagSeparator)\n\t\tif len(names) > 1 && names[len(names)-1] == config.DefaultDockerTag {\n\t\t\tcapabilities = appendNameOnlyAttribute(capabilities, attributePrefix+capabilityDockerPluginInfix+strings.Join(names[:len(names)-1], attributeSeparator))\n\t\t}\n\n\t\tcapabilities = appendNameOnlyAttribute(capabilities,\n\t\t\tattributePrefix+capabilityDockerPluginInfix+strings.Replace(pluginName, config.DockerTagSeparator, attributeSeparator, -1))\n\t}\n\treturn capabilities\n}\n\nfunc (agent *ecsAgent) appendNvidiaDriverVersionAttribute(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\tif agent.resourceFields != nil && agent.resourceFields.NvidiaGPUManager != nil {\n\t\tdriverVersion := agent.resourceFields.NvidiaGPUManager.GetDriverVersion()\n\t\tif driverVersion != \"\" {\n\t\t\tcapabilities = appendNameOnlyAttribute(capabilities, attributePrefix+capabilityNvidiaDriverVersionInfix+driverVersion)\n\t\t}\n\t}\n\treturn capabilities\n}\n\nfunc (agent *ecsAgent) appendENITrunkingCapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\tif !agent.cfg.ENITrunkingEnabled {\n\t\treturn capabilities\n\t}\n\tcapabilities = appendNameOnlyAttribute(capabilities, attributePrefix+taskENITrunkingAttributeSuffix)\n\treturn agent.appendBranchENIPluginVersionAttribute(capabilities)\n}\n\nfunc (agent *ecsAgent) appendBranchENIPluginVersionAttribute(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\tversion, err := agent.cniClient.Version(ecscni.ECSBranchENIPluginName)\n\tif err != nil {\n\t\tseelog.Warnf(\n\t\t\t\"Unable to determine the version of the plugin '%s': %v\",\n\t\t\tecscni.ECSBranchENIPluginName, err)\n\t\treturn capabilities\n\t}\n\n\treturn append(capabilities, &ecs.Attribute{\n\t\tName:  aws.String(attributePrefix + branchCNIPluginVersionSuffix),\n\t\tValue: aws.String(version),\n\t})\n}\n\nfunc (agent *ecsAgent) appendPIDAndIPCNamespaceSharingCapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\treturn appendNameOnlyAttribute(capabilities, attributePrefix+capabiltyPIDAndIPCNamespaceSharing)\n}\n\nfunc (agent *ecsAgent) appendAppMeshCapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\treturn appendNameOnlyAttribute(capabilities, attributePrefix+appMeshAttributeSuffix)\n}\n\nfunc (agent *ecsAgent) appendTaskEIACapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\n\tcapabilities = appendNameOnlyAttribute(capabilities, attributePrefix+taskEIAAttributeSuffix)\n\n\teiaRequiredFlags := []string{AVX, AVX2, SSE41, SSE42}\n\tcpuInfo, err := utils.ReadCPUInfo(CpuInfoPath)\n\tif err != nil {\n\t\tseelog.Warnf(\"Unable to read cpuinfo: %v\", err)\n\t\treturn capabilities\n\t}\n\n\tflagMap := utils.GetCPUFlags(cpuInfo)\n\tmissingFlags := []string{}\n\tfor _, requiredFlag := range eiaRequiredFlags {\n\t\tif _, ok := flagMap[requiredFlag]; !ok {\n\t\t\tmissingFlags = append(missingFlags, requiredFlag)\n\t\t}\n\t}\n\n\tif len(missingFlags) > 0 {\n\t\tseelog.Infof(\"Missing cpu flags for EIA support: %v\", strings.Join(missingFlags, \",\"))\n\t\treturn capabilities\n\t}\n\n\treturn appendNameOnlyAttribute(capabilities, attributePrefix+taskEIAWithOptimizedCPU)\n}\n\nfunc (agent *ecsAgent) appendFirelensFluentdCapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\treturn appendNameOnlyAttribute(capabilities, attributePrefix+capabilityFirelensFluentd)\n}\n\nfunc (agent *ecsAgent) appendFirelensFluentbitCapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\treturn appendNameOnlyAttribute(capabilities, attributePrefix+capabilityFirelensFluentbit)\n}\n\nfunc (agent *ecsAgent) appendFirelensLoggingDriverCapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\treturn appendNameOnlyAttribute(capabilities, capabilityPrefix+capabilityFirelensLoggingDriver)\n}\n\nfunc (agent *ecsAgent) appendFirelensConfigCapabilities(capabilities []*ecs.Attribute) []*ecs.Attribute {\n\tcapabilities = appendNameOnlyAttribute(capabilities, attributePrefix+capabilityFirelensConfigFile)\n\treturn appendNameOnlyAttribute(capabilities, attributePrefix+capabilityFirelensConfigS3)\n}\n","lang_cluster":"Go","length":163,"code_uid":"9e3e9cbe5abb4afb89ffdbb494b58f00"}
{"diff_hunk":"@@ -1,9 +1,25 @@\n+\/*\n+Copyright 2019 The KubeEdge Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*\/\n package status\n \n import (\n-\tedgeapi \"github.com\/kubeedge\/kubeedge\/common\/types\"\n \t\"time\"\n \n+\tedgeapi \"github.com\/kubeedge\/kubeedge\/common\/types\"\n+\n \t\"github.com\/kubeedge\/beehive\/pkg\/common\/log\"\n \t\"github.com\/kubeedge\/kubeedge\/edge\/pkg\/edged\/podmanager\"\n \t\"github.com\/kubeedge\/kubeedge\/edge\/pkg\/metamanager\/client\"","old_code":"package status\n\nimport (\n\tedgeapi \"github.com\/kubeedge\/kubeedge\/common\/types\"\n\t\"time\"\n\n\t\"github.com\/kubeedge\/beehive\/pkg\/common\/log\"\n\t\"github.com\/kubeedge\/kubeedge\/edge\/pkg\/edged\/podmanager\"\n\t\"github.com\/kubeedge\/kubeedge\/edge\/pkg\/metamanager\/client\"\n\t\"k8s.io\/api\/core\/v1\"\n\tapiequality \"k8s.io\/apimachinery\/pkg\/api\/equality\"\n\t\"k8s.io\/apimachinery\/pkg\/types\"\n\t\"k8s.io\/apimachinery\/pkg\/util\/wait\"\n\tclientset \"k8s.io\/client-go\/kubernetes\"\n\t\"k8s.io\/kubernetes\/pkg\/kubelet\/status\"\n)\n\n\/\/ manager as status manager, embedded a k8s.io\/kubernetes\/pkg\/kubelet\/status.Manager\n\/\/ inherit it's method but refactored Start() function to periodicity update status to IEF\ntype manager struct {\n\tstatus.Manager\n\t\/\/ TODO: consider need lock?\n\tpodManager        podmanager.Manager\n\tapiStatusVersions map[types.UID]*v1.PodStatus\n\tmetaClient        client.CoreInterface\n}\n\n\/\/NewManager creates and returns a new manager object\nfunc NewManager(kubeClient clientset.Interface, podManager podmanager.Manager, podDeletionSafety status.PodDeletionSafetyProvider, metaClient client.CoreInterface) status.Manager {\n\tkubeManager := status.NewManager(kubeClient, podManager, podDeletionSafety)\n\treturn &manager{\n\t\tManager:           kubeManager,\n\t\tmetaClient:        metaClient,\n\t\tpodManager:        podManager,\n\t\tapiStatusVersions: make(map[types.UID]*v1.PodStatus),\n\t}\n}\n\nconst syncPeriod = 10 * time.Second\n\nfunc (m *manager) Start() {\n\tlog.LOGGER.Info(\"Starting to sync pod status with apiserver\")\n\tsyncTicker := time.Tick(syncPeriod)\n\n\tgo wait.Forever(func() {\n\t\tselect {\n\t\tcase <-syncTicker:\n\t\t\tm.updatePodStatus()\n\t\t}\n\t}, 0)\n}\n\nfunc (m *manager) updatePodStatus() {\n\tfor _, pod := range m.podManager.GetPods() {\n\t\tuid := pod.UID\n\t\tpodStatus, ok := m.GetPodStatus(uid)\n\t\tif !ok || &podStatus == nil {\n\t\t\tcontinue\n\t\t}\n\t\tlatestStatus, ok := m.apiStatusVersions[uid]\n\t\tif ok && apiequality.Semantic.DeepEqual(latestStatus, &podStatus) {\n\t\t\tcontinue\n\t\t}\n\t\ts := *podStatus.DeepCopy()\n\t\tvar conditionFlag bool\n\t\tpodCondition := v1.PodCondition{Type: v1.PodReady, Status: v1.ConditionFalse, Reason: \"ContainersNotReady\"}\n\t\tfor idx, cs := range podStatus.ContainerStatuses {\n\t\t\tif cs.State.Running != nil && cs.State.Running.StartedAt.Unix() == 0 {\n\t\t\t\tnewState := v1.ContainerState{Waiting: &v1.ContainerStateWaiting{\n\t\t\t\t\tReason:  \"CrashLoopBackOff\",\n\t\t\t\t\tMessage: \"Container restarting in container runtime\",\n\t\t\t\t}}\n\t\t\t\ts.ContainerStatuses[idx].State = newState\n\t\t\t\tconditionFlag = true\n\t\t\t}\n\t\t}\n\t\tvar podReadyFlag bool\n\t\tif conditionFlag {\n\t\t\tif s.Conditions == nil {\n\t\t\t\ts.Conditions = append(s.Conditions, podCondition)\n\t\t\t} else {\n\t\t\t\tfor index, condition := range s.Conditions {\n\t\t\t\t\tif condition.Type == v1.PodReady {\n\t\t\t\t\t\ts.Conditions[index].Status = v1.ConditionFalse\n\t\t\t\t\t\ts.Conditions[index].Reason = \"ContainersNotReady\"\n\t\t\t\t\t\tpodReadyFlag = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !podReadyFlag {\n\t\t\t\t\ts.Conditions = append(s.Conditions, podCondition)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\terr := m.metaClient.PodStatus(pod.Namespace).Update(pod.Name, edgeapi.PodStatusRequest{UID: pod.UID, Name: pod.Name, Status: s})\n\t\tif err != nil {\n\t\t\tlog.LOGGER.Errorf(\"Update pod status failed err :%v\", err)\n\t\t}\n\t\tlog.LOGGER.Infof(\"Status for pod %s updated successfully: %+v\", pod.Name, podStatus)\n\t\tm.apiStatusVersions[pod.UID] = podStatus.DeepCopy()\n\t}\n}\n","lang_cluster":"Go","length":103,"code_uid":"1792601b429941c7a465c8d34895d998"}
{"diff_hunk":"@@ -48,7 +48,7 @@ func TestBroadcast(t *testing.T) {\n \t\t}\n \t}\n \tu := func(_ context.Context, _ uint32, _ peerstore.PeerInfo, _ proto.Message) {}\n-\tbootnodePort := testutil.RandomPort()\n+\tbootnodePort := 14689\n \tcfg := config.Config{\n \t\tNetwork: config.Network{Host: \"127.0.0.1\", Port: bootnodePort},\n \t}","old_code":"\/\/ Copyright (c) 2019 IoTeX Foundation\n\/\/ This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n\/\/ warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n\/\/ permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n\/\/ License 2.0 that can be found in the LICENSE file.\n\npackage p2p\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com\/golang\/protobuf\/proto\"\n\tpeerstore \"github.com\/libp2p\/go-libp2p-peerstore\"\n\t\"github.com\/stretchr\/testify\/require\"\n\n\t\"github.com\/iotexproject\/iotex-core\/config\"\n\t\"github.com\/iotexproject\/iotex-core\/testutil\"\n\t\"github.com\/iotexproject\/iotex-proto\/golang\/testingpb\"\n)\n\nfunc TestBroadcast(t *testing.T) {\n\tctx := context.Background()\n\tn := 10\n\tagents := make([]*Agent, 0)\n\tdefer func() {\n\t\tvar err error\n\t\tfor _, agent := range agents {\n\t\t\terr = agent.Stop(ctx)\n\t\t}\n\t\trequire.NoError(t, err)\n\t}()\n\tcounts := make(map[uint8]int)\n\tvar mutex sync.RWMutex\n\tb := func(_ context.Context, _ uint32, msg proto.Message) {\n\t\tmutex.Lock()\n\t\tdefer mutex.Unlock()\n\t\ttestMsg, ok := msg.(*testingpb.TestPayload)\n\t\trequire.True(t, ok)\n\t\tidx := testMsg.MsgBody[0]\n\t\tif _, ok = counts[idx]; ok {\n\t\t\tcounts[idx]++\n\t\t} else {\n\t\t\tcounts[idx] = 1\n\t\t}\n\t}\n\tu := func(_ context.Context, _ uint32, _ peerstore.PeerInfo, _ proto.Message) {}\n\tbootnodePort := testutil.RandomPort()\n\tcfg := config.Config{\n\t\tNetwork: config.Network{Host: \"127.0.0.1\", Port: bootnodePort},\n\t}\n\tbootnode := NewAgent(cfg, b, u)\n\trequire.NoError(t, bootnode.Start(ctx))\n\trequire.NoError(t, testutil.WaitUntil(100*time.Millisecond, 10*time.Second, func() (b bool, e error) {\n\t\tip := net.ParseIP(\"127.0.0.1\")\n\t\ttcpAddr := net.TCPAddr{\n\t\t\tIP:   ip,\n\t\t\tPort: bootnodePort,\n\t\t}\n\t\t_, err := net.DialTCP(\"tcp\", nil, &tcpAddr)\n\t\treturn err == nil, nil\n\t}))\n\tfor i := 0; i < n; i++ {\n\t\tport := testutil.RandomPort()\n\t\tcfg := config.Config{\n\t\t\tNetwork: config.Network{\n\t\t\t\tHost:           \"127.0.0.1\",\n\t\t\t\tPort:           port,\n\t\t\t\tBootstrapNodes: []string{bootnode.Self()[0].String()},\n\t\t\t},\n\t\t}\n\t\tagent := NewAgent(cfg, b, u)\n\t\trequire.NoError(t, agent.Start(ctx))\n\t\trequire.NoError(t, testutil.WaitUntil(100*time.Millisecond, 10*time.Second, func() (b bool, e error) {\n\t\t\tip := net.ParseIP(\"127.0.0.1\")\n\t\t\ttcpAddr := net.TCPAddr{\n\t\t\t\tIP:   ip,\n\t\t\t\tPort: port,\n\t\t\t}\n\t\t\t_, err := net.DialTCP(\"tcp\", nil, &tcpAddr)\n\t\t\treturn err == nil, nil\n\t\t}))\n\t\tagents = append(agents, agent)\n\t}\n\n\tfor i := 0; i < n; i++ {\n\t\trequire.NoError(t, agents[i].BroadcastOutbound(WitContext(ctx, Context{ChainID: 1}), &testingpb.TestPayload{\n\t\t\tMsgBody: []byte{uint8(i)},\n\t\t}))\n\t\trequire.NoError(t, testutil.WaitUntil(100*time.Millisecond, 20*time.Second, func() (bool, error) {\n\t\t\tmutex.RLock()\n\t\t\tdefer mutex.RUnlock()\n\t\t\t\/\/ Broadcast message will be skipped by the source node\n\t\t\treturn counts[uint8(i)] == n, nil\n\t\t}))\n\t}\n}\n\nfunc TestUnicast(t *testing.T) {\n\tctx := context.Background()\n\tn := 10\n\tagents := make([]*Agent, 0)\n\tdefer func() {\n\t\tvar err error\n\t\tfor _, agent := range agents {\n\t\t\terr = agent.Stop(ctx)\n\t\t}\n\t\trequire.NoError(t, err)\n\t}()\n\tcounts := make(map[uint8]int)\n\tvar src string\n\tvar mutex sync.RWMutex\n\tb := func(_ context.Context, _ uint32, _ proto.Message) {}\n\tu := func(_ context.Context, _ uint32, peer peerstore.PeerInfo, msg proto.Message) {\n\t\tmutex.Lock()\n\t\tdefer mutex.Unlock()\n\t\ttestMsg, ok := msg.(*testingpb.TestPayload)\n\t\trequire.True(t, ok)\n\t\tidx := testMsg.MsgBody[0]\n\t\tif _, ok = counts[idx]; ok {\n\t\t\tcounts[idx]++\n\t\t} else {\n\t\t\tcounts[idx] = 1\n\t\t}\n\t\tsrc = peer.ID.Pretty()\n\t}\n\n\tbootnode := NewAgent(config.Config{\n\t\tNetwork: config.Network{Host: \"127.0.0.1\", Port: testutil.RandomPort()},\n\t}, b, u)\n\trequire.NoError(t, bootnode.Start(ctx))\n\n\tfor i := 0; i < n; i++ {\n\t\tcfg := config.Config{\n\t\t\tNetwork: config.Network{\n\t\t\t\tHost:           \"127.0.0.1\",\n\t\t\t\tPort:           testutil.RandomPort(),\n\t\t\t\tBootstrapNodes: []string{bootnode.Self()[0].String()},\n\t\t\t},\n\t\t}\n\t\tagent := NewAgent(cfg, b, u)\n\t\trequire.NoError(t, agent.Start(ctx))\n\t\tagents = append(agents, agent)\n\t}\n\n\tfor i := 0; i < n; i++ {\n\t\tneighbors, err := agents[i].Neighbors(ctx)\n\t\trequire.NoError(t, err)\n\t\trequire.True(t, len(neighbors) > 0)\n\t\tfor _, neighbor := range neighbors {\n\t\t\trequire.NoError(t, agents[i].UnicastOutbound(WitContext(ctx, Context{ChainID: 1}), neighbor, &testingpb.TestPayload{\n\t\t\t\tMsgBody: []byte{uint8(i)},\n\t\t\t}))\n\t\t}\n\t\trequire.NoError(t, testutil.WaitUntil(100*time.Millisecond, 20*time.Second, func() (bool, error) {\n\t\t\tmutex.RLock()\n\t\t\tdefer mutex.RUnlock()\n\t\t\treturn counts[uint8(i)] == len(neighbors) && src == agents[i].Info().ID.Pretty(), nil\n\t\t}))\n\t}\n}\n","lang_cluster":"Go","length":164,"code_uid":"5f6003f1c5e84d55a2b2d6e7dd8a2e28"}
{"diff_hunk":"@@ -41,11 +41,19 @@ func (w *watcher) ContainerStarted(id string) {\n \t\t\tcontinue\n \t\t}\n \t\tif network.isOurs {\n-\t\t\tfqdn := fmt.Sprintf(\"%s.%s\", info.Config.Hostname, info.Config.Domainname)\n-\t\t\tif err := w.weave.RegisterWithDNS(id, fqdn, net.IPAddress); err != nil {\n-\t\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to register %s with weaveDNS: %s\", id, err)\n+\t\t\tif !w.driver.noDNS {\n+\t\t\t\tfqdn := fmt.Sprintf(\"%s.%s\", info.Config.Hostname, info.Config.Domainname)\n+\t\t\t\tif err := w.weave.RegisterWithDNS(id, fqdn, net.IPAddress); err != nil {\n+\t\t\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to register %s with weaveDNS: %s\", id, err)\n+\t\t\t\t}\n \t\t\t}\n-\t\t\tif _, err := weavenet.WithNetNSByPid(info.State.Pid, \"configure-arp\", weavenet.VethName); err != nil {\n+\t\t\trootDir := \"\/\"\n+\t\t\tif w.driver.isPluginV2 {\n+\t\t\t\t\/\/ We bind mount host's \/proc to \/host\/proc for plugin-v2\n+\t\t\t\trootDir = \"\/host\"\n+\t\t\t}\n+\t\t\tnetNSPath := weavenet.NSPathByPidWithRoot(rootDir, info.State.Pid)\n+\t\t\tif _, err := weavenet.WithNetNS(netNSPath, \"configure-arp\", weavenet.VethName); err != nil {\n \t\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to configure interfaces: %s\", err)\n \t\t\t}\n \t\t}","old_code":"package plugin\n\nimport (\n\t\"fmt\"\n\n\tweaveapi \"github.com\/weaveworks\/weave\/api\"\n\t\"github.com\/weaveworks\/weave\/common\/docker\"\n\tweavenet \"github.com\/weaveworks\/weave\/net\"\n)\n\nconst (\n\tWeaveDomain = \"weave.local\"\n)\n\ntype watcher struct {\n\tclient *docker.Client\n\tweave  *weaveapi.Client\n\tdriver *driver\n}\n\ntype Watcher interface {\n}\n\nfunc NewWatcher(client *docker.Client, weave *weaveapi.Client, driver *driver) (Watcher, error) {\n\tw := &watcher{client: client, weave: weave, driver: driver}\n\treturn w, client.AddObserver(w)\n}\n\nfunc (w *watcher) ContainerStarted(id string) {\n\tw.driver.debug(\"ContainerStarted\", \"%s\", id)\n\tinfo, err := w.client.InspectContainer(id)\n\tif err != nil {\n\t\tw.driver.warn(\"ContainerStarted\", \"error inspecting container %s: %s\", id, err)\n\t\treturn\n\t}\n\t\/\/ check that it's on our network\n\tfor _, net := range info.NetworkSettings.Networks {\n\t\tnetwork, err := w.driver.findNetworkInfo(net.NetworkID)\n\t\tif err != nil {\n\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to find network %s info: %s\", net.NetworkID, err)\n\t\t\tcontinue\n\t\t}\n\t\tif network.isOurs {\n\t\t\tfqdn := fmt.Sprintf(\"%s.%s\", info.Config.Hostname, info.Config.Domainname)\n\t\t\tif err := w.weave.RegisterWithDNS(id, fqdn, net.IPAddress); err != nil {\n\t\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to register %s with weaveDNS: %s\", id, err)\n\t\t\t}\n\t\t\tif _, err := weavenet.WithNetNSByPid(info.State.Pid, \"configure-arp\", weavenet.VethName); err != nil {\n\t\t\t\tw.driver.warn(\"ContainerStarted\", \"unable to configure interfaces: %s\", err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (w *watcher) ContainerDied(id string) {\n\t\/\/ don't need to do this as WeaveDNS removes names on container died anyway\n\t\/\/ (note by the time we get this event we can't see the EndpointID)\n}\n\nfunc (w *watcher) ContainerDestroyed(id string) {}\n","lang_cluster":"Go","length":60,"code_uid":"bf131cb631834d90813896152abb8032"}
{"diff_hunk":"@@ -59,10 +59,11 @@ func (client *clientRest) NodeRegister(proposal dto_discovery.ServiceProposal) (\n \treturn\n }\n \n-func (client *clientRest) NodeSendStats(nodeKey string, sessionList []dto.SessionStatsDeprecated) (err error) {\n+func (client *clientRest) NodeSendStats(nodeKey string) (err error) {\n \tresponse, err := client.doPostRequest(\"node_send_stats\", dto.NodeStatsRequest{\n-\t\tNodeKey:  nodeKey,\n-\t\tSessions: sessionList,\n+\t\tNodeKey: nodeKey,\n+\t\t\/\/ TODO: remove this struct in favor of `SessionStats`\n+\t\tSessions: []dto.SessionStats{},\n \t})\n \tif err == nil {\n \t\tdefer response.Body.Close()","old_code":"package server\n\nimport (\n\t\"bytes\"\n\t\"encoding\/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\/ioutil\"\n\t\"net\/http\"\n\n\tlog \"github.com\/cihub\/seelog\"\n\t\"github.com\/mysterium\/node\/identity\"\n\t\"github.com\/mysterium\/node\/server\/dto\"\n\tdto_discovery \"github.com\/mysterium\/node\/service_discovery\/dto\"\n\t\"net\/url\"\n)\n\nvar mysteriumApiUrl string\n\nconst MYSTERIUM_API_CLIENT = \"goclient-v0.1\"\nconst MYSTERIUM_API_LOG_PREFIX = \"[Mysterium.api] \"\n\nfunc NewClient() Client {\n\thttpClient := http.Client{\n\t\tTransport: &http.Transport{},\n\t}\n\treturn &clientRest{\n\t\thttpClient: httpClient,\n\t}\n}\n\ntype clientRest struct {\n\thttpClient http.Client\n}\n\nfunc (client *clientRest) RegisterIdentity(identity identity.Identity) (err error) {\n\tresponse, err := client.doPostRequest(\"identities\", dto.CreateIdentityRequest{\n\t\tIdentity: identity.Address,\n\t})\n\n\tif err == nil {\n\t\tdefer response.Body.Close()\n\t\tlog.Info(MYSTERIUM_API_LOG_PREFIX, \"Identity registered: \", identity)\n\t}\n\n\treturn\n}\n\nfunc (client *clientRest) NodeRegister(proposal dto_discovery.ServiceProposal) (err error) {\n\tresponse, err := client.doPostRequest(\"node_register\", dto.NodeRegisterRequest{\n\t\tServiceProposal: proposal,\n\t})\n\n\tif err == nil {\n\t\tdefer response.Body.Close()\n\t\tlog.Info(MYSTERIUM_API_LOG_PREFIX, \"Node registered: \", proposal.ProviderId)\n\t}\n\n\treturn\n}\n\nfunc (client *clientRest) NodeSendStats(nodeKey string, sessionList []dto.SessionStatsDeprecated) (err error) {\n\tresponse, err := client.doPostRequest(\"node_send_stats\", dto.NodeStatsRequest{\n\t\tNodeKey:  nodeKey,\n\t\tSessions: sessionList,\n\t})\n\tif err == nil {\n\t\tdefer response.Body.Close()\n\t\tlog.Info(MYSTERIUM_API_LOG_PREFIX, \"Node stats sent: \", nodeKey)\n\t}\n\n\treturn nil\n}\n\nfunc (client *clientRest) FindProposals(nodeKey string) (proposals []dto_discovery.ServiceProposal, err error) {\n\tvalues := url.Values{}\n\tvalues.Set(\"node_key\", nodeKey)\n\tresponse, err := client.doGetRequest(\"proposals\", values)\n\n\tif err != nil {\n\t\treturn\n\t}\n\n\tdefer response.Body.Close()\n\n\tvar proposalsResponse dto.ProposalsResponse\n\terr = parseResponseJson(response, &proposalsResponse)\n\tif err != nil {\n\t\treturn\n\t}\n\tproposals = proposalsResponse.Proposals\n\n\tlog.Info(MYSTERIUM_API_LOG_PREFIX, \"FindProposals fetched: \", proposals)\n\n\treturn\n}\n\nfunc (client *clientRest) SendSessionStats(sessionId string, sessionStats dto.SessionStats) (err error) {\n\tpath := fmt.Sprintf(\"sessions\/%s\/stats\", sessionId)\n\tresponse, err := client.doPostRequest(path, sessionStats)\n\tif err == nil {\n\t\tdefer response.Body.Close()\n\t\tlog.Info(MYSTERIUM_API_LOG_PREFIX, \"Session stats sent: \", sessionId)\n\t}\n\n\treturn nil\n}\n\nfunc (client *clientRest) doGetRequest(path string, values url.Values) (*http.Response, error) {\n\tfullPath := fmt.Sprintf(\"%v\/%v?%v\", mysteriumApiUrl, path, values.Encode())\n\treturn client.executeRequest(\"GET\", fullPath, nil)\n}\n\nfunc (client *clientRest) doPostRequest(path string, payload interface{}) (*http.Response, error) {\n\treturn client.doPayloadRequest(\"POST\", path, payload)\n}\n\nfunc (client *clientRest) doPayloadRequest(method, path string, payload interface{}) (*http.Response, error) {\n\tpayloadJson, err := json.Marshal(payload)\n\tif err != nil {\n\t\tlog.Critical(MYSTERIUM_API_LOG_PREFIX, err)\n\t\treturn nil, err\n\t}\n\n\treturn client.executeRequest(method, mysteriumApiUrl+\"\/\"+path, payloadJson)\n}\n\nfunc (client *clientRest) executeRequest(method, fullPath string, payloadJson []byte) (*http.Response, error) {\n\trequest, err := http.NewRequest(method, fullPath, bytes.NewBuffer(payloadJson))\n\trequest.Header.Set(\"User-Agent\", MYSTERIUM_API_CLIENT)\n\trequest.Header.Set(\"Content-Type\", \"application\/json\")\n\trequest.Header.Set(\"Accept\", \"application\/json\")\n\tif err != nil {\n\t\tlog.Critical(MYSTERIUM_API_LOG_PREFIX, err)\n\t\treturn nil, err\n\t}\n\n\tresponse, err := client.httpClient.Do(request)\n\n\tif err != nil {\n\t\tlog.Error(MYSTERIUM_API_LOG_PREFIX, err)\n\t\treturn response, err\n\t}\n\n\terr = parseResponseError(response)\n\tif err != nil {\n\t\tlog.Error(MYSTERIUM_API_LOG_PREFIX, err)\n\t\treturn response, err\n\t}\n\n\treturn response, nil\n}\n\nfunc parseResponseJson(response *http.Response, dto interface{}) error {\n\tresponseJson, err := ioutil.ReadAll(response.Body)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = json.Unmarshal(responseJson, dto)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc parseResponseError(response *http.Response) error {\n\tif response.StatusCode < 200 || response.StatusCode >= 300 {\n\t\treturn errors.New(fmt.Sprintf(\"Server response invalid: %s (%s)\", response.Status, response.Request.URL))\n\t}\n\n\treturn nil\n}\n","lang_cluster":"Go","length":174,"code_uid":"eb2ea421f3c74bdab33e9d863a6c66fe"}
{"diff_hunk":"@@ -3,28 +3,35 @@ package ipam\n import (\n \t\"fmt\"\n \n+\t\"github.com\/weaveworks\/weave\/common\"\n \t\"github.com\/weaveworks\/weave\/ipam\/address\"\n \t\"github.com\/weaveworks\/weave\/router\"\n )\n \n type claim struct {\n-\tresultChan       chan<- error\n-\tident            string\n-\taddr             address.Address\n-\thasBeenCancelled func() bool\n+\tresultChan chan<- error\n+\tident      string\n+\taddr       address.Address\n }\n \n-\/\/ Try returns true for success (or failure), false if we need to try again later\n-func (c *claim) Try(alloc *Allocator) bool {\n-\tif (c.hasBeenCancelled)() {\n-\t\tc.Cancel()\n-\t\treturn true\n+func (c *claim) sendResult(result error) {\n+\t\/\/ Make sure we only send a result once, since listener stops listening after that\n+\tif c.resultChan != nil {\n+\t\tc.resultChan <- result\n+\t\tclose(c.resultChan)\n+\t\tc.resultChan = nil\n+\t}\n+\tif result != nil {\n+\t\tcommon.Error.Println(\"[allocator] \" + result.Error())\n \t}\n+}\n \n+\/\/ Try returns true for success (or failure), false if we need to try again later\n+func (c *claim) Try(alloc *Allocator) bool {\n \tif !alloc.ring.Contains(c.addr) {\n \t\t\/\/ Address not within our universe; assume user knows what they are doing\n \t\talloc.infof(\"Ignored address %s claimed by %s - not in our universe\\n\", c.addr, c.ident)\n-\t\tc.resultChan <- nil\n+\t\tc.sendResult(nil)\n \t\treturn true\n \t}\n ","old_code":"package ipam\n\nimport (\n\t\"fmt\"\n\n\t\"github.com\/weaveworks\/weave\/ipam\/address\"\n\t\"github.com\/weaveworks\/weave\/router\"\n)\n\ntype claim struct {\n\tresultChan       chan<- error\n\tident            string\n\taddr             address.Address\n\thasBeenCancelled func() bool\n}\n\n\/\/ Try returns true for success (or failure), false if we need to try again later\nfunc (c *claim) Try(alloc *Allocator) bool {\n\tif (c.hasBeenCancelled)() {\n\t\tc.Cancel()\n\t\treturn true\n\t}\n\n\tif !alloc.ring.Contains(c.addr) {\n\t\t\/\/ Address not within our universe; assume user knows what they are doing\n\t\talloc.infof(\"Ignored address %s claimed by %s - not in our universe\\n\", c.addr, c.ident)\n\t\tc.resultChan <- nil\n\t\treturn true\n\t}\n\n\t\/\/ If our ring doesn't know, it must be empty.  We will have initiated the\n\t\/\/ bootstrap of the ring, so wait until we find some owner for this\n\t\/\/ range (might be us).\n\towner := alloc.ring.Owner(c.addr)\n\tif owner == router.UnknownPeerName {\n\t\talloc.infof(\"Ring is empty; will try later.\\n\", c.addr, owner)\n\t\treturn false\n\t}\n\tif owner != alloc.ourName {\n\t\tname, found := alloc.nicknames[owner]\n\t\tif found {\n\t\t\tname = \" (\" + name + \")\"\n\t\t}\n\t\tc.resultChan <- fmt.Errorf(\"address %s is owned by other peer %s%s\", c.addr.String(), owner, name)\n\t\treturn true\n\t}\n\t\/\/ We are the owner, check we haven't given it to another container\n\texistingIdent := alloc.findOwner(c.addr)\n\tif existingIdent == c.ident {\n\t\t\/\/ same identifier is claiming same address; that's OK\n\t\tc.resultChan <- nil\n\t\treturn true\n\t}\n\tif existingIdent == \"\" {\n\t\terr := alloc.space.Claim(c.addr)\n\t\tif err != nil {\n\t\t\tc.resultChan <- err\n\t\t\treturn true\n\t\t}\n\t\talloc.debugln(\"Claimed\", c.addr, \"for\", c.ident)\n\t\talloc.addOwned(c.ident, c.addr)\n\t\tc.resultChan <- nil\n\t\treturn true\n\t}\n\t\/\/ Addr already owned by container on this machine\n\tc.resultChan <- fmt.Errorf(\"address %s is already owned by %s\", c.addr.String(), existingIdent)\n\treturn true\n}\n\nfunc (c *claim) Cancel() {\n\tc.resultChan <- fmt.Errorf(\"Operation cancelled.\")\n}\n\nfunc (c *claim) String() string {\n\treturn fmt.Sprintf(\"Claim %s -> %s\", c.ident, c.addr.String())\n}\n\nfunc (c *claim) ForContainer(ident string) bool {\n\treturn c.ident == ident\n}\n","lang_cluster":"Go","length":80,"code_uid":"a9f024f2d18e4527bec96a580c05e037"}
{"diff_hunk":"@@ -73,7 +73,7 @@ type GcpChaosSpec struct {\n \t\/\/ The device name of the disk to detach.\n \t\/\/ Needed in disk-loss.\n \t\/\/ +optional\n-\tDeviceName *string `json:\"deviceName,omitempty\"`\n+\tDeviceName *[]string `json:\"deviceName,omitempty\"`\n }\n \n \/\/ GcpChaosStatus represents the status of a GcpChaos","old_code":"\/\/ Copyright 2021 Chaos Mesh Authors.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage v1alpha1\n\nimport (\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n)\n\n\/\/ +kubebuilder:object:root=true\n\/\/ +chaos-mesh:base\n\n\/\/ GcpChaos is the Schema for the gcpchaos API\ntype GcpChaos struct {\n\tmetav1.TypeMeta   `json:\",inline\"`\n\tmetav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n\tSpec   GcpChaosSpec   `json:\"spec\"`\n\tStatus GcpChaosStatus `json:\"status,omitempty\"`\n}\n\n\/\/ GcpChaosAction represents the chaos action about gcp.\ntype GcpChaosAction string\n\nconst (\n\t\/\/ NodeStop represents the chaos action of stopping the node.\n\tNodeStop GcpChaosAction = \"node-stop\"\n\t\/\/ NodeReset represents the chaos action of resetting the node.\n\tNodeReset GcpChaosAction = \"node-reset\"\n\t\/\/ DiskLoss represents the chaos action of detaching the disk.\n\tDiskLoss GcpChaosAction = \"disk-loss\"\n)\n\n\/\/ GcpChaosSpec is the content of the specification for a GcpChaos\ntype GcpChaosSpec struct {\n\t\/\/ Action defines the specific gcp chaos action.\n\t\/\/ Supported action: node-stop \/ node-reset \/ disk-loss\n\t\/\/ Default action: node-stop\n\t\/\/ +kubebuilder:validation:Enum=node-stop;node-reset;disk-loss\n\tAction GcpChaosAction `json:\"action\"`\n\n\t\/\/ Duration represents the duration of the chaos action.\n\t\/\/ +optional\n\tDuration *string `json:\"duration,omitempty\"`\n\n\t\/\/ Scheduler defines some schedule rules to control the running time of the chaos experiment about time.\n\t\/\/ +optional\n\tScheduler *SchedulerSpec `json:\"scheduler,omitempty\"`\n\n\t\/\/ SecretName defines the name of kubernetes secret. It is used for GCP credentials.\n\t\/\/ +optional\n\tSecretName *string `json:\"secretName,omitempty\"`\n\n\t\/\/ Project defines the name of gcp project.\n\tProject string `json:\"project\"`\n\n\t\/\/ Zone defines the zone of gcp project.\n\tZone string `json:\"zone\"`\n\n\t\/\/ Instance defines the name of the instance\n\tInstance string `json:\"instance\"`\n\n\t\/\/ The device name of the disk to detach.\n\t\/\/ Needed in disk-loss.\n\t\/\/ +optional\n\tDeviceName *string `json:\"deviceName,omitempty\"`\n}\n\n\/\/ GcpChaosStatus represents the status of a GcpChaos\ntype GcpChaosStatus struct {\n\tChaosStatus `json:\",inline\"`\n\n\t\/\/ The attached disk info string.\n\t\/\/ Needed in disk-loss.\n\tAttachedDiskString string `json:\"attachedDiskString,omitempty\"`\n}\n","lang_cluster":"Go","length":86,"code_uid":"ac9ca2af53dd4b3687ae97549d6406e5"}
{"diff_hunk":"@@ -155,11 +155,11 @@ func (r *DeliveryReporter) reportEventProcessingTime(ctx context.Context, end ti\n }\n \n func filterTypeValue(v string) string {\n-\tif v != \"\" {\n-\t\treturn v\n+\tif v == \"\" {\n+\t\t\/\/ the default value if the filter attributes are empty.\n+\t\treturn \"any\"\n \t}\n-\t\/\/ the default value if the filter attributes are empty.\n-\treturn \"any\"\n+\treturn EventTypeMetricValue(v)\n }\n \n func (r *DeliveryReporter) AddTags(ctx context.Context) (context.Context, error) {","old_code":"\/*\nCopyright 2020 Google LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage metrics\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com\/google\/knative-gcp\/pkg\/broker\/config\"\n\t\"go.opencensus.io\/stats\"\n\t\"go.opencensus.io\/stats\/view\"\n\t\"go.opencensus.io\/tag\"\n\t\"knative.dev\/pkg\/metrics\"\n)\n\ntype DeliveryMetricsKey int\n\nconst (\n\tstartDeliveryProcessingTime DeliveryMetricsKey = iota\n)\n\ntype DeliveryReporter struct {\n\tpodName               PodName\n\tcontainerName         ContainerName\n\tdispatchTimeInMsecM   *stats.Float64Measure\n\tprocessingTimeInMsecM *stats.Float64Measure\n}\n\nfunc (r *DeliveryReporter) register() error {\n\treturn metrics.RegisterResourceView(\n\t\t&view.View{\n\t\t\tName:        \"event_count\",\n\t\t\tDescription: \"Number of events delivered to a Trigger subscriber\",\n\t\t\tMeasure:     r.dispatchTimeInMsecM,\n\t\t\tAggregation: view.Count(),\n\t\t\tTagKeys: []tag.Key{\n\t\t\t\tNamespaceNameKey,\n\t\t\t\tBrokerNameKey,\n\t\t\t\tTriggerNameKey,\n\t\t\t\tTriggerFilterTypeKey,\n\t\t\t\tResponseCodeKey,\n\t\t\t\tResponseCodeClassKey,\n\t\t\t\tPodNameKey,\n\t\t\t\tContainerNameKey,\n\t\t\t},\n\t\t},\n\t\t&view.View{\n\t\t\tName:        r.dispatchTimeInMsecM.Name(),\n\t\t\tDescription: r.dispatchTimeInMsecM.Description(),\n\t\t\tMeasure:     r.dispatchTimeInMsecM,\n\t\t\tAggregation: view.Distribution(metrics.Buckets125(1, 10000)...), \/\/ 1, 2, 5, 10, 20, 50, 100, 1000, 5000, 10000\n\t\t\tTagKeys: []tag.Key{\n\t\t\t\tNamespaceNameKey,\n\t\t\t\tBrokerNameKey,\n\t\t\t\tTriggerNameKey,\n\t\t\t\tTriggerFilterTypeKey,\n\t\t\t\tResponseCodeKey,\n\t\t\t\tResponseCodeClassKey,\n\t\t\t\tPodNameKey,\n\t\t\t\tContainerNameKey,\n\t\t\t},\n\t\t},\n\t\t&view.View{\n\t\t\tName:        r.processingTimeInMsecM.Name(),\n\t\t\tDescription: r.processingTimeInMsecM.Description(),\n\t\t\tMeasure:     r.processingTimeInMsecM,\n\t\t\tAggregation: view.Distribution(metrics.Buckets125(1, 10000)...), \/\/ 1, 2, 5, 10, 20, 50, 100, 1000, 5000, 10000\n\t\t\tTagKeys: []tag.Key{\n\t\t\t\tNamespaceNameKey,\n\t\t\t\tBrokerNameKey,\n\t\t\t\tTriggerNameKey,\n\t\t\t\tTriggerFilterTypeKey,\n\t\t\t\tPodNameKey,\n\t\t\t\tContainerNameKey,\n\t\t\t},\n\t\t},\n\t)\n}\n\n\/\/ NewDeliveryReporter creates a new DeliveryReporter.\nfunc NewDeliveryReporter(podName PodName, containerName ContainerName) (*DeliveryReporter, error) {\n\tr := &DeliveryReporter{\n\t\tpodName:       podName,\n\t\tcontainerName: containerName,\n\t\t\/\/ dispatchTimeInMsecM records the time spent dispatching an event to\n\t\t\/\/ a Trigger subscriber, in milliseconds.\n\t\tdispatchTimeInMsecM: stats.Float64(\n\t\t\t\"event_dispatch_latencies\",\n\t\t\t\"The time spent dispatching an event to a Trigger subscriber\",\n\t\t\tstats.UnitMilliseconds,\n\t\t),\n\t\t\/\/ processingTimeInMsecM records the time spent between arrival at the Broker\n\t\t\/\/ and the delivery to the Trigger subscriber.\n\t\tprocessingTimeInMsecM: stats.Float64(\n\t\t\t\"event_processing_latencies\",\n\t\t\t\"The time spent processing an event before it is dispatched to a Trigger subscriber\",\n\t\t\tstats.UnitMilliseconds,\n\t\t),\n\t}\n\n\tif err := r.register(); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to register delivery stats: %w\", err)\n\t}\n\treturn r, nil\n}\n\n\/\/ ReportEventDispatchTime captures dispatch times.\nfunc (r *DeliveryReporter) ReportEventDispatchTime(ctx context.Context, d time.Duration, responseCode int) {\n\t\/\/ convert time.Duration in nanoseconds to milliseconds.\n\tmetrics.Record(ctx, r.dispatchTimeInMsecM.M(float64(d\/time.Millisecond)),\n\t\tstats.WithTags(\n\t\t\ttag.Insert(ResponseCodeKey, strconv.Itoa(responseCode)),\n\t\t\ttag.Insert(ResponseCodeClassKey, metrics.ResponseCodeClass(responseCode)),\n\t\t),\n\t)\n}\n\n\/\/ StartEventProcessing records the start of event processing for delivery within the given context.\nfunc StartEventProcessing(ctx context.Context) context.Context {\n\treturn context.WithValue(ctx, startDeliveryProcessingTime, time.Now())\n}\n\n\/\/ FinishEventProcessing captures event processing times. Requires StartDelivery to have been\n\/\/ called previously using ctx.\nfunc (r *DeliveryReporter) FinishEventProcessing(ctx context.Context) error {\n\treturn r.reportEventProcessingTime(ctx, time.Now())\n}\n\n\/\/ ReportEventProcessingTime captures event processing times. Requires StartDelivery to have been\n\/\/ called previously using ctx.\nfunc (r *DeliveryReporter) reportEventProcessingTime(ctx context.Context, end time.Time) error {\n\tstart, err := getStartDeliveryProcessingTime(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\/\/ convert time.Duration in nanoseconds to milliseconds.\n\tmetrics.Record(ctx, r.processingTimeInMsecM.M(float64(end.Sub(start)\/time.Millisecond)))\n\treturn nil\n}\n\nfunc filterTypeValue(v string) string {\n\tif v != \"\" {\n\t\treturn v\n\t}\n\t\/\/ the default value if the filter attributes are empty.\n\treturn \"any\"\n}\n\nfunc (r *DeliveryReporter) AddTags(ctx context.Context) (context.Context, error) {\n\treturn tag.New(ctx,\n\t\ttag.Insert(PodNameKey, string(r.podName)),\n\t\ttag.Insert(ContainerNameKey, string(r.containerName)),\n\t)\n}\n\nfunc AddTargetTags(ctx context.Context, target *config.Target) (context.Context, error) {\n\treturn tag.New(ctx,\n\t\ttag.Insert(NamespaceNameKey, target.Namespace),\n\t\ttag.Insert(BrokerNameKey, target.Broker),\n\t\ttag.Insert(TriggerNameKey, target.Name),\n\t\ttag.Insert(TriggerFilterTypeKey, filterTypeValue(target.FilterAttributes[\"type\"])),\n\t)\n}\n\nfunc getStartDeliveryProcessingTime(ctx context.Context) (time.Time, error) {\n\tv := ctx.Value(startDeliveryProcessingTime)\n\tif time, ok := v.(time.Time); ok {\n\t\treturn time, nil\n\t}\n\treturn time.Time{}, fmt.Errorf(\"missing or invalid start time: %v\", v)\n}\n","lang_cluster":"Go","length":187,"code_uid":"eb66f587ecd7480d90a7d1710961190c"}
{"diff_hunk":"@@ -50,6 +50,7 @@ func deps() {\n \t\t\"go get -u github.com\/alecthomas\/gometalinter\",\n \t\t\"gometalinter --install\",\n \t\t\"go get -u github.com\/stretchr\/testify\",\n+\t\t\"go get -u github.com\/xeipuuv\/gojsonschema\",\n \t}\n \n \tfor _, name := range list {","old_code":"package main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"os\/exec\"\n\t\"runtime\"\n\t\"strings\"\n)\n\nvar lineBreak = \"\\n\"\n\nfunc init() {\n\tlog.SetFlags(0)\n\tif runtime.GOOS == \"windows\" {\n\t\tlineBreak = \"\\r\\n\"\n\t}\n}\n\n\/\/ run executes a given command on the shell, like\n\/\/ `run(\"git status\")`\nfunc run(name string) string {\n\targs := strings.Split(name, \" \")\n\treturn runParts(args...)\n}\n\nfunc runParts(args ...string) string {\n\tname := strings.Join(args, \" \")\n\tcmd := exec.Command(args[0], args[1:]...) \/\/ #nosec\n\tlog.Println(name)\n\tout, err := cmd.CombinedOutput()\n\tif err != nil {\n\t\tlog.Printf(\"%s\", out)\n\t\tlog.Fatalf(\"Command '%s' failed: %s\\n\", name, err)\n\t}\n\n\treturn strings.Trim(string(out), lineBreak)\n}\n\n\/\/ deps installs all dependencies\nfunc deps() {\n\tlog.Println(\"Installing dependencies...\")\n\n\tlist := []string{\n\t\t\"go get -u github.com\/whyrusleeping\/gx\",\n\t\t\"go get -u github.com\/whyrusleeping\/gx-go\",\n\t\t\".\/fetch_go-ipfs_deps.sh\",\n\t\t\"gx install\",\n\t\t\"go get -u github.com\/alecthomas\/gometalinter\",\n\t\t\"gometalinter --install\",\n\t\t\"go get -u github.com\/stretchr\/testify\",\n\t}\n\n\tfor _, name := range list {\n\t\tlog.Println(run(name))\n\t}\n}\n\n\/\/ lint runs linting using gometalinter\nfunc lint(packages ...string) {\n\tif len(packages) == 0 {\n\t\tpackages = []string{\".\/...\"}\n\t}\n\n\tlog.Printf(\"Linting %s ...\\n\", strings.Join(packages, \" \"))\n\n\t\/\/ Run fast linters batched together\n\tconfigs := []string{\n\t\t\"gometalinter\",\n\t\t\"--skip=sharness\",\n\t\t\"--skip=vendor\",\n\t\t\"--disable-all\",\n\t}\n\n\tfastLinters := []string{\n\t\t\"--enable=vet\",\n\t\t\"--enable=gofmt\",\n\t\t\"--enable=misspell\",\n\t\t\"--enable=goconst\",\n\t\t\"--enable=golint\",\n\t\t\"--enable=errcheck\",\n\t\t\"--min-occurrences=6\", \/\/ for goconst\n\t}\n\n\tlog.Println(runParts(append(append(configs, fastLinters...), packages...)...))\n\n\tslowLinters := []string{\n\t\t\"--deadline=10m\",\n\t\t\"--enable=unconvert\",\n\t\t\"--enable=gosimple\",\n\t\t\"--enable=megacheck\",\n\t\t\"--enable=varcheck\",\n\t\t\"--enable=structcheck\",\n\t\t\"--enable=deadcode\",\n\t}\n\n\tlog.Println(runParts(append(append(configs, slowLinters...), packages...)...))\n}\n\nfunc build() {\n\tlog.Println(\"Building...\")\n\n\tcommit := run(\"git log -n 1 --format=%H\")\n\n\tlog.Println(\n\t\trunParts(\n\t\t\t\"go\", \"build\",\n\t\t\t\"-ldflags\", fmt.Sprintf(\"-X github.com\/filecoin-project\/go-filecoin\/flags.Commit=%s\", commit),\n\t\t\t\"-v\", \"-o\", \"go-filecoin\", \".\",\n\t\t),\n\t)\n}\n\nfunc install() {\n\tlog.Println(\"Installing...\")\n\n\tlog.Println(runParts(\"go\", \"install\"))\n}\n\n\/\/ test executes tests and passes along all additional arguments to `go test`.\nfunc test(args ...string) {\n\tlog.Println(\"Testing...\")\n\n\tlog.Println(run(fmt.Sprintf(\"go test .\/... %s\", strings.Join(args, \" \"))))\n}\n\nfunc main() {\n\targs := os.Args[1:]\n\n\tif len(args) == 0 {\n\t\tlog.Fatalf(\"Missing command\")\n\t}\n\n\tcmd := args[0]\n\n\tswitch cmd {\n\tcase \"deps\":\n\t\tdeps()\n\tcase \"lint\":\n\t\tlint(args[1:]...)\n\tcase \"build\":\n\t\tbuild()\n\tcase \"test\":\n\t\ttest(args[1:]...)\n\tcase \"install\":\n\t\tinstall()\n\tcase \"best\":\n\t\tbuild()\n\t\ttest(args[1:]...)\n\tcase \"all\":\n\t\tdeps()\n\t\tlint()\n\t\tbuild()\n\t\ttest(args[1:]...)\n\tdefault:\n\t\tlog.Fatalf(\"Unknown command: %s\\n\", cmd)\n\t}\n}\n","lang_cluster":"Go","length":159,"code_uid":"eec889c75a0d4cc6b16b42c94a2fc79f"}
{"diff_hunk":"@@ -22,39 +22,6 @@ const (\n \tDeviceTwinModuleName = \"twin\"\n )\n \n-\/\/ ormerMock is mocked Ormer implementation.\n-var ormerMock *beego.MockOrmer\n-\n-\/\/ querySeterMock is mocked QuerySeter implementation.\n-var querySeterMock *beego.MockQuerySeter\n-\n-\/\/ fakeModule is mocked implementation of TestModule.\n-var fakeModule *beehive.MockModule\n-\n-\/\/ mainContext is beehive context used for communication between modules.\n-var mainContext *context.Context\n-\n-\/\/test is for sending test messages from devicetwin module.\n-var test model.Message\n-\n-\/\/ initMocks is function to initialize mocks.\n-func initMocks(t *testing.T) {\n-\tmockCtrl := gomock.NewController(t)\n-\tdefer mockCtrl.Finish()\n-\tormerMock = beego.NewMockOrmer(mockCtrl)\n-\tquerySeterMock = beego.NewMockQuerySeter(mockCtrl)\n-\tfakeModule = beehive.NewMockModule(mockCtrl)\n-\tdbm.DBAccess = ormerMock\n-}\n-\n-\/\/ registerFakeModule is fuction to register all mock modules used.\n-func registerFakeModule() {\n-\tfakeModule.EXPECT().Name().Return(TestModule).Times(3)\n-\tcore.Register(fakeModule)\n-\tmainContext = context.GetContext(context.MsgCtxTypeChannel)\n-\tmainContext.AddModule(TestModule)\n-}\n-\n \/\/ TestName is function to test Name().\n func TestName(t *testing.T) {\n \ttests := []struct {","old_code":"package devicetwin\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com\/golang\/mock\/gomock\"\n\n\t\"github.com\/kubeedge\/beehive\/pkg\/core\"\n\t\"github.com\/kubeedge\/beehive\/pkg\/core\/context\"\n\t\"github.com\/kubeedge\/beehive\/pkg\/core\/model\"\n\t\"github.com\/kubeedge\/kubeedge\/edge\/mocks\/beego\"\n\t\"github.com\/kubeedge\/kubeedge\/edge\/mocks\/beehive\"\n\t\"github.com\/kubeedge\/kubeedge\/edge\/pkg\/common\/dbm\"\n\t\"github.com\/kubeedge\/kubeedge\/edge\/pkg\/devicetwin\/dtcommon\"\n)\n\nconst (\n\t\/\/TestModule is name of test.\n\tTestModule = \"test\"\n\t\/\/DeviceTwinModuleName is name of twin\n\tDeviceTwinModuleName = \"twin\"\n)\n\n\/\/ ormerMock is mocked Ormer implementation.\nvar ormerMock *beego.MockOrmer\n\n\/\/ querySeterMock is mocked QuerySeter implementation.\nvar querySeterMock *beego.MockQuerySeter\n\n\/\/ fakeModule is mocked implementation of TestModule.\nvar fakeModule *beehive.MockModule\n\n\/\/ mainContext is beehive context used for communication between modules.\nvar mainContext *context.Context\n\n\/\/test is for sending test messages from devicetwin module.\nvar test model.Message\n\n\/\/ initMocks is function to initialize mocks.\nfunc initMocks(t *testing.T) {\n\tmockCtrl := gomock.NewController(t)\n\tdefer mockCtrl.Finish()\n\tormerMock = beego.NewMockOrmer(mockCtrl)\n\tquerySeterMock = beego.NewMockQuerySeter(mockCtrl)\n\tfakeModule = beehive.NewMockModule(mockCtrl)\n\tdbm.DBAccess = ormerMock\n}\n\n\/\/ registerFakeModule is fuction to register all mock modules used.\nfunc registerFakeModule() {\n\tfakeModule.EXPECT().Name().Return(TestModule).Times(3)\n\tcore.Register(fakeModule)\n\tmainContext = context.GetContext(context.MsgCtxTypeChannel)\n\tmainContext.AddModule(TestModule)\n}\n\n\/\/ TestName is function to test Name().\nfunc TestName(t *testing.T) {\n\ttests := []struct {\n\t\tname string\n\t\twant string\n\t}{\n\t\t{\n\t\t\tname: \"DeviceTwinNametest\",\n\t\t\twant: \"twin\",\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tdt := &DeviceTwin{}\n\t\t\tif got := dt.Name(); got != tt.want {\n\t\t\t\tt.Errorf(\"DeviceTwin.Name() = %v, want %v\", got, tt.want)\n\t\t\t}\n\t\t})\n\t}\n}\n\n\/\/ TestGroup is function to test Group().\nfunc TestGroup(t *testing.T) {\n\ttests := []struct {\n\t\tname string\n\t\twant string\n\t}{\n\t\t{\n\t\t\tname: \"DeviceTwinGroupTest\",\n\t\t\twant: \"twin\",\n\t\t},\n\t}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tdt := &DeviceTwin{}\n\t\t\tif got := dt.Group(); got != tt.want {\n\t\t\t\tt.Errorf(\"DeviceTwin.Group() = %v, want %v\", got, tt.want)\n\t\t\t}\n\t\t})\n\t}\n}\n\n\/\/ TestStart is function to test Start().\nfunc TestStart(t *testing.T) {\n\tinitMocks(t)\n\tregisterFakeModule()\n\tcore.Register(&DeviceTwin{})\n\tdt := DeviceTwin{}\n\tmainContext.AddModule(dt.Name())\n\tmainContext.AddModuleGroup(dt.Name(), dt.Group())\n\tdt.context = mainContext\n\tormerMock.EXPECT().QueryTable(gomock.Any()).Return(querySeterMock).Times(1)\n\tquerySeterMock.EXPECT().All(gomock.Any()).Return(int64(1), nil).Times(1)\n\tgo dt.Start(mainContext)\n\ttime.Sleep(1 * time.Millisecond)\n\t\/\/ Sending a message from devicetwin module to the created fake module(TestModule) to check context is initialized properly.\n\tdt.context.Send(TestModule, test)\n\t_, err := mainContext.Receive(TestModule)\n\tt.Run(\"MessagePingTest\", func(t *testing.T) {\n\t\tif err != nil {\n\t\t\tt.Errorf(\"Error while receiving message: %v\", err)\n\t\t\treturn\n\t\t}\n\t})\n\t\/\/Checking whether Mem,Twin,Device and Comm modules are registered and started successfully.\n\ttests := []struct {\n\t\tname       string\n\t\tmoduleName string\n\t}{\n\t\t{\n\t\t\tname:       \"MemModuleHealthCheck\",\n\t\t\tmoduleName: dtcommon.MemModule,\n\t\t},\n\t\t{\n\t\t\tname:       \"TwinModuleHealthCheck\",\n\t\t\tmoduleName: dtcommon.TwinModule,\n\t\t},\n\t\t{\n\t\t\tname:       \"DeviceModuleHealthCheck\",\n\t\t\tmoduleName: dtcommon.DeviceModule,\n\t\t},\n\t\t{\n\t\t\tname:       \"CommModuleHealthCheck\",\n\t\t\tmoduleName: dtcommon.CommModule,\n\t\t},\n\t}\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tmoduleCheck := false\n\t\t\tfor _, module := range dt.dtcontroller.DTModules {\n\t\t\t\tif test.moduleName == module.Name {\n\t\t\t\t\tmoduleCheck = true\n\t\t\t\t\terr := dt.dtcontroller.DTContexts.HeartBeat(test.moduleName, \"ping\")\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Errorf(\"Heartbeat of module %v is expired and dtcontroller will start it again\", test.moduleName)\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif moduleCheck == false {\n\t\t\t\tt.Errorf(\"Registration of module %v failed\", test.moduleName)\n\t\t\t}\n\t\t})\n\t}\n}\n\n\/\/ TestCleanup is function to test Cleanup().\nfunc TestCleanup(t *testing.T) {\n\tdeviceTwin := DeviceTwin{\n\t\tcontext: mainContext,\n\t\tdtcontroller: &DTController{\n\t\t\tStop: make(chan bool, 1),\n\t\t},\n\t}\n\tdeviceTwin.Cleanup()\n\t\/\/Testing the value of stop channel in dtcontroller\n\tt.Run(\"CleanUpTestStopChanTest\", func(t *testing.T) {\n\t\tif <-deviceTwin.dtcontroller.Stop == false {\n\t\t\tt.Errorf(\"Want %v on StopChan Got %v\", true, false)\n\t\t}\n\t})\n\t\/\/Send message to avoid deadlock if channel deletion has failed after cleanup\n\tgo mainContext.Send(DeviceTwinModuleName, test)\n\t_, err := mainContext.Receive(DeviceTwinModuleName)\n\tt.Run(\"CheckCleanUp\", func(t *testing.T) {\n\t\tif err == nil {\n\t\t\tt.Errorf(\"DeviceTwin Module still has channel after cleanup\")\n\t\t}\n\t})\n}\n","lang_cluster":"Go","length":187,"code_uid":"d383a1989ba64c1ab0042556201e6fa0"}
{"diff_hunk":"@@ -18,9 +18,9 @@\n package service\n \n import (\n+\t\"encoding\/json\"\n \t\"errors\"\n \n-\tlog \"github.com\/cihub\/seelog\"\n \t\"github.com\/mysteriumnetwork\/node\/communication\"\n \t\"github.com\/mysteriumnetwork\/node\/identity\"\n \tidentity_selector \"github.com\/mysteriumnetwork\/node\/identity\/selector\"","old_code":"\/*\n * Copyright (C) 2018 The \"MysteriumNetwork\/node\" Authors.\n *\n * This program is free software: you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n *\/\n\npackage service\n\nimport (\n\t\"errors\"\n\n\tlog \"github.com\/cihub\/seelog\"\n\t\"github.com\/mysteriumnetwork\/node\/communication\"\n\t\"github.com\/mysteriumnetwork\/node\/identity\"\n\tidentity_selector \"github.com\/mysteriumnetwork\/node\/identity\/selector\"\n\t\"github.com\/mysteriumnetwork\/node\/market\"\n\t\"github.com\/mysteriumnetwork\/node\/market\/proposals\/registry\"\n\t\"github.com\/mysteriumnetwork\/node\/session\"\n)\n\nconst logPrefix = \"[service-manager] \"\n\nvar (\n\t\/\/ ErrorLocation error indicates that action (i.e. disconnect)\n\tErrorLocation = errors.New(\"failed to detect service location\")\n\t\/\/ ErrUnsupportedServiceType indicates that manager tried to create an unsupported service type\n\tErrUnsupportedServiceType = errors.New(\"unsupported service type\")\n)\n\n\/\/ ServiceFactory initiates instance which is able to serve connections\ntype ServiceFactory func(Options) (Service, error)\n\n\/\/ Service interface represents pluggable Mysterium service\ntype Service interface {\n\tStart(providerID identity.Identity) (market.ServiceProposal, session.ConfigNegotiator, error)\n\tWait() error\n\tStop() error\n}\n\n\/\/ DialogWaiterFactory initiates communication channel which waits for incoming dialogs\ntype DialogWaiterFactory func(providerID identity.Identity, serviceType string) (communication.DialogWaiter, error)\n\n\/\/ DialogHandlerFactory initiates instance which is able to handle incoming dialogs\ntype DialogHandlerFactory func(market.ServiceProposal, session.ConfigNegotiator) communication.DialogHandler\n\n\/\/ NewManager creates new instance of pluggable services manager\nfunc NewManager(\n\tidentityLoader identity_selector.Handler,\n\tserviceFactory ServiceFactory,\n\tdialogWaiterFactory DialogWaiterFactory,\n\tdialogHandlerFactory DialogHandlerFactory,\n\tdiscoveryService *registry.Discovery,\n) *Manager {\n\treturn &Manager{\n\t\tidentityHandler:      identityLoader,\n\t\tserviceFactory:       serviceFactory,\n\t\tdialogWaiterFactory:  dialogWaiterFactory,\n\t\tdialogHandlerFactory: dialogHandlerFactory,\n\t\tdiscovery:            discoveryService,\n\t}\n}\n\n\/\/ Manager entrypoint which knows how to start pluggable Mysterium services\ntype Manager struct {\n\tidentityHandler identity_selector.Handler\n\n\tdialogWaiterFactory  DialogWaiterFactory\n\tdialogWaiter         communication.DialogWaiter\n\tdialogHandlerFactory DialogHandlerFactory\n\n\tserviceFactory ServiceFactory\n\tservice        Service\n\n\tdiscovery *registry.Discovery\n}\n\n\/\/ Start starts service - does not block\nfunc (manager *Manager) Start(options Options) (err error) {\n\tloadIdentity := identity_selector.NewLoader(manager.identityHandler, options.Identity, options.Passphrase)\n\tproviderID, err := loadIdentity()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tmanager.service, err = manager.serviceFactory(options)\n\tif err != nil {\n\t\treturn err\n\t}\n\tproposal, sessionConfigProvider, err := manager.service.Start(providerID)\n\tif err != nil {\n\t\treturn err\n\t}\n\tmanager.dialogWaiter, err = manager.dialogWaiterFactory(providerID, proposal.ServiceType)\n\tif err != nil {\n\t\treturn err\n\t}\n\tproviderContact, err := manager.dialogWaiter.Start()\n\tif err != nil {\n\t\treturn err\n\t}\n\tproposal.SetProviderContact(providerID, providerContact)\n\n\tdialogHandler := manager.dialogHandlerFactory(proposal, sessionConfigProvider)\n\tif err = manager.dialogWaiter.ServeDialogs(dialogHandler); err != nil {\n\t\treturn err\n\t}\n\n\tmanager.discovery.Start(providerID, proposal)\n\treturn nil\n}\n\n\/\/ Wait blocks until service is stopped\nfunc (manager *Manager) Wait() error {\n\tlog.Info(logPrefix, \"Waiting for discovery service to finish\")\n\tmanager.discovery.Wait()\n\n\tlog.Info(logPrefix, \"Waiting for service to finish\")\n\treturn manager.service.Wait()\n}\n\n\/\/ Kill stops service\nfunc (manager *Manager) Kill() error {\n\tvar errDialogWaiter, errService error\n\n\tif manager.discovery != nil {\n\t\tmanager.discovery.Stop()\n\t}\n\tif manager.dialogWaiter != nil {\n\t\terrDialogWaiter = manager.dialogWaiter.Stop()\n\t}\n\tif manager.service != nil {\n\t\terrService = manager.service.Stop()\n\t}\n\n\tif errDialogWaiter != nil {\n\t\treturn errDialogWaiter\n\t}\n\tif errService != nil {\n\t\treturn errService\n\t}\n\treturn nil\n}\n","lang_cluster":"Go","length":153,"code_uid":"6f547ef61b71498c85f4dc62b9be718b"}
{"diff_hunk":"@@ -25,6 +25,10 @@ const (\n \t\/\/ AttachmentEvent is used to define the attachment state transition events\n \t\/\/ emitted by ENI watcher\n \tAttachmentEvent\n+\n+\t\/\/ ManagedAgentEvent is used to define the managed agent state transition events\n+\t\/\/ emitted by the engine\n+\tManagedAgentEvent\n )\n \n \/\/ Event defines the type of state change event","old_code":"\/\/ Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\"). You may\n\/\/ not use this file except in compliance with the License. A copy of the\n\/\/ License is located at\n\/\/\n\/\/\thttp:\/\/aws.amazon.com\/apache2.0\/\n\/\/\n\/\/ or in the \"license\" file accompanying this file. This file is distributed\n\/\/ on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n\/\/ express or implied. See the License for the specific language governing\n\/\/ permissions and limitations under the License.\n\npackage statechange\n\nconst (\n\t\/\/ ContainerEvent is used to define the container state transition events\n\t\/\/ emitted by the engine\n\tContainerEvent EventType = iota\n\n\t\/\/ TaskEvent is used to define the task state transition events emitted by\n\t\/\/ the engine\n\tTaskEvent\n\n\t\/\/ AttachmentEvent is used to define the attachment state transition events\n\t\/\/ emitted by ENI watcher\n\tAttachmentEvent\n)\n\n\/\/ Event defines the type of state change event\ntype EventType int32\n\n\/\/ Event is used to abstract away the two transition event types\n\/\/ passed up through a single channel from the the engine\ntype Event interface {\n\n\t\/\/ GetEventType implementations should return one the enums defined above to\n\t\/\/ identify the type of event being emitted\n\tGetEventType() EventType\n}\n","lang_cluster":"Go","length":40,"code_uid":"84ff39bf771845c2905dfd8c66314a7b"}
{"diff_hunk":"@@ -1,16 +1,21 @@\n package cli\n \n import (\n-\t\"log\"\n+\tstdlog \"log\"\n \n \t\"github.com\/mitchellh\/cli\"\n \t\"github.com\/spiffe\/spire\/cmd\/spire-agent\/cli\/api\"\n \t\"github.com\/spiffe\/spire\/cmd\/spire-agent\/cli\/healthcheck\"\n \t\"github.com\/spiffe\/spire\/cmd\/spire-agent\/cli\/run\"\n+\t\"github.com\/spiffe\/spire\/pkg\/common\/log\"\n \t\"github.com\/spiffe\/spire\/pkg\/common\/version\"\n )\n \n-func Run(args []string) int {\n+type CLI struct {\n+\tLogOptions []log.Option\n+}\n+\n+func (cc *CLI) Run(args []string) int {\n \tc := cli.NewCLI(\"spire-agent\", version.Version())\n \tc.Args = args\n \tc.Commands = map[string]cli.CommandFactory{","old_code":"package cli\n\nimport (\n\t\"log\"\n\n\t\"github.com\/mitchellh\/cli\"\n\t\"github.com\/spiffe\/spire\/cmd\/spire-agent\/cli\/api\"\n\t\"github.com\/spiffe\/spire\/cmd\/spire-agent\/cli\/healthcheck\"\n\t\"github.com\/spiffe\/spire\/cmd\/spire-agent\/cli\/run\"\n\t\"github.com\/spiffe\/spire\/pkg\/common\/version\"\n)\n\nfunc Run(args []string) int {\n\tc := cli.NewCLI(\"spire-agent\", version.Version())\n\tc.Args = args\n\tc.Commands = map[string]cli.CommandFactory{\n\t\t\"api fetch\": func() (cli.Command, error) {\n\t\t\treturn api.NewFetchX509Command(), nil\n\t\t},\n\t\t\"api fetch x509\": func() (cli.Command, error) {\n\t\t\treturn api.NewFetchX509Command(), nil\n\t\t},\n\t\t\"api fetch jwt\": func() (cli.Command, error) {\n\t\t\treturn api.NewFetchJWTCommand(), nil\n\t\t},\n\t\t\"api validate jwt\": func() (cli.Command, error) {\n\t\t\treturn api.NewValidateJWTCommand(), nil\n\t\t},\n\t\t\"api watch\": func() (cli.Command, error) {\n\t\t\treturn &api.WatchCLI{}, nil\n\t\t},\n\t\t\"run\": func() (cli.Command, error) {\n\t\t\treturn &run.Command{}, nil\n\t\t},\n\t\t\"healthcheck\": func() (cli.Command, error) {\n\t\t\treturn healthcheck.NewHealthCheckCommand(), nil\n\t\t},\n\t}\n\n\texitStatus, err := c.Run()\n\tif err != nil {\n\t\tlog.Println(err)\n\t}\n\treturn exitStatus\n}\n","lang_cluster":"Go","length":45,"code_uid":"deb6524933e143049caae17c5a7ab941"}
{"diff_hunk":"@@ -182,7 +182,8 @@ func TestSerialiseBlockWitness(t *testing.T) {\n \tif err := bwb.WriteTo(&b); err != nil {\n \t\tt.Errorf(\"Could not make block witness: %v\", err)\n \t}\n-\texpected := common.FromHex(\"0xa76862616c616e6365730065636f64657300666861736865731822646b65797300666e6f6e63657300697374727563747572650b6676616c75657300582023181a62d35fe01562158be610f84e047f99f5e74d896da21682d925964ece3a0601024704010402040304\")\n+\n+\texpected := common.FromHex(\"0xa76862616c616e6365730065636f64657300666861736865731822646b65797300666e6f6e63657300697374727563747572650b6676616c756573005820858f70a4b1e6aa71a7edc574d2ca946495a038aa37ce13dc7b7ed15661a6ff2f0601024704010402040304\")\n \tif !bytes.Equal(expected, b.Bytes()) {\n \t\tt.Errorf(\"Expected %x, got: %x\", expected, b.Bytes())\n \t}","old_code":"package trie\n\nimport (\n\t\"bytes\"\n\t\"testing\"\n\n\t\"github.com\/ledgerwatch\/turbo-geth\/common\"\n)\n\nfunc TestSupplyKeyValue(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.supplyKey([]byte(\"key\")); err != nil {\n\t\tt.Errorf(\"Could not supply key: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x436b6579\"), bwb.Keys.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x436b6579 in keys tape, got: %x\", bwb.Keys.buffer.Bytes())\n\t}\n\tif err := bwb.supplyValue([]byte(\"value\")); err != nil {\n\t\tt.Errorf(\"Could not supply value: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x4576616c7565\"), bwb.Values.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x4576616c7565 in values tape, got: %x\", bwb.Values.buffer.Bytes())\n\t}\n}\n\nfunc TestSupplyHash(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.supplyHash(common.HexToHash(\"0x9583498348fc48393abc\")); err != nil {\n\t\tt.Errorf(\"Could not supply hash: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x5820000000000000000000000000000000000000000000009583498348fc48393abc\"), bwb.Hashes.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x5820000000000000000000000000000000000000000000009583498348fc48393abc in hash tape, got: %x\", bwb.Hashes.buffer.Bytes())\n\t}\n}\n\nfunc TestSupplyCode(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.supplyCode(common.FromHex(\"0x9583498348fc48393abc58bc\")); err != nil {\n\t\tt.Errorf(\"Could not supply code: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x4c9583498348fc48393abc58bc\"), bwb.Codes.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x4c9583498348fc48393abc58bc in codes tape, got: %x\", bwb.Codes.buffer.Bytes())\n\t}\n}\n\nfunc TestOpLeaf(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.leaf(56); err != nil {\n\t\tt.Errorf(\"Could not call leaf: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x001838\"), bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x001838 in structure tape, got: %x\", bwb.Structure.buffer.Bytes())\n\t}\n}\nfunc TestOpLeafHash(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.leafHash(56); err != nil {\n\t\tt.Errorf(\"Could not call leafHash: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x011838\"), bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x011838 in structure tape, got: %x\", bwb.Structure.buffer.Bytes())\n\t}\n}\n\nfunc TestOpExtension(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.extension(common.FromHex(\"0x0f05\")); err != nil {\n\t\tt.Errorf(\"Could not call extension: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x02420f05\"), bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x02420f05 in structure tape, got: %x\", bwb.Structure.buffer.Bytes())\n\t}\n}\n\nfunc TestOpExtensionHash(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.extensionHash(common.FromHex(\"0x0f05\")); err != nil {\n\t\tt.Errorf(\"Could not call extensionHash: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x03420f05\"), bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x03420f05 in structure tape, got: %x\", bwb.Structure.buffer.Bytes())\n\t}\n}\n\nfunc TestOpBranch(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.branch(1 + 4); err != nil {\n\t\tt.Errorf(\"Could not call branch: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x0405\"), bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x0405 in structure tape, got: %x\", bwb.Structure.buffer.Bytes())\n\t}\n}\n\nfunc TestOpBranchHash(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.branchHash(1 + 4); err != nil {\n\t\tt.Errorf(\"Could not call branchHash: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x0505\"), bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x0505 in structure tape, got: %x\", bwb.Structure.buffer.Bytes())\n\t}\n}\n\nfunc TestOpHash(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.hash(3); err != nil {\n\t\tt.Errorf(\"Could not call hash: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x0603\"), bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x0603 in structure tape, got: %x\", bwb.Structure.buffer.Bytes())\n\t}\n}\n\nfunc TestOpCode(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.code(); err != nil {\n\t\tt.Errorf(\"Could not call code: %v\", err)\n\t}\n\tif !bytes.Equal(common.FromHex(\"0x07\"), bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected 0x07 in structure tape, got: %x\", bwb.Structure.buffer.Bytes())\n\t}\n}\n\nfunc TestOpAccountLeaf(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.accountLeaf(56, 3); err != nil {\n\t\tt.Errorf(\"Could not call acccountLeaf: %v\", err)\n\t}\n\texpected := common.FromHex(\"0x08183803\")\n\tif !bytes.Equal(expected, bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected %x in structure tape, got: %x\", expected, bwb.Structure.buffer.Bytes())\n\t}\n}\nfunc TestOpAccountLeafHash(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.accountLeafHash(56, 3); err != nil {\n\t\tt.Errorf(\"Could not call accountLeafHash: %v\", err)\n\t}\n\texpected := common.FromHex(\"0x09183803\")\n\tif !bytes.Equal(expected, bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected %x in structure tape, got: %x\", expected, bwb.Structure.buffer.Bytes())\n\t}\n}\n\nfunc TestOpEmptyRoot(t *testing.T) {\n\tbwb := NewBlockWitnessBuilder(false)\n\tif err := bwb.emptyRoot(); err != nil {\n\t\tt.Errorf(\"Could not call emptyRoot: %v\", err)\n\t}\n\texpected := common.FromHex(\"0x0a\")\n\tif !bytes.Equal(expected, bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected %x in structure tape, got: %x\", expected, bwb.Structure.buffer.Bytes())\n\t}\n}\n\nfunc TestMakeBlockWitness(t *testing.T) {\n\ttr := New(common.Hash{})\n\ttr.Update([]byte(\"ABCD0001\"), []byte(\"val1\"), 0)\n\ttr.Update([]byte(\"ABCE0002\"), []byte(\"val2\"), 0)\n\tbwb := NewBlockWitnessBuilder(false)\n\trs := NewResolveSet(2)\n\tif err := bwb.MakeBlockWitness(tr, rs, nil, nil); err != nil {\n\t\tt.Errorf(\"Could not make block witness: %v\", err)\n\t}\n\texpected := common.FromHex(\"0x0601024704010402040304\")\n\tif !bytes.Equal(expected, bwb.Structure.buffer.Bytes()) {\n\t\tt.Errorf(\"Expected %x in structure tape, got: %x\", expected, bwb.Structure.buffer.Bytes())\n\t}\n}\n\nfunc TestSerialiseBlockWitness(t *testing.T) {\n\ttr := New(common.Hash{})\n\ttr.Update([]byte(\"ABCD0001\"), []byte(\"val1\"), 0)\n\ttr.Update([]byte(\"ABCE0002\"), []byte(\"val2\"), 0)\n\tbwb := NewBlockWitnessBuilder(false)\n\trs := NewResolveSet(2)\n\tif err := bwb.MakeBlockWitness(tr, rs, nil, nil); err != nil {\n\t\tt.Errorf(\"Could not make block witness: %v\", err)\n\t}\n\tvar b bytes.Buffer\n\tif err := bwb.WriteTo(&b); err != nil {\n\t\tt.Errorf(\"Could not make block witness: %v\", err)\n\t}\n\texpected := common.FromHex(\"0xa76862616c616e6365730065636f64657300666861736865731822646b65797300666e6f6e63657300697374727563747572650b6676616c75657300582023181a62d35fe01562158be610f84e047f99f5e74d896da21682d925964ece3a0601024704010402040304\")\n\tif !bytes.Equal(expected, b.Bytes()) {\n\t\tt.Errorf(\"Expected %x, got: %x\", expected, b.Bytes())\n\t}\n\ttr1, _, err := BlockWitnessToTrie(b.Bytes(), false)\n\tif err != nil {\n\t\tt.Errorf(\"Could not restore trie from the block witness: %v\", err)\n\t}\n\tif tr.Hash() != tr1.Hash() {\n\t\tt.Errorf(\"Reconstructed block witness has different root hash than source trie\")\n\t}\n}\n","lang_cluster":"Go","length":196,"code_uid":"7966920593004788bfc700061ef253c7"}
{"diff_hunk":"@@ -4,22 +4,17 @@ package gpu\n \n import (\n \t\"context\"\n-\t\"net\"\n \t\"os\"\n \n \t\"github.com\/docker\/docker\/api\/types\/container\"\n-\t\"github.com\/docker\/go-plugins-helpers\/volume\"\n-\tlog \"github.com\/noxiouz\/zapctx\/ctxlog\"\n \t\"github.com\/sonm-io\/core\/proto\"\n-\t\"github.com\/sshaman1101\/nvidia-docker\/nvidia\"\n-\t\"go.uber.org\/zap\"\n )\n \n type radeonTuner struct {\n \tvolumePluginHandler\n }\n \n-func newRadeonTuner(ctx context.Context, opts ...Option) (Tuner, error) {\n+func newRadeonTuner(_ context.Context, opts ...Option) (Tuner, error) {\n \toptions := radeonDefaultOptions()\n \tfor _, f := range opts {\n \t\tf(options)","old_code":"\/\/ +build !darwin,cl\n\npackage gpu\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"os\"\n\n\t\"github.com\/docker\/docker\/api\/types\/container\"\n\t\"github.com\/docker\/go-plugins-helpers\/volume\"\n\tlog \"github.com\/noxiouz\/zapctx\/ctxlog\"\n\t\"github.com\/sonm-io\/core\/proto\"\n\t\"github.com\/sshaman1101\/nvidia-docker\/nvidia\"\n\t\"go.uber.org\/zap\"\n)\n\ntype radeonTuner struct {\n\tvolumePluginHandler\n}\n\nfunc newRadeonTuner(ctx context.Context, opts ...Option) (Tuner, error) {\n\toptions := radeonDefaultOptions()\n\tfor _, f := range opts {\n\t\tf(options)\n\t}\n\n\ttun := radeonTuner{}\n\ttun.options = options\n\n\tif err := hasGPUWithVendor(sonm.GPUVendorType_RADEON); err != nil {\n\t\treturn nil, err\n\t}\n\n\ttun.devices = tun.getDevices()\n\n\tif _, err := os.Stat(openCLVendorDir); err == nil {\n\t\ttun.OpenCLVendorDir = openCLVendorDir\n\t}\n\n\tvolInfo := []nvidia.VolumeInfo{\n\t\t{\n\t\t\tName:         tun.options.VolumeDriverName,\n\t\t\tMountpoint:   \"\/opt\/amdgpu-pro\",\n\t\t\tMountOptions: \"ro\",\n\t\t\tComponents: map[string][]string{\n\t\t\t\t\"libraries\": {\n\t\t\t\t\t\"amdvlk64.so\",\n\t\t\t\t\t\"libEGL.so\",\n\t\t\t\t\t\"libGL.so\",\n\t\t\t\t\t\"libGLESv2.so\",\n\t\t\t\t\t\"libOpenCL.so\",\n\t\t\t\t\t\"libamdocl12cl64.so\",\n\t\t\t\t\t\"libamdocl64.so\",\n\t\t\t\t\t\/\/ vdpau\n\t\t\t\t\t\"libvdpau_amdgpu.so.1.0.0\",\n\t\t\t\t\t\/\/ dri\n\t\t\t\t\t\"radeonsi_drv_video.so\",\n\t\t\t\t\t\/\/ gbm\n\t\t\t\t\t\"gbm_amdgpu.so\",\n\n\t\t\t\t\t\/\/ by noxiouz from prev impl\n\t\t\t\t\t\"libMesaOpenCL.so\",\n\t\t\t\t\t\"pipe_vmwgfx.so\",\n\t\t\t\t\t\"pipe_r600.so\",\n\t\t\t\t\t\"pipe_r300.so\",\n\t\t\t\t\t\"pipe_radeonsi.so\",\n\t\t\t\t\t\"pipe_swrast.so\",\n\t\t\t\t\t\"pipe_nouveau.so\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tlog.G(ctx).Info(\"provisioning volumes\", zap.String(\"at\", tun.options.VolumePath))\n\tvolumes, err := nvidia.LookupVolumes(tun.options.VolumePath, tun.options.DriverVersion, volInfo)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttun.handler = volume.NewHandler(NewPlugin(volumes))\n\ttun.listener, err = net.Listen(\"unix\", tun.options.SocketPath)\n\tif err != nil {\n\t\tlog.G(ctx).Error(\"failed to create listening socket for to communicate with Docker as plugin\",\n\t\t\tzap.String(\"path\", tun.options.SocketPath), zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tgo func() {\n\t\ttun.handler.Serve(tun.listener)\n\t}()\n\n\treturn tun, nil\n}\n\nfunc (radeonTuner) getDevices() []string {\n\tvar dev []string\n\tif _, err := os.Stat(\"\/dev\/dri\"); err == nil {\n\t\tdev = append(dev, \"\/dev\/dri\")\n\t}\n\n\tif _, err := os.Stat(\"\/dev\/kfd\"); err == nil {\n\t\tdev = append(dev, \"\/dev\/kfd\")\n\t}\n\n\treturn dev\n}\n\nfunc (tun radeonTuner) Tune(hostconfig *container.HostConfig) error {\n\treturn tun.tune(hostconfig)\n}\n\nfunc (tun radeonTuner) Close() error {\n\tif err := tun.listener.Close(); err != nil {\n\t\treturn err\n\t}\n\n\treturn os.Remove(tun.options.SocketPath)\n}\n","lang_cluster":"Go","length":119,"code_uid":"13a7756247374d679ab8f285b41b4881"}
{"diff_hunk":"@@ -76,10 +76,10 @@ func buildBinary(source, target string) error {\n \tif !ok {\n \t\ttargetArch = runtime.GOARCH\n \t}\n-\treturn buildBinaryFor(source, target, targetOS, targetArch)\n+\treturn buildBinaryFor(source, target, targetOS, targetArch, false)\n }\n \n-func buildBinaryFor(source, target, targetOS, targetArch string) error {\n+func buildBinaryFor(source, target, targetOS, targetArch string, buildStatic bool) error {\n \tlog.Info().Msgf(\"Building %s -> %s %s\/%s\", source, target, targetOS, targetArch)\n \n \tbuildDir, err := filepath.Abs(path.Join(\"build\", target))","old_code":"\/*\n * Copyright (C) 2020 The \"MysteriumNetwork\/node\" Authors.\n *\n * This program is free software: you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n *\/\n\npackage packages\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"path\"\n\t\"path\/filepath\"\n\t\"runtime\"\n\t\"strings\"\n\n\t\"github.com\/magefile\/mage\/sh\"\n\t\"github.com\/mysteriumnetwork\/go-ci\/env\"\n\t\"github.com\/mysteriumnetwork\/node\/logconfig\"\n\t\"github.com\/mysteriumnetwork\/node\/utils\/fileutil\"\n\t\"github.com\/rs\/zerolog\/log\"\n)\n\n\/\/ Build builds the project. Like go tool, it supports cross-platform build with env vars: GOOS, GOARCH.\nfunc Build() error {\n\tlogconfig.Bootstrap()\n\tif err := buildBinary(path.Join(\"cmd\", \"mysterium_node\", \"mysterium_node.go\"), \"myst\"); err != nil {\n\t\treturn err\n\t}\n\tif err := copyConfig(\"myst\"); err != nil {\n\t\treturn err\n\t}\n\tif err := buildBinary(path.Join(\"cmd\", \"supervisor\", \"supervisor.go\"), \"myst_supervisor\"); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc linkerFlags() (flags []string) {\n\tif env.Str(env.BuildBranch) != \"\" {\n\t\tflags = append(flags, \"-X\", fmt.Sprintf(\"'github.com\/mysteriumnetwork\/node\/metadata.BuildBranch=%s'\", env.Str(env.BuildBranch)))\n\t}\n\tif env.Str(\"BUILD_COMMIT\") != \"\" {\n\t\tflags = append(flags, \"-X\", fmt.Sprintf(\"'github.com\/mysteriumnetwork\/node\/metadata.BuildCommit=%s'\", env.Str(\"BUILD_COMMIT\")))\n\t}\n\tif env.Str(env.BuildNumber) != \"\" {\n\t\tflags = append(flags, \"-X\", fmt.Sprintf(\"'github.com\/mysteriumnetwork\/node\/metadata.BuildNumber=%s'\", env.Str(env.BuildNumber)))\n\t}\n\tif env.Str(env.BuildVersion) != \"\" {\n\t\tflags = append(flags, \"-X\", fmt.Sprintf(\"'github.com\/mysteriumnetwork\/node\/metadata.Version=%s'\", env.Str(env.BuildVersion)))\n\t}\n\treturn flags\n}\n\nfunc buildCrossBinary(os, arch string) error {\n\treturn sh.Run(\"bin\/build_xgo\", os+\"\/\"+arch)\n}\n\nfunc buildBinary(source, target string) error {\n\ttargetOS, ok := os.LookupEnv(\"GOOS\")\n\tif !ok {\n\t\ttargetOS = runtime.GOOS\n\t}\n\ttargetArch, ok := os.LookupEnv(\"GOARCH\")\n\tif !ok {\n\t\ttargetArch = runtime.GOARCH\n\t}\n\treturn buildBinaryFor(source, target, targetOS, targetArch)\n}\n\nfunc buildBinaryFor(source, target, targetOS, targetArch string) error {\n\tlog.Info().Msgf(\"Building %s -> %s %s\/%s\", source, target, targetOS, targetArch)\n\n\tbuildDir, err := filepath.Abs(path.Join(\"build\", target))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar flags = []string{\"build\"}\n\tif env.Bool(\"FLAG_RACE\") {\n\t\tflags = append(flags, \"-race\")\n\t}\n\n\tldFlags := linkerFlags()\n\tflags = append(flags, fmt.Sprintf(`-ldflags=-w -s %s`, strings.Join(ldFlags, \" \")))\n\n\tif targetOS == \"windows\" {\n\t\ttarget += \".exe\"\n\t}\n\tflags = append(flags, \"-o\", path.Join(buildDir, target), source)\n\n\tenvi := map[string]string{\n\t\t\"GOOS\":   targetOS,\n\t\t\"GOARCH\": targetArch,\n\t}\n\treturn sh.RunWith(envi, \"go\", flags...)\n}\n\nfunc copyConfig(target string) error {\n\tdest, err := filepath.Abs(path.Join(\"build\", target, \"config\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tcommon, err := filepath.Abs(path.Join(\"bin\", \"package\", \"config\", \"common\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := fileutil.CopyDirs(common, dest); err != nil {\n\t\treturn err\n\t}\n\n\ttargetOS, ok := os.LookupEnv(\"GOOS\")\n\tif !ok {\n\t\ttargetOS = runtime.GOOS\n\t}\n\tosSpecific, err := filepath.Abs(path.Join(\"bin\", \"package\", \"config\", targetOS))\n\tif err := fileutil.CopyDirs(osSpecific, dest); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n","lang_cluster":"Go","length":134,"code_uid":"c5d3a080e3184b5a919fc48b11b2b774"}
{"diff_hunk":"@@ -30,7 +30,7 @@ var DBATag = \"v0.2.0\"\n var RouterImage = \"drud\/nginx-proxy\" \/\/ Note that this is overridden by make\n \n \/\/ RouterTag defines the tag used for the router.\n-var RouterTag = \"v0.3.0\" \/\/ Note that this is overridden by make\n+var RouterTag = \"router-expose\" \/\/ Note that this is overridden by make\n \n \/\/ COMMIT is the actual committish, supplied by make\n var COMMIT = \"COMMIT should be overridden\"","old_code":"package version\n\n\/\/ VERSION is supplied with the git committish this is built from\nvar VERSION = \"\"\n\n\/\/ IMPORTANT: These versions are overridden by version ldflags specifications VERSION_VARIABLES in the Makefile\n\n\/\/ DdevVersion is the current version of ddev, by default the git committish (should be current git tag)\nvar DdevVersion = \"v0.3.0-dev\" \/\/ Note that this is overridden by make\n\n\/\/ WebImg defines the default web image used for applications.\nvar WebImg = \"drud\/nginx-php-fpm7-local\" \/\/ Note that this is overridden by make\n\n\/\/ WebTag defines the default web image tag for drud dev\nvar WebTag = \"v0.4.0\" \/\/ Note that this is overridden by make\n\n\/\/ DBImg defines the default db image used for applications.\nvar DBImg = \"drud\/mysql-docker-local-57\" \/\/ Note that this is overridden by make\n\n\/\/ DBTag defines the default db image tag for drud dev\nvar DBTag = \"v0.3.0\" \/\/ Note that this is overridden by make\n\n\/\/ DBAImg defines the default phpmyadmin image tag used for applications.\nvar DBAImg = \"drud\/phpmyadmin\"\n\n\/\/ DBATag defines the default phpmyadmin image tag used for applications.\nvar DBATag = \"v0.2.0\"\n\n\/\/ RouterImage defines the image used for the router.\nvar RouterImage = \"drud\/nginx-proxy\" \/\/ Note that this is overridden by make\n\n\/\/ RouterTag defines the tag used for the router.\nvar RouterTag = \"v0.3.0\" \/\/ Note that this is overridden by make\n\n\/\/ COMMIT is the actual committish, supplied by make\nvar COMMIT = \"COMMIT should be overridden\"\n\n\/\/ BUILDINFO is information with date and context, supplied by make\nvar BUILDINFO = \"BUILDINFO should have new info\"\n","lang_cluster":"Go","length":39,"code_uid":"53709817e934467a9d36edd6caf12ea0"}
{"diff_hunk":"@@ -49,7 +49,7 @@ func (s *Server) handleSignals() {\n \t\t\t\ts.Debugf(\"Trapped %q signal\", sig)\n \t\t\t\tswitch sig {\n \t\t\t\tcase syscall.SIGINT:\n-\t\t\t\t\ts.Noticef(\"Server Exiting..\")\n+\t\t\t\t\ts.Shutdown()\n \t\t\t\t\tos.Exit(0)\n \t\t\t\tcase syscall.SIGUSR1:\n \t\t\t\t\t\/\/ File log re-open for rotating file logs.","old_code":"\/\/ Copyright 2012-2019 The NATS Authors\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\n\/\/ +build !windows\n\npackage server\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"os\/exec\"\n\t\"os\/signal\"\n\t\"strconv\"\n\t\"strings\"\n\t\"syscall\"\n)\n\nvar processName = \"nats-server\"\n\n\/\/ SetProcessName allows to change the expected name of the process.\nfunc SetProcessName(name string) {\n\tprocessName = name\n}\n\n\/\/ Signal Handling\nfunc (s *Server) handleSignals() {\n\tif s.getOpts().NoSigs {\n\t\treturn\n\t}\n\tc := make(chan os.Signal, 1)\n\n\tsignal.Notify(c, syscall.SIGINT, syscall.SIGUSR1, syscall.SIGUSR2, syscall.SIGHUP)\n\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase sig := <-c:\n\t\t\t\ts.Debugf(\"Trapped %q signal\", sig)\n\t\t\t\tswitch sig {\n\t\t\t\tcase syscall.SIGINT:\n\t\t\t\t\ts.Noticef(\"Server Exiting..\")\n\t\t\t\t\tos.Exit(0)\n\t\t\t\tcase syscall.SIGUSR1:\n\t\t\t\t\t\/\/ File log re-open for rotating file logs.\n\t\t\t\t\ts.ReOpenLogFile()\n\t\t\t\tcase syscall.SIGUSR2:\n\t\t\t\t\tgo s.lameDuckMode()\n\t\t\t\tcase syscall.SIGHUP:\n\t\t\t\t\t\/\/ Config reload.\n\t\t\t\t\tif err := s.Reload(); err != nil {\n\t\t\t\t\t\ts.Errorf(\"Failed to reload server configuration: %s\", err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase <-s.quitCh:\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n}\n\n\/\/ ProcessSignal sends the given signal command to the given process. If pidStr\n\/\/ is empty, this will send the signal to the single running instance of\n\/\/ nats-server. If multiple instances are running, it returns an error. This returns\n\/\/ an error if the given process is not running or the command is invalid.\nfunc ProcessSignal(command Command, pidStr string) error {\n\tvar pid int\n\tif pidStr == \"\" {\n\t\tpids, err := resolvePids()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif len(pids) == 0 {\n\t\t\treturn fmt.Errorf(\"no %s processes running\", processName)\n\t\t}\n\t\tif len(pids) > 1 {\n\t\t\terrStr := fmt.Sprintf(\"multiple %s processes running:\\n\", processName)\n\t\t\tprefix := \"\"\n\t\t\tfor _, p := range pids {\n\t\t\t\terrStr += fmt.Sprintf(\"%s%d\", prefix, p)\n\t\t\t\tprefix = \"\\n\"\n\t\t\t}\n\t\t\treturn errors.New(errStr)\n\t\t}\n\t\tpid = pids[0]\n\t} else {\n\t\tp, err := strconv.Atoi(pidStr)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"invalid pid: %s\", pidStr)\n\t\t}\n\t\tpid = p\n\t}\n\n\tvar err error\n\tswitch command {\n\tcase CommandStop:\n\t\terr = kill(pid, syscall.SIGKILL)\n\tcase CommandQuit:\n\t\terr = kill(pid, syscall.SIGINT)\n\tcase CommandReopen:\n\t\terr = kill(pid, syscall.SIGUSR1)\n\tcase CommandReload:\n\t\terr = kill(pid, syscall.SIGHUP)\n\tcase commandLDMode:\n\t\terr = kill(pid, syscall.SIGUSR2)\n\tdefault:\n\t\terr = fmt.Errorf(\"unknown signal %q\", command)\n\t}\n\treturn err\n}\n\n\/\/ resolvePids returns the pids for all running nats-server processes.\nfunc resolvePids() ([]int, error) {\n\t\/\/ If pgrep isn't available, this will just bail out and the user will be\n\t\/\/ required to specify a pid.\n\toutput, err := pgrep()\n\tif err != nil {\n\t\tswitch err.(type) {\n\t\tcase *exec.ExitError:\n\t\t\t\/\/ ExitError indicates non-zero exit code, meaning no processes\n\t\t\t\/\/ found.\n\t\t\tbreak\n\t\tdefault:\n\t\t\treturn nil, errors.New(\"unable to resolve pid, try providing one\")\n\t\t}\n\t}\n\tvar (\n\t\tmyPid   = os.Getpid()\n\t\tpidStrs = strings.Split(string(output), \"\\n\")\n\t\tpids    = make([]int, 0, len(pidStrs))\n\t)\n\tfor _, pidStr := range pidStrs {\n\t\tif pidStr == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tpid, err := strconv.Atoi(pidStr)\n\t\tif err != nil {\n\t\t\treturn nil, errors.New(\"unable to resolve pid, try providing one\")\n\t\t}\n\t\t\/\/ Ignore the current process.\n\t\tif pid == myPid {\n\t\t\tcontinue\n\t\t}\n\t\tpids = append(pids, pid)\n\t}\n\treturn pids, nil\n}\n\nvar kill = func(pid int, signal syscall.Signal) error {\n\treturn syscall.Kill(pid, signal)\n}\n\nvar pgrep = func() ([]byte, error) {\n\treturn exec.Command(\"pgrep\", processName).Output()\n}\n","lang_cluster":"Go","length":165,"code_uid":"f3a623adc3b24228a512e292e951532c"}
{"diff_hunk":"@@ -53,6 +53,16 @@ func mustSpanIDFromHex(s string) (t trace.SpanID) {\n \treturn\n }\n \n+func TestBytesMapCarrier(t *testing.T) {\n+\tcarrier := make(propagation.BytesMapCarrier)\n+\tcarrier.Set(\"foo\", \"bar\")\n+\tcarrier.Set(\"baz\", \"qux\")\n+\n+\tassert.Equal(t, carrier.Get(\"foo\"), \"bar\")\n+\tassert.Equal(t, carrier.Get(\"baz\"), \"qux\")\n+\tassert.Equal(t, carrier.Keys(), []string{\"foo\", \"baz\"})\n+}\n+\n type outOfThinAirPropagator struct {\n \tt *testing.T\n }","old_code":"\/\/ Copyright The OpenTelemetry Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage propagation_test\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com\/stretchr\/testify\/assert\"\n\t\"github.com\/stretchr\/testify\/require\"\n\n\t\"go.opentelemetry.io\/otel\/propagation\"\n\t\"go.opentelemetry.io\/otel\/trace\"\n)\n\nconst (\n\ttraceIDStr = \"4bf92f3577b34da6a3ce929d0e0e4736\"\n\tspanIDStr  = \"00f067aa0ba902b7\"\n)\n\nvar (\n\ttraceID = mustTraceIDFromHex(traceIDStr)\n\tspanID  = mustSpanIDFromHex(spanIDStr)\n)\n\nfunc mustTraceIDFromHex(s string) (t trace.TraceID) {\n\tvar err error\n\tt, err = trace.TraceIDFromHex(s)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn\n}\n\nfunc mustSpanIDFromHex(s string) (t trace.SpanID) {\n\tvar err error\n\tt, err = trace.SpanIDFromHex(s)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn\n}\n\ntype outOfThinAirPropagator struct {\n\tt *testing.T\n}\n\nvar _ propagation.TextMapPropagator = outOfThinAirPropagator{}\n\nfunc (p outOfThinAirPropagator) Extract(ctx context.Context, carrier propagation.TextMapCarrier) context.Context {\n\tsc := trace.NewSpanContext(trace.SpanContextConfig{\n\t\tTraceID:    traceID,\n\t\tSpanID:     spanID,\n\t\tTraceFlags: 0,\n\t})\n\trequire.True(p.t, sc.IsValid())\n\treturn trace.ContextWithRemoteSpanContext(ctx, sc)\n}\n\nfunc (outOfThinAirPropagator) Inject(context.Context, propagation.TextMapCarrier) {}\n\nfunc (outOfThinAirPropagator) Fields() []string {\n\treturn nil\n}\n\ntype nilCarrier struct{}\n\nvar _ propagation.TextMapCarrier = nilCarrier{}\n\nfunc (nilCarrier) Keys() []string {\n\treturn nil\n}\n\nfunc (nilCarrier) Get(key string) string {\n\treturn \"\"\n}\n\nfunc (nilCarrier) Set(key string, value string) {}\n\nfunc TestMultiplePropagators(t *testing.T) {\n\tootaProp := outOfThinAirPropagator{t: t}\n\tns := nilCarrier{}\n\ttestProps := []propagation.TextMapPropagator{\n\t\tpropagation.TraceContext{},\n\t}\n\tbg := context.Background()\n\t\/\/ sanity check of oota propagator, ensuring that it really\n\t\/\/ generates the valid span context out of thin air\n\t{\n\t\tctx := ootaProp.Extract(bg, ns)\n\t\tsc := trace.SpanContextFromContext(ctx)\n\t\trequire.True(t, sc.IsValid(), \"oota prop failed sanity check\")\n\t\trequire.True(t, sc.IsRemote(), \"oota prop is remote\")\n\t}\n\t\/\/ sanity check for real propagators, ensuring that they\n\t\/\/ really are not putting any valid span context into an empty\n\t\/\/ go context in absence of the HTTP headers.\n\tfor _, prop := range testProps {\n\t\tctx := prop.Extract(bg, ns)\n\t\tsc := trace.SpanContextFromContext(ctx)\n\t\trequire.Falsef(t, sc.IsValid(), \"%#v failed sanity check\", prop)\n\t\trequire.Falsef(t, sc.IsRemote(), \"%#v prop set a remote\", prop)\n\t}\n\tfor _, prop := range testProps {\n\t\tprops := propagation.NewCompositeTextMapPropagator(ootaProp, prop)\n\t\tctx := props.Extract(bg, ns)\n\t\tsc := trace.SpanContextFromContext(ctx)\n\t\tassert.Truef(t, sc.IsRemote(), \"%#v prop is remote\", prop)\n\t\tassert.Truef(t, sc.IsValid(), \"%#v clobbers span context\", prop)\n\t}\n}\n","lang_cluster":"Go","length":123,"code_uid":"de3bb04b80154cd5b6e6a7d9712c9f14"}
{"diff_hunk":"@@ -1,3 +1,5 @@\n+\/\/ The MIT License\n+\/\/\n \/\/ Copyright (c) 2017 Uber Technologies, Inc.\n \/\/\n \/\/ Permission is hereby granted, free of charge, to any person obtaining a copy","old_code":"\/\/ Copyright (c) 2017 Uber Technologies, Inc.\n\/\/\n\/\/ Permission is hereby granted, free of charge, to any person obtaining a copy\n\/\/ of this software and associated documentation files (the \"Software\"), to deal\n\/\/ in the Software without restriction, including without limitation the rights\n\/\/ to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n\/\/ copies of the Software, and to permit persons to whom the Software is\n\/\/ furnished to do so, subject to the following conditions:\n\/\/\n\/\/ The above copyright notice and this permission notice shall be included in\n\/\/ all copies or substantial portions of the Software.\n\/\/\n\/\/ THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\/\/ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\/\/ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\/\/ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\/\/ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\/\/ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n\/\/ THE SOFTWARE.\n\n\/\/go:generate mockgen -copyright_file ..\/..\/LICENSE -package $GOPACKAGE -source $GOFILE -destination interface_mock.go\n\npackage frontend\n\nimport (\n\t\"go.temporal.io\/temporal-proto\/workflowservice\"\n\n\t\"github.com\/temporalio\/temporal\/common\"\n\t\"github.com\/temporalio\/temporal\/common\/resource\"\n\n\thealthpb \"google.golang.org\/grpc\/health\/grpc_health_v1\"\n)\n\ntype (\n\t\/\/ Handler is interface wrapping frontend handler\n\tHandler interface {\n\t\tworkflowservice.WorkflowServiceServer\n\t\tcommon.Daemon\n\n\t\t\/\/ Health is the health check method for this rpc handler\n\t\thealthpb.HealthServer\n\t\t\/\/ UpdateHealthStatus sets the health status for this rpc handler.\n\t\t\/\/ This health status will be used within the rpc health check handler\n\t\tUpdateHealthStatus(status HealthStatus)\n\n\t\tGetResource() resource.Resource\n\t\tGetConfig() *Config\n\t}\n)\n","lang_cluster":"Go","length":49,"code_uid":"6feacce25e1446bd9b50e81060f4a1d1"}
{"diff_hunk":"@@ -26,12 +26,18 @@ import (\n \n var ipamDrivers map[string]IPAMDriver\n \n-type IPAMConfig struct {\n-\tType    string `json:\"type,omitempty\"`\n-\tSubnet  string `json:\"subnet,omitempty\"`\n+type Range struct {\n+\tSubnet  string `json:\"subnet\"`\n \tGateway string `json:\"gateway,omitempty\"`\n }\n \n+type RangeSet []Range\n+\n+type IPAMConfig struct {\n+\tType   string     `json:\"type,omitempty\"`\n+\tRanges []RangeSet `json:\"ranges,omitempty\"`\n+}\n+\n type IPAMDriver interface {\n \tAdd(args *invoke.Args, networkConfig []byte) (*current.Result, error)\n \tDel(args *invoke.Args, networkConfig []byte) error","old_code":"\/\/ Copyright 2019 Antrea Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage ipam\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\n\t\"github.com\/containernetworking\/cni\/pkg\/invoke\"\n\t\"github.com\/containernetworking\/cni\/pkg\/types\/current\"\n\n\tcnipb \"github.com\/vmware-tanzu\/antrea\/pkg\/apis\/cni\/v1beta1\"\n)\n\nvar ipamDrivers map[string]IPAMDriver\n\ntype IPAMConfig struct {\n\tType    string `json:\"type,omitempty\"`\n\tSubnet  string `json:\"subnet,omitempty\"`\n\tGateway string `json:\"gateway,omitempty\"`\n}\n\ntype IPAMDriver interface {\n\tAdd(args *invoke.Args, networkConfig []byte) (*current.Result, error)\n\tDel(args *invoke.Args, networkConfig []byte) error\n\tCheck(args *invoke.Args, networkConfig []byte) error\n}\n\nvar ipamResults = sync.Map{}\n\nfunc RegisterIPAMDriver(ipamType string, ipamDriver IPAMDriver) error {\n\tif ipamDrivers == nil {\n\t\tipamDrivers = make(map[string]IPAMDriver)\n\t}\n\tif _, existed := ipamDrivers[ipamType]; existed {\n\t\treturn fmt.Errorf(\"Already registered IPAM with type %s\", ipamType)\n\t}\n\tipamDrivers[ipamType] = ipamDriver\n\treturn nil\n}\n\nfunc argsFromEnv(cniArgs *cnipb.CniCmdArgs) *invoke.Args {\n\treturn &invoke.Args{\n\t\tContainerID: cniArgs.ContainerId,\n\t\tNetNS:       cniArgs.Netns,\n\t\tIfName:      cniArgs.Ifname,\n\t\tPath:        cniArgs.Path,\n\t}\n}\n\nfunc ExecIPAMAdd(cniArgs *cnipb.CniCmdArgs, ipamType string, resultKey string) (*current.Result, error) {\n\t\/\/ Return the cached IPAM result for the same Pod. This cache helps to ensure CNIAdd is idempotent. There are two\n\t\/\/ usages of CNIAdd message on Windows: 1) add container network configuration, and 2) query Pod network status.\n\t\/\/ kubelet on Windows sends CNIAdd messages to query Pod status periodically before the sandbox container is ready.\n\t\/\/ The cache here is to ensure only one IP address is allocated to one Pod.\n\t\/\/ TODO: A risk of IP re-allocation exists if agent restarts before kubelet queries Pod status and after the\n\t\/\/       container networking configurations is added.\n\tobj, ok := GetIPFromCache(resultKey)\n\tif ok {\n\t\treturn obj, nil\n\t}\n\n\targs := argsFromEnv(cniArgs)\n\tdriver := ipamDrivers[ipamType]\n\tresult, err := driver.Add(args, cniArgs.NetworkConfiguration)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tipamResults.Store(resultKey, result)\n\treturn result, nil\n}\n\nfunc ExecIPAMDelete(cniArgs *cnipb.CniCmdArgs, ipamType string, resultKey string) error {\n\targs := argsFromEnv(cniArgs)\n\tdriver := ipamDrivers[ipamType]\n\terr := driver.Del(args, cniArgs.NetworkConfiguration)\n\tif err != nil {\n\t\treturn err\n\t}\n\tipamResults.Delete(resultKey)\n\treturn nil\n}\n\nfunc ExecIPAMCheck(cniArgs *cnipb.CniCmdArgs, ipamType string) error {\n\targs := argsFromEnv(cniArgs)\n\tdriver := ipamDrivers[ipamType]\n\treturn driver.Check(args, cniArgs.NetworkConfiguration)\n}\n\nfunc GetIPFromCache(resultKey string) (*current.Result, bool) {\n\tobj, ok := ipamResults.Load(resultKey)\n\tif ok {\n\t\tresult := obj.(*current.Result)\n\t\treturn result, ok\n\t}\n\treturn nil, ok\n}\n\nfunc IsIPAMTypeValid(ipamType string) bool {\n\t_, valid := ipamDrivers[ipamType]\n\treturn valid\n}\n","lang_cluster":"Go","length":114,"code_uid":"e20c7bc3ecf94c9c885fe2a0b3038f4f"}
{"diff_hunk":"@@ -65,13 +65,10 @@ const (\n \n \/\/ SPI on the Trinket M0.\n var (\n-\tSPI0 = SPI{Bus: sam.SERCOM0_SPI,\n-\t\tSCK:     SPI0_SCK_PIN,\n-\t\tMOSI:    SPI0_MOSI_PIN,\n-\t\tMISO:    SPI0_MISO_PIN,\n-\t\tDOpad:   spiTXPad2SCK3,\n-\t\tDIpad:   sercomRXPad0,\n-\t\tPinMode: PinSERCOMAlt}\n+\tSPI0 = SPI{\n+\t\tBus:    sam.SERCOM0_SPI,\n+\t\tSERCOM: 0,\n+\t}\n )\n \n \/\/ I2C pins","old_code":"\/\/ +build sam,atsamd21,trinket_m0\n\npackage machine\n\nimport \"device\/sam\"\n\n\/\/ used to reset into bootloader\nconst RESET_MAGIC_VALUE = 0xf01669ef\n\n\/\/ GPIO Pins\nconst (\n\tD0  = PA08 \/\/ PWM available\n\tD1  = PA02\n\tD2  = PA09 \/\/ PWM available\n\tD3  = PA07 \/\/ PWM available \/ UART0 RX\n\tD4  = PA06 \/\/ PWM available \/ UART0 TX\n\tD13 = PA10 \/\/ LED\n)\n\n\/\/ Analog pins\nconst (\n\tA0 = D1\n\tA1 = D2\n\tA2 = D0\n\tA3 = D3\n\tA4 = D4\n)\n\nconst (\n\tLED = D13\n)\n\n\/\/ UART0 aka USBCDC pins\nconst (\n\tUSBCDC_DM_PIN = PA24\n\tUSBCDC_DP_PIN = PA25\n)\n\n\/\/ UART1 pins\nconst (\n\tUART_TX_PIN = D4\n\tUART_RX_PIN = D3\n)\n\n\/\/ UART1 on the Trinket M0.\nvar (\n\tUART1 = UART{Bus: sam.SERCOM1_USART,\n\t\tBuffer: NewRingBuffer(),\n\t\tMode:   PinSERCOM,\n\t\tIRQVal: sam.IRQ_SERCOM1,\n\t}\n)\n\n\/\/go:export SERCOM1_IRQHandler\nfunc handleUART1() {\n\tdefaultUART1Handler()\n}\n\n\/\/ SPI pins\nconst (\n\tSPI0_SCK_PIN  = D3\n\tSPI0_MOSI_PIN = D4\n\tSPI0_MISO_PIN = D2\n)\n\n\/\/ SPI on the Trinket M0.\nvar (\n\tSPI0 = SPI{Bus: sam.SERCOM0_SPI,\n\t\tSCK:     SPI0_SCK_PIN,\n\t\tMOSI:    SPI0_MOSI_PIN,\n\t\tMISO:    SPI0_MISO_PIN,\n\t\tDOpad:   spiTXPad2SCK3,\n\t\tDIpad:   sercomRXPad0,\n\t\tPinMode: PinSERCOMAlt}\n)\n\n\/\/ I2C pins\nconst (\n\tSDA_PIN = D0 \/\/ SDA\n\tSCL_PIN = D2 \/\/ SCL\n)\n\n\/\/ I2C on the Trinket M0.\nvar (\n\tI2C0 = I2C{Bus: sam.SERCOM2_I2CM,\n\t\tSDA:     SDA_PIN,\n\t\tSCL:     SCL_PIN,\n\t\tPinMode: PinSERCOMAlt}\n)\n\n\/\/ I2S pins\nconst (\n\tI2S_SCK_PIN = PA10\n\tI2S_SD_PIN  = PA08\n\tI2S_WS_PIN  = NoPin \/\/ TODO: figure out what this is on Trinket M0.\n)\n","lang_cluster":"Go","length":96,"code_uid":"926d2bee3c0f4d49abfeb6784cb7686f"}
{"diff_hunk":"@@ -6,20 +6,20 @@ package agent_test\n \n import (\n \t\"log\"\n-\t\"os\"\n \t\"net\"\n+\t\"os\"\n \n-        \"golang.org\/x\/crypto\/ssh\"\n-        \"golang.org\/x\/crypto\/ssh\/agent\"\n+\t\"golang.org\/x\/crypto\/ssh\"\n+\t\"golang.org\/x\/crypto\/ssh\/agent\"\n )\n \n func ExampleClientAgent() {\n \t\/\/ ssh-agent has a UNIX socket under $SSH_AUTH_SOCK\n \tsocket := os.Getenv(\"SSH_AUTH_SOCK\")\n-        conn, err := net.Dial(\"unix\", socket)\n-        if err != nil {\n-                log.Fatalf(\"net.Dial: %v\", err)\n-        }\n+\tconn, err := net.Dial(\"unix\", socket)\n+\tif err != nil {\n+\t\tlog.Fatalf(\"net.Dial: %v\", err)\n+\t}\n \tagentClient := agent.NewClient(conn)\n \tconfig := &ssh.ClientConfig{\n \t\tUser: \"username\",","old_code":"\/\/ Copyright 2016 The Go Authors. All rights reserved.\n\/\/ Use of this source code is governed by a BSD-style\n\/\/ license that can be found in the LICENSE file.\n\npackage agent_test\n\nimport (\n\t\"log\"\n\t\"os\"\n\t\"net\"\n\n        \"golang.org\/x\/crypto\/ssh\"\n        \"golang.org\/x\/crypto\/ssh\/agent\"\n)\n\nfunc ExampleClientAgent() {\n\t\/\/ ssh-agent has a UNIX socket under $SSH_AUTH_SOCK\n\tsocket := os.Getenv(\"SSH_AUTH_SOCK\")\n        conn, err := net.Dial(\"unix\", socket)\n        if err != nil {\n                log.Fatalf(\"net.Dial: %v\", err)\n        }\n\tagentClient := agent.NewClient(conn)\n\tconfig := &ssh.ClientConfig{\n\t\tUser: \"username\",\n\t\tAuth: []ssh.AuthMethod{\n\t\t\t\/\/ Use a callback rather than PublicKeys\n\t\t\t\/\/ so we only consult the agent once the remote server\n\t\t\t\/\/ wants it.\n\t\t\tssh.PublicKeysCallback(agentClient.Signers),\n\t\t},\n\t}\n\n\tsshc, err := ssh.Dial(\"tcp\", \"localhost:22\", config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Dial: %v\", err)\n\t}\n\t\/\/ .. use sshc\n\tsshc.Close()\n}\n","lang_cluster":"Go","length":40,"code_uid":"5a1d7f4edc164efbbccd5eaa01dbd2e8"}
{"diff_hunk":"@@ -39,4 +39,9 @@ func Start() {\n func Stop() {\n \ttch.Stop()\n \tyarpc.Stop()\n+\thttp.Stop()\n+\tapachethrift.Stop()\n }\n+\n+\/\/ TODO(abg): We should probably use defers to ensure things that started up\n+\/\/ successfully are stopped before we exit.","old_code":"\/\/ Copyright (c) 2016 Uber Technologies, Inc.\n\/\/\n\/\/ Permission is hereby granted, free of charge, to any person obtaining a copy\n\/\/ of this software and associated documentation files (the \"Software\"), to deal\n\/\/ in the Software without restriction, including without limitation the rights\n\/\/ to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n\/\/ copies of the Software, and to permit persons to whom the Software is\n\/\/ furnished to do so, subject to the following conditions:\n\/\/\n\/\/ The above copyright notice and this permission notice shall be included in\n\/\/ all copies or substantial portions of the Software.\n\/\/\n\/\/ THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\/\/ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\/\/ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\/\/ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\/\/ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\/\/ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n\/\/ THE SOFTWARE.\n\npackage server\n\nimport (\n\t\"github.com\/yarpc\/yarpc-go\/crossdock\/server\/apachethrift\"\n\t\"github.com\/yarpc\/yarpc-go\/crossdock\/server\/http\"\n\t\"github.com\/yarpc\/yarpc-go\/crossdock\/server\/tch\"\n\t\"github.com\/yarpc\/yarpc-go\/crossdock\/server\/yarpc\"\n)\n\n\/\/ Start starts all required Crossdock test servers\nfunc Start() {\n\ttch.Start()\n\tyarpc.Start()\n\thttp.Start()\n\tapachethrift.Start()\n}\n\n\/\/ Stop stops all required Crossdock test servers\nfunc Stop() {\n\ttch.Stop()\n\tyarpc.Stop()\n}\n","lang_cluster":"Go","length":42,"code_uid":"116b1109a9734292b7b893e5cedaeea3"}
{"diff_hunk":"@@ -35,6 +35,10 @@ type ConfigRunner struct {\n \n \t\/\/ Must not be nil if using default subnets.\n \tVPCGetter VPCGetter\n+\n+\t\/\/ Platform configuration\n+\tOS   string\n+\tArch string\n }\n \n \/\/ Run runs tasks given subnets, security groups and the cluster, and returns the tasks.","old_code":"\/\/ Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\/\/ SPDX-License-Identifier: Apache-2.0\n\npackage task\n\nimport (\n\t\"fmt\"\n\n\t\"github.com\/aws\/copilot-cli\/internal\/pkg\/aws\/ec2\"\n\t\"github.com\/aws\/copilot-cli\/internal\/pkg\/aws\/ecs\"\n)\n\nconst (\n\tfmtErrDefaultSubnets = \"get default subnet IDs: %w\"\n)\n\n\/\/ ConfigRunner runs an Amazon ECS task in the subnets, security groups, and cluster.\n\/\/ It uses the default subnets and the default cluster if the corresponding field is empty.\ntype ConfigRunner struct {\n\t\/\/ Count of the tasks to be launched.\n\tCount int\n\t\/\/ Group Name of the tasks that use the same task definition.\n\tGroupName string\n\n\t\/\/ The ARN of the cluster to run the task.\n\tCluster string\n\n\t\/\/ Network configuration\n\tSubnets        []string\n\tSecurityGroups []string\n\n\t\/\/ Interfaces to interact with dependencies. Must not be nil.\n\tClusterGetter DefaultClusterGetter\n\tStarter       Runner\n\n\t\/\/ Must not be nil if using default subnets.\n\tVPCGetter VPCGetter\n}\n\n\/\/ Run runs tasks given subnets, security groups and the cluster, and returns the tasks.\n\/\/ If subnets are not provided, it uses the default subnets.\n\/\/ If cluster is not provided, it uses the default cluster.\nfunc (r *ConfigRunner) Run() ([]*Task, error) {\n\tif err := r.validateDependencies(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif r.Cluster == \"\" {\n\t\tcluster, err := r.ClusterGetter.DefaultCluster()\n\t\tif err != nil {\n\t\t\treturn nil, &errGetDefaultCluster{\n\t\t\t\tparentErr: err,\n\t\t\t}\n\t\t}\n\t\tr.Cluster = cluster\n\t}\n\n\tif r.Subnets == nil {\n\t\tsubnets, err := r.VPCGetter.SubnetIDs(ec2.FilterForDefaultVPCSubnets)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(fmtErrDefaultSubnets, err)\n\t\t}\n\t\tif len(subnets) == 0 {\n\t\t\treturn nil, errNoSubnetFound\n\t\t}\n\t\tr.Subnets = subnets\n\t}\n\n\tecsTasks, err := r.Starter.RunTask(ecs.RunTaskInput{\n\t\tCluster:        r.Cluster,\n\t\tCount:          r.Count,\n\t\tSubnets:        r.Subnets,\n\t\tSecurityGroups: r.SecurityGroups,\n\t\tTaskFamilyName: taskFamilyName(r.GroupName),\n\t\tStartedBy:      startedBy,\n\t})\n\tif err != nil {\n\t\treturn nil, &errRunTask{\n\t\t\tgroupName: r.GroupName,\n\t\t\tparentErr: err,\n\t\t}\n\t}\n\n\treturn convertECSTasks(ecsTasks), nil\n}\n\nfunc (r *ConfigRunner) validateDependencies() error {\n\tif r.ClusterGetter == nil {\n\t\treturn errClusterGetterNil\n\t}\n\n\tif r.Starter == nil {\n\t\treturn errStarterNil\n\t}\n\n\treturn nil\n}\n","lang_cluster":"Go","length":97,"code_uid":"0fec481a691f499693a3a6ac6881b26f"}
{"diff_hunk":"@@ -7,6 +7,15 @@ type NodeEntry struct {\n \tGossipVersion string\n }\n \n+\/\/ Equals method compares two NodeEntries, and returns TRUE if the NodeEntry's data is equal.\n+func (ne NodeEntry) Equals(other *NodeEntry) bool {\n+\tif other == nil {\n+\t\treturn false\n+\t} else {\n+\t\treturn ne.Id == other.Id && ne.Ip == other.Ip && ne.GossipVersion == other.GossipVersion\n+\t}\n+}\n+\n \/\/ ClusterInfo is the cluster info used while discoveryping nodes\n \/\/ and discovering peer nodes using gossip\n type ClusterInfo struct {","old_code":"package discovery\n\n\/\/ NodeEntry is used to discovery nodes in the cluster\ntype NodeEntry struct {\n\tId            string\n\tIp            string\n\tGossipVersion string\n}\n\n\/\/ ClusterInfo is the cluster info used while discoveryping nodes\n\/\/ and discovering peer nodes using gossip\ntype ClusterInfo struct {\n\tSize    int\n\tNodes   map[string]NodeEntry\n\tVersion uint64\n}\n\ntype WatchClusterCB func(*ClusterInfo, error) error\n\ntype Cluster interface {\n\t\/\/ AddNode adds a new node into a cluster so that other can discover\n\tAddNode(dne NodeEntry) (*ClusterInfo, error)\n\n\t\/\/ RemoveNode removes a node from a cluster\n\tRemoveNode(dne NodeEntry) (*ClusterInfo, error)\n\n\t\/\/ Enumerate enumerates the nodes that have been discovered in the cluster\n\tEnumerate() (*ClusterInfo, error)\n\n\t\/\/ WatchCluster starts a watch on the cluster and calls the provided\n\t\/\/ callback function when a node is added or removed.\n\tWatchCluster(wcb WatchClusterCB, index uint64) error\n}\n","lang_cluster":"Go","length":33,"code_uid":"efd2821b22c54a508812af73b95820a7"}
{"diff_hunk":"@@ -31,5 +31,5 @@ func (t Template) eksControlPlanePolicies() []string {\n }\n \n func eksAssumeRolePolicy() *iamv1.PolicyDocument {\n-\treturn assumeRolePolicy(\"eks.amazonaws.com\")\n+\treturn assumeRolePolicy([]string{\"eks.amazonaws.com\"})\n }","old_code":"\/*\nCopyright 2020 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage bootstrap\n\nimport iamv1 \"sigs.k8s.io\/cluster-api-provider-aws\/cmd\/clusterawsadm\/api\/iam\/v1alpha1\"\n\nfunc (t Template) eksControlPlanePolicies() []string {\n\tpolicies := []string{EKSClusterPolicy}\n\tif t.Spec.EKS.DefaultControlPlaneRole.ExtraPolicyAttachments != nil {\n\t\tfor _, policy := range t.Spec.EKS.DefaultControlPlaneRole.ExtraPolicyAttachments {\n\t\t\tadditionalPolicy := policy\n\t\t\tpolicies = append(policies, additionalPolicy)\n\t\t}\n\t}\n\n\treturn policies\n}\n\nfunc eksAssumeRolePolicy() *iamv1.PolicyDocument {\n\treturn assumeRolePolicy(\"eks.amazonaws.com\")\n}\n","lang_cluster":"Go","length":35,"code_uid":"9f68baa1b2e841098379dd6b05e69221"}
{"diff_hunk":"@@ -39,10 +39,23 @@ func Test_translateAnnotations(t *testing.T) {\n \n \tvalidAnnotations := func() map[string]string {\n \t\treturn map[string]string{\n-\t\t\tcmapi.CommonNameAnnotationKey:  \"www.example.com\",\n-\t\t\tcmapi.DurationAnnotationKey:    \"168h\", \/\/ 1 week\n-\t\t\tcmapi.RenewBeforeAnnotationKey: \"24h\",\n-\t\t\tcmapi.UsagesAnnotationKey:      \"server auth,signing\",\n+\t\t\tcmapi.CommonNameAnnotationKey:                 \"www.example.com\",\n+\t\t\tcmapi.EmailsAnnotationKey:                     \"test@example.com\",\n+\t\t\tcmapi.SubjectOrganizationsAnnotationKey:       \"Test Organization\",\n+\t\t\tcmapi.SubjectOrganizationalUnitsAnnotationKey: \"Test Organizational Unit\",\n+\t\t\tcmapi.SubjectCountriesAnnotationKey:           \"Country\",\n+\t\t\tcmapi.SubjectProvincesAnnotationKey:           \"Province\",\n+\t\t\tcmapi.SubjectLocalitiesAnnotationKey:          \"City\",\n+<<<<<<< HEAD\n+\t\t\tcmapi.SubjectStreetAddressesAnnotationKey:     \"\\\"1725 Slough Avenue, Suite 200, Scranton Business Park\\\"\",\n+=======\n+\t\t\tcmapi.SubjectStreetAddressesAnnotationKey:     \"Address\",\n+>>>>>>> 10a9535a99ece4b45f2ea7d8c4aa57bd35d52768\n+\t\t\tcmapi.SubjectPostalCodesAnnotationKey:         \"ABC123\",\n+\t\t\tcmapi.SubjectSerialNumberAnnotationKey:        \"123456\",\n+\t\t\tcmapi.DurationAnnotationKey:                   \"168h\", \/\/ 1 week\n+\t\t\tcmapi.RenewBeforeAnnotationKey:                \"24h\",\n+\t\t\tcmapi.UsagesAnnotationKey:                     \"server auth,signing\",\n \t\t}\n \t}\n ","old_code":"\/*\nCopyright 2020 The cert-manager Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage shimhelper\n\nimport (\n\t\"errors\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com\/stretchr\/testify\/assert\"\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n\n\tcmapi \"github.com\/jetstack\/cert-manager\/pkg\/apis\/certmanager\/v1\"\n\t\"github.com\/jetstack\/cert-manager\/test\/unit\/gen\"\n)\n\nfunc Test_translateAnnotations(t *testing.T) {\n\ttype testCase struct {\n\t\tcrt           *cmapi.Certificate\n\t\tannotations   map[string]string\n\t\tmutate        func(*testCase)\n\t\tcheck         func(*assert.Assertions, *cmapi.Certificate)\n\t\texpectedError error\n\t}\n\n\tvalidAnnotations := func() map[string]string {\n\t\treturn map[string]string{\n\t\t\tcmapi.CommonNameAnnotationKey:  \"www.example.com\",\n\t\t\tcmapi.DurationAnnotationKey:    \"168h\", \/\/ 1 week\n\t\t\tcmapi.RenewBeforeAnnotationKey: \"24h\",\n\t\t\tcmapi.UsagesAnnotationKey:      \"server auth,signing\",\n\t\t}\n\t}\n\n\ttests := map[string]testCase{\n\t\t\"success\": {\n\t\t\tcrt:         gen.Certificate(\"example-cert\"),\n\t\t\tannotations: validAnnotations(),\n\t\t\tcheck: func(a *assert.Assertions, crt *cmapi.Certificate) {\n\t\t\t\ta.Equal(\"www.example.com\", crt.Spec.CommonName)\n\t\t\t\ta.Equal(&metav1.Duration{Duration: time.Hour * 24 * 7}, crt.Spec.Duration)\n\t\t\t\ta.Equal(&metav1.Duration{Duration: time.Hour * 24}, crt.Spec.RenewBefore)\n\t\t\t\ta.Equal([]cmapi.KeyUsage{cmapi.UsageServerAuth, cmapi.UsageSigning}, crt.Spec.Usages)\n\t\t\t},\n\t\t},\n\t\t\"nil annotations\": {\n\t\t\tcrt:         gen.Certificate(\"example-cert\"),\n\t\t\tannotations: nil,\n\t\t},\n\t\t\"empty annotations\": {\n\t\t\tcrt:         gen.Certificate(\"example-cert\"),\n\t\t\tannotations: map[string]string{},\n\t\t},\n\t\t\"nil certificate\": {\n\t\t\tcrt:           nil,\n\t\t\tannotations:   validAnnotations(),\n\t\t\texpectedError: errNilCertificate,\n\t\t},\n\t\t\"bad duration\": {\n\t\t\tcrt:         gen.Certificate(\"example-cert\"),\n\t\t\tannotations: validAnnotations(),\n\t\t\tmutate: func(tc *testCase) {\n\t\t\t\ttc.annotations[cmapi.DurationAnnotationKey] = \"an un-parsable duration string\"\n\t\t\t},\n\t\t\texpectedError: errInvalidIngressAnnotation,\n\t\t},\n\t\t\"bad renewBefore\": {\n\t\t\tcrt:         gen.Certificate(\"example-cert\"),\n\t\t\tannotations: validAnnotations(),\n\t\t\tmutate: func(tc *testCase) {\n\t\t\t\ttc.annotations[cmapi.RenewBeforeAnnotationKey] = \"an un-parsable duration string\"\n\t\t\t},\n\t\t\texpectedError: errInvalidIngressAnnotation,\n\t\t},\n\t\t\"bad usages\": {\n\t\t\tcrt:         gen.Certificate(\"example-cert\"),\n\t\t\tannotations: validAnnotations(),\n\t\t\tmutate: func(tc *testCase) {\n\t\t\t\ttc.annotations[cmapi.UsagesAnnotationKey] = \"playing ping pong\"\n\t\t\t},\n\t\t\texpectedError: errInvalidIngressAnnotation,\n\t\t},\n\t\t\"bad usage list\": {\n\t\t\tcrt:         gen.Certificate(\"example-cert\"),\n\t\t\tannotations: validAnnotations(),\n\t\t\tmutate: func(tc *testCase) {\n\t\t\t\ttc.annotations[cmapi.UsagesAnnotationKey] = \"server auth,,signing\"\n\t\t\t},\n\t\t\texpectedError: errInvalidIngressAnnotation,\n\t\t},\n\t}\n\tfor name, tc := range tests {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\tif tc.mutate != nil {\n\t\t\t\ttc.mutate(&tc)\n\t\t\t}\n\t\t\tcrt := tc.crt.DeepCopy()\n\n\t\t\terr := translateAnnotations(crt, tc.annotations)\n\n\t\t\tif tc.expectedError != nil {\n\t\t\t\tassertErrorIs(t, err, tc.expectedError)\n\t\t\t} else {\n\t\t\t\tassert.NoError(t, err)\n\t\t\t}\n\t\t\tif tc.check != nil {\n\t\t\t\ttc.check(assert.New(t), crt)\n\t\t\t}\n\t\t})\n\t}\n}\n\n\/\/ assertErrorIs checks that the supplied error has the target error in its chain.\n\/\/ TODO Upgrade to next release of testify package which has this built in.\nfunc assertErrorIs(t *testing.T, err, target error) {\n\tif assert.Error(t, err) {\n\t\tassert.Truef(t, errors.Is(err, target), \"unexpected error type. err: %v, target: %v\", err, target)\n\t}\n}\n","lang_cluster":"Go","length":133,"code_uid":"7459ed7b20f24afa83b8cbf64b1b88e2"}
{"diff_hunk":"@@ -75,7 +75,7 @@ func TestCtlInstall(t *testing.T) {\n \t\t\ttestApiServer, cleanup := install_framework.NewTestInstallApiServer(t)\n \t\t\tdefer cleanup()\n \n-\t\t\tctx, cancel := context.WithTimeout(context.TODO(), time.Second*20)\n+\t\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second*40)\n \t\t\tdefer cancel()\n \n \t\t\tif test.prerun {","old_code":"\/*\nCopyright 2021 The cert-manager Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage ctl\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com\/sergi\/go-diff\/diffmatchpatch\"\n\n\t\"github.com\/jetstack\/cert-manager\/cmd\/ctl\/cmd\"\n\t\"github.com\/jetstack\/cert-manager\/test\/integration\/ctl\/install_framework\"\n\t\"github.com\/jetstack\/cert-manager\/test\/internal\/util\"\n)\n\nfunc TestCtlInstall(t *testing.T) {\n\ttests := map[string]struct {\n\t\tprerun       bool\n\t\tpreInputArgs []string\n\t\tpreExpErr    bool\n\t\tpreExpOutput string\n\n\t\tinputArgs []string\n\t\texpErr    bool\n\t\texpOutput string\n\t}{\n\t\t\"install cert-manager\": {\n\t\t\tinputArgs: []string{},\n\t\t\texpErr:    false,\n\t\t\texpOutput: `STATUS: deployed`,\n\t\t},\n\t\t\"install cert-manager (already installed)\": {\n\t\t\tprerun:       true,\n\t\t\tpreInputArgs: []string{},\n\t\t\tpreExpErr:    false,\n\t\t\tpreExpOutput: `STATUS: deployed`,\n\n\t\t\tinputArgs: []string{},\n\t\t\texpErr:    true,\n\t\t\texpOutput: `^Found existing installed cert-manager CRDs! Cannot continue with installation.$`,\n\t\t},\n\t\t\"install cert-manager (already installed, in other namespace)\": {\n\t\t\tprerun:       true,\n\t\t\tpreInputArgs: []string{\"--namespace=test\"},\n\t\t\tpreExpErr:    false,\n\t\t\tpreExpOutput: `STATUS: deployed`,\n\n\t\t\tinputArgs: []string{},\n\t\t\texpErr:    true,\n\t\t\texpOutput: `^Found existing installed cert-manager CRDs! Cannot continue with installation.$`,\n\t\t},\n\t}\n\n\tfor name, test := range tests {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\ttestApiServer, cleanup := install_framework.NewTestInstallApiServer(t)\n\t\t\tdefer cleanup()\n\n\t\t\tctx, cancel := context.WithTimeout(context.TODO(), time.Second*20)\n\t\t\tdefer cancel()\n\n\t\t\tif test.prerun {\n\t\t\t\texecuteCommandAndCheckOutput(t, ctx, testApiServer.KubeConfig(), test.preInputArgs, test.preExpErr, test.preExpOutput)\n\t\t\t}\n\n\t\t\texecuteCommandAndCheckOutput(t, ctx, testApiServer.KubeConfig(), test.inputArgs, test.expErr, test.expOutput)\n\t\t})\n\t}\n}\n\nfunc executeCommandAndCheckOutput(\n\tt *testing.T,\n\tctx context.Context,\n\tkubeConfig string,\n\tinputArgs []string,\n\texpErr bool,\n\texpOutput string,\n) {\n\t\/\/ Options to run status command\n\tstdin := bytes.NewBufferString(\"\")\n\tstdout := bytes.NewBufferString(\"\")\n\n\tchartPath := util.GetTestPath(\"deploy\", \"charts\", \"cert-manager\", \"cert-manager.tgz\")\n\tcmd := cmd.NewCertManagerCtlCommand(ctx, stdin, stdout, stdout)\n\tcmd.SetArgs(append([]string{\n\t\tfmt.Sprintf(\"--kubeconfig=%s\", kubeConfig),\n\t\t\"--wait=false\",\n\t\tfmt.Sprintf(\"--chart-name=%s\", chartPath),\n\t\t\"x\",\n\t\t\"install\",\n\t}, inputArgs...))\n\n\terr := cmd.Execute()\n\tif err != nil {\n\t\tfmt.Fprintf(stdout, \"%s\\n\", err)\n\n\t\tif !expErr {\n\t\t\tt.Errorf(\"got unexpected error: %v\", err)\n\t\t} else {\n\t\t\tt.Logf(\"got an error, which was expected, details: %v\", err)\n\t\t}\n\t} else if expErr {\n\t\t\/\/ expected error but error is nil\n\t\tt.Errorf(\"expected but got no error\")\n\t}\n\n\tmatch, err := regexp.MatchString(strings.TrimSpace(expOutput), strings.TrimSpace(stdout.String()))\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n\tdmp := diffmatchpatch.New()\n\tif !match {\n\t\tdiffs := dmp.DiffMain(strings.TrimSpace(expOutput), strings.TrimSpace(stdout.String()), false)\n\t\tt.Errorf(\n\t\t\t\"got unexpected output, diff (ignoring line anchors ^ and $ and regex for creation time):\\n\"+\n\t\t\t\t\"diff: %s\\n\\n\"+\n\t\t\t\t\" exp: %s\\n\\n\"+\n\t\t\t\t\" got: %s\",\n\t\t\tdmp.DiffPrettyText(diffs),\n\t\t\texpOutput,\n\t\t\tstdout.String(),\n\t\t)\n\t}\n}\n","lang_cluster":"Go","length":143,"code_uid":"a6c172f932254bb1b7d14c6452621181"}
{"diff_hunk":"@@ -123,36 +123,37 @@ func (p *PoolAttach) Execute() ([]byte, error) {\n \t\treturn nil, err\n \t}\n \t\/\/ execute command here\n-\treturn exec.Command(bin.ZPOOL, p.Command).CombinedOutput()\n+\treturn exec.Command(bin.BASH, \"-c\", p.Command).CombinedOutput()\n }\n \n \/\/ Build returns the PoolAttach object generated by builder\n func (p *PoolAttach) Build() (*PoolAttach, error) {\n \tvar c strings.Builder\n \tp = p.Validate()\n-\tp.appendCommand(c, fmt.Sprintf(\" %s \", Operation))\n+\tp.appendCommand(&c, bin.ZPOOL)\n+\tp.appendCommand(&c, fmt.Sprintf(\" %s \", Operation))\n \n \tif IsForcefullySet()(p) {\n-\t\tp.appendCommand(c, fmt.Sprintf(\" -f \"))\n+\t\tp.appendCommand(&c, fmt.Sprintf(\" -f \"))\n \t}\n \n \tif IsPropertySet()(p) {\n \t\tfor _, v := range p.Property {\n-\t\t\tp.appendCommand(c, fmt.Sprintf(\" -o %s \", v))\n+\t\t\tp.appendCommand(&c, fmt.Sprintf(\" -o %s \", v))\n \t\t}\n \t}\n \n-\tp.appendCommand(c, p.Pool)\n+\tp.appendCommand(&c, p.Pool)\n \n-\tp.appendCommand(c, fmt.Sprintf(\" %s \", p.Device))\n-\tp.appendCommand(c, fmt.Sprintf(\" %s \", p.NewDevice))\n+\tp.appendCommand(&c, fmt.Sprintf(\" %s \", p.Device))\n+\tp.appendCommand(&c, fmt.Sprintf(\" %s \", p.NewDevice))\n \n \tp.Command = c.String()\n \treturn p, p.err\n }\n \n \/\/ appendCommand append string to given string builder\n-func (p *PoolAttach) appendCommand(c strings.Builder, cmd string) {\n+func (p *PoolAttach) appendCommand(c *strings.Builder, cmd string) {\n \t_, err := c.WriteString(cmd)\n \tif err != nil {\n \t\tp.err = errors.Wrapf(p.err, \"Failed to append cmd{%s} : %s\", cmd, err.Error())","old_code":"\/*\nCopyright 2019 The OpenEBS Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage pattach\n\nimport (\n\t\"fmt\"\n\t\"os\/exec\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"strings\"\n\n\t\"github.com\/openebs\/maya\/pkg\/zfs\/cmd\/v1alpha1\/bin\"\n\t\"github.com\/pkg\/errors\"\n)\n\nconst (\n\t\/\/ Operation defines type of zfs operation\n\tOperation = \"attach\"\n)\n\n\/\/PoolAttach defines structure for pool 'Attach' operation\ntype PoolAttach struct {\n\t\/\/list of property\n\tProperty []string\n\n\t\/\/forcefully attach\n\tForcefully bool\n\n\t\/\/device name\n\tDevice string\n\n\t\/\/new device name\n\tNewDevice string\n\n\t\/\/pool name\n\tPool string\n\n\t\/\/ command string\n\tCommand string\n\n\t\/\/ checks is list of predicate function used for validating object\n\tchecks []PredicateFunc\n\n\t\/\/ error\n\terr error\n}\n\n\/\/ NewPoolAttach returns new instance of object PoolAttach\nfunc NewPoolAttach() *PoolAttach {\n\treturn &PoolAttach{}\n}\n\n\/\/ WithCheck add given check to checks list\nfunc (p *PoolAttach) WithCheck(check ...PredicateFunc) *PoolAttach {\n\tp.checks = append(p.checks, check...)\n\treturn p\n}\n\n\/\/ WithProperty method fills the Property field of PoolAttach object.\nfunc (p *PoolAttach) WithProperty(key, value string) *PoolAttach {\n\tp.Property = append(p.Property, fmt.Sprintf(\"%s=%s\", key, value))\n\treturn p\n}\n\n\/\/ WithForcefully method fills the Forcefully field of PoolAttach object.\nfunc (p *PoolAttach) WithForcefully(Forcefully bool) *PoolAttach {\n\tp.Forcefully = Forcefully\n\treturn p\n}\n\n\/\/ WithDevice method fills the Device field of PoolAttach object.\nfunc (p *PoolAttach) WithDevice(Device string) *PoolAttach {\n\tp.Device = Device\n\treturn p\n}\n\n\/\/ WithNewDevice method fills the NewDevice field of PoolAttach object.\nfunc (p *PoolAttach) WithNewDevice(NewDevice string) *PoolAttach {\n\tp.NewDevice = NewDevice\n\treturn p\n}\n\n\/\/ WithPool method fills the Pool field of PoolAttach object.\nfunc (p *PoolAttach) WithPool(Pool string) *PoolAttach {\n\tp.Pool = Pool\n\treturn p\n}\n\n\/\/ WithCommand method fills the Command field of PoolAttach object.\nfunc (p *PoolAttach) WithCommand(Command string) *PoolAttach {\n\tp.Command = Command\n\treturn p\n}\n\n\/\/ Validate is to validate generated PoolAttach object by builder\nfunc (p *PoolAttach) Validate() *PoolAttach {\n\tfor _, check := range p.checks {\n\t\tif !check(p) {\n\t\t\tp.err = errors.Wrapf(p.err, \"validation failed {%v}\", runtime.FuncForPC(reflect.ValueOf(check).Pointer()).Name())\n\t\t}\n\t}\n\treturn p\n}\n\n\/\/ Execute is to execute generated PoolAttach object\nfunc (p *PoolAttach) Execute() ([]byte, error) {\n\tp, err := p.Build()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t\/\/ execute command here\n\treturn exec.Command(bin.ZPOOL, p.Command).CombinedOutput()\n}\n\n\/\/ Build returns the PoolAttach object generated by builder\nfunc (p *PoolAttach) Build() (*PoolAttach, error) {\n\tvar c strings.Builder\n\tp = p.Validate()\n\tp.appendCommand(c, fmt.Sprintf(\" %s \", Operation))\n\n\tif IsForcefullySet()(p) {\n\t\tp.appendCommand(c, fmt.Sprintf(\" -f \"))\n\t}\n\n\tif IsPropertySet()(p) {\n\t\tfor _, v := range p.Property {\n\t\t\tp.appendCommand(c, fmt.Sprintf(\" -o %s \", v))\n\t\t}\n\t}\n\n\tp.appendCommand(c, p.Pool)\n\n\tp.appendCommand(c, fmt.Sprintf(\" %s \", p.Device))\n\tp.appendCommand(c, fmt.Sprintf(\" %s \", p.NewDevice))\n\n\tp.Command = c.String()\n\treturn p, p.err\n}\n\n\/\/ appendCommand append string to given string builder\nfunc (p *PoolAttach) appendCommand(c strings.Builder, cmd string) {\n\t_, err := c.WriteString(cmd)\n\tif err != nil {\n\t\tp.err = errors.Wrapf(p.err, \"Failed to append cmd{%s} : %s\", cmd, err.Error())\n\t}\n}\n","lang_cluster":"Go","length":160,"code_uid":"ef4547225c354f429bc2bc07180b415c"}
{"diff_hunk":"@@ -10,11 +10,10 @@ import (\n \n \t\"github.com\/filecoin-project\/go-filecoin\/address\"\n \t\"github.com\/filecoin-project\/go-filecoin\/porcelain\"\n-\t\"github.com\/filecoin-project\/go-filecoin\/types\"\n )\n \n \/\/ MinerCreate runs the `miner create` command against the filecoin process\n-func (f *Filecoin) MinerCreate(ctx context.Context, pledge uint64, collateral *types.AttoFIL, options ...ActionOption) (address.Address, error) {\n+func (f *Filecoin) MinerCreate(ctx context.Context, pledge uint64, collateral *big.Int, options ...ActionOption) (address.Address, error) {\n \tvar out address.Address\n \n \tsPledge := fmt.Sprintf(\"%d\", pledge)","old_code":"package fat\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math\/big\"\n\n\tcid \"gx\/ipfs\/QmR8BauakNcBa3RbE4nbQu76PDiJgoQgz8AJdhJuiU4TAw\/go-cid\"\n\t\"gx\/ipfs\/QmY5Grm8pJdiSSVsYxx4uNRgweY72EmYwuSDbRnbFok3iY\/go-libp2p-peer\"\n\n\t\"github.com\/filecoin-project\/go-filecoin\/address\"\n\t\"github.com\/filecoin-project\/go-filecoin\/porcelain\"\n\t\"github.com\/filecoin-project\/go-filecoin\/types\"\n)\n\n\/\/ MinerCreate runs the `miner create` command against the filecoin process\nfunc (f *Filecoin) MinerCreate(ctx context.Context, pledge uint64, collateral *types.AttoFIL, options ...ActionOption) (address.Address, error) {\n\tvar out address.Address\n\n\tsPledge := fmt.Sprintf(\"%d\", pledge)\n\tsCollateral := collateral.String()\n\n\targs := []string{\"go-filecoin\", \"miner\", \"create\"}\n\n\tfor _, option := range options {\n\t\targs = append(args, option()...)\n\t}\n\n\targs = append(args, sPledge, sCollateral)\n\n\tif err := f.RunCmdJSONWithStdin(ctx, nil, &out, args...); err != nil {\n\t\treturn address.Address{}, err\n\t}\n\n\treturn out, nil\n}\n\n\/\/ MinerUpdatePeerid runs the `miner update-peerid` command against the filecoin process\nfunc (f *Filecoin) MinerUpdatePeerid(ctx context.Context, minerAddr address.Address, pid peer.ID, options ...ActionOption) (cid.Cid, error) {\n\tvar out cid.Cid\n\n\targs := []string{\"go-filecoin\", \"miner\", \"update-peerid\"}\n\n\tfor _, option := range options {\n\t\targs = append(args, option()...)\n\t}\n\n\targs = append(args, minerAddr.String(), pid.Pretty())\n\n\tif err := f.RunCmdJSONWithStdin(ctx, nil, &out, args...); err != nil {\n\t\treturn cid.Undef, err\n\t}\n\n\treturn out, nil\n}\n\n\/\/ MinerAddAsk runs the `miner add-ask` command against the filecoin process\nfunc (f *Filecoin) MinerAddAsk(ctx context.Context, minerAddr address.Address, fil *big.Float, expiry big.Int, options ...ActionOption) (cid.Cid, error) {\n\tvar out cid.Cid\n\n\tsMinerAddr := minerAddr.String()\n\tsExpiry := expiry.String()\n\tsFil := fil.String()\n\n\targs := []string{\"go-filecoin\", \"miner\", \"add-ask\"}\n\n\tfor _, option := range options {\n\t\targs = append(args, option()...)\n\t}\n\n\targs = append(args, sMinerAddr, sFil, sExpiry)\n\n\tif err := f.RunCmdJSONWithStdin(ctx, nil, &out, args...); err != nil {\n\t\treturn cid.Undef, err\n\t}\n\treturn out, nil\n}\n\n\/\/ MinerOwner runs the `miner owner` command against the filecoin process\nfunc (f *Filecoin) MinerOwner(ctx context.Context, minerAddr address.Address) (address.Address, error) {\n\tvar out address.Address\n\n\tsMinerAddr := minerAddr.String()\n\n\tif err := f.RunCmdJSONWithStdin(ctx, nil, &out, \"go-filecoin\", \"miner\", \"owner\", sMinerAddr); err != nil {\n\t\treturn address.Address{}, err\n\t}\n\n\treturn out, nil\n}\n\n\/\/ MinerPledge runs the `miner pledge` command against the filecoin process\nfunc (f *Filecoin) MinerPledge(ctx context.Context, minerAddr address.Address) (*big.Int, error) {\n\tvar out big.Int\n\n\tsMinerAddr := minerAddr.String()\n\n\tif err := f.RunCmdJSONWithStdin(ctx, nil, &out, \"go-filecoin\", \"miner\", \"pledge\", sMinerAddr); err != nil {\n\t\treturn big.NewInt(0), err\n\t}\n\n\treturn &out, nil\n}\n\n\/\/ MinerPower runs the `miner power` command against the filecoin process\nfunc (f *Filecoin) MinerPower(ctx context.Context, minerAddr address.Address) (*big.Int, error) {\n\tvar out big.Int\n\n\tsMinerAddr := minerAddr.String()\n\n\tif err := f.RunCmdJSONWithStdin(ctx, nil, &out, \"go-filecoin\", \"miner\", \"power\", sMinerAddr); err != nil {\n\t\treturn big.NewInt(0), err\n\t}\n\n\treturn &out, nil\n}\n\n\/\/ MinerSetPrice runs the `miner set-price` command against the filecoin process\nfunc (f *Filecoin) MinerSetPrice(ctx context.Context, fil *big.Float, expiry *big.Int, options ...ActionOption) (*porcelain.MinerSetPriceResponse, error) {\n\tvar out porcelain.MinerSetPriceResponse\n\n\tsExpiry := expiry.String()\n\tsFil := fil.String()\n\n\targs := []string{\"go-filecoin\", \"miner\", \"set-price\"}\n\n\tfor _, option := range options {\n\t\targs = append(args, option()...)\n\t}\n\n\targs = append(args, sFil, sExpiry)\n\n\tif err := f.RunCmdJSONWithStdin(ctx, nil, &out, args...); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &out, nil\n}\n","lang_cluster":"Go","length":138,"code_uid":"d09c5bfbc96947f1b488fa76d0340fc6"}
{"diff_hunk":"@@ -37,6 +37,11 @@ const (\n \tStoragePoolClaimCPK CasPoolKey = \"openebs.io\/storage-pool-claim\"\n \t\/\/ CStorPoolClusterCPK is the CStorPoolcluster label\n \tCStorPoolClusterCPK CasPoolKey = \"openebs.io\/cstor-pool-cluster\"\n+\t\/\/ CStorPoolInstanceCPK is the CStorPoolInstance label\n+\tCStorPoolInstanceCPK CasPoolKey = \"openebs.io\/cstor-pool-instance\"\n+\t\/\/ PredecessorBlockDeviceCPK is the annotation on the block device claim\n+\t\/\/ holding previous block device name\n+\tPredecessorBlockDeviceCPK CasPoolKey = \"openebs.io\/bd-predecessor\"\n \t\/\/ NdmDiskTypeCPK is the node-disk-manager disk type e.g. 'sparse' or 'disk'\n \tNdmDiskTypeCPK CasPoolKey = \"ndm.io\/disk-type\"\n \t\/\/ NdmBlockDeviceTypeCPK is the node-disk-manager blockdevice type e.g. \/\/ 'blockdevice'","old_code":"\/*\nCopyright 2017 The OpenEBS Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\npackage v1alpha1\n\nimport (\n\tndm \"github.com\/openebs\/maya\/pkg\/apis\/openebs.io\/ndm\/v1alpha1\"\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n)\n\n\/\/ CasPoolKey is the key for the CasPool.\ntype CasPoolKey string\n\n\/\/ CasPoolValString represents the string value for a CasPoolKey.\ntype CasPoolValString string\n\n\/\/ CasPoolValInt represents the integer value for a CasPoolKey\ntype CasPoolValInt int\n\nconst (\n\t\/\/ HostNameCPK is the kubernetes host name label\n\tHostNameCPK CasPoolKey = \"kubernetes.io\/hostname\"\n\t\/\/ StoragePoolClaimCPK is the storage pool claim label\n\tStoragePoolClaimCPK CasPoolKey = \"openebs.io\/storage-pool-claim\"\n\t\/\/ CStorPoolClusterCPK is the CStorPoolcluster label\n\tCStorPoolClusterCPK CasPoolKey = \"openebs.io\/cstor-pool-cluster\"\n\t\/\/ NdmDiskTypeCPK is the node-disk-manager disk type e.g. 'sparse' or 'disk'\n\tNdmDiskTypeCPK CasPoolKey = \"ndm.io\/disk-type\"\n\t\/\/ NdmBlockDeviceTypeCPK is the node-disk-manager blockdevice type e.g. \/\/ 'blockdevice'\n\tNdmBlockDeviceTypeCPK CasPoolKey = \"ndm.io\/blockdevice-type\"\n\t\/\/ PoolTypeMirroredCPV is a key for mirrored for pool\n\tPoolTypeMirroredCPV CasPoolValString = \"mirrored\"\n\t\/\/ PoolTypeStripedCPV is a key for striped for pool\n\tPoolTypeStripedCPV CasPoolValString = \"striped\"\n\t\/\/ PoolTypeRaidzCPV is a key for raidz for pool\n\tPoolTypeRaidzCPV CasPoolValString = \"raidz\"\n\t\/\/ PoolTypeRaidz2CPV is a key for raidz for pool\n\tPoolTypeRaidz2CPV CasPoolValString = \"raidz2\"\n\t\/\/ TypeSparseCPV is a key for sparse disk pool\n\tTypeSparseCPV CasPoolValString = \"sparse\"\n\t\/\/ TypeDiskCPV is a key for physical,iscsi,virtual etc disk pool\n\tTypeDiskCPV CasPoolValString = \"disk\"\n\t\/\/ TypeBlockDeviceCPV is a key for physical,iscsi,virtual etc disk pool\n\tTypeBlockDeviceCPV CasPoolValString = \"blockdevice\"\n\t\/\/ StripedBlockDeviceCountCPV is the count for striped type pool\n\tStripedBlockDeviceCountCPV CasPoolValInt = 1\n\t\/\/ MirroredBlockDeviceCountCPV is the count for mirrored type pool\n\tMirroredBlockDeviceCountCPV CasPoolValInt = 2\n\t\/\/ RaidzBlockDeviceCountCPV is the count for raidz type pool\n\tRaidzBlockDeviceCountCPV CasPoolValInt = 3\n\t\/\/ Raidz2BlockDeviceCountCPV is the count for raidz2 type pool\n\tRaidz2BlockDeviceCountCPV CasPoolValInt = 6\n)\n\n\/\/ CasPool is a type which will be utilised by CAS engine to perform\n\/\/ storagepool related operation.\n\/\/ TODO: Restrucutre CasPool struct.\ntype CasPool struct {\n\tmetav1.TypeMeta   `json:\",inline\"`\n\tmetav1.ObjectMeta `json:\"metadata,omitempty\"`\n\t\/\/ StoragePoolClaim is the name of the storagepoolclaim object\n\tStoragePoolClaim string\n\n\t\/\/ CasCreateTemplate is the cas template that will be used for storagepool create\n\t\/\/ operation\n\tCasCreateTemplate string\n\n\t\/\/ CasDeleteTemplate is the cas template that will be used for storagepool delete\n\t\/\/ operation\n\tCasDeleteTemplate string\n\n\t\/\/ Namespace can be passed via storagepoolclaim as labels to decide on the\n\t\/\/ execution of namespaced resources with respect to storagepool\n\tNamespace string\n\n\t\/\/ BlockDeviceList is the list of block devices over which a storagepool will be provisioned\n\tBlockDeviceList []BlockDeviceGroup\n\n\t\/\/ PoolType is the type of pool to be provisioned e.g. striped or mirrored\n\tPoolType string\n\n\t\/\/ PoolCacheFile is cache file which used at the time of importing a pool\n\tPoolCacheFile string\n\n\t\/\/ MaxPool is the maximum number of pool that should be provisioned\n\tMaxPools int\n\n\t\/\/ MinPool is the minimum number of pool that should be provisioned\n\tMinPools int\n\n\t\/\/ Type is the CasPool type e.g. sparse or openebs-cstor\n\tType string\n\n\t\/\/ NodeName is the node where cstor pool will be created\n\tNodeName string\n\n\t\/\/ reSync will decide whether the event is a reconciliation event\n\tReSync bool\n\n\t\/\/ PendingPoolCount is the number of pools that will be tried for creation as a part of reconciliation.\n\tPendingPoolCount int\n\n\tDeviceID           []string\n\tAPIBlockDeviceList ndm.BlockDeviceList\n}\n","lang_cluster":"Go","length":118,"code_uid":"fb2d96af841f4f7cba0468815da47480"}
{"diff_hunk":"@@ -6,11 +6,16 @@ import (\n \t\"encoding\/json\"\n \n \t\"github.com\/influxdata\/flux\/ast\"\n+\t\"github.com\/influxdata\/flux\/internal\/parser\"\n \t\"github.com\/influxdata\/flux\/internal\/token\"\n \t\"github.com\/influxdata\/flux\/libflux\/go\/libflux\"\n )\n \n func parseFile(f *token.File, src []byte) (*ast.File, error) {\n+\tif !useRustParser() {\n+\t\treturn parser.ParseFile(f, src), nil\n+\t}\n+\n \tastFile := libflux.Parse(string(src))\n \tdefer astFile.Free()\n ","old_code":"\/\/ +build libflux\n\npackage parser\n\nimport (\n\t\"encoding\/json\"\n\n\t\"github.com\/influxdata\/flux\/ast\"\n\t\"github.com\/influxdata\/flux\/internal\/token\"\n\t\"github.com\/influxdata\/flux\/libflux\/go\/libflux\"\n)\n\nfunc parseFile(f *token.File, src []byte) (*ast.File, error) {\n\tastFile := libflux.Parse(string(src))\n\tdefer astFile.Free()\n\n\tdata, err := astFile.MarshalJSON()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar file ast.File\n\tif err := json.Unmarshal(data, &file); err != nil {\n\t\treturn nil, err\n\t}\n\tfile.Name = f.Name()\n\n\t\/\/ The go parser will not fill in the imports if there are\n\t\/\/ none so we remove them here to retain compatibility.\n\tif len(file.Imports) == 0 {\n\t\tfile.Imports = nil\n\t}\n\treturn &file, nil\n}\n","lang_cluster":"Go","length":34,"code_uid":"de4bccd601784286a1c02e39bee2ac15"}
{"diff_hunk":"@@ -48,20 +48,29 @@ const codeHeader = `\/\/ Copyright 2020 Chaos Mesh Authors.\n \/\/ limitations under the License.\n \n package v1alpha1\n+`\n \n+func main() {\n+\tgeneratedCode := codeHeader + `\n import (\n \t\"reflect\"\n \t\"time\"\n \n \tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n )\n-\n `\n+\tgeneratedTest := codeHeader + `\n+import (\n+\t\"reflect\"\n+\t\"testing\"\n+\t\"time\"\n \n-func main() {\n-\tgeneratedCode := codeHeader\n-\n+\t\"github.com\/bxcodec\/faker\"\n+\t. \"github.com\/onsi\/gomega\"\n+)\n+`\n \tinitImpl := \"\"\n+\n \tfilepath.Walk(\".\/api\/v1alpha1\", func(path string, info os.FileInfo, err error) error {\n \t\tlog := log.WithValues(\"file\", path)\n ","old_code":"\/\/ Copyright 2020 Chaos Mesh Authors.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage main\n\nimport (\n\t\"fmt\"\n\t\"go\/ast\"\n\t\"go\/parser\"\n\t\"go\/token\"\n\t\"os\"\n\t\"path\/filepath\"\n\t\"strings\"\n\n\t\"github.com\/pingcap\/errors\"\n\t\"sigs.k8s.io\/controller-runtime\/pkg\/log\/zap\"\n)\n\nvar (\n\tlog = zap.Logger(true)\n)\n\ntype metadata struct {\n\tType string\n}\n\nconst codeHeader = `\/\/ Copyright 2020 Chaos Mesh Authors.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\npackage v1alpha1\n\nimport (\n\t\"reflect\"\n\t\"time\"\n\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n)\n\n`\n\nfunc main() {\n\tgeneratedCode := codeHeader\n\n\tinitImpl := \"\"\n\tfilepath.Walk(\".\/api\/v1alpha1\", func(path string, info os.FileInfo, err error) error {\n\t\tlog := log.WithValues(\"file\", path)\n\n\t\tif err != nil {\n\t\t\tlog.Error(err, \"fail to walk in directory\")\n\t\t\treturn err\n\t\t}\n\t\tif info.IsDir() {\n\t\t\treturn nil\n\t\t}\n\n\t\tfset := token.NewFileSet()\n\t\tfile, err := parser.ParseFile(fset, path, nil, parser.ParseComments)\n\t\tif err != nil {\n\t\t\tlog.Error(err, \"fail to parse file\")\n\t\t\treturn err\n\t\t}\n\n\t\tcmap := ast.NewCommentMap(fset, file, file.Comments)\n\n\tout:\n\t\tfor node, commentGroups := range cmap {\n\t\t\tfor _, commentGroup := range commentGroups {\n\t\t\t\tvar err error\n\t\t\t\tfor _, comment := range commentGroup.List {\n\t\t\t\t\tif strings.Contains(comment.Text, \"+chaos-mesh:base\") {\n\t\t\t\t\t\tlog.Info(\"build\", \"pos\", fset.Position(comment.Pos()))\n\t\t\t\t\t\tbaseDecl, ok := node.(*ast.GenDecl)\n\t\t\t\t\t\tif !ok {\n\t\t\t\t\t\t\terr = errors.Errorf(\"node is not a *ast.GenDecl\")\n\t\t\t\t\t\t\tlog.Error(err, \"fail to get type\")\n\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif baseDecl.Tok != token.TYPE {\n\t\t\t\t\t\t\terr = errors.Errorf(\"node.Tok is not token.TYPE\")\n\t\t\t\t\t\t\tlog.Error(err, \"fail to get type\")\n\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tbaseType, ok := baseDecl.Specs[0].(*ast.TypeSpec)\n\t\t\t\t\t\tif !ok {\n\t\t\t\t\t\t\terr = errors.Errorf(\"node is not a *ast.TypeSpec\")\n\t\t\t\t\t\t\tlog.Error(err, \"fail to get type\")\n\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tgeneratedCode += generateImpl(baseType.Name.Name)\n\t\t\t\t\t\tinitImpl += generateInit(baseType.Name.Name)\n\t\t\t\t\t\tcontinue out\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n\n\tgeneratedCode += fmt.Sprintf(`\nfunc init() {\n%s\n}\n`, initImpl)\n\tfile, err := os.Create(\".\/api\/v1alpha1\/zz_generated.chaosmesh.go\")\n\tif err != nil {\n\t\tlog.Error(err, \"fail to create file\")\n\t}\n\tfmt.Fprint(file, generatedCode)\n\n\treturn\n}\n","lang_cluster":"Go","length":135,"code_uid":"a63de08b56d84b88a54aa9a55090a1a4"}
{"diff_hunk":"@@ -3,10 +3,8 @@ package admissioncontroller\n import (\n \t\"fmt\"\n \t\"net\/http\"\n-\t\"strings\"\n \n \tadmissionv1beta1 \"k8s.io\/api\/admission\/v1beta1\"\n-\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n \t\"k8s.io\/klog\/v2\"\n \n \trulesv1 \"github.com\/kubeedge\/kubeedge\/cloud\/pkg\/apis\/rules\/v1\"","old_code":"package admissioncontroller\n\nimport (\n\t\"fmt\"\n\t\"net\/http\"\n\t\"strings\"\n\n\tadmissionv1beta1 \"k8s.io\/api\/admission\/v1beta1\"\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\n\t\"k8s.io\/klog\/v2\"\n\n\trulesv1 \"github.com\/kubeedge\/kubeedge\/cloud\/pkg\/apis\/rules\/v1\"\n)\n\nfunc admitRuleEndpoint(review admissionv1beta1.AdmissionReview) *admissionv1beta1.AdmissionResponse {\n\treviewResponse := admissionv1beta1.AdmissionResponse{}\n\tvar msg string\n\tswitch review.Request.Operation {\n\tcase admissionv1beta1.Create:\n\t\traw := review.Request.Object.Raw\n\t\truleEndpoint := rulesv1.RuleEndpoint{}\n\t\tdeserializer := codecs.UniversalDeserializer()\n\t\tif _, _, err := deserializer.Decode(raw, nil, &ruleEndpoint); err != nil {\n\t\t\tklog.Errorf(\"validation failed with error: %v\", err)\n\t\t\tmsg = err.Error()\n\t\t\tbreak\n\t\t}\n\t\terr := validateRuleEndpoint(&ruleEndpoint)\n\t\tif err != nil {\n\t\t\tmsg = err.Error()\n\t\t\tbreak\n\t\t}\n\t\treviewResponse.Allowed = true\n\t\tklog.Info(\"admission validation passed!\")\n\tcase admissionv1beta1.Delete, admissionv1beta1.Connect:\n\t\t\/\/no rule defined for above operations, greenlight for all of above.\n\t\treviewResponse.Allowed = true\n\t\tklog.Info(\"admission validation passed!\")\n\tdefault:\n\t\tmsg = fmt.Sprintf(\"Unsupported webhook operation %v\", review.Request.Operation)\n\t\tklog.Warning(msg)\n\t}\n\tif !reviewResponse.Allowed {\n\t\treviewResponse.Result = &metav1.Status{Message: strings.TrimSpace(msg)}\n\t}\n\treturn &reviewResponse\n}\n\nfunc validateRuleEndpoint(ruleEndpoint *rulesv1.RuleEndpoint) error {\n\tswitch ruleEndpoint.Spec.RuleEndpointType {\n\tcase rulesv1.RuleEndpointTypeServiceBus:\n\t\t_, exist := ruleEndpoint.Spec.Properties[\"service_port\"]\n\t\tif !exist {\n\t\t\treturn fmt.Errorf(\"\\\"service_port\\\" property missed in property when ruleEndpoint is \\\"servicebus\\\"\")\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc serveRuleEndpoint(w http.ResponseWriter, r *http.Request) {\n\tserve(w, r, admitRuleEndpoint)\n}\n","lang_cluster":"Go","length":62,"code_uid":"89e45fdcd87348beb2a5a35fba20c467"}
{"diff_hunk":"@@ -3,49 +3,24 @@ package miner\n import (\n \t\"crypto\/ecdsa\"\n \n-\t\"github.com\/ccding\/go-stun\/stun\"\n-\tlog \"github.com\/noxiouz\/zapctx\/ctxlog\"\n \t\"github.com\/pkg\/errors\"\n-\t\"github.com\/sonm-io\/core\/insonmnia\/hardware\"\n+\t\"github.com\/sonm-io\/core\/insonmnia\/benchmarks\"\n \t\"github.com\/sonm-io\/core\/util\"\n \t\"golang.org\/x\/net\/context\"\n )\n \n type options struct {\n \tctx       context.Context\n-\thardware  hardware.Info\n-\tnat       stun.NATType\n \tovs       Overseer\n \tssh       SSH\n \tkey       *ecdsa.PrivateKey\n \tpublicIPs []string\n+\tbenchList benchmarks.BenchList\n }\n \n func (o *options) setupNetworkOptions(cfg Config) error {\n \tvar pubIPs []string\n \n-\t\/\/ Discover IP if we're behind a NAT.\n-\tif cfg.Firewall() != nil {\n-\t\tlog.G(o.ctx).Debug(\"discovering public IP address with NAT type, this might be slow\")\n-\n-\t\tclient := stun.NewClient()\n-\t\tif cfg.Firewall().Server != \"\" {\n-\t\t\tclient.SetServerAddr(cfg.Firewall().Server)\n-\t\t}\n-\n-\t\tnat, addr, err := client.Discover()\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tpubIPs = append(pubIPs, addr.IP())\n-\t\to.nat, o.publicIPs = nat, SortedIPs(pubIPs)\n-\n-\t\treturn nil\n-\t}\n-\n-\to.nat = stun.NATNone\n-\n \t\/\/ Use public IPs from config (if provided).\n \tpubIPs = cfg.PublicIPs()\n \tif len(pubIPs) > 0 {","old_code":"package miner\n\nimport (\n\t\"crypto\/ecdsa\"\n\n\t\"github.com\/ccding\/go-stun\/stun\"\n\tlog \"github.com\/noxiouz\/zapctx\/ctxlog\"\n\t\"github.com\/pkg\/errors\"\n\t\"github.com\/sonm-io\/core\/insonmnia\/hardware\"\n\t\"github.com\/sonm-io\/core\/util\"\n\t\"golang.org\/x\/net\/context\"\n)\n\ntype options struct {\n\tctx       context.Context\n\thardware  hardware.Info\n\tnat       stun.NATType\n\tovs       Overseer\n\tssh       SSH\n\tkey       *ecdsa.PrivateKey\n\tpublicIPs []string\n}\n\nfunc (o *options) setupNetworkOptions(cfg Config) error {\n\tvar pubIPs []string\n\n\t\/\/ Discover IP if we're behind a NAT.\n\tif cfg.Firewall() != nil {\n\t\tlog.G(o.ctx).Debug(\"discovering public IP address with NAT type, this might be slow\")\n\n\t\tclient := stun.NewClient()\n\t\tif cfg.Firewall().Server != \"\" {\n\t\t\tclient.SetServerAddr(cfg.Firewall().Server)\n\t\t}\n\n\t\tnat, addr, err := client.Discover()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tpubIPs = append(pubIPs, addr.IP())\n\t\to.nat, o.publicIPs = nat, SortedIPs(pubIPs)\n\n\t\treturn nil\n\t}\n\n\to.nat = stun.NATNone\n\n\t\/\/ Use public IPs from config (if provided).\n\tpubIPs = cfg.PublicIPs()\n\tif len(pubIPs) > 0 {\n\t\to.publicIPs = SortedIPs(pubIPs)\n\t\treturn nil\n\t}\n\n\t\/\/ Scan interfaces if there's no config and no NAT.\n\tsystemIPs, err := util.GetAvailableIPs()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, ip := range systemIPs {\n\t\tpubIPs = append(pubIPs, ip.String())\n\t}\n\tif len(pubIPs) > 0 {\n\t\to.publicIPs = SortedIPs(pubIPs)\n\t\treturn nil\n\t}\n\n\treturn errors.New(\"failed to get public IPs\")\n}\n\ntype Option func(*options)\n\nfunc WithContext(ctx context.Context) Option {\n\treturn func(opts *options) {\n\t\topts.ctx = ctx\n\t}\n}\n\nfunc WithHardware(hardwareInfo hardware.Info) Option {\n\treturn func(opts *options) {\n\t\topts.hardware = hardwareInfo\n\t}\n}\n\nfunc WithNat(nat stun.NATType) Option {\n\treturn func(opts *options) {\n\t\topts.nat = nat\n\t}\n}\n\nfunc WithOverseer(ovs Overseer) Option {\n\treturn func(opts *options) {\n\t\topts.ovs = ovs\n\t}\n}\n\nfunc WithSSH(ssh SSH) Option {\n\treturn func(opts *options) {\n\t\topts.ssh = ssh\n\t}\n}\n\nfunc WithKey(key *ecdsa.PrivateKey) Option {\n\treturn func(opts *options) {\n\t\topts.key = key\n\t}\n}\n\nfunc makeCgroupManager(cfg *ResourcesConfig) (cGroup, cGroupManager, error) {\n\tif !platformSupportCGroups || cfg == nil {\n\t\treturn newNilCgroupManager()\n\t}\n\treturn newCgroupManager(cfg.Cgroup, cfg.Resources)\n}\n","lang_cluster":"Go","length":116,"code_uid":"2322edc2d3fb4c12a6a210779a15370a"}
{"diff_hunk":"@@ -1,9 +1,11 @@\n package chain_test\n \n import (\n+\t\"context\"\n \t\"testing\"\n \n \t\"github.com\/stretchr\/testify\/assert\"\n+\t\"github.com\/stretchr\/testify\/require\"\n \n \t\"github.com\/filecoin-project\/go-filecoin\/chain\"\n \tth \"github.com\/filecoin-project\/go-filecoin\/testhelpers\"","old_code":"package chain_test\n\nimport (\n\t\"testing\"\n\n\t\"github.com\/stretchr\/testify\/assert\"\n\n\t\"github.com\/filecoin-project\/go-filecoin\/chain\"\n\tth \"github.com\/filecoin-project\/go-filecoin\/testhelpers\"\n\ttf \"github.com\/filecoin-project\/go-filecoin\/testhelpers\/testflags\"\n\t\"github.com\/filecoin-project\/go-filecoin\/types\"\n)\n\nfunc TestIsReorg(t *testing.T) {\n\ttf.UnitTest(t)\n\n\t\/\/ Only need dummy blocks for this test\n\tvar reorgGen types.Block\n\treorgGenTS := th.RequireNewTipSet(t, &reorgGen)\n\n\tmockSigner, _ := types.NewMockSignersAndKeyInfo(1)\n\tmockSignerPubKey := mockSigner.PubKeys[0]\n\n\tt.Run(\"if chain is a fork of another chain, IsReorg is true\", func(t *testing.T) {\n\t\tparams := th.FakeChildParams{\n\t\t\tMinerPubKey: mockSignerPubKey,\n\t\t\tSigner:      mockSigner,\n\t\t\tGenesisCid:  reorgGen.Cid(),\n\t\t\tStateRoot:   reorgGen.StateRoot}\n\t\tchn := th.RequireMkFakeChain(t, reorgGenTS, 10, params)\n\t\tcurHead := chn[len(chn)-1]\n\n\t\tparams.Nonce = uint64(32)\n\t\tforkChain := th.RequireMkFakeChain(t, reorgGenTS, 15, params)\n\t\tforkChain = append([]types.TipSet{reorgGenTS}, forkChain...)\n\t\tassert.True(t, chain.IsReorg(curHead, forkChain))\n\t})\n\n\tt.Run(\"if new chain has existing chain as prefix, IsReorg is false\", func(t *testing.T) {\n\t\tparams := th.FakeChildParams{\n\t\t\tMinerPubKey: mockSignerPubKey,\n\t\t\tSigner:      mockSigner,\n\t\t\tGenesisCid:  reorgGen.Cid(),\n\t\t\tStateRoot:   reorgGen.StateRoot}\n\t\tchn := th.RequireMkFakeChain(t, reorgGenTS, 20, params)\n\t\tcurHead := chn[10]\n\n\t\tassert.False(t, chain.IsReorg(curHead, chn))\n\t})\n\n\tt.Run(\"if chain has head that is a subset of new chain head, IsReorg is false\", func(t *testing.T) {\n\t\tparams := th.FakeChildParams{\n\t\t\tGenesisCid:  reorgGen.Cid(),\n\t\t\tMinerPubKey: mockSignerPubKey,\n\t\t\tSigner:      mockSigner,\n\t\t\tStateRoot:   reorgGen.StateRoot}\n\t\tchn := th.RequireMkFakeChain(t, reorgGenTS, 10, params)\n\t\tcurHead := chn[len(chn)-1]\n\t\theadBlock := curHead.ToSlice()[0]\n\t\tblock2 := th.RequireMkFakeChild(t, th.FakeChildParams{\n\t\t\tParent:      chn[len(chn)-2],\n\t\t\tMinerPubKey: mockSignerPubKey,\n\t\t\tSigner:      mockSigner,\n\t\t\tGenesisCid:  reorgGen.Cid(),\n\t\t\tStateRoot:   reorgGen.StateRoot})\n\t\tsuperset := th.RequireNewTipSet(t, headBlock, block2)\n\t\tchn[len(chn)-1] = superset\n\n\t\tassert.False(t, chain.IsReorg(curHead, chn))\n\t})\n}\n","lang_cluster":"Go","length":71,"code_uid":"5fee309e986144d1b3406e869d312813"}
{"diff_hunk":"@@ -5,11 +5,12 @@\n package ens\n \n import (\n+\t\"bytes\"\n \t\"errors\"\n \t\"fmt\"\n \t\"strings\"\n \n-\t\"github.com\/ethereum\/go-ethereum\/accounts\/abi\/bind\"\n+\t\"github.com\/ethereum\/go-ethereum\/common\"\n \t\"github.com\/ethereum\/go-ethereum\/ethclient\"\n \tgoens \"github.com\/wealdtech\/go-ens\/v3\"\n ","old_code":"\/\/ Copyright 2020 The Swarm Authors. All rights reserved.\n\/\/ Use of this source code is governed by a BSD-style\n\/\/ license that can be found in the LICENSE file.\n\npackage ens\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com\/ethereum\/go-ethereum\/accounts\/abi\/bind\"\n\t\"github.com\/ethereum\/go-ethereum\/ethclient\"\n\tgoens \"github.com\/wealdtech\/go-ens\/v3\"\n\n\t\"github.com\/ethersphere\/bee\/pkg\/resolver\/client\"\n\t\"github.com\/ethersphere\/bee\/pkg\/swarm\"\n)\n\nconst swarmContentHashPrefix = \"\/swarm\/\"\n\n\/\/ Address is the swarm bzz address.\ntype Address = swarm.Address\n\n\/\/ Make sure Client implements the resolver.Client interface.\nvar _ client.Interface = (*Client)(nil)\n\nvar (\n\t\/\/ ErrFailedToConnect denotes that the resolver failed to connect to the\n\t\/\/ provided endpoint.\n\tErrFailedToConnect = errors.New(\"failed to connect\")\n\t\/\/ ErrResolveFailed denotes that a name could not be resolved.\n\tErrResolveFailed = errors.New(\"resolve failed\")\n\t\/\/ ErrInvalidContentHash denotes that the value of the contenthash record is\n\t\/\/ not valid.\n\tErrInvalidContentHash = errors.New(\"invalid swarm content hash\")\n\t\/\/ errNotImplemented denotes that the function has not been implemented.\n\terrNotImplemented = errors.New(\"function not implemented\")\n)\n\n\/\/ Client is a name resolution client that can connect to ENS via an\n\/\/ Ethereum endpoint.\ntype Client struct {\n\tendpoint  string\n\tethCl     *ethclient.Client\n\tdialFn    func(string) (*ethclient.Client, error)\n\tresolveFn func(bind.ContractBackend, string) (string, error)\n}\n\n\/\/ Option is a function that applies an option to a Client.\ntype Option func(*Client)\n\n\/\/ NewClient will return a new Client.\nfunc NewClient(endpoint string, opts ...Option) (client.Interface, error) {\n\tc := &Client{\n\t\tendpoint:  endpoint,\n\t\tdialFn:    ethclient.Dial,\n\t\tresolveFn: wrapResolve,\n\t}\n\n\t\/\/ Apply all options to the Client.\n\tfor _, o := range opts {\n\t\to(c)\n\t}\n\n\t\/\/ Connect to the name resolution service.\n\tif c.dialFn == nil {\n\t\treturn nil, fmt.Errorf(\"dialFn: %w\", errNotImplemented)\n\t}\n\n\tethCl, err := c.dialFn(c.endpoint)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"%v: %w\", err, ErrFailedToConnect)\n\t}\n\tc.ethCl = ethCl\n\n\treturn c, nil\n}\n\n\/\/ IsConnected returns true if there is an active RPC connection with an\n\/\/ Ethereum node at the configured endpoint.\nfunc (c *Client) IsConnected() bool {\n\treturn c.ethCl != nil\n}\n\n\/\/ Endpoint returns the endpoint the client was connected to.\nfunc (c *Client) Endpoint() string {\n\treturn c.endpoint\n}\n\n\/\/ Resolve implements the resolver.Client interface.\nfunc (c *Client) Resolve(name string) (Address, error) {\n\tif c.resolveFn == nil {\n\t\treturn swarm.ZeroAddress, fmt.Errorf(\"resolveFn: %w\", errNotImplemented)\n\t}\n\n\thash, err := c.resolveFn(c.ethCl, name)\n\tif err != nil {\n\t\treturn swarm.ZeroAddress, fmt.Errorf(\"%v: %w\", err, ErrResolveFailed)\n\t}\n\n\t\/\/ Ensure that the content hash string is in a valid format, eg.\n\t\/\/ \"\/swarm\/<address>\".\n\tif !strings.HasPrefix(hash, swarmContentHashPrefix) {\n\t\treturn swarm.ZeroAddress, fmt.Errorf(\"contenthash %s: %w\", hash, ErrInvalidContentHash)\n\t}\n\n\t\/\/ Trim the prefix and try to parse the result as a bzz address.\n\treturn swarm.ParseHexAddress(strings.TrimPrefix(hash, swarmContentHashPrefix))\n}\n\n\/\/ Close closes the RPC connection with the client, terminating all unfinished\n\/\/ requests. If the connection is already closed, this call is a noop.\nfunc (c *Client) Close() error {\n\tif c.ethCl != nil {\n\t\tc.ethCl.Close()\n\n\t}\n\tc.ethCl = nil\n\n\treturn nil\n}\n\nfunc wrapResolve(backend bind.ContractBackend, name string) (string, error) {\n\n\t\/\/ Connect to the ENS resolver for the provided name.\n\tensR, err := goens.NewResolver(backend, name)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t\/\/ Try and read out the content hash record.\n\tch, err := ensR.Contenthash()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn goens.ContenthashToString(ch)\n}\n","lang_cluster":"Go","length":139,"code_uid":"37504dda08864391b21322fc903bcaad"}
{"diff_hunk":"@@ -29,8 +29,10 @@ limitations under the License.\n \/\/ limitations under the License.\n \n Modifies:\n-- Remove interface \"Provider\"\n-- Remove import \"k8s.io\/kubernetes\/pkg\/proxy\/config\"\n+- Replace import \"k8s.io\/kubernetes\/pkg\/proxy\/config\" with \"github.com\/vmware-tanzu\/antrea\/third_party\/proxy\/config\"\n+- Remove config.EndpointSliceHandler, config.NodeHandler from Provider interface type\n+- Remove NodeHandler, EndpointSliceHandler, Sync() from Provider interface\n+- Add Run(), GetServiceByIP() to Provider interface\n *\/\n \n package proxy","old_code":"\/*\nCopyright 2015 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*\/\n\/*\n\/\/ Copyright 2020 Antrea Authors\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\nModifies:\n- Remove interface \"Provider\"\n- Remove import \"k8s.io\/kubernetes\/pkg\/proxy\/config\"\n*\/\n\npackage proxy\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\n\tv1 \"k8s.io\/api\/core\/v1\"\n\t\"k8s.io\/apimachinery\/pkg\/types\"\n)\n\n\/\/ ServicePortName carries a namespace + name + portname.  This is the unique\n\/\/ identifier for a load-balanced service.\ntype ServicePortName struct {\n\ttypes.NamespacedName\n\tPort     string\n\tProtocol v1.Protocol\n}\n\nfunc (spn ServicePortName) String() string {\n\treturn fmt.Sprintf(\"%s:%s\", spn.NamespacedName.String(), spn.Port)\n}\n\n\/\/ ServicePort is an interface which abstracts information about a service.\ntype ServicePort interface {\n\t\/\/ String returns service string.  An example format can be: `IP:Port\/Protocol`.\n\tString() string\n\t\/\/ GetClusterIP returns service cluster IP in net.IP format.\n\tClusterIP() net.IP\n\t\/\/ GetPort returns service port if present. If return 0 means not present.\n\tPort() int\n\t\/\/ GetSessionAffinityType returns service session affinity type\n\tSessionAffinityType() v1.ServiceAffinity\n\t\/\/ GetStickyMaxAgeSeconds returns service max connection age\n\tStickyMaxAgeSeconds() int\n\t\/\/ ExternalIPStrings returns service ExternalIPs as a string array.\n\tExternalIPStrings() []string\n\t\/\/ LoadBalancerIPStrings returns service LoadBalancerIPs as a string array.\n\tLoadBalancerIPStrings() []string\n\t\/\/ GetProtocol returns service protocol.\n\tProtocol() v1.Protocol\n\t\/\/ LoadBalancerSourceRanges returns service LoadBalancerSourceRanges if present empty array if not\n\tLoadBalancerSourceRanges() []string\n\t\/\/ GetHealthCheckNodePort returns service health check node port if present.  If return 0, it means not present.\n\tHealthCheckNodePort() int\n\t\/\/ GetNodePort returns a service Node port if present. If return 0, it means not present.\n\tNodePort() int\n\t\/\/ GetOnlyNodeLocalEndpoints returns if a service has only node local endpoints\n\tOnlyNodeLocalEndpoints() bool\n\t\/\/ TopologyKeys returns service TopologyKeys as a string array.\n\tTopologyKeys() []string\n}\n\n\/\/ Endpoint in an interface which abstracts information about an endpoint.\n\/\/ TODO: Rename functions to be consistent with ServicePort.\ntype Endpoint interface {\n\t\/\/ String returns endpoint string.  An example format can be: `IP:Port`.\n\t\/\/ We take the returned value as ServiceEndpoint.Endpoint.\n\tString() string\n\t\/\/ GetIsLocal returns true if the endpoint is running in same host as kube-proxy, otherwise returns false.\n\tGetIsLocal() bool\n\t\/\/ GetTopology returns the topology information of the endpoint.\n\tGetTopology() map[string]string\n\t\/\/ IP returns IP part of the endpoint.\n\tIP() string\n\t\/\/ Port returns the Port part of the endpoint.\n\tPort() (int, error)\n\t\/\/ Equal checks if two endpoints are equal.\n\tEqual(Endpoint) bool\n}\n\n\/\/ ServiceEndpoint is used to identify a service and one of its endpoint pair.\ntype ServiceEndpoint struct {\n\tEndpoint        string\n\tServicePortName ServicePortName\n}\n","lang_cluster":"Go","length":110,"code_uid":"89e06cbe492841e287b137a09ab0567f"}
{"diff_hunk":"@@ -41,4 +41,8 @@ public interface Accountable {\n     return Collections.emptyList();\n   }\n \n+  \/**\n+   * An accountable that always returns 0\n+   *\/\n+  Accountable NULL_ACCOUNTABLE = () -> 0;\n }","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage org.apache.lucene.util;\n\n\nimport java.util.Collection;\nimport java.util.Collections;\n\n\/**\n * An object whose RAM usage can be computed.\n *\n * @lucene.internal\n *\/\npublic interface Accountable {\n\n  \/**\n   * Return the memory usage of this object in bytes. Negative values are illegal.\n   *\/\n  long ramBytesUsed();\n\n  \/**\n   * Returns nested resources of this class. \n   * The result should be a point-in-time snapshot (to avoid race conditions).\n   * @see Accountables\n   *\/\n  default Collection<Accountable> getChildResources() {\n    return Collections.emptyList();\n  }\n\n}\n","lang_cluster":"Java","length":44,"code_uid":"0881320141f24468a2419305a796eb10"}
{"diff_hunk":"@@ -72,12 +72,29 @@ public class AppsController {\n         });\n \n         this.view.setOnSelectScript(\n-                scriptDTO -> scriptInterpreter.runScript(scriptDTO.getScript(), e -> Platform.runLater(() -> {\n-                    \/\/ no exception if installation is cancelled\n-                    if (!(e.getCause() instanceof InterruptedException)) {\n-                        new ErrorMessage(tr(\"The script ended unexpectedly\"), e, this.view);\n-                    }\n-                })));\n+                scriptDTO -> {\n+                    final StringBuilder environmentBuilder = new StringBuilder();\n+                    environmentBuilder.append(\"TYPE_ID=\\\"\");\n+                    environmentBuilder.append(scriptDTO.getTypeId());\n+                    environmentBuilder.append(\"\\\";\\n\");\n+                    environmentBuilder.append(\"CATEGORY_ID=\\\"\");\n+                    environmentBuilder.append(scriptDTO.getCategoryId());\n+                    environmentBuilder.append(\"\\\";\\n\");\n+                    environmentBuilder.append(\"APPLICATION_ID=\\\"\");\n+                    environmentBuilder.append(scriptDTO.getApplicationId());\n+                    environmentBuilder.append(\"\\\";\\n\");\n+                    environmentBuilder.append(\"SCRIPT_ID=\\\"\");\n+                    environmentBuilder.append(scriptDTO.getId());\n+                    environmentBuilder.append(\"\\\";\\n\");\n+                    final String environment = environmentBuilder.toString();\n+                    final String execute = environment + scriptDTO.getScript();\n+                    scriptInterpreter.runScript(execute, e -> Platform.runLater(() -> {\n+                        \/\/ no exception if installation is cancelled\n+                        if (!(e.getCause() instanceof InterruptedException)) {\n+                            new ErrorMessage(tr(\"The script ended unexpectedly\"), e, this.view);\n+                        }\n+                    }));\n+                });\n \n         onAppLoaded.run();\n     }","old_code":"\/*\n * Copyright (C) 2015-2017 P\u00c2RIS Quentin\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n *\/\n\npackage org.phoenicis.javafx.controller.apps;\n\nimport javafx.application.Platform;\nimport org.phoenicis.javafx.views.common.ErrorMessage;\nimport org.phoenicis.javafx.views.common.ThemeManager;\nimport org.phoenicis.javafx.views.mainwindow.apps.ApplicationsView;\nimport org.phoenicis.repository.RepositoryManager;\nimport org.phoenicis.repository.dto.CategoryDTO;\nimport org.phoenicis.repository.dto.RepositoryDTO;\nimport org.phoenicis.scripts.interpreter.ScriptInterpreter;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.net.URI;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.List;\nimport java.util.Optional;\n\nimport static org.phoenicis.configuration.localisation.Localisation.tr;\n\npublic class AppsController {\n    private final org.slf4j.Logger LOGGER = LoggerFactory.getLogger(AppsController.class);\n    private final ApplicationsView view;\n    private final RepositoryManager repositoryManager;\n    private final ScriptInterpreter scriptInterpreter;\n    private ThemeManager themeManager;\n\n    private Runnable onAppLoaded = () -> {\n    };\n\n    public AppsController(ApplicationsView view, RepositoryManager repositoryManager,\n            ScriptInterpreter scriptInterpreter,\n            ThemeManager themeManager) {\n        this.view = view;\n        this.repositoryManager = repositoryManager;\n        this.scriptInterpreter = scriptInterpreter;\n        this.themeManager = themeManager;\n\n        this.repositoryManager.addCallbacks(this::populateView,\n                e -> Platform.runLater(() -> view.showFailure(\n                        tr(\"Connecting to the repository failed.\\nPlease check your connection and try again.\"),\n                        Optional.of(e))));\n    }\n\n    public void loadApps() {\n        this.view.showWait();\n        this.repositoryManager.triggerRepositoryChange();\n\n        this.view.setOnRetryButtonClicked(event -> {\n            this.view.showWait();\n            this.repositoryManager.triggerRepositoryChange();\n        });\n\n        this.view.setOnSelectScript(\n                scriptDTO -> scriptInterpreter.runScript(scriptDTO.getScript(), e -> Platform.runLater(() -> {\n                    \/\/ no exception if installation is cancelled\n                    if (!(e.getCause() instanceof InterruptedException)) {\n                        new ErrorMessage(tr(\"The script ended unexpectedly\"), e, this.view);\n                    }\n                })));\n\n        onAppLoaded.run();\n    }\n\n    public void setOnAppLoaded(Runnable onAppLoaded) {\n        this.onAppLoaded = onAppLoaded;\n    }\n\n    public ApplicationsView getView() {\n        return view;\n    }\n\n    private void populateView(RepositoryDTO repositoryDTO) {\n        Platform.runLater(() -> {\n            List<CategoryDTO> categoryDTOS = repositoryDTO.getTypes().get(0).getCategories();\n            setDefaultCategoryIcons(categoryDTOS);\n            this.view.populate(categoryDTOS);\n        });\n    }\n\n    private void setDefaultCategoryIcons(List<CategoryDTO> categoryDTOS) {\n        try {\n            StringBuilder cssBuilder = new StringBuilder();\n            for (CategoryDTO category : categoryDTOS) {\n                cssBuilder.append(\"#\" + category.getId().toLowerCase() + \"Button{\\n\");\n                URI categoryIcon = category.getIcon();\n                if (categoryIcon == null) {\n                    cssBuilder\n                            .append(\"-fx-background-image: url('\/org\/phoenicis\/javafx\/views\/common\/phoenicis.png');\\n\");\n                } else {\n                    cssBuilder.append(\"-fx-background-image: url('\" + categoryIcon + \"');\\n\");\n                }\n                cssBuilder.append(\"}\\n\");\n            }\n            String css = cssBuilder.toString();\n            Path temp = Files.createTempFile(\"defaultCategoryIcons\", \".css\").toAbsolutePath();\n            File tempFile = temp.toFile();\n            tempFile.deleteOnExit();\n            Files.write(temp, css.getBytes());\n            String defaultCategoryIconsCss = temp.toUri().toString();\n            themeManager.setDefaultCategoryIconsCss(defaultCategoryIconsCss);\n        } catch (IOException e) {\n            LOGGER.warn(\"Could not set default category icons.\", e);\n        }\n    }\n}\n","lang_cluster":"Java","length":126,"code_uid":"d7397c0b72294302ae18d3d69ffda7f3"}
{"diff_hunk":"@@ -102,7 +102,7 @@ class ExternalDriverSupplier implements Supplier<WebDriver> {\n     Optional<Class<? extends Supplier<WebDriver>>> supplierClass = getDelegateClass();\n     if (supplierClass.isPresent()) {\n       Class<? extends Supplier<WebDriver>> clazz = supplierClass.get();\n-      logger.info(\"Using delegate supplier: \" + clazz.getName());\n+      logger.finest(\"Using delegate supplier: \" + clazz.getName());\n       try {\n         @SuppressWarnings(\"unchecked\")\n         Constructor<Supplier<WebDriver>> ctor =","old_code":"\/\/ Licensed to the Software Freedom Conservancy (SFC) under one\n\/\/ or more contributor license agreements.  See the NOTICE file\n\/\/ distributed with this work for additional information\n\/\/ regarding copyright ownership.  The SFC licenses this file\n\/\/ to you under the Apache License, Version 2.0 (the\n\/\/ \"License\"); you may not use this file except in compliance\n\/\/ with the License.  You may obtain a copy of the License at\n\/\/\n\/\/   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing,\n\/\/ software distributed under the License is distributed on an\n\/\/ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\/\/ KIND, either express or implied.  See the License for the\n\/\/ specific language governing permissions and limitations\n\/\/ under the License.\n\npackage org.openqa.selenium.testing.drivers;\n\nimport static java.util.concurrent.TimeUnit.SECONDS;\n\nimport com.google.common.base.Suppliers;\n\nimport org.openqa.selenium.Capabilities;\nimport org.openqa.selenium.ImmutableCapabilities;\nimport org.openqa.selenium.WebDriver;\nimport org.openqa.selenium.net.UrlChecker;\nimport org.openqa.selenium.remote.LocalFileDetector;\nimport org.openqa.selenium.remote.RemoteWebDriver;\n\nimport java.lang.reflect.Constructor;\nimport java.lang.reflect.InvocationTargetException;\nimport java.net.MalformedURLException;\nimport java.net.URL;\nimport java.util.Optional;\nimport java.util.function.Supplier;\nimport java.util.logging.Logger;\n\n\/**\n * Supports providing WebDriver instances from an external source using the following system\n * properties:\n * <dl>\n *   <dt>selenium.external.serverUrl<\/dt>\n *   <dd>Defines the fully qualified URL of an external WebDriver server to send commands to.\n *       This server <i>must<\/i> be compliant with the\n *       <a href=\"https:\/\/github.com\/SeleniumHQ\/selenium\/wiki\/JsonWireProtocol\">JSON wire protocol<\/a>.\n *       If only this property is provided, then this supplier will provide a new\n *       {@link RemoteWebDriver} instance pointed at the designated server. Otherwise, if a\n *       custom supplier is also defined (see below), this supplier will wait for the server to\n *       be accepting commands before delegating to the designated class for the actual client\n *       creation.\n *   <\/dd>\n *   <dt>selenium.external.supplierClass<\/dt>\n *   <dd>Specifies the fully qualified name of another class on the classpath. This class must\n *       implement {@code Supplier<WebDriver>} and have a public constructor that accepts two\n *       {@link Capabilities} objects as arguments (for the desired and required capabilities,\n *       respectively).\n *   <\/dd>\n * <\/dl>\n *\/\nclass ExternalDriverSupplier implements Supplier<WebDriver> {\n  private static final Logger logger = Logger.getLogger(ExternalDriverSupplier.class.getName());\n\n  private static final String DELEGATE_SUPPLIER_CLASS_PROPERTY = \"selenium.external.supplierClass\";\n  private static final String EXTERNAL_SERVER_URL_PROPERTY = \"selenium.external.serverUrl\";\n\n  private final Capabilities desiredCapabilities;\n\n  ExternalDriverSupplier(Capabilities desiredCapabilities) {\n    this.desiredCapabilities = new ImmutableCapabilities(desiredCapabilities);\n  }\n\n  @Override\n  public WebDriver get() {\n    Optional<Supplier<WebDriver>> delegate = createDelegate(desiredCapabilities);\n    delegate = createForExternalServer(desiredCapabilities, delegate);\n\n    return delegate.orElse(Suppliers.ofInstance(null)).get();\n  }\n\n  private static Optional<Supplier<WebDriver>> createForExternalServer(\n      Capabilities desiredCapabilities,\n      Optional<Supplier<WebDriver>> delegate) {\n    String externalUrl = System.getProperty(EXTERNAL_SERVER_URL_PROPERTY);\n    if (externalUrl != null) {\n      logger.info(\"Using external WebDriver server: \" + externalUrl);\n      URL url;\n      try {\n        url = new URL(externalUrl);\n      } catch (MalformedURLException e) {\n        throw new RuntimeException(\"Invalid server URL: \" + externalUrl, e);\n      }\n      Supplier<WebDriver> defaultSupplier = new DefaultRemoteSupplier(url, desiredCapabilities);\n      Supplier<WebDriver> supplier = new ExternalServerDriverSupplier(\n          url, delegate.orElse(defaultSupplier));\n      return Optional.of(supplier);\n    }\n    return delegate;\n  }\n\n  private static Optional<Supplier<WebDriver>> createDelegate(Capabilities desiredCapabilities) {\n    Optional<Class<? extends Supplier<WebDriver>>> supplierClass = getDelegateClass();\n    if (supplierClass.isPresent()) {\n      Class<? extends Supplier<WebDriver>> clazz = supplierClass.get();\n      logger.info(\"Using delegate supplier: \" + clazz.getName());\n      try {\n        @SuppressWarnings(\"unchecked\")\n        Constructor<Supplier<WebDriver>> ctor =\n            (Constructor<Supplier<WebDriver>>) clazz.getConstructor(Capabilities.class);\n        return Optional.of(ctor.newInstance(desiredCapabilities));\n      } catch (InvocationTargetException e) {\n        throw new RuntimeException(e.getTargetException());\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n    }\n    return Optional.empty();\n  }\n\n  @SuppressWarnings(\"unchecked\")\n  private static Optional<Class<? extends Supplier<WebDriver>>> getDelegateClass() {\n    String delegateClassName = System.getProperty(DELEGATE_SUPPLIER_CLASS_PROPERTY);\n    if (delegateClassName != null) {\n      try {\n        logger.info(\"Loading custom supplier: \" + delegateClassName);\n        Class<? extends Supplier<WebDriver>> clazz =\n            (Class<? extends Supplier<WebDriver>>) Class.forName(delegateClassName);\n        return Optional.of(clazz);\n      } catch (Exception e) {\n        throw new RuntimeException(e);\n      }\n    }\n    return Optional.empty();\n  }\n\n  \/**\n   * Waits for an external WebDriver server to be ready before delegating to another supplier\n   * for driver creation.\n   *\/\n  private static class ExternalServerDriverSupplier implements Supplier<WebDriver> {\n\n    private final URL serverUrl;\n    private final Supplier<WebDriver> delegateSupplier;\n\n    private ExternalServerDriverSupplier(\n        URL serverUrl, Supplier<WebDriver> delegateSupplier) {\n      this.serverUrl = serverUrl;\n      this.delegateSupplier = delegateSupplier;\n    }\n\n    @Override\n    public WebDriver get() {\n      try {\n        logger.info(\"Waiting for server to be ready at \" + serverUrl);\n        new UrlChecker().waitUntilAvailable(60, SECONDS, new URL(serverUrl + \"\/status\"));\n        logger.info(\"Server is ready\");\n      } catch (UrlChecker.TimeoutException e) {\n        throw new RuntimeException(\"The external server is not accepting commands\", e);\n      } catch (MalformedURLException e) {\n        throw new RuntimeException(e);\n      }\n      return delegateSupplier.get();\n    }\n  }\n\n  \/**\n   * Creates basic {@link RemoteWebDriver} instances.\n   *\/\n  private static class DefaultRemoteSupplier implements Supplier<WebDriver> {\n    private final URL url;\n    private final Capabilities desiredCapabilities;\n\n    private DefaultRemoteSupplier(URL url, Capabilities desiredCapabilities) {\n      this.url = url;\n      this.desiredCapabilities = desiredCapabilities;\n    }\n\n    @Override\n    public WebDriver get() {\n      RemoteWebDriver driver = new RemoteWebDriver(url, desiredCapabilities);\n      driver.setFileDetector(new LocalFileDetector());\n      return driver;\n    }\n  }\n}\n","lang_cluster":"Java","length":185,"code_uid":"3ffce1e77a754c9cb2133db2a344e0c1"}
{"diff_hunk":"@@ -54,8 +54,8 @@ public class SDKInfoPluginTest extends InstrumentationTestCase {\n \t\tContext ctx = getInstrumentation().getTargetContext();\n \t\tJSONObject sdkInfo = SDKInfoPlugin.getSDKInfo(ctx);\n \t\tBootConfig bootconfig = BootConfig.getBootConfig(ctx);\n-\t\tassertEquals(\"Wrong app name\", \"SalesforceSDKTest\", sdkInfo.getString(\"appName\"));\n-\t\tassertEquals(\"Wrong app version\", \"1.0\", sdkInfo.getString(\"appVersion\"));\n+\t\tassertEquals(\"Wrong app name\", \"\", sdkInfo.getString(\"appName\"));\n+\t\tassertEquals(\"Wrong app version\", \"\", sdkInfo.getString(\"appVersion\"));\n \t\tList<String> sdkInfoPlugins = toList(sdkInfo.getJSONArray(\"forcePluginsAvailable\"));\n \t\tassertEquals(\"Wrong number of plugins\", 3, sdkInfoPlugins.size());\n \t\tassertTrue(\"oauth plugin should have been returned\", sdkInfoPlugins.contains(\"com.salesforce.oauth\"));","old_code":"\/*\n * Copyright (c) 2012, salesforce.com, inc.\n * All rights reserved.\n * Redistribution and use of this software in source and binary forms, with or\n * without modification, are permitted provided that the following conditions\n * are met:\n * - Redistributions of source code must retain the above copyright notice, this\n * list of conditions and the following disclaimer.\n * - Redistributions in binary form must reproduce the above copyright notice,\n * this list of conditions and the following disclaimer in the documentation\n * and\/or other materials provided with the distribution.\n * - Neither the name of salesforce.com, inc. nor the names of its contributors\n * may be used to endorse or promote products derived from this software without\n * specific prior written permission of salesforce.com, inc.\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n *\/\npackage com.salesforce.androidsdk.phonegap;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.json.JSONArray;\nimport org.json.JSONException;\nimport org.json.JSONObject;\n\nimport com.salesforce.androidsdk.app.SalesforceSDKManager;\nimport com.salesforce.androidsdk.rest.BootConfig;\n\nimport android.content.Context;\nimport android.content.pm.PackageManager.NameNotFoundException;\nimport android.test.InstrumentationTestCase;\n\n\n\/**\n * Tests for SDKInfoPlugin\n *\n *\/\npublic class SDKInfoPluginTest extends InstrumentationTestCase {\n\n\t\/**\n\t * Test for getSDKInfo\n\t *\/\n\tpublic void testGetSDKInfo() throws NameNotFoundException, JSONException {\n\t\tContext ctx = getInstrumentation().getTargetContext();\n\t\tJSONObject sdkInfo = SDKInfoPlugin.getSDKInfo(ctx);\n\t\tBootConfig bootconfig = BootConfig.getBootConfig(ctx);\n\t\tassertEquals(\"Wrong app name\", \"SalesforceSDKTest\", sdkInfo.getString(\"appName\"));\n\t\tassertEquals(\"Wrong app version\", \"1.0\", sdkInfo.getString(\"appVersion\"));\n\t\tList<String> sdkInfoPlugins = toList(sdkInfo.getJSONArray(\"forcePluginsAvailable\"));\n\t\tassertEquals(\"Wrong number of plugins\", 3, sdkInfoPlugins.size());\n\t\tassertTrue(\"oauth plugin should have been returned\", sdkInfoPlugins.contains(\"com.salesforce.oauth\"));\n\t\tassertTrue(\"sdkinfo plugin should have been returned\", sdkInfoPlugins.contains(\"com.salesforce.sdkinfo\"));\n\t\tassertTrue(\"sfaccountmanager plugin should have been returned\", sdkInfoPlugins.contains(\"com.salesforce.sfaccountmanager\"));\n\t\tassertEquals(\"Wrong version\", SalesforceSDKManager.SDK_VERSION, sdkInfo.getString(\"sdkVersion\"));\n\t\n\t\tJSONObject sdkInfoBootConfig = sdkInfo.getJSONObject(\"bootConfig\");\n\t\tassertEquals(\"Wrong bootconfig shouldAuthenticate\", bootconfig.shouldAuthenticate(), sdkInfoBootConfig.getBoolean(\"shouldAuthenticate\"));\n\t\tassertEquals(\"Wrong bootconfig attemptOfflineLoad\", bootconfig.attemptOfflineLoad(), sdkInfoBootConfig.getBoolean(\"attemptOfflineLoad\"));\n\t\tassertEquals(\"Wrong bootconfig isLocal\", bootconfig.isLocal(), sdkInfoBootConfig.getBoolean(\"isLocal\"));\n\t\tList<String> sdkInfoOAuthScopes = toList(sdkInfoBootConfig.getJSONArray(\"oauthScopes\"));\n\t\tassertEquals(\"Wrong bootconfig oauthScopes\", 1, sdkInfoOAuthScopes.size());\n\t\tassertTrue(\"Wrong bootconfig oauthScopes\", sdkInfoOAuthScopes.contains(\"api\"));\n\t\tassertEquals(\"Wrong bootconfig oauthRedirectURI\", bootconfig.getOauthRedirectURI(), sdkInfoBootConfig.getString(\"oauthRedirectURI\"));\n\t\tassertEquals(\"Wrong bootconfig remoteAccessConsumerKey\", bootconfig.getRemoteAccessConsumerKey(), sdkInfoBootConfig.getString(\"remoteAccessConsumerKey\"));\n\t\tassertEquals(\"Wrong bootconfig androidPushNotificationClientId\", bootconfig.getPushNotificationClientId(), sdkInfoBootConfig.getString(\"androidPushNotificationClientId\"));\n\t\tassertEquals(\"Wrong bootconfig startPage\", \"\", sdkInfoBootConfig.optString(\"startPage\")); \/\/ this is a native app\n\t\tassertEquals(\"Wrong bootconfig errorPage\", \"\", sdkInfoBootConfig.optString(\"errorPage\")); \/\/ this is a native app\n\t}\n\n\t\/**\n\t * Test for getForcePluginsFromXML\n\t *\/\n\tpublic void testGetForcePluginsFromXML() {\n\t\tList<String> plugins = SDKInfoPlugin.getForcePluginsFromXML(getInstrumentation().getTargetContext());\n\t\tassertEquals(\"Wrong number of force plugins\", 3, plugins.size());\n\t\tassertTrue(\"oauth plugin should have been returned\", plugins.contains(\"com.salesforce.oauth\"));\n\t\tassertTrue(\"sdkinfo plugin should have been returned\", plugins.contains(\"com.salesforce.sdkinfo\"));\n\t\tassertTrue(\"sfaccountmanager plugin should have been returned\", plugins.contains(\"com.salesforce.sfaccountmanager\"));\n\t}\n\t\n\t\/**\n\t * Helper method\n\t * @param jsonArray\n\t * @return\n\t * @throws JSONException \n\t *\/\n\tprivate List<String> toList(JSONArray jsonArray) throws JSONException {\n\t\tList<String> list = new ArrayList<String>(jsonArray.length());\n\t\tfor (int i=0; i<jsonArray.length(); i++) {\n\t\t\tlist.add(jsonArray.getString(i));\n\t\t}\n\t\treturn list;\n\t}\n}\n","lang_cluster":"Java","length":104,"code_uid":"234156e43b8548b5844af8dae1d92864"}
{"diff_hunk":"@@ -40,6 +40,12 @@ public abstract class MethodDescriptorView {\n \n   public abstract List<HeaderRequestParamView> headerRequestParams();\n \n+  @Nullable\n+  public abstract String requestFormatterName();\n+\n+  @Nullable\n+  public abstract String responseFormatterName();\n+\n   public boolean hasHeaderRequestParams() {\n     return headerRequestParams() != null && !headerRequestParams().isEmpty();\n   }","old_code":"\/* Copyright 2017 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage com.google.api.codegen.viewmodel;\n\nimport com.google.api.codegen.config.GrpcStreamingConfig.GrpcStreamingType;\nimport com.google.auto.value.AutoValue;\nimport java.util.List;\nimport javax.annotation.Nullable;\n\n@AutoValue\npublic abstract class MethodDescriptorView {\n\n  public abstract String requestTypeName();\n\n  public abstract String responseTypeName();\n\n  public abstract boolean hasResponse();\n\n  public abstract GrpcStreamingType grpcStreamingType();\n\n  public abstract String name();\n\n  public abstract String protoMethodName();\n\n  public abstract String fullServiceName();\n\n  public abstract String transportSettingsVar();\n\n  public abstract List<HeaderRequestParamView> headerRequestParams();\n\n  public boolean hasHeaderRequestParams() {\n    return headerRequestParams() != null && !headerRequestParams().isEmpty();\n  }\n\n  @Nullable\n  public abstract HttpMethodView httpMethod();\n\n  public static Builder newBuilder() {\n    return new AutoValue_MethodDescriptorView.Builder()\n        .grpcStreamingType(GrpcStreamingType.NonStreaming);\n  }\n\n  @AutoValue.Builder\n  public abstract static class Builder {\n\n    public abstract Builder requestTypeName(String name);\n\n    public abstract Builder responseTypeName(String name);\n\n    public abstract Builder hasResponse(boolean val);\n\n    public abstract Builder grpcStreamingType(GrpcStreamingType val);\n\n    public abstract Builder name(String directCallableName);\n\n    public abstract Builder protoMethodName(String val);\n\n    public abstract Builder fullServiceName(String val);\n\n    public abstract Builder transportSettingsVar(String val);\n\n    public abstract Builder headerRequestParams(List<HeaderRequestParamView> val);\n\n    public abstract Builder httpMethod(HttpMethodView val);\n\n    public abstract MethodDescriptorView build();\n  }\n}\n","lang_cluster":"Java","length":80,"code_uid":"f8b32452f92e4900a597d311fd461c57"}
{"diff_hunk":"@@ -36,25 +36,30 @@ import org.apache.iceberg.flink.source.FlinkSource;\n \n \/**\n  * Flink Iceberg table source.\n- * TODO: Implement {@link FilterableTableSource} and {@link LimitableTableSource}.\n+ * TODO: Implement {@link FilterableTableSource}\n  *\/\n-public class IcebergTableSource implements StreamTableSource<RowData>, ProjectableTableSource<RowData> {\n+public class IcebergTableSource\n+    implements StreamTableSource<RowData>, ProjectableTableSource<RowData>, LimitableTableSource<RowData> {\n \n   private final TableLoader loader;\n   private final TableSchema schema;\n   private final Map<String, String> properties;\n   private final int[] projectedFields;\n+  private boolean isLimitPushDown = false;\n+  private long limit = -1L;\n \n   public IcebergTableSource(TableLoader loader, TableSchema schema, Map<String, String> properties) {\n-    this(loader, schema, properties, null);\n+    this(loader, schema, properties, null, false, -1);\n   }\n \n   private IcebergTableSource(TableLoader loader, TableSchema schema, Map<String, String> properties,\n-                             int[] projectedFields) {\n+                             int[] projectedFields, boolean isLimitPushDown, long limit) {\n     this.loader = loader;\n     this.schema = schema;\n     this.properties = properties;\n     this.projectedFields = projectedFields;\n+    this.isLimitPushDown = isLimitPushDown;\n+    this.limit = limit;\n   }\n \n   @Override","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.flink;\n\nimport java.util.Arrays;\nimport java.util.Map;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.table.api.TableSchema;\nimport org.apache.flink.table.data.RowData;\nimport org.apache.flink.table.sources.FilterableTableSource;\nimport org.apache.flink.table.sources.LimitableTableSource;\nimport org.apache.flink.table.sources.ProjectableTableSource;\nimport org.apache.flink.table.sources.StreamTableSource;\nimport org.apache.flink.table.sources.TableSource;\nimport org.apache.flink.table.types.DataType;\nimport org.apache.flink.table.utils.TableConnectorUtils;\nimport org.apache.iceberg.flink.source.FlinkSource;\n\n\/**\n * Flink Iceberg table source.\n * TODO: Implement {@link FilterableTableSource} and {@link LimitableTableSource}.\n *\/\npublic class IcebergTableSource implements StreamTableSource<RowData>, ProjectableTableSource<RowData> {\n\n  private final TableLoader loader;\n  private final TableSchema schema;\n  private final Map<String, String> properties;\n  private final int[] projectedFields;\n\n  public IcebergTableSource(TableLoader loader, TableSchema schema, Map<String, String> properties) {\n    this(loader, schema, properties, null);\n  }\n\n  private IcebergTableSource(TableLoader loader, TableSchema schema, Map<String, String> properties,\n                             int[] projectedFields) {\n    this.loader = loader;\n    this.schema = schema;\n    this.properties = properties;\n    this.projectedFields = projectedFields;\n  }\n\n  @Override\n  public boolean isBounded() {\n    return FlinkSource.isBounded(properties);\n  }\n\n  @Override\n  public TableSource<RowData> projectFields(int[] fields) {\n    return new IcebergTableSource(loader, schema, properties, fields);\n  }\n\n  @Override\n  public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n    return FlinkSource.forRowData().env(execEnv).tableLoader(loader).project(getProjectedSchema())\n        .properties(properties).build();\n  }\n\n  @Override\n  public TableSchema getTableSchema() {\n    return schema;\n  }\n\n  @Override\n  public DataType getProducedDataType() {\n    return getProjectedSchema().toRowDataType().bridgedTo(RowData.class);\n  }\n\n  private TableSchema getProjectedSchema() {\n    TableSchema fullSchema = getTableSchema();\n    if (projectedFields == null) {\n      return fullSchema;\n    } else {\n      String[] fullNames = fullSchema.getFieldNames();\n      DataType[] fullTypes = fullSchema.getFieldDataTypes();\n      return TableSchema.builder().fields(\n          Arrays.stream(projectedFields).mapToObj(i -> fullNames[i]).toArray(String[]::new),\n          Arrays.stream(projectedFields).mapToObj(i -> fullTypes[i]).toArray(DataType[]::new)).build();\n    }\n  }\n\n  @Override\n  public String explainSource() {\n    String explain = \"Iceberg table: \" + loader.toString();\n    if (projectedFields != null) {\n      explain += \", ProjectedFields: \" + Arrays.toString(projectedFields);\n    }\n    return TableConnectorUtils.generateRuntimeName(getClass(), getTableSchema().getFieldNames()) + explain;\n  }\n}\n","lang_cluster":"Java","length":107,"code_uid":"e8684cae43ea478da9562d5200894a48"}
{"diff_hunk":"@@ -15,8 +15,10 @@\n \n package com.pingcap.tikv;\n \n+import com.pingcap.tikv.key.IndexKey;\n import com.pingcap.tikv.key.Key;\n import com.pingcap.tikv.key.RowKey;\n+import com.pingcap.tikv.meta.TiIndexInfo;\n import com.pingcap.tikv.meta.TiTableInfo;\n import com.pingcap.tikv.region.TiRegion;\n import java.util.ArrayList;","old_code":"\/*\n * Copyright 2019 PingCAP, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage com.pingcap.tikv;\n\nimport com.pingcap.tikv.key.Key;\nimport com.pingcap.tikv.key.RowKey;\nimport com.pingcap.tikv.meta.TiTableInfo;\nimport com.pingcap.tikv.region.TiRegion;\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class TiBatchWriteUtils {\n\n  public static List<TiRegion> getRegionsByTable(TiSession session, TiTableInfo table) {\n    ArrayList<TiRegion> regionList = new ArrayList<>();\n    Key key = RowKey.createMin(table.getId());\n    RowKey endRowKey = RowKey.createBeyondMax(table.getId());\n\n    while (key.compareTo(endRowKey) < 0) {\n      TiRegion region = session.getRegionManager().getRegionByKey(key.toByteString());\n      regionList.add(region);\n      key = Key.toRawKey(region.getEndKey());\n    }\n    return regionList;\n  }\n}\n","lang_cluster":"Java","length":39,"code_uid":"6f184812ac0643f0a4bc5a94b4efdd18"}
{"diff_hunk":"@@ -29,11 +29,13 @@ import java.util.Map;\n  *\/\n public class CommonDiscoveryProvider implements DiscoveryProvider {\n   private final DiscoveryContext context;\n-  private final SnippetSetRunner<Method> snippetSetRunner;\n+  private final SnippetSetRunner.Generator<Method> snippetSetRunner;\n   private final String snippetFileName;\n \n   public CommonDiscoveryProvider(\n-      DiscoveryContext context, SnippetSetRunner<Method> snippetSetRunner, String snippetFileName) {\n+      DiscoveryContext context,\n+      SnippetSetRunner.Generator<Method> snippetSetRunner,\n+      String snippetFileName) {\n     this.context = context;\n     this.snippetSetRunner = snippetSetRunner;\n     this.snippetFileName = snippetFileName;","old_code":"\/* Copyright 2016 Google Inc\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage com.google.api.codegen.discovery;\n\nimport com.google.api.codegen.DiscoveryContext;\nimport com.google.api.codegen.GeneratedResult;\nimport com.google.api.codegen.SnippetSetRunner;\nimport com.google.api.tools.framework.snippet.Doc;\nimport com.google.protobuf.Api;\nimport com.google.protobuf.Method;\n\nimport java.util.Collections;\nimport java.util.Map;\n\n\/**\n * Common DiscoveryProvider which runs code generation.\n *\/\npublic class CommonDiscoveryProvider implements DiscoveryProvider {\n  private final DiscoveryContext context;\n  private final SnippetSetRunner<Method> snippetSetRunner;\n  private final String snippetFileName;\n\n  public CommonDiscoveryProvider(\n      DiscoveryContext context, SnippetSetRunner<Method> snippetSetRunner, String snippetFileName) {\n    this.context = context;\n    this.snippetSetRunner = snippetSetRunner;\n    this.snippetFileName = snippetFileName;\n  }\n\n  @Override\n  public Map<String, Doc> generate(Method method) {\n    GeneratedResult result = snippetSetRunner.generate(method, snippetFileName, context);\n\n    Api api = context.getApi();\n    String outputRoot =\n        \"autogenerated\/\"\n            + api.getName()\n            + \"\/\"\n            + api.getVersion()\n            + \"\/\"\n            + context.getService().getDocumentation().getOverview();\n    String resultPath = outputRoot + \"\/\" + result.getFilename();\n\n    return Collections.singletonMap(resultPath, result.getDoc());\n  }\n\n  public static Builder newBuilder() {\n    return new Builder();\n  }\n\n  public static class Builder {\n    private DiscoveryContext context;\n    private SnippetSetRunner<Method> snippetSetRunner;\n    private String snippetFileName;\n\n    private Builder() {}\n\n    public Builder setContext(DiscoveryContext context) {\n      this.context = context;\n      return this;\n    }\n\n    public Builder setSnippetSetRunner(SnippetSetRunner<Method> snippetSetRunner) {\n      this.snippetSetRunner = snippetSetRunner;\n      return this;\n    }\n\n    public Builder setSnippetFileName(String snippetFileName) {\n      this.snippetFileName = snippetFileName;\n      return this;\n    }\n\n    public CommonDiscoveryProvider build() {\n      return new CommonDiscoveryProvider(context, snippetSetRunner, snippetFileName);\n    }\n  }\n}\n","lang_cluster":"Java","length":89,"code_uid":"05718fff9e014af1bd5188d8500d0c6e"}
{"diff_hunk":"@@ -78,6 +78,10 @@ public class StringLiteralExpr extends LiteralStringValueExpr {\n         return super.remove(node);\n     }\n \n+    public String asString() {\n+        return value;\n+    }\n+\n     @Override\n     public StringLiteralExpr clone() {\n         return (StringLiteralExpr) accept(new CloneVisitor(), null);","old_code":"\/*\n * Copyright (C) 2007-2010 J\u00falio Vilmar Gesser.\n * Copyright (C) 2011, 2013-2016 The JavaParser Team.\n *\n * This file is part of JavaParser.\n *\n * JavaParser can be used either under the terms of\n * a) the GNU Lesser General Public License as published by\n *     the Free Software Foundation, either version 3 of the License, or\n *     (at your option) any later version.\n * b) the terms of the Apache License\n *\n * You should have received a copy of both licenses in LICENCE.LGPL and\n * LICENCE.APACHE. Please refer to those files for details.\n *\n * JavaParser is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU Lesser General Public License for more details.\n *\/\npackage com.github.javaparser.ast.expr;\n\nimport com.github.javaparser.Range;\nimport com.github.javaparser.ast.AllFieldsConstructor;\nimport com.github.javaparser.ast.visitor.GenericVisitor;\nimport com.github.javaparser.ast.visitor.VoidVisitor;\nimport com.github.javaparser.utils.Utils;\nimport com.github.javaparser.ast.Node;\nimport com.github.javaparser.ast.visitor.CloneVisitor;\nimport com.github.javaparser.metamodel.StringLiteralExprMetaModel;\nimport com.github.javaparser.metamodel.JavaParserMetaModel;\n\n\/**\n * A literal string.\n * <br\/><code>\"Hello World!\"<\/code>\n * <br\/><code>\"\\\"\\n\"<\/code>\n * <br\/><code>\"\u2122\"<\/code>\n * <br\/><code>\"\ud83d\udca9\"<\/code>\n *\n * @author Julio Vilmar Gesser\n *\/\npublic class StringLiteralExpr extends LiteralStringValueExpr {\n\n    public StringLiteralExpr() {\n        this(null, \"empty\");\n    }\n\n    @AllFieldsConstructor\n    public StringLiteralExpr(final String value) {\n        this(null, value);\n    }\n\n    \/**\n     * Utility method that creates a new StringLiteralExpr. Escapes EOL characters.\n     *\/\n    public static StringLiteralExpr escape(String string) {\n        return new StringLiteralExpr(Utils.escapeEndOfLines(string));\n    }\n\n    public StringLiteralExpr(final Range range, final String value) {\n        super(range, value);\n    }\n\n    @Override\n    public <R, A> R accept(final GenericVisitor<R, A> v, final A arg) {\n        return v.visit(this, arg);\n    }\n\n    @Override\n    public <A> void accept(final VoidVisitor<A> v, final A arg) {\n        v.visit(this, arg);\n    }\n\n    @Override\n    public boolean remove(Node node) {\n        if (node == null)\n            return false;\n        return super.remove(node);\n    }\n\n    @Override\n    public StringLiteralExpr clone() {\n        return (StringLiteralExpr) accept(new CloneVisitor(), null);\n    }\n\n    @Override\n    public StringLiteralExprMetaModel getMetaModel() {\n        return JavaParserMetaModel.stringLiteralExprMetaModel;\n    }\n}\n","lang_cluster":"Java","length":90,"code_uid":"905ee95451e140d5b14fa8e3a6e5534b"}
{"diff_hunk":"@@ -191,7 +191,7 @@ final class Collections {\n         if (iterable instanceof Seq) {\n             return (Seq<T>) iterable;\n         } else {\n-            return List.ofAll(iterable);\n+            return Stream.ofAll(iterable);\n         }\n     }\n }","old_code":"\/*     \/ \\____  _    _  ____   ______  \/ \\ ____  __    _______\n *    \/  \/    \\\/ \\  \/ \\\/    \\ \/  \/\\__\\\/  \/\/    \\\/  \\  \/\/  \/\\__\\   J\u039bV\u039bSL\u039bNG\n *  _\/  \/  \/\\  \\  \\\/  \/  \/\\  \\\\__\\\\  \\  \/\/  \/\\  \\ \/\\\\\/ \\ \/__\\ \\   Copyright 2014-2016 Javaslang, http:\/\/javaslang.io\n * \/___\/\\_\/  \\_\/\\____\/\\_\/  \\_\/\\__\\\/__\/\\__\\_\/  \\_\/\/  \\__\/\\_____\/   Licensed under the Apache License, Version 2.0\n *\/\npackage javaslang.collection;\n\nimport javaslang.control.Option;\n\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Objects;\nimport java.util.function.BiFunction;\nimport java.util.function.Function;\nimport java.util.function.Predicate;\nimport java.util.function.Supplier;\n\n\/**\n * Internal class, containing helpers.\n *\n * @author Daniel Dietrich\n * @since 2.0.0\n *\/\nfinal class Collections {\n    @SuppressWarnings(\"unchecked\")\n    static <C extends Traversable<T>, T> C removeAll(C collection, T element) {\n        Objects.requireNonNull(element, \"element is null\");\n        return removeAll(collection, List.of(element));\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    static <C extends Traversable<T>, T> C removeAll(C collection, Iterable<? extends T> elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        final Set<T> removed = HashSet.ofAll(elements);\n        return (C) collection.filter(e -> !removed.contains(e));\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    static <C extends Traversable<T>, T> C removeAll(C collection, Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        return (C) collection.filter(predicate.negate());\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    static <C extends Traversable<T>, T> C retainAll(C collection, Iterable<? extends T> elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        final Set<T> removed = HashSet.ofAll(elements);\n        return (C) collection.filter(removed::contains);\n    }\n\n    public static <T, C, R extends Iterable<T>> Map<C, R> groupBy(Traversable<T> collection, Function<? super T, ? extends C> classifier, Function<? super Iterable<T>, R> mapper) {\n        Objects.requireNonNull(collection, \"collection is null\");\n        Objects.requireNonNull(classifier, \"classifier is null\");\n        Objects.requireNonNull(mapper, \"mapper is null\");\n\n        final java.util.Map<C, Collection<T>> mutableResults = new java.util.LinkedHashMap<>();\n        for (T value : collection) {\n            final C key = classifier.apply(value);\n            mutableResults.computeIfAbsent(key, k -> new ArrayList<>()).add(value);\n        }\n\n        Map<C, R> results = LinkedHashMap.empty();\n        for (java.util.Map.Entry<C, Collection<T>> entry : mutableResults.entrySet()) {\n            results = results.put(entry.getKey(), mapper.apply(entry.getValue()));\n        }\n        return results;\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    public static <K, V, K2, U extends Map<K2, V>> U mapKeys(Map<K, V> source, U zero, Function<? super K, ? extends K2> keyMapper, BiFunction<? super V, ? super V, ? extends V> valueMerge) {\n        Objects.requireNonNull(source, \"source is null\");\n        Objects.requireNonNull(zero, \"zero is null\");\n        Objects.requireNonNull(keyMapper, \"keyMapper is null\");\n        Objects.requireNonNull(valueMerge, \"valueMerge is null\");\n        return source.foldLeft(zero, (acc, entry) -> {\n            final K2 k2 = keyMapper.apply(entry._1);\n            final V v2 = entry._2;\n            final Option<V> v1 = acc.get(k2);\n            final V v = v1.isDefined() ? valueMerge.apply(v1.get(), v2) : v2;\n            return (U) acc.put(k2, v);\n        });\n    }\n\n    static Option<Integer> indexOption(int index) {\n        return Option.when(index >= 0, index);\n    }\n\n    \/\/ checks, if the *elements* of the given iterables are equal\n    static boolean areEqual(Iterable<?> iterable1, Iterable<?> iterable2) {\n        final java.util.Iterator<?> iter1 = iterable1.iterator();\n        final java.util.Iterator<?> iter2 = iterable2.iterator();\n        while (iter1.hasNext() && iter2.hasNext()) {\n            if (!Objects.equals(iter1.next(), iter2.next())) {\n                return false;\n            }\n        }\n        return iter1.hasNext() == iter2.hasNext();\n    }\n\n    \/\/ hashes the elements of an iterable\n    static int hash(Iterable<?> iterable) {\n        int hashCode = 1;\n        for (Object o : iterable) {\n            hashCode = 31 * hashCode + Objects.hashCode(o);\n        }\n        return hashCode;\n    }\n\n    static <T, U, C extends Iterable<U>, R extends Traversable<U>> R scanLeft(Iterable<? extends T> elements,\n                                                                              U zero, BiFunction<? super U, ? super T, ? extends U> operation,\n                                                                              C cumulativeResult, BiFunction<C, U, C> combiner,\n                                                                              Function<C, R> finisher) {\n        U acc = zero;\n        cumulativeResult = combiner.apply(cumulativeResult, acc);\n        for (T a : elements) {\n            acc = operation.apply(acc, a);\n            cumulativeResult = combiner.apply(cumulativeResult, acc);\n        }\n        return finisher.apply(cumulativeResult);\n    }\n\n    static <T, U, C extends Iterable<U>, R extends Traversable<U>> R scanRight(Iterable<? extends T> elements,\n                                                                               U zero, BiFunction<? super T, ? super U, ? extends U> operation,\n                                                                               C cumulativeResult, BiFunction<C, U, C> combiner,\n                                                                               Function<C, R> finisher) {\n        final Iterator<? extends T> reversedElements = seq(elements).reverseIterator(); \/\/ TODO a List will be reversed too many times this way\n        return scanLeft(reversedElements, zero, (u, t) -> operation.apply(t, u), cumulativeResult, combiner, finisher);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    static <T, S extends Seq<T>> Iterator<S> crossProduct(S empty, S seq, int power) {\n        if (power < 0) {\n            return Iterator.empty();\n        }\n        return Iterator.range(0, power)\n                       .foldLeft(Iterator.of(empty), (product, ignored) -> product.flatMap(el -> seq.map(t -> (S) el.append(t))));\n    }\n\n    static <C extends Traversable<T>, T> C tabulate(int n, Function<? super Integer, ? extends T> f, C empty, Function<T[], C> of) {\n        Objects.requireNonNull(f, \"f is null\");\n        Objects.requireNonNull(empty, \"empty is null\");\n        Objects.requireNonNull(of, \"of is null\");\n        if (n <= 0) {\n            return empty;\n        } else {\n            @SuppressWarnings(\"unchecked\")\n            final T[] elements = (T[]) new Object[n];\n            for (int i = 0; i < n; i++) {\n                elements[i] = f.apply(i);\n            }\n            return of.apply(elements);\n        }\n    }\n\n    static <C extends Traversable<T>, T> C fill(int n, Supplier<? extends T> s, C empty, Function<T[], C> of) {\n        Objects.requireNonNull(s, \"s is null\");\n        Objects.requireNonNull(empty, \"empty is null\");\n        Objects.requireNonNull(of, \"of is null\");\n        return tabulate(n, anything -> s.get(), empty, of);\n    }\n\n    static <T> Iterator<T> tabulate(int n, Function<? super Integer, ? extends T> f) {\n        Objects.requireNonNull(f, \"f is null\");\n        if (n <= 0) {\n            return Iterator.empty();\n        } else {\n            return new AbstractIterator<T>() {\n\n                int i = 0;\n\n                @Override\n                public boolean hasNext() {\n                    return i < n;\n                }\n\n                @Override\n                protected T getNext() {\n                    return f.apply(i++);\n                }\n            };\n        }\n    }\n\n    static <T> Iterator<T> fill(int n, Supplier<? extends T> supplier) {\n        Objects.requireNonNull(supplier, \"supplier is null\");\n        return tabulate(n, ignored -> supplier.get());\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    static <T> Seq<T> seq(Iterable<? extends T> iterable) {\n        if (iterable instanceof Seq) {\n            return (Seq<T>) iterable;\n        } else {\n            return List.ofAll(iterable);\n        }\n    }\n}\n","lang_cluster":"Java","length":197,"code_uid":"db7c231d4d1340c197603b1d8cf1a2b7"}
{"diff_hunk":"@@ -34,4 +34,6 @@ public interface DefinitionConst {\n   String VERSION_RULE_LATEST = \"latest\";\n \n   String VERSION_RULE_ALL = \"0.0.0+\";\n+\n+  String DEFAULT_REVISION = \"0\";\n }","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage io.servicecomb.serviceregistry.definition;\n\npublic interface DefinitionConst {\n\n  String CONFIG_QUALIFIED_INSTANCE_ENVIRONMENT_KEY = \"instance_description.environment\";\n\n  String CONFIG_ALLOW_CROSS_APP_KEY = \"allowCrossApp\";\n\n  String DEFAULT_APPLICATION_ID = \"default\";\n\n  String DEFAULT_MICROSERVICE_VERSION = \"1.0.0\";\n\n  String DEFAULT_STAGE = \"prod\";\n\n  String DEFAULT_INSTANCE_ENVIRONMENT = \"production\";\n\n  String VERSION_RULE_LATEST = \"latest\";\n\n  String VERSION_RULE_ALL = \"0.0.0+\";\n}\n","lang_cluster":"Java","length":37,"code_uid":"835ea53d35ea4333a43916c9281d2ce5"}
{"diff_hunk":"@@ -38,25 +38,21 @@ public class PrivateTransactionValidator {\n       final PrivateTransaction transaction,\n       final Long accountNonce,\n       final boolean allowFutureNonces) {\n-    LOG.debug(\"Validating private transaction fields of {}\", transaction.getHash());\n+    LOG.debug(\"Validating private transaction {}\", transaction);\n     final ValidationResult<TransactionInvalidReason> privateFieldsValidationResult =\n         validatePrivateTransactionFields(transaction);\n     if (!privateFieldsValidationResult.isValid()) {\n       LOG.debug(\n-          \"Private Transaction fields are invalid {}, {}\",\n-          transaction.getHash(),\n+          \"Private Transaction fields are invalid {}\",\n           privateFieldsValidationResult.getErrorMessage());\n       return privateFieldsValidationResult;\n     }\n \n-    LOG.debug(\"Validating the signature of Private Transaction {} \", transaction.getHash());\n-\n     final ValidationResult<TransactionValidator.TransactionInvalidReason>\n         signatureValidationResult = validateTransactionSignature(transaction);\n     if (!signatureValidationResult.isValid()) {\n       LOG.debug(\n-          \"Private Transaction {}, failed validation {}, {}\",\n-          transaction.getHash(),\n+          \"Private Transaction failed signature validation {}, {}\",\n           signatureValidationResult.getInvalidReason(),\n           signatureValidationResult.getErrorMessage());\n       return signatureValidationResult;","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.ethereum.privacy;\n\nimport org.hyperledger.besu.ethereum.mainnet.TransactionValidator;\nimport org.hyperledger.besu.ethereum.mainnet.TransactionValidator.TransactionInvalidReason;\nimport org.hyperledger.besu.ethereum.mainnet.ValidationResult;\n\nimport java.math.BigInteger;\nimport java.util.Optional;\n\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\n\npublic class PrivateTransactionValidator {\n\n  private static final Logger LOG = LogManager.getLogger();\n\n  private final Optional<BigInteger> chainId;\n\n  public PrivateTransactionValidator(final Optional<BigInteger> chainId) {\n    this.chainId = chainId;\n  }\n\n  public ValidationResult<TransactionValidator.TransactionInvalidReason> validate(\n      final PrivateTransaction transaction,\n      final Long accountNonce,\n      final boolean allowFutureNonces) {\n    LOG.debug(\"Validating private transaction fields of {}\", transaction.getHash());\n    final ValidationResult<TransactionInvalidReason> privateFieldsValidationResult =\n        validatePrivateTransactionFields(transaction);\n    if (!privateFieldsValidationResult.isValid()) {\n      LOG.debug(\n          \"Private Transaction fields are invalid {}, {}\",\n          transaction.getHash(),\n          privateFieldsValidationResult.getErrorMessage());\n      return privateFieldsValidationResult;\n    }\n\n    LOG.debug(\"Validating the signature of Private Transaction {} \", transaction.getHash());\n\n    final ValidationResult<TransactionValidator.TransactionInvalidReason>\n        signatureValidationResult = validateTransactionSignature(transaction);\n    if (!signatureValidationResult.isValid()) {\n      LOG.debug(\n          \"Private Transaction {}, failed validation {}, {}\",\n          transaction.getHash(),\n          signatureValidationResult.getInvalidReason(),\n          signatureValidationResult.getErrorMessage());\n      return signatureValidationResult;\n    }\n\n    final long transactionNonce = transaction.getNonce();\n\n    LOG.debug(\"Validating actual nonce {}, with expected nonce {}\", transactionNonce, accountNonce);\n\n    if (accountNonce > transactionNonce) {\n      final String errorMessage =\n          String.format(\n              \"Private Transaction nonce %s, is lower than sender account nonce %s.\",\n              transactionNonce, accountNonce);\n      LOG.debug(errorMessage);\n      return ValidationResult.invalid(\n          TransactionValidator.TransactionInvalidReason.PRIVATE_NONCE_TOO_LOW, errorMessage);\n    }\n\n    if (!allowFutureNonces && accountNonce != transactionNonce) {\n      final String errorMessage =\n          String.format(\n              \"Private Transaction nonce %s, does not match sender account nonce %s.\",\n              transactionNonce, accountNonce);\n      LOG.debug(errorMessage);\n      return ValidationResult.invalid(\n          TransactionValidator.TransactionInvalidReason.INCORRECT_PRIVATE_NONCE, errorMessage);\n    }\n\n    return ValidationResult.valid();\n  }\n\n  private ValidationResult<TransactionValidator.TransactionInvalidReason>\n      validatePrivateTransactionFields(final PrivateTransaction privateTransaction) {\n    if (!privateTransaction.getValue().isZero()) {\n      return ValidationResult.invalid(\n          TransactionValidator.TransactionInvalidReason.PRIVATE_VALUE_NOT_ZERO);\n    }\n    if (!privateTransaction.getRestriction().equals(Restriction.RESTRICTED)) {\n      return ValidationResult.invalid(\n          TransactionValidator.TransactionInvalidReason.PRIVATE_UNIMPLEMENTED_TRANSACTION_TYPE);\n    }\n\n    return ValidationResult.valid();\n  }\n\n  private ValidationResult<TransactionValidator.TransactionInvalidReason>\n      validateTransactionSignature(final PrivateTransaction transaction) {\n    if (chainId.isPresent()\n        && (transaction.getChainId().isPresent() && !transaction.getChainId().equals(chainId))) {\n      return ValidationResult.invalid(\n          TransactionValidator.TransactionInvalidReason.WRONG_CHAIN_ID,\n          String.format(\n              \"Transaction was meant for chain id %s, not this chain id %s\",\n              transaction.getChainId().get(), chainId.get()));\n    }\n\n    if (chainId.isEmpty() && transaction.getChainId().isPresent()) {\n      return ValidationResult.invalid(\n          TransactionValidator.TransactionInvalidReason.REPLAY_PROTECTED_SIGNATURES_NOT_SUPPORTED,\n          \"Replay protection (chainId) is not supported\");\n    }\n\n    \/\/ org.bouncycastle.math.ec.ECCurve.AbstractFp.decompressPoint throws an\n    \/\/ IllegalArgumentException for \"Invalid point compression\" for bad signatures.\n    try {\n      transaction.getSender();\n    } catch (final IllegalArgumentException e) {\n      return ValidationResult.invalid(\n          TransactionValidator.TransactionInvalidReason.INVALID_SIGNATURE,\n          \"Sender could not be extracted from transaction signature\");\n    }\n\n    return ValidationResult.valid();\n  }\n}\n","lang_cluster":"Java","length":135,"code_uid":"fdb2d9b18c404b3ca065e00008208d20"}
{"diff_hunk":"@@ -17,20 +17,17 @@\n package io.servicecomb.transport.rest.servlet;\n \n import io.servicecomb.common.rest.RestProducerInvocation;\n-import io.servicecomb.common.rest.definition.RestOperationMeta;\n import io.servicecomb.common.rest.filter.HttpServerFilter;\n import io.servicecomb.core.definition.OperationMeta;\n import io.servicecomb.foundation.vertx.http.StandardHttpServletRequestEx;\n \n public class RestServletProducerInvocation extends RestProducerInvocation {\n   @Override\n-  protected RestOperationMeta findRestOperation() {\n-    RestOperationMeta restOperationMeta = super.findRestOperation();\n+  protected void findRestOperation() {\n+    super.findRestOperation();\n \n     boolean cacheRequest = collectCacheRequest(restOperationMeta.getOperationMeta());\n     ((StandardHttpServletRequestEx) requestEx).setCacheRequest(cacheRequest);\n-\n-    return restOperationMeta;\n   }\n \n   protected boolean collectCacheRequest(OperationMeta operationMeta) {","old_code":"\/*\n * Copyright 2017 Huawei Technologies Co., Ltd\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage io.servicecomb.transport.rest.servlet;\n\nimport io.servicecomb.common.rest.RestProducerInvocation;\nimport io.servicecomb.common.rest.definition.RestOperationMeta;\nimport io.servicecomb.common.rest.filter.HttpServerFilter;\nimport io.servicecomb.core.definition.OperationMeta;\nimport io.servicecomb.foundation.vertx.http.StandardHttpServletRequestEx;\n\npublic class RestServletProducerInvocation extends RestProducerInvocation {\n  @Override\n  protected RestOperationMeta findRestOperation() {\n    RestOperationMeta restOperationMeta = super.findRestOperation();\n\n    boolean cacheRequest = collectCacheRequest(restOperationMeta.getOperationMeta());\n    ((StandardHttpServletRequestEx) requestEx).setCacheRequest(cacheRequest);\n\n    return restOperationMeta;\n  }\n\n  protected boolean collectCacheRequest(OperationMeta operationMeta) {\n    for (HttpServerFilter filter : httpServerFilters) {\n      if (filter.needCacheRequest(operationMeta)) {\n        return true;\n      }\n    }\n    return false;\n  }\n}\n","lang_cluster":"Java","length":44,"code_uid":"abb5e7732b524faabebf5fa3074dfac3"}
{"diff_hunk":"@@ -58,14 +58,16 @@ public final class BaselineReleaseCompatibility extends AbstractBaselinePlugin {\n \n         @Override\n         public Iterable<String> asArguments() {\n-            JavaVersion jdkVersion = getJdkVersion(javaCompile);\n-\n-            if (!supportsReleaseFlag(jdkVersion)) {\n+            if (javaCompile.getProject().getPlugins().hasPlugin(BaselineJavaVersion.class)) {\n+                return Collections.emptyList();\n+            }\n+            JavaVersion compilerVersion = JavaVersion.current();\n+            if (!compilerVersion.isJava9Compatible()) {\n                 log.debug(\n                         \"BaselineReleaseCompatibility is a no-op for {} in {} as {} doesn't support --release\",\n                         javaCompile.getName(),\n                         javaCompile.getProject(),\n-                        jdkVersion);\n+                        compilerVersion);\n                 return Collections.emptyList();\n             }\n ","old_code":"\/*\n * (c) Copyright 2019 Palantir Technologies Inc. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage com.palantir.baseline.plugins;\n\nimport com.google.common.collect.ImmutableList;\nimport java.util.Collections;\nimport java.util.Optional;\nimport org.gradle.api.JavaVersion;\nimport org.gradle.api.Project;\nimport org.gradle.api.tasks.compile.JavaCompile;\nimport org.gradle.jvm.toolchain.JavaCompiler;\nimport org.gradle.jvm.toolchain.JavaInstallationMetadata;\nimport org.gradle.process.CommandLineArgumentProvider;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n\/**\n * When using JDK 9+ to compile with a targetCompatibility less than JDK 9, this plugin adds compiler arguments per <a\n * href=\"https:\/\/openjdk.java.net\/jeps\/247\">JEP 247<\/a> to explicitly set the target JDK platform API to maintain binary\n * compatibility.\n *\n * <p>See also <a href=\"https:\/\/github.com\/gradle\/gradle\/issues\/2510\">Gradle JDK release issue<\/a>.\n *\/\npublic final class BaselineReleaseCompatibility extends AbstractBaselinePlugin {\n    private static final Logger log = LoggerFactory.getLogger(BaselineReleaseCompatibility.class);\n\n    @Override\n    public void apply(Project project) {\n        this.project = project;\n\n        project.getTasks().withType(JavaCompile.class).configureEach(javaCompile -> {\n            javaCompile.getOptions().getCompilerArgumentProviders().add(new ReleaseFlagProvider(javaCompile));\n        });\n    }\n\n    \/\/ using a lazy argument provider is crucial because otherwise we'd try to read sourceCompat \/ targetCompat\n    \/\/ before the user has even set it in their build.gradle!\n    private static final class ReleaseFlagProvider implements CommandLineArgumentProvider {\n        private final JavaCompile javaCompile;\n\n        private ReleaseFlagProvider(JavaCompile javaCompile) {\n            this.javaCompile = javaCompile;\n        }\n\n        @Override\n        public Iterable<String> asArguments() {\n            JavaVersion jdkVersion = getJdkVersion(javaCompile);\n\n            if (!supportsReleaseFlag(jdkVersion)) {\n                log.debug(\n                        \"BaselineReleaseCompatibility is a no-op for {} in {} as {} doesn't support --release\",\n                        javaCompile.getName(),\n                        javaCompile.getProject(),\n                        jdkVersion);\n                return Collections.emptyList();\n            }\n\n            \/\/ The java compiler does not allow using --add-exports in combination with --release\n            if (javaCompile.getOptions().getCompilerArgs().stream().anyMatch(arg -> arg.startsWith(\"--add-exports\"))) {\n                log.debug(\n                        \"BaselineReleaseCompatibility is a no-op for {} in {} as --add-exports flag is also used\",\n                        javaCompile.getName(),\n                        javaCompile.getProject());\n                return Collections.emptyList();\n            }\n\n            Optional<JavaVersion> taskTarget =\n                    Optional.ofNullable(javaCompile.getTargetCompatibility()).map(JavaVersion::toVersion);\n\n            if (!taskTarget.isPresent()) {\n                log.debug(\n                        \"BaselineReleaseCompatibility is a no-op for {} in {} as no targetCompatibility is set\",\n                        javaCompile.getName(),\n                        javaCompile.getProject());\n                return Collections.emptyList();\n            }\n            JavaVersion target = taskTarget.get();\n\n            if (jdkVersion.compareTo(target) <= 0) {\n                log.debug(\n                        \"BaselineReleaseCompatibility is a no-op for {} in {} as targetCompatibility is higher\",\n                        javaCompile.getName(),\n                        javaCompile.getProject());\n                return Collections.emptyList();\n            }\n\n            return ImmutableList.of(\"--release\", target.getMajorVersion());\n        }\n\n        \/\/ The --release flag was added in Java 9: https:\/\/openjdk.java.net\/jeps\/247\n        private static boolean supportsReleaseFlag(JavaVersion jdkVersion) {\n            return jdkVersion.isJava9Compatible();\n        }\n    }\n\n    private static JavaVersion getJdkVersion(JavaCompile javaCompile) {\n        return javaCompile\n                .getJavaCompiler()\n                .map(JavaCompiler::getMetadata)\n                .map(JavaInstallationMetadata::getLanguageVersion)\n                .map(version -> JavaVersion.toVersion(version.asInt()))\n                \/\/ Fallback to current java version if toolchain is not configured\n                .getOrElse(JavaVersion.current());\n    }\n}\n","lang_cluster":"Java","length":119,"code_uid":"f23abf76b96a445fb1ce9cdd507b1134"}
{"diff_hunk":"@@ -13,14 +13,14 @@\n  * License for the specific language governing permissions and limitations under\n  * the License.\n  *\/\n-\n package azkaban.execapp.event;\n \n import azkaban.executor.Status;\n \n+\n public class BlockingStatus {\n \n-  private static final long WAIT_TIME = 5 * 60 * 1000;\n+  private static final long WAIT_TIME = 300000;  \/\/ 5 * 60 * 1000\n   private final int execId;\n   private final String jobId;\n   private Status status;","old_code":"\/*\n * Copyright 2012 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n *\/\n\npackage azkaban.execapp.event;\n\nimport azkaban.executor.Status;\n\npublic class BlockingStatus {\n\n  private static final long WAIT_TIME = 5 * 60 * 1000;\n  private final int execId;\n  private final String jobId;\n  private Status status;\n\n  public BlockingStatus(final int execId, final String jobId, final Status initialStatus) {\n    this.execId = execId;\n    this.jobId = jobId;\n    this.status = initialStatus;\n  }\n\n  public Status blockOnFinishedStatus() {\n    if (this.status == null) {\n      return null;\n    }\n\n    while (!Status.isStatusFinished(this.status)) {\n      synchronized (this) {\n        try {\n          this.wait(WAIT_TIME);\n        } catch (final InterruptedException e) {\n        }\n      }\n    }\n\n    return this.status;\n  }\n\n  public Status viewStatus() {\n    return this.status;\n  }\n\n  public void unblock() {\n    synchronized (this) {\n      this.notifyAll();\n    }\n  }\n\n  public void changeStatus(final Status status) {\n    synchronized (this) {\n      this.status = status;\n      if (Status.isStatusFinished(status)) {\n        unblock();\n      }\n    }\n  }\n\n  public int getExecId() {\n    return this.execId;\n  }\n\n  public String getJobId() {\n    return this.jobId;\n  }\n}\n","lang_cluster":"Java","length":77,"code_uid":"3e36cb65fbb7440aa640f1865b7de551"}
{"diff_hunk":"@@ -45,7 +45,7 @@ import org.slf4j.LoggerFactory;\n public class QuartzScheduler {\n \n   \/\/Unless specified, all Quartz jobs's identities comes with the default job name.\n-  private static final String DEFAULT_JOB_NAME = \"job1\";\n+  public static final String DEFAULT_JOB_NAME = \"job1\";\n   private static final Logger logger = LoggerFactory.getLogger(QuartzScheduler.class);\n   private Scheduler scheduler = null;\n ","old_code":"\/*\n * Copyright 2017 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n *\/\n\npackage azkaban.scheduler;\n\nimport static azkaban.ServiceProvider.SERVICE_PROVIDER;\nimport static java.util.Objects.requireNonNull;\n\nimport azkaban.Constants.ConfigurationKeys;\nimport azkaban.utils.Props;\nimport java.util.Set;\nimport javax.inject.Inject;\nimport javax.inject.Singleton;\nimport org.quartz.CronExpression;\nimport org.quartz.CronScheduleBuilder;\nimport org.quartz.JobBuilder;\nimport org.quartz.JobDetail;\nimport org.quartz.JobKey;\nimport org.quartz.Scheduler;\nimport org.quartz.SchedulerException;\nimport org.quartz.Trigger;\nimport org.quartz.TriggerBuilder;\nimport org.quartz.impl.StdSchedulerFactory;\nimport org.quartz.impl.matchers.GroupMatcher;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n\/**\n * Manages Quartz schedules. Azkaban regards QuartzJob and QuartzTrigger as an one-to-one mapping.\n *\/\n@Singleton\npublic class QuartzScheduler {\n\n  \/\/Unless specified, all Quartz jobs's identities comes with the default job name.\n  private static final String DEFAULT_JOB_NAME = \"job1\";\n  private static final Logger logger = LoggerFactory.getLogger(QuartzScheduler.class);\n  private Scheduler scheduler = null;\n\n  @Inject\n  public QuartzScheduler(final Props azProps) throws SchedulerException {\n    if (!azProps.getBoolean(ConfigurationKeys.ENABLE_QUARTZ, false)) {\n      return;\n    }\n    final StdSchedulerFactory schedulerFactory =\n        new StdSchedulerFactory(azProps.toProperties());\n    this.scheduler = schedulerFactory.getScheduler();\n\n    \/\/ Currently Quartz only support internal job schedules. When we migrate to User Production\n    \/\/ flows, we need to construct a Guice-Free JobFactory for use.\n    this.scheduler.setJobFactory(SERVICE_PROVIDER.getInstance(SchedulerJobFactory.class));\n  }\n\n  public void start() {\n    try {\n      this.scheduler.start();\n    } catch (final SchedulerException e) {\n      logger.error(\"Error starting Quartz scheduler: \", e);\n    }\n    logger.info(\"Quartz Scheduler started.\");\n  }\n\n  public void cleanup() {\n    logger.info(\"Cleaning up schedules in scheduler\");\n    try {\n      this.scheduler.clear();\n    } catch (final SchedulerException e) {\n      logger.error(\"Exception clearing scheduler: \", e);\n    }\n  }\n\n  public void pause() {\n    logger.info(\"pausing all schedules in Quartz\");\n    try {\n      this.scheduler.pauseAll();\n    } catch (final SchedulerException e) {\n      logger.error(\"Exception pausing scheduler: \", e);\n    }\n  }\n\n  public void resume() {\n    logger.info(\"resuming all schedules in Quartz\");\n    try {\n      this.scheduler.resumeAll();\n    } catch (final SchedulerException e) {\n      logger.error(\"Exception resuming scheduler: \", e);\n    }\n  }\n\n  public void shutdown() {\n    logger.info(\"Shutting down scheduler\");\n    try {\n      this.scheduler.shutdown();\n    } catch (final SchedulerException e) {\n      logger.error(\"Exception shutting down scheduler: \", e);\n    }\n  }\n\n  public void unregisterJob(final String groupName) throws SchedulerException {\n    if (!ifJobExist(groupName)) {\n      logger.warn(\"can not find job with \" + groupName + \" in quartz.\");\n    } else {\n      this.scheduler.deleteJob(new JobKey(DEFAULT_JOB_NAME, groupName));\n    }\n  }\n\n  \/**\n   * Only cron schedule register is supported.\n   *\n   * @param cronExpression the cron schedule for this job\n   * @param jobDescription Regarding QuartzJobDescription#groupName, in order to guarantee no\n   * duplicate quartz schedules, we design the naming convention depending on use cases: <ul>\n   * <li>User flow schedule: we use {@link org.quartz.JobKey#JobKey} to represent the identity of a\n   * flow's schedule. The format follows \"$projectID_$flowName\" to guarantee no duplicates.\n   * <li>Quartz schedule for AZ internal use: the groupName should start with letters,\n   * rather than\n   * number, which is the first case.<\/ul>\n   *\/\n  public void registerJob(final String cronExpression, final QuartzJobDescription jobDescription)\n      throws SchedulerException {\n\n    requireNonNull(jobDescription, \"jobDescription is null\");\n\n    \/\/ Not allowed to register duplicate job name.\n    if (ifJobExist(jobDescription.getGroupName())) {\n      throw new SchedulerException(\n          \"can not register existing job \" + jobDescription.getGroupName());\n    }\n\n    if (!CronExpression.isValidExpression(cronExpression)) {\n      throw new SchedulerException(\n          \"The cron expression string <\" + cronExpression + \"> is not valid.\");\n    }\n\n    \/\/ TODO kunkun-tang: we will modify this when we start supporting multi schedules per flow.\n    final JobDetail job = JobBuilder.newJob(jobDescription.getJobClass())\n        .withIdentity(DEFAULT_JOB_NAME, jobDescription.getGroupName()).build();\n\n    \/\/ Add external dependencies to Job Data Map.\n    job.getJobDataMap().putAll(jobDescription.getContextMap());\n\n    \/\/ TODO kunkun-tang: Need management code to deal with different misfire policy\n    final Trigger trigger = TriggerBuilder\n        .newTrigger()\n        .withSchedule(\n            CronScheduleBuilder.cronSchedule(cronExpression)\n                .withMisfireHandlingInstructionFireAndProceed()\n\/\/            .withMisfireHandlingInstructionDoNothing()\n\/\/            .withMisfireHandlingInstructionIgnoreMisfires()\n        )\n        .build();\n\n    this.scheduler.scheduleJob(job, trigger);\n    logger.info(\"Quartz Schedule with jobDetail \" + job.getDescription() + \" is registered.\");\n  }\n\n\n  public boolean ifJobExist(final String groupName) throws SchedulerException {\n    final Set<JobKey> jobKeySet = this.scheduler.getJobKeys(GroupMatcher.jobGroupEquals(groupName));\n    return jobKeySet != null && jobKeySet.size() > 0;\n  }\n\n  public Scheduler getScheduler() {\n    return this.scheduler;\n  }\n}\n","lang_cluster":"Java","length":178,"code_uid":"30d35322f3734dafb89e07b0f33e6e8c"}
{"diff_hunk":"@@ -118,6 +118,26 @@ public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n+  @Test\n+  public void testDropTable() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+\n+    Object result = scalarSql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName);\n+    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s\", tableName));\n+\n+    sql(\"DROP TABLE %s\", tableName);\n+\n+    assertEquals(\"Source table should be intact\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s\", sourceName));\n+  }\n+\n   @Test\n   public void testInvalidSnapshotsCases() throws IOException {\n     String location = temp.newFolder().toString();","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.spark.extensions;\n\nimport java.io.IOException;\nimport java.util.Map;\nimport org.apache.iceberg.AssertHelpers;\nimport org.apache.iceberg.Table;\nimport org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\nimport org.apache.spark.sql.AnalysisException;\nimport org.junit.After;\nimport org.junit.Assert;\nimport org.junit.Assume;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.TemporaryFolder;\n\npublic class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n  private static final String sourceName = \"spark_catalog.default.source\";\n  \/\/ Currently we can only Snapshot only out of the Spark Session Catalog\n\n  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n    super(catalogName, implementation, config);\n  }\n\n  @Rule\n  public TemporaryFolder temp = new TemporaryFolder();\n\n  @After\n  public void removeTables() {\n    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n  }\n\n  @Test\n  public void testSnapshot() throws IOException {\n    String location = temp.newFolder().toString();\n    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName, location);\n    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n    Object result = scalarSql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName);\n\n    Assert.assertEquals(\"Should have added one file\", 1L, result);\n\n    Table createdTable = validationCatalog.loadTable(tableIdent);\n    String tableLocation = createdTable.location();\n    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n\n    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n\n    assertEquals(\"Should have expected rows\",\n        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n  }\n\n  @Test\n  public void testSnapshotWithProperties() throws IOException {\n    String location = temp.newFolder().toString();\n    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName, location);\n    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n    Object result = scalarSql(\n        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n        catalogName, sourceName, tableName);\n\n    Assert.assertEquals(\"Should have added one file\", 1L, result);\n\n    Table createdTable = validationCatalog.loadTable(tableIdent);\n\n    String tableLocation = createdTable.location();\n    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n\n    Map<String, String> props = createdTable.properties();\n    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n\n    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n\n    assertEquals(\"Should have expected rows\",\n        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n  }\n\n  @Test\n  public void testSnapshotWithAlternateLocation() throws IOException {\n    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n    String location = temp.newFolder().toString();\n    String snapshotLocation = temp.newFolder().toString();\n    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName, location);\n    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n    Object[] result = sql(\n        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', location => '%s')\",\n        catalogName, sourceName, tableName, snapshotLocation).get(0);\n\n    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n\n    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n\n    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n\n    assertEquals(\"Should have expected rows\",\n        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n  }\n\n  @Test\n  public void testInvalidSnapshotsCases() throws IOException {\n    String location = temp.newFolder().toString();\n    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName, location);\n\n    AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n        AnalysisException.class, \"Missing required parameters\",\n        () -> sql(\"CALL %s.system.snapshot('foo')\", catalogName));\n\n    AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n        AnalysisException.class, \"Wrong arg type\",\n        () -> sql(\"CALL %s.system.snapshot('n', 't', map('foo', 'bar'))\", catalogName));\n\n    AssertHelpers.assertThrows(\"Should reject calls with invalid map args\",\n        AnalysisException.class, \"cannot resolve 'map\",\n        () -> sql(\"CALL %s.system.snapshot('%s', 'fable', 'loc', map(2, 1, 1))\", catalogName, sourceName));\n\n    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n        () -> sql(\"CALL %s.system.snapshot('', 'dest')\", catalogName));\n\n    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n        () -> sql(\"CALL %s.system.snapshot('src', '')\", catalogName));\n  }\n}\n","lang_cluster":"Java","length":146,"code_uid":"42e42f3623ac483eaa834f7694370e5c"}
{"diff_hunk":"@@ -39,11 +39,8 @@ public class ViewsConfigurationLibrary {\n \n     @Bean\n     public ViewLibrary viewLibrary() {\n-        return new ViewLibrary(\n-                applicationName,\n-                themeConfiguration.themeManager(),\n-                phoenicisGlobalConfiguration.objectMapper()\n-        );\n+        return new ViewLibrary(applicationName, themeConfiguration.themeManager(),\n+                phoenicisGlobalConfiguration.objectMapper());\n     }\n \n     @Bean","old_code":"\/*\n * Copyright (C) 2015-2017 P\u00c2RIS Quentin\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n *\/\n\npackage org.phoenicis.javafx.views.mainwindow.library;\n\nimport org.phoenicis.configuration.PhoenicisGlobalConfiguration;\nimport org.phoenicis.javafx.views.common.ThemeConfiguration;\nimport org.phoenicis.javafx.views.mainwindow.console.ConsoleTabFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class ViewsConfigurationLibrary {\n    @Value(\"${application.name}\")\n    private String applicationName;\n\n    @Autowired\n    private ThemeConfiguration themeConfiguration;\n\n    @Autowired\n    private PhoenicisGlobalConfiguration phoenicisGlobalConfiguration;\n\n    @Bean\n    public ViewLibrary viewLibrary() {\n        return new ViewLibrary(\n                applicationName,\n                themeConfiguration.themeManager(),\n                phoenicisGlobalConfiguration.objectMapper()\n        );\n    }\n\n    @Bean\n    public ConsoleTabFactory consoleTabFactory() {\n        return new ConsoleTabFactory();\n    }\n}\n","lang_cluster":"Java","length":53,"code_uid":"3bb82e04a13a4b8dafefebb5703ab0cd"}
{"diff_hunk":"@@ -32,6 +32,7 @@ import org.apache.tuweni.bytes.Bytes32;\n \n public class KeyPairUtil {\n   private static final Logger LOG = LogManager.getLogger();\n+  private static final EllipticCurveSignature ELLIPTIC_CURVE_SIGNATURE = EllipticCurveSignatureFactory.getInstance();\n \n   public static String loadResourceFile(final String resourcePath) {\n     try {","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.crypto;\n\nimport static java.nio.file.StandardCopyOption.ATOMIC_MOVE;\nimport static java.nio.file.StandardCopyOption.REPLACE_EXISTING;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.net.URL;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.List;\n\nimport com.google.common.io.Resources;\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\nimport org.apache.tuweni.bytes.Bytes32;\n\npublic class KeyPairUtil {\n  private static final Logger LOG = LogManager.getLogger();\n\n  public static String loadResourceFile(final String resourcePath) {\n    try {\n      URL path = KeyPairUtil.class.getClassLoader().getResource(resourcePath);\n      return Resources.toString(path, StandardCharsets.UTF_8).trim();\n    } catch (Exception e) {\n      throw new RuntimeException(\"Unable to load resource: \" + resourcePath, e);\n    }\n  }\n\n  public static SECP256K1.KeyPair loadKeyPairFromResource(final String resourcePath) {\n    final SECP256K1.KeyPair keyPair;\n    String keyData = loadResourceFile(resourcePath);\n    if (keyData == null || keyData.isEmpty()) {\n      throw new IllegalArgumentException(\"Unable to load resource: \" + resourcePath);\n    }\n    SECP256K1.PrivateKey privateKey = SECP256K1.PrivateKey.create(Bytes32.fromHexString((keyData)));\n    keyPair = SECP256K1.KeyPair.create(privateKey);\n\n    LOG.info(\"Loaded keyPair {} from {}\", keyPair.getPublicKey().toString(), resourcePath);\n    return keyPair;\n  }\n\n  public static SECP256K1.KeyPair loadKeyPair(final File keyFile) {\n\n    final SECP256K1.KeyPair key;\n    if (keyFile.exists()) {\n\n      key = load(keyFile);\n      LOG.info(\n          \"Loaded public key {} from {}\", key.getPublicKey().toString(), keyFile.getAbsolutePath());\n    } else {\n      key = SECP256K1.KeyPair.generate();\n      try {\n        storeKeyPair(key, keyFile);\n      } catch (IOException e) {\n        throw new IllegalArgumentException(\"Cannot store generated private key.\");\n      }\n      LOG.info(\n          \"Generated new public key {} and stored it to {}\",\n          key.getPublicKey().toString(),\n          keyFile.getAbsolutePath());\n    }\n    return key;\n  }\n\n  public static SECP256K1.KeyPair loadKeyPair(final Path homeDirectory) {\n    return loadKeyPair(getDefaultKeyFile(homeDirectory));\n  }\n\n  public static File getDefaultKeyFile(final Path homeDirectory) {\n    return homeDirectory.resolve(\"key\").toFile();\n  }\n\n  public static SECP256K1.KeyPair load(final File file) {\n    return SECP256K1.KeyPair.create(loadPrivateKey(file));\n  }\n\n  static SECP256K1.PrivateKey loadPrivateKey(final File file) {\n    try {\n      final List<String> info = Files.readAllLines(file.toPath());\n      if (info.size() != 1) {\n        throw new IllegalArgumentException(\"Supplied file does not contain valid keyPair pair.\");\n      }\n      return SECP256K1.PrivateKey.create(Bytes32.fromHexString((info.get(0))));\n    } catch (IOException ex) {\n      throw new IllegalArgumentException(\"Supplied file does not contain valid keyPair pair.\");\n    }\n  }\n\n  static void storeKeyPair(final SECP256K1.KeyPair keyKair, final File file) throws IOException {\n    final File privateKeyDir = file.getParentFile();\n    privateKeyDir.mkdirs();\n    final Path tempPath = Files.createTempFile(privateKeyDir.toPath(), \".tmp\", \"\");\n    Files.write(\n        tempPath,\n        keyKair.getPrivateKey().getEncodedBytes().toString().getBytes(StandardCharsets.UTF_8));\n    Files.move(tempPath, file.toPath(), REPLACE_EXISTING, ATOMIC_MOVE);\n  }\n}\n","lang_cluster":"Java","length":114,"code_uid":"742f204ffc3845ea93a1b31c04ba327a"}
{"diff_hunk":"@@ -70,6 +70,17 @@ public interface WorldUpdater extends MutableWorldView {\n     return account == null ? createAccount(address) : account;\n   }\n \n+  \/**\n+   * Retrieves the provided account for a sender of a transaction if it exists, or creates it if it\n+   * doesn't.\n+   *\n+   * @param address the address of the account.\n+   * @return the account {@code address}, or {@code null} if the account does not exist.\n+   *\/\n+  default EvmAccount getOrCreateSenderAccount(final Address address) {\n+    return getOrCreate(address);\n+  }\n+\n   \/**\n    * Retrieves the provided account, returning a modifiable object (whose updates are accumulated by\n    * this updater).","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.ethereum.core;\n\nimport java.util.Collection;\nimport java.util.Optional;\n\n\/**\n * An object that buffers updates made over a particular {@link WorldView}.\n *\n * <p>All changes made to this object, being it account creation\/deletion or account modifications\n * through {@link MutableAccount}, are immediately reflected on this object (so for instance,\n * deleting an account and trying to get it afterwards will return {@code null}) but do not impact\n * whichever {@link WorldView} this is an updater for until the {@link #commit} method is called.\n *\/\npublic interface WorldUpdater extends MutableWorldView {\n\n  \/**\n   * Creates a new account, or reset it (that is, act as if it was deleted and created anew) if it\n   * already exists.\n   *\n   * <p>After this call, the account will exists and will have the provided nonce and balance. His\n   * code and storage will be empty.\n   *\n   * @param address the address of the account to create (or reset).\n   * @param nonce the nonce for created\/reset account.\n   * @param balance the balance for created\/reset account.\n   * @return the account {@code address}, which will have nonce {@code nonce}, balance {@code\n   *     balance} and empty code and storage.\n   *\/\n  EvmAccount createAccount(Address address, long nonce, Wei balance);\n\n  \/**\n   * Creates a new account, or reset it (that is, act as if it was deleted and created anew) if it\n   * already exists.\n   *\n   * <p>This call is equivalent to {@link #createAccount(Address, long, Wei)} but defaults both the\n   * nonce and balance to zero.\n   *\n   * @param address the address of the account to create (or reset).\n   * @return the account {@code address}, which will have 0 for the nonce and balance and empty code\n   *     and storage.\n   *\/\n  default EvmAccount createAccount(final Address address) {\n    return createAccount(address, Account.DEFAULT_NONCE, Account.DEFAULT_BALANCE);\n  }\n\n  \/**\n   * Retrieves the provided account if it exists, or create it if it doesn't.\n   *\n   * @param address the address of the account.\n   * @return the account {@code address}. If that account exists, it is returned as if by {@link\n   *     #getAccount(Address)}, otherwise, it is created and returned as if by {@link\n   *     #createAccount(Address)} (and thus all his fields will be zero\/empty).\n   *\/\n  default EvmAccount getOrCreate(final Address address) {\n    final EvmAccount account = getAccount(address);\n    return account == null ? createAccount(address) : account;\n  }\n\n  \/**\n   * Retrieves the provided account, returning a modifiable object (whose updates are accumulated by\n   * this updater).\n   *\n   * @param address the address of the account.\n   * @return the account {@code address}, or {@code null} if the account does not exist.\n   *\/\n  EvmAccount getAccount(Address address);\n\n  \/**\n   * Deletes the provided account.\n   *\n   * @param address the address of the account to delete. If that account doesn't exists prior to\n   *     this call, this is a no-op.\n   *\/\n  void deleteAccount(Address address);\n\n  \/**\n   * Returns the accounts that have been touched within the scope of this updater.\n   *\n   * @return the accounts that have been touched within the scope of this updater\n   *\/\n  Collection<? extends Account> getTouchedAccounts();\n\n  \/**\n   * Returns the account addresses that have been deleted within the scope of this updater.\n   *\n   * @return the account addresses that have been deleted within the scope of this updater\n   *\/\n  Collection<Address> getDeletedAccountAddresses();\n\n  \/** Removes the changes that were made to this updater. *\/\n  void revert();\n\n  \/**\n   * Commits the changes made to this updater to the underlying {@link WorldView} this is an updater\n   * of.\n   *\/\n  void commit();\n\n  \/**\n   * The parent updater (if it exists).\n   *\n   * @return The parent WorldUpdater if this wraps another one, empty otherwise\n   *\/\n  Optional<WorldUpdater> parentUpdater();\n}\n","lang_cluster":"Java","length":119,"code_uid":"c99528941ffe46648bebdddf0fbe14ff"}
{"diff_hunk":"@@ -37,6 +37,7 @@ import java.util.function.Consumer;\n  * A literal string.\n  * <br\/><code>\"Hello World!\"<\/code>\n  * <br\/><code>\"\\\"\\n\"<\/code>\n+ * <br\/><code>\"\\u2122\"<\/code>\n  * <br\/><code>\"\u2122\"<\/code>\n  * <br\/><code>\"\ud83d\udca9\"<\/code>\n  *","old_code":"\/*\n * Copyright (C) 2007-2010 J\u00falio Vilmar Gesser.\n * Copyright (C) 2011, 2013-2016 The JavaParser Team.\n *\n * This file is part of JavaParser.\n *\n * JavaParser can be used either under the terms of\n * a) the GNU Lesser General Public License as published by\n *     the Free Software Foundation, either version 3 of the License, or\n *     (at your option) any later version.\n * b) the terms of the Apache License\n *\n * You should have received a copy of both licenses in LICENCE.LGPL and\n * LICENCE.APACHE. Please refer to those files for details.\n *\n * JavaParser is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU Lesser General Public License for more details.\n *\/\npackage com.github.javaparser.ast.expr;\n\nimport com.github.javaparser.ast.AllFieldsConstructor;\nimport com.github.javaparser.ast.Node;\nimport com.github.javaparser.ast.visitor.CloneVisitor;\nimport com.github.javaparser.ast.visitor.GenericVisitor;\nimport com.github.javaparser.ast.visitor.VoidVisitor;\nimport com.github.javaparser.metamodel.JavaParserMetaModel;\nimport com.github.javaparser.metamodel.StringLiteralExprMetaModel;\nimport com.github.javaparser.utils.StringEscapeUtils;\nimport com.github.javaparser.utils.Utils;\nimport javax.annotation.Generated;\nimport com.github.javaparser.TokenRange;\nimport java.util.function.Consumer;\n\n\/**\n * A literal string.\n * <br\/><code>\"Hello World!\"<\/code>\n * <br\/><code>\"\\\"\\n\"<\/code>\n * <br\/><code>\"\u2122\"<\/code>\n * <br\/><code>\"\ud83d\udca9\"<\/code>\n *\n * @author Julio Vilmar Gesser\n *\/\npublic final class StringLiteralExpr extends LiteralStringValueExpr {\n\n    public StringLiteralExpr() {\n        this(null, \"empty\");\n    }\n\n    \/**\n     * Creates a string literal expression from given string. Escapes EOL characters.\n     *\n     * @param value the value of the literal\n     *\/\n    @AllFieldsConstructor\n    public StringLiteralExpr(final String value) {\n        this(null, Utils.escapeEndOfLines(value));\n    }\n\n    \/**\n     * Utility method that creates a new StringLiteralExpr. Escapes EOL characters.\n     *\n     * @deprecated Use {@link #StringLiteralExpr(String)} instead.\n     *\/\n    @Deprecated\n    public static StringLiteralExpr escape(String string) {\n        return new StringLiteralExpr(Utils.escapeEndOfLines(string));\n    }\n\n    \/**\n     * This constructor is used by the parser and is considered private.\n     *\/\n    @Generated(\"com.github.javaparser.generator.core.node.MainConstructorGenerator\")\n    public StringLiteralExpr(TokenRange tokenRange, String value) {\n        super(tokenRange, value);\n        customInitialization();\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.AcceptGenerator\")\n    public <R, A> R accept(final GenericVisitor<R, A> v, final A arg) {\n        return v.visit(this, arg);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.AcceptGenerator\")\n    public <A> void accept(final VoidVisitor<A> v, final A arg) {\n        v.visit(this, arg);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.RemoveMethodGenerator\")\n    public boolean remove(Node node) {\n        if (node == null)\n            return false;\n        return super.remove(node);\n    }\n\n    \/**\n     * Sets the content of this expressions to given value. Escapes EOL characters.\n     *\n     * @param value the new literal value\n     * @return self\n     *\/\n    public StringLiteralExpr setEscapedValue(String value) {\n        this.value = Utils.escapeEndOfLines(value);\n        return this;\n    }\n\n    \/**\n     * @return the unescaped literal value\n     *\/\n    public String asString() {\n        return StringEscapeUtils.unescapeJava(value);\n    }\n\n    \/**\n     * Escapes the given string from special characters and uses it as the literal value.\n     *\n     * @param value unescaped string\n     * @return this literal expression\n     *\/\n    public StringLiteralExpr setString(String value) {\n        this.value = StringEscapeUtils.escapeJava(value);\n        return this;\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.CloneGenerator\")\n    public StringLiteralExpr clone() {\n        return (StringLiteralExpr) accept(new CloneVisitor(), null);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.GetMetaModelGenerator\")\n    public StringLiteralExprMetaModel getMetaModel() {\n        return JavaParserMetaModel.stringLiteralExprMetaModel;\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.ReplaceMethodGenerator\")\n    public boolean replace(Node node, Node replacementNode) {\n        if (node == null)\n            return false;\n        return super.replace(node, replacementNode);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.TypeCastingGenerator\")\n    public boolean isStringLiteralExpr() {\n        return true;\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.TypeCastingGenerator\")\n    public StringLiteralExpr asStringLiteralExpr() {\n        return this;\n    }\n\n    @Generated(\"com.github.javaparser.generator.core.node.TypeCastingGenerator\")\n    public void ifStringLiteralExpr(Consumer<StringLiteralExpr> action) {\n        action.accept(this);\n    }\n}\n","lang_cluster":"Java","length":165,"code_uid":"649c1c3f91ac47e5a084bde7820e3555"}
{"diff_hunk":"@@ -90,7 +90,7 @@ public class ViewEngines extends MainWindowView<EngineSidebar> {\n \n         Bindings.bindContent(availableEngines.getTabs(), mappedSubCategoryTabs);\n \n-        this.initializeSidebar();\n+        this.setSidebar(this.sidebar);\n     }\n \n     public void setOnInstallEngine(Consumer<EngineDTO> onInstallEngine) {","old_code":"\/*\n * Copyright (C) 2015-2017 P\u00c2RIS Quentin\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n *\/\n\npackage org.phoenicis.javafx.views.mainwindow.engines;\n\nimport javafx.animation.PauseTransition;\nimport javafx.application.Platform;\nimport javafx.beans.binding.Bindings;\nimport javafx.collections.FXCollections;\nimport javafx.collections.ObservableList;\nimport javafx.scene.control.TabPane;\nimport javafx.scene.layout.VBox;\nimport javafx.util.Duration;\nimport org.phoenicis.engines.dto.EngineCategoryDTO;\nimport org.phoenicis.engines.dto.EngineDTO;\nimport org.phoenicis.engines.dto.EngineSubCategoryDTO;\nimport org.phoenicis.engines.dto.EngineVersionDTO;\nimport org.phoenicis.javafx.views.common.MappedList;\nimport org.phoenicis.javafx.views.common.ThemeManager;\nimport org.phoenicis.javafx.views.common.widgets.lists.CombinedListWidget;\nimport org.phoenicis.javafx.views.mainwindow.MainWindowView;\n\nimport java.util.List;\nimport java.util.function.Consumer;\n\npublic class ViewEngines extends MainWindowView<EngineSidebar> {\n    private TabPane availableEngines;\n\n    private EnginePanel currentEnginePanel;\n\n    private EngineCategoryDTO selectedCategory;\n\n    private ObservableList<EngineCategoryDTO> engineCategories;\n\n    private ObservableList<EngineSubCategoryDTO> engineSubCategories;\n    private MappedList<EngineSubCategoryTab, EngineSubCategoryDTO> mappedSubCategoryTabs;\n    private MappedList<CombinedListWidget<EngineVersionDTO>, EngineSubCategoryTab> mappedListWidgets;\n\n    private Consumer<EngineDTO> setOnInstallEngine = (engine) -> {\n    };\n    private Consumer<EngineDTO> setOnDeleteEngine = (engine) -> {\n    };\n\n    private PauseTransition pause = new PauseTransition(Duration.seconds(0.5));\n\n    public ViewEngines(ThemeManager themeManager, String enginesPath) {\n        super(\"Engines\", themeManager);\n\n        this.engineCategories = FXCollections.observableArrayList();\n        this.engineSubCategories = FXCollections.observableArrayList();\n        this.mappedSubCategoryTabs = new MappedList<>(engineSubCategories, engineSubCategory -> {\n            EngineSubCategoryTab result = new EngineSubCategoryTab(selectedCategory, engineSubCategory, enginesPath);\n\n            result.setOnSelectEngine(this::showEngineDetails);\n\n            return result;\n        });\n        this.mappedListWidgets = new MappedList<>(mappedSubCategoryTabs, tab -> tab.getEngineVersionsView());\n\n        this.sidebar = new EngineSidebar(mappedListWidgets);\n\n        this.sidebar.setOnCategorySelection(this::selectCategory);\n        this.sidebar.setOnApplyInstalledFilter(newValue -> availableEngines.getTabs()\n                .forEach(tab -> ((EngineSubCategoryTab) tab).setFilterForInstalled(newValue)));\n        this.sidebar.setOnApplyUninstalledFilter(newValue -> availableEngines.getTabs()\n                .forEach(tab -> ((EngineSubCategoryTab) tab).setFilterForNotInstalled(newValue)));\n        this.sidebar.setOnSearchTermClear(() -> availableEngines.getTabs()\n                .forEach(tab -> ((EngineSubCategoryTab) tab).setFilterForSearchTerm(\"\")));\n        this.sidebar.setOnApplySearchTerm(this::processFilterText);\n\n        this.sidebar.bindEngineCategories(engineCategories);\n\n        this.initFailure();\n        this.initWineVersions();\n\n        Bindings.bindContent(availableEngines.getTabs(), mappedSubCategoryTabs);\n\n        this.initializeSidebar();\n    }\n\n    public void setOnInstallEngine(Consumer<EngineDTO> onInstallEngine) {\n        this.setOnInstallEngine = onInstallEngine;\n    }\n\n    public void setOnDeleteEngine(Consumer<EngineDTO> onDeleteEngine) {\n        this.setOnDeleteEngine = onDeleteEngine;\n    }\n\n    private void initFailure() {\n\n    }\n\n    private void initWineVersions() {\n        availableEngines = new TabPane();\n        availableEngines.getStyleClass().add(\"rightPane\");\n\n        availableEngines.setTabClosingPolicy(TabPane.TabClosingPolicy.UNAVAILABLE);\n    }\n\n    \/\/ TODO: delete this method because it doesn't do what it promises, namely showing the wine versions tab\n    @Deprecated\n    public void showWineVersions() {\n        setCenter(availableEngines);\n    }\n\n    public void populate(List<EngineCategoryDTO> engineCategoryDTOS) {\n        Platform.runLater(() -> {\n            this.engineCategories.setAll(engineCategoryDTOS);\n\n            if (!engineCategoryDTOS.isEmpty()) {\n                this.sidebar.selectFirstEngineCategory();\n            }\n\n            this.closeDetailsView();\n            this.setCenter(availableEngines);\n        });\n    }\n\n    public void populateEngines(EngineCategoryDTO category) {\n        this.selectedCategory = category;\n        this.engineSubCategories.setAll(category.getSubCategories());\n    }\n\n    private void selectCategory(EngineCategoryDTO category) {\n        this.setCenter(availableEngines);\n        this.populateEngines(category);\n    }\n\n    private void showEngineDetails(EngineDTO engineDTO) {\n        currentEnginePanel = new EnginePanel(engineDTO);\n        currentEnginePanel.setOnClose(this::closeDetailsView);\n        currentEnginePanel.setOnEngineInstall(this::installEngine);\n        currentEnginePanel.setOnEngineDelete(this::deleteEngine);\n\n        this.showDetailsView(currentEnginePanel);\n    }\n\n    private void processFilterText(String filterText) {\n        this.pause.setOnFinished(event -> {\n            String text = filterText.toLowerCase();\n\n            availableEngines.getTabs().forEach(tab -> ((EngineSubCategoryTab) tab).setFilterForSearchTerm(text));\n        });\n\n        this.pause.playFromStart();\n    }\n\n    private void installEngine(EngineDTO engineDTO) {\n        this.setOnInstallEngine.accept(engineDTO);\n    }\n\n    private void deleteEngine(EngineDTO engineDTO) {\n        this.setOnDeleteEngine.accept(engineDTO);\n    }\n\n    public void showProgress(VBox progressUi) {\n        currentEnginePanel.showProgress(progressUi);\n    }\n}\n","lang_cluster":"Java","length":174,"code_uid":"084c2b9505b14b6fb4cf926b2cfc72f2"}
{"diff_hunk":"@@ -7,7 +7,8 @@ import com.fsck.k9.message.preview.PreviewResult.PreviewType;\n public enum DatabasePreviewType {\n     NONE(\"none\", PreviewType.NONE),\n     TEXT(\"text\", PreviewType.TEXT),\n-    ENCRYPTED(\"encrypted\", PreviewType.ENCRYPTED);\n+    ENCRYPTED(\"encrypted\", PreviewType.ENCRYPTED),\n+    FAILED_TO_LOAD(\"failedToLoad\", PreviewType.FAILED_TO_LOAD);\n \n \n     private final String databaseValue;","old_code":"package com.fsck.k9.mailstore;\n\n\nimport com.fsck.k9.message.preview.PreviewResult.PreviewType;\n\n\npublic enum DatabasePreviewType {\n    NONE(\"none\", PreviewType.NONE),\n    TEXT(\"text\", PreviewType.TEXT),\n    ENCRYPTED(\"encrypted\", PreviewType.ENCRYPTED);\n\n\n    private final String databaseValue;\n    private final PreviewType previewType;\n\n\n    DatabasePreviewType(String databaseValue, PreviewType previewType) {\n        this.databaseValue = databaseValue;\n        this.previewType = previewType;\n    }\n\n    public static DatabasePreviewType fromDatabaseValue(String databaseValue) {\n        for (DatabasePreviewType databasePreviewType : values()) {\n            if (databasePreviewType.getDatabaseValue().equals(databaseValue)) {\n                return databasePreviewType;\n            }\n        }\n\n        throw new AssertionError(\"Unknown database value: \" + databaseValue);\n    }\n\n    public static DatabasePreviewType fromPreviewType(PreviewType previewType) {\n        for (DatabasePreviewType databasePreviewType : values()) {\n            if (databasePreviewType.previewType == previewType) {\n                return databasePreviewType;\n            }\n        }\n\n        throw new AssertionError(\"Unknown preview type: \" + previewType);\n    }\n\n    public String getDatabaseValue() {\n        return databaseValue;\n    }\n\n    public PreviewType getPreviewType() {\n        return previewType;\n    }\n}\n","lang_cluster":"Java","length":49,"code_uid":"18bcd69ca44445a1b7279b970b0b2753"}
{"diff_hunk":"@@ -18,14 +18,19 @@\n package org.apache.servicecomb.foundation.vertx.server;\n \n import java.net.InetSocketAddress;\n+import java.util.concurrent.atomic.AtomicInteger;\n \n+import org.apache.servicecomb.foundation.common.event.EventManager;\n import org.apache.servicecomb.foundation.common.net.URIEndpointObject;\n import org.apache.servicecomb.foundation.ssl.SSLCustom;\n import org.apache.servicecomb.foundation.ssl.SSLOption;\n import org.apache.servicecomb.foundation.ssl.SSLOptionFactory;\n import org.apache.servicecomb.foundation.vertx.AsyncResultCallback;\n+import org.apache.servicecomb.foundation.vertx.ClientConnectedEvent;\n import org.apache.servicecomb.foundation.vertx.VertxTLSBuilder;\n \n+import com.netflix.config.DynamicPropertyFactory;\n+\n import io.vertx.core.Vertx;\n import io.vertx.core.net.NetServer;\n import io.vertx.core.net.NetServerOptions;","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage org.apache.servicecomb.foundation.vertx.server;\n\nimport java.net.InetSocketAddress;\n\nimport org.apache.servicecomb.foundation.common.net.URIEndpointObject;\nimport org.apache.servicecomb.foundation.ssl.SSLCustom;\nimport org.apache.servicecomb.foundation.ssl.SSLOption;\nimport org.apache.servicecomb.foundation.ssl.SSLOptionFactory;\nimport org.apache.servicecomb.foundation.vertx.AsyncResultCallback;\nimport org.apache.servicecomb.foundation.vertx.VertxTLSBuilder;\n\nimport io.vertx.core.Vertx;\nimport io.vertx.core.net.NetServer;\nimport io.vertx.core.net.NetServerOptions;\n\npublic class TcpServer {\n  private URIEndpointObject endpointObject;\n\n  public TcpServer(URIEndpointObject endpointObject) {\n    this.endpointObject = endpointObject;\n  }\n\n  public void init(Vertx vertx, String sslKey, AsyncResultCallback<InetSocketAddress> callback) {\n    NetServer netServer;\n    if (endpointObject.isSslEnabled()) {\n      SSLOptionFactory factory =\n          SSLOptionFactory.createSSLOptionFactory(sslKey, null);\n      SSLOption sslOption;\n      if (factory == null) {\n        sslOption = SSLOption.buildFromYaml(sslKey);\n      } else {\n        sslOption = factory.createSSLOption();\n      }\n      SSLCustom sslCustom = SSLCustom.createSSLCustom(sslOption.getSslCustomClass());\n      NetServerOptions serverOptions = new NetServerOptions();\n      VertxTLSBuilder.buildNetServerOptions(sslOption, sslCustom, serverOptions);\n      netServer = vertx.createNetServer(serverOptions);\n    } else {\n      netServer = vertx.createNetServer();\n    }\n\n    netServer.connectHandler(netSocket -> {\n      TcpServerConnection connection = createTcpServerConnection();\n      connection.init(netSocket);\n    });\n\n    InetSocketAddress socketAddress = endpointObject.getSocketAddress();\n    netServer.listen(socketAddress.getPort(), socketAddress.getHostString(), ar -> {\n      if (ar.succeeded()) {\n        callback.success(socketAddress);\n        return;\n      }\n\n      \/\/ \u76d1\u542c\u5931\u8d25\n      String msg = String.format(\"listen failed, address=%s\", socketAddress.toString());\n      callback.fail(new Exception(msg, ar.cause()));\n    });\n  }\n\n  protected TcpServerConnection createTcpServerConnection() {\n    return new TcpServerConnection();\n  }\n}\n","lang_cluster":"Java","length":80,"code_uid":"75629350d62f4f9d9d9e76dcfe5204c3"}
{"diff_hunk":"@@ -61,6 +61,13 @@ public class OperationConfig {\n   @InjectProperty(keys = {\"request.${op-any-priority}.timeout\", \"request.timeout\"}, defaultValue = \"30000\")\n   private long msRequestTimeout;\n \n+  \/**\n+   * whether to remove certain headers from the 3rd party invocations\n+   *\/\n+  @InjectProperty(keys = {\"request.${op-any-priority}.clientRequestHeaderFilterEnabled\",\n+      \"request.clientRequestHeaderFilterEnabled\"}, defaultValue = \"true\")\n+  private boolean clientRequestHeaderFilterEnabled = true;\n+\n   \/**\n    * producer wait in thread pool timeout\n    *\/","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage org.apache.servicecomb.core.definition;\n\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.concurrent.TimeUnit;\n\nimport org.apache.servicecomb.config.inject.InjectProperties;\nimport org.apache.servicecomb.config.inject.InjectProperty;\n\n@InjectProperties(prefix = \"servicecomb\")\npublic class OperationConfig {\n  public static final List<String> CONSUMER_OP_ANY_PRIORITY = Arrays.asList(\n      \"${service}.${schema}.${operation}\",\n      \"${service}.${schema}\",\n      \"${service}\");\n\n  public static final List<String> PRODUCER_OP_ANY_PRIORITY = Arrays.asList(\n      \"${schema}.${operation}\",\n      \"${schema}\");\n\n  public static final List<String> CONSUMER_OP_PRIORITY = Arrays.asList(\n      \".${service}.${schema}.${operation}\",\n      \".${service}.${schema}\",\n      \".${service}\",\n      \"\");\n\n  public static final List<String> PRODUCER_OP_PRIORITY = Arrays.asList(\n      \".${schema}.${operation}\",\n      \".${schema}\",\n      \"\");\n\n  @InjectProperty(keys = {\"metrics.${consumer-producer}.invocation.slow.enabled${op-priority}\",\n      \"${consumer-producer}.invocation.slow.enabled${op-priority}\"}, defaultValue = \"false\")\n  private boolean slowInvocationEnabled;\n\n  @InjectProperty(keys = {\"metrics.${consumer-producer}.invocation.slow.msTime${op-priority}\",\n      \"${consumer-producer}.invocation.slow.msTime${op-priority}\"}, defaultValue = \"1000\")\n  private long msSlowInvocation;\n\n  private long nanoSlowInvocation;\n\n  \/**\n   * consumer request timeout\n   *\/\n  @InjectProperty(keys = {\"request.${op-any-priority}.timeout\", \"request.timeout\"}, defaultValue = \"30000\")\n  private long msRequestTimeout;\n\n  \/**\n   * producer wait in thread pool timeout\n   *\/\n  @InjectProperty(keys = {\n      \"Provider.requestWaitInPoolTimeout${op-priority}\",\n      \"highway.server.requestWaitInPoolTimeout\"}, defaultValue = \"30000\")\n  private long msHighwayRequestWaitInPoolTimeout;\n\n  private long nanoHighwayRequestWaitInPoolTimeout;\n\n  @InjectProperty(keys = {\n      \"Provider.requestWaitInPoolTimeout${op-priority}\",\n      \"rest.server.requestWaitInPoolTimeout\"}, defaultValue = \"30000\")\n  private long msRestRequestWaitInPoolTimeout;\n\n  private long nanoRestRequestWaitInPoolTimeout;\n\n  public boolean isSlowInvocationEnabled() {\n    return slowInvocationEnabled;\n  }\n\n  public void setSlowInvocationEnabled(boolean slowInvocationEnabled) {\n    this.slowInvocationEnabled = slowInvocationEnabled;\n  }\n\n  public long getMsSlowInvocation() {\n    return msSlowInvocation;\n  }\n\n  public void setMsSlowInvocation(long msSlowInvocation) {\n    this.msSlowInvocation = msSlowInvocation;\n    this.nanoSlowInvocation = TimeUnit.MILLISECONDS.toNanos(msSlowInvocation);\n  }\n\n  public long getNanoSlowInvocation() {\n    return nanoSlowInvocation;\n  }\n\n  public long getMsRequestTimeout() {\n    return msRequestTimeout;\n  }\n\n  public void setMsRequestTimeout(long msRequestTimeout) {\n    this.msRequestTimeout = msRequestTimeout;\n  }\n\n  public long getMsHighwayRequestWaitInPoolTimeout() {\n    return msHighwayRequestWaitInPoolTimeout;\n  }\n\n  public void setMsHighwayRequestWaitInPoolTimeout(long msHighwayRequestWaitInPoolTimeout) {\n    this.msHighwayRequestWaitInPoolTimeout = msHighwayRequestWaitInPoolTimeout;\n    this.nanoHighwayRequestWaitInPoolTimeout = TimeUnit.MILLISECONDS.toNanos(msHighwayRequestWaitInPoolTimeout);\n  }\n\n  public long getNanoHighwayRequestWaitInPoolTimeout() {\n    return nanoHighwayRequestWaitInPoolTimeout;\n  }\n\n  public long getMsRestRequestWaitInPoolTimeout() {\n    return msRestRequestWaitInPoolTimeout;\n  }\n\n  public void setMsRestRequestWaitInPoolTimeout(long msRestRequestWaitInPoolTimeout) {\n    this.msRestRequestWaitInPoolTimeout = msRestRequestWaitInPoolTimeout;\n    this.nanoRestRequestWaitInPoolTimeout = TimeUnit.MILLISECONDS.toNanos(msRestRequestWaitInPoolTimeout);\n  }\n\n  public long getNanoRestRequestWaitInPoolTimeout() {\n    return nanoRestRequestWaitInPoolTimeout;\n  }\n}\n","lang_cluster":"Java","length":135,"code_uid":"0ed13589430d4084a23d14abe32a980b"}
{"diff_hunk":"@@ -76,11 +76,13 @@ public final class ArrayCreationLevel extends Node implements NodeWithAnnotation\n     }\n \n     @Override\n+    @Generated(\"com.github.javaparser.generator.core.node.AcceptGenerator\")\n     public <R, A> R accept(final GenericVisitor<R, A> v, final A arg) {\n         return v.visit(this, arg);\n     }\n \n     @Override\n+    @Generated(\"com.github.javaparser.generator.core.node.AcceptGenerator\")\n     public <A> void accept(final VoidVisitor<A> v, final A arg) {\n         v.visit(this, arg);\n     }","old_code":"\/*\n * Copyright (C) 2007-2010 J\u00falio Vilmar Gesser.\n * Copyright (C) 2011, 2013-2016 The JavaParser Team.\n *\n * This file is part of JavaParser.\n *\n * JavaParser can be used either under the terms of\n * a) the GNU Lesser General Public License as published by\n *     the Free Software Foundation, either version 3 of the License, or\n *     (at your option) any later version.\n * b) the terms of the Apache License\n *\n * You should have received a copy of both licenses in LICENCE.LGPL and\n * LICENCE.APACHE. Please refer to those files for details.\n *\n * JavaParser is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU Lesser General Public License for more details.\n *\/\npackage com.github.javaparser.ast;\n\nimport com.github.javaparser.ast.expr.AnnotationExpr;\nimport com.github.javaparser.ast.expr.Expression;\nimport com.github.javaparser.ast.expr.IntegerLiteralExpr;\nimport com.github.javaparser.ast.nodeTypes.NodeWithAnnotations;\nimport com.github.javaparser.ast.observer.ObservableProperty;\nimport com.github.javaparser.ast.visitor.GenericVisitor;\nimport com.github.javaparser.ast.visitor.VoidVisitor;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Optional;\nimport static com.github.javaparser.utils.Utils.assertNotNull;\nimport com.github.javaparser.ast.Node;\nimport com.github.javaparser.ast.visitor.CloneVisitor;\nimport com.github.javaparser.metamodel.ArrayCreationLevelMetaModel;\nimport com.github.javaparser.metamodel.JavaParserMetaModel;\nimport javax.annotation.Generated;\nimport com.github.javaparser.TokenRange;\n\n\/**\n * In <code>new int[1][2];<\/code> there are two ArrayCreationLevel objects,\n * the first one contains the expression \"1\",\n * the second the expression \"2\".\n *\/\npublic final class ArrayCreationLevel extends Node implements NodeWithAnnotations<ArrayCreationLevel> {\n\n    private Expression dimension;\n\n    private NodeList<AnnotationExpr> annotations = new NodeList<>();\n\n    public ArrayCreationLevel() {\n        this(null, null, new NodeList<>());\n    }\n\n    public ArrayCreationLevel(int dimension) {\n        this(null, new IntegerLiteralExpr(\"\" + dimension), new NodeList<>());\n    }\n\n    public ArrayCreationLevel(Expression dimension) {\n        this(null, dimension, new NodeList<>());\n    }\n\n    @AllFieldsConstructor\n    public ArrayCreationLevel(Expression dimension, NodeList<AnnotationExpr> annotations) {\n        this(null, dimension, annotations);\n    }\n\n    \/**This constructor is used by the parser and is considered private.*\/\n    @Generated(\"com.github.javaparser.generator.core.node.MainConstructorGenerator\")\n    public ArrayCreationLevel(TokenRange tokenRange, Expression dimension, NodeList<AnnotationExpr> annotations) {\n        super(tokenRange);\n        setDimension(dimension);\n        setAnnotations(annotations);\n        customInitialization();\n    }\n\n    @Override\n    public <R, A> R accept(final GenericVisitor<R, A> v, final A arg) {\n        return v.visit(this, arg);\n    }\n\n    @Override\n    public <A> void accept(final VoidVisitor<A> v, final A arg) {\n        v.visit(this, arg);\n    }\n\n    \/**\n     * Sets the dimension\n     *\n     * @param dimension the dimension, can be null\n     * @return this, the ArrayCreationLevel\n     *\/\n    @Generated(\"com.github.javaparser.generator.core.node.PropertyGenerator\")\n    public ArrayCreationLevel setDimension(final Expression dimension) {\n        if (dimension == this.dimension) {\n            return (ArrayCreationLevel) this;\n        }\n        notifyPropertyChange(ObservableProperty.DIMENSION, this.dimension, dimension);\n        if (this.dimension != null)\n            this.dimension.setParentNode(null);\n        this.dimension = dimension;\n        setAsParentNodeOf(dimension);\n        return this;\n    }\n\n    @Generated(\"com.github.javaparser.generator.core.node.PropertyGenerator\")\n    public Optional<Expression> getDimension() {\n        return Optional.ofNullable(dimension);\n    }\n\n    @Generated(\"com.github.javaparser.generator.core.node.PropertyGenerator\")\n    public NodeList<AnnotationExpr> getAnnotations() {\n        return annotations;\n    }\n\n    @Generated(\"com.github.javaparser.generator.core.node.PropertyGenerator\")\n    public ArrayCreationLevel setAnnotations(final NodeList<AnnotationExpr> annotations) {\n        assertNotNull(annotations);\n        if (annotations == this.annotations) {\n            return (ArrayCreationLevel) this;\n        }\n        notifyPropertyChange(ObservableProperty.ANNOTATIONS, this.annotations, annotations);\n        if (this.annotations != null)\n            this.annotations.setParentNode(null);\n        this.annotations = annotations;\n        setAsParentNodeOf(annotations);\n        return this;\n    }\n\n    @Generated(\"com.github.javaparser.generator.core.node.RemoveMethodGenerator\")\n    public ArrayCreationLevel removeDimension() {\n        return setDimension((Expression) null);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.RemoveMethodGenerator\")\n    public boolean remove(Node node) {\n        if (node == null)\n            return false;\n        for (int i = 0; i < annotations.size(); i++) {\n            if (annotations.get(i) == node) {\n                annotations.remove(i);\n                return true;\n            }\n        }\n        if (dimension != null) {\n            if (node == dimension) {\n                removeDimension();\n                return true;\n            }\n        }\n        return super.remove(node);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.CloneGenerator\")\n    public ArrayCreationLevel clone() {\n        return (ArrayCreationLevel) accept(new CloneVisitor(), null);\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.GetMetaModelGenerator\")\n    public ArrayCreationLevelMetaModel getMetaModel() {\n        return JavaParserMetaModel.arrayCreationLevelMetaModel;\n    }\n\n    @Override\n    @Generated(\"com.github.javaparser.generator.core.node.ReplaceMethodGenerator\")\n    public boolean replace(Node node, Node replacementNode) {\n        if (node == null)\n            return false;\n        for (int i = 0; i < annotations.size(); i++) {\n            if (annotations.get(i) == node) {\n                annotations.set(i, (AnnotationExpr) replacementNode);\n                return true;\n            }\n        }\n        if (dimension != null) {\n            if (node == dimension) {\n                setDimension((Expression) replacementNode);\n                return true;\n            }\n        }\n        return super.replace(node, replacementNode);\n    }\n}\n","lang_cluster":"Java","length":187,"code_uid":"3f17d1c4040747d8ae14332d43710f8f"}
{"diff_hunk":"@@ -4,11 +4,14 @@\n \n package net.sourceforge.pmd.lang.rule.properties;\n \n+import java.util.Collections;\n import java.util.Enumeration;\n import java.util.Map;\n \n+import net.sourceforge.pmd.EnumeratedPropertyDescriptor;\n import net.sourceforge.pmd.PropertyDescriptorFactory;\n-import net.sourceforge.pmd.lang.rule.properties.factories.BasicPropertyDescriptorFactory;\n+import net.sourceforge.pmd.PropertyDescriptorField;\n+import net.sourceforge.pmd.util.CollectionUtil;\n \n \/**\n  * Defines a datatype with a set of preset values of any type as held within a","old_code":"\/**\n * BSD-style license; for more info see http:\/\/pmd.sourceforge.net\/license.html\n *\/\n\npackage net.sourceforge.pmd.lang.rule.properties;\n\nimport java.util.Enumeration;\nimport java.util.Map;\n\nimport net.sourceforge.pmd.PropertyDescriptorFactory;\nimport net.sourceforge.pmd.lang.rule.properties.factories.BasicPropertyDescriptorFactory;\n\n\/**\n * Defines a datatype with a set of preset values of any type as held within a\n * pair of maps. While the values are not serialized out, the labels are and\n * serve as keys to obtain the values. The choices() method provides the ordered\n * selections to be used in an editor widget.\n *\n * @author Brian Remedios\n * @param <E>\n *\/\npublic class EnumeratedProperty<E> extends AbstractEnumeratedProperty<E, Object> {\n\n    public static final PropertyDescriptorFactory FACTORY = new BasicPropertyDescriptorFactory<EnumeratedProperty>(\n            Enumeration.class) {\n\n        @Override\n        public EnumeratedProperty createWith(Map<String, String> valuesById) {\n\n            return new EnumeratedProperty(nameIn(valuesById), descriptionIn(valuesById), labelsIn(valuesById),\n                    choicesIn(valuesById), indexIn(valuesById), 0f);\n        }\n    };\n\n    \/**\n     * Constructor for EnumeratedProperty.\n     *\n     * @param theName\n     *            String\n     * @param theDescription\n     *            String\n     * @param theLabels\n     *            String[]\n     * @param theChoices\n     *            E[]\n     * @param defaultIndex\n     *            int\n     * @param theUIOrder\n     *            float\n     * @throws IllegalArgumentException\n     *\/\n    public EnumeratedProperty(String theName, String theDescription, String[] theLabels, E[] theChoices,\n            int defaultIndex, float theUIOrder) {\n        super(theName, theDescription, theLabels, theChoices, new int[] { defaultIndex }, theUIOrder, false);\n    }\n\n    \/**\n     * @return Class\n     * @see net.sourceforge.pmd.PropertyDescriptor#type()\n     *\/\n    @Override\n    public Class<Object> type() {\n        return Object.class;\n    }\n\n    \/**\n     * @param value\n     *            Object\n     * @return String\n     * @see net.sourceforge.pmd.PropertyDescriptor#errorFor(Object)\n     *\/\n    @Override\n    public String errorFor(Object value) {\n        return labelsByChoice.containsKey(value) ? null : nonLegalValueMsgFor(value);\n    }\n\n    \/**\n     * @param value\n     *            String\n     * @return Object\n     * @throws IllegalArgumentException\n     * @see net.sourceforge.pmd.PropertyDescriptor#valueFrom(String)\n     *\/\n    @Override\n    public Object valueFrom(String value) throws IllegalArgumentException {\n        return choiceFrom(value);\n    }\n\n    \/**\n     *\n     * @param value\n     *            Object\n     * @return String\n     * @see net.sourceforge.pmd.PropertyDescriptor#asDelimitedString(Object)\n     *\/\n    @Override\n    public String asDelimitedString(Object value) {\n        return labelsByChoice.get(value);\n    }\n}\n","lang_cluster":"Java","length":100,"code_uid":"380a8f7d111c46a7bda4edec57827a3b"}
{"diff_hunk":"@@ -59,6 +59,7 @@ public class EdgeInvocation extends AbstractRestInvocation {\n     this.responseEx = new VertxServerResponseToHttpServletResponse(context.response());\n     this.httpServerFilters = httpServerFilters;\n     requestEx.setAttribute(RestConst.REST_REQUEST, requestEx);\n+    setAfterCreateInvocationHandler(invocation -> context.put(RestConst.REST_INVOCATION_CONTEXT, invocation));\n   }\n \n   public void edgeInvoke() {","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage org.apache.servicecomb.edge.core;\n\nimport java.util.List;\n\nimport org.apache.servicecomb.common.rest.AbstractRestInvocation;\nimport org.apache.servicecomb.common.rest.RestConst;\nimport org.apache.servicecomb.common.rest.filter.HttpServerFilter;\nimport org.apache.servicecomb.common.rest.locator.OperationLocator;\nimport org.apache.servicecomb.common.rest.locator.ServicePathManager;\nimport org.apache.servicecomb.core.Const;\nimport org.apache.servicecomb.core.definition.MicroserviceVersionMeta;\nimport org.apache.servicecomb.core.invocation.InvocationFactory;\nimport org.apache.servicecomb.core.provider.consumer.ReactiveResponseExecutor;\nimport org.apache.servicecomb.core.provider.consumer.ReferenceConfig;\nimport org.apache.servicecomb.foundation.common.exceptions.ServiceCombException;\nimport org.apache.servicecomb.foundation.vertx.http.VertxServerRequestToHttpServletRequest;\nimport org.apache.servicecomb.foundation.vertx.http.VertxServerResponseToHttpServletResponse;\nimport org.apache.servicecomb.serviceregistry.RegistryUtils;\nimport org.apache.servicecomb.serviceregistry.consumer.MicroserviceVersionRule;\nimport org.apache.servicecomb.serviceregistry.definition.DefinitionConst;\n\nimport io.vertx.core.Vertx;\nimport io.vertx.ext.web.RoutingContext;\n\npublic class EdgeInvocation extends AbstractRestInvocation {\n  public static final String EDGE_INVOCATION_CONTEXT = \"edgeInvocationContext\";\n\n  protected String microserviceName;\n\n  protected MicroserviceVersionRule microserviceVersionRule;\n\n  protected MicroserviceVersionMeta latestMicroserviceVersionMeta;\n\n  protected ReferenceConfig referenceConfig;\n\n  protected String versionRule = DefinitionConst.VERSION_RULE_ALL;\n\n  public void init(String microserviceName, RoutingContext context, String path,\n      List<HttpServerFilter> httpServerFilters) {\n    this.microserviceName = microserviceName;\n    this.requestEx = new VertxServerRequestToHttpServletRequest(context, path);\n    this.responseEx = new VertxServerResponseToHttpServletResponse(context.response());\n    this.httpServerFilters = httpServerFilters;\n    requestEx.setAttribute(RestConst.REST_REQUEST, requestEx);\n  }\n\n  public void edgeInvoke() {\n    findMicroserviceVersionMeta();\n    findRestOperation(latestMicroserviceVersionMeta.getMicroserviceMeta());\n\n    scheduleInvocation();\n  }\n\n  protected void findMicroserviceVersionMeta() {\n    String versionRule = chooseVersionRule();\n\n    String appId = RegistryUtils.getAppId();\n    int idxAt = microserviceName.indexOf(org.apache.servicecomb.serviceregistry.api.Const.APP_SERVICE_SEPARATOR);\n    if (idxAt != -1) {\n      appId = microserviceName.substring(0, idxAt);\n    }\n\n    microserviceVersionRule = RegistryUtils.getServiceRegistry()\n        .getAppManager()\n        .getOrCreateMicroserviceVersionRule(appId, microserviceName, versionRule);\n    latestMicroserviceVersionMeta = microserviceVersionRule.getLatestMicroserviceVersion();\n\n    if (latestMicroserviceVersionMeta == null) {\n      throw new ServiceCombException(\n          String.format(\"Failed to find latest MicroserviceVersionMeta, appId=%s, microserviceName=%s, versionRule=%s.\",\n              appId,\n              microserviceName,\n              versionRule));\n    }\n  }\n\n  public void setVersionRule(String versionRule) {\n    this.versionRule = versionRule;\n  }\n\n  \/\/ another possible rule:\n  \/\/ path is: \/msName\/version\/.....\n  \/\/ version in path is v1 or v2 and so on\n  \/\/ map version to VersionRule:\n  \/\/   v1->1.0.0-2.0.0\n  \/\/   v2->2.0.0-3.0.0\n  \/\/ that means if a(1.x.x) bigger then b(1.y.y), then a compatible to b\n  \/\/        but a(2.x.x) not compatible to b   \n  protected String chooseVersionRule() {\n    \/\/ this will use all instance of the microservice\n    \/\/ and this required all new version compatible to old version\n    return versionRule;\n  }\n\n  @Override\n  protected OperationLocator locateOperation(ServicePathManager servicePathManager) {\n    return servicePathManager.consumerLocateOperation(requestEx.getRequestURI(), requestEx.getMethod());\n  }\n\n  @Override\n  protected void createInvocation() {\n    ReferenceConfig referenceConfig = new ReferenceConfig();\n    referenceConfig.setMicroserviceMeta(latestMicroserviceVersionMeta.getMicroserviceMeta());\n    referenceConfig.setMicroserviceVersionRule(microserviceVersionRule.getVersionRule().getVersionRule());\n    referenceConfig.setTransport(Const.ANY_TRANSPORT);\n\n    this.invocation = InvocationFactory.forConsumer(referenceConfig,\n        restOperationMeta.getOperationMeta(),\n        null);\n    this.invocation.setSync(false);\n    this.invocation.getHandlerContext().put(EDGE_INVOCATION_CONTEXT, Vertx.currentContext());\n    this.invocation.setResponseExecutor(new ReactiveResponseExecutor());\n  }\n}\n","lang_cluster":"Java","length":131,"code_uid":"579326079d3e4a7086da876451124321"}
{"diff_hunk":"@@ -17,11 +17,12 @@\n \n package org.apache.servicecomb.metrics.core.monitor;\n \n+import java.util.HashMap;\n+import java.util.Map;\n \n-import org.apache.servicecomb.metrics.common.ConsumerInvocationMetric;\n-import org.apache.servicecomb.metrics.common.MetricsConst;\n+import org.apache.servicecomb.foundation.metrics.MetricsConst;\n \n-public class ConsumerInvocationMonitor extends InvocationMonitor {\n+public class ConsumerInvocationMonitor {\n   private final TimerMonitor consumerLatency;\n \n   private final CallMonitor consumerCall;","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage org.apache.servicecomb.metrics.core.monitor;\n\n\nimport org.apache.servicecomb.metrics.common.ConsumerInvocationMetric;\nimport org.apache.servicecomb.metrics.common.MetricsConst;\n\npublic class ConsumerInvocationMonitor extends InvocationMonitor {\n  private final TimerMonitor consumerLatency;\n\n  private final CallMonitor consumerCall;\n\n  public TimerMonitor getConsumerLatency() {\n    return consumerLatency;\n  }\n\n  public CallMonitor getConsumerCall() {\n    return consumerCall;\n  }\n\n  public ConsumerInvocationMonitor(String operationName) {\n    super(operationName, String.format(MetricsConst.CONSUMER_PREFIX_TEMPLATE, operationName));\n    this.consumerLatency = new TimerMonitor(this.getPrefix() + \".consumerLatency\");\n    this.consumerCall = new CallMonitor(this.getPrefix() + \".consumerCall\");\n  }\n\n  public ConsumerInvocationMetric toMetric(int windowTimeIndex) {\n    return new ConsumerInvocationMetric(this.getOperationName(), this.getPrefix(),\n        consumerLatency.toMetric(windowTimeIndex), consumerCall.toMetric(windowTimeIndex));\n  }\n}\n","lang_cluster":"Java","length":47,"code_uid":"8f8537d68ca34d41941738adaacd0184"}
{"diff_hunk":"@@ -63,6 +63,14 @@ class UpdateManager {\n                 UserPreferences.enableSonic();\n             }\n         }\n+\n+        if(oldVersionCode < 1070196) {\n+            \/\/ migrate episode cleanup value (unit changed from days to hours)\n+            int oldValueInDays = UserPreferences.getEpisodeCleanupValue();\n+            if (oldValueInDays > 0) {\n+                UserPreferences.setEpisodeCleanupValue(oldValueInDays * 24);\n+            } \/\/ else 0 or special negative values, no change needed\n+        }\n     }\n \n }","old_code":"package de.danoeh.antennapod.core;\n\n\nimport android.content.Context;\nimport android.content.SharedPreferences;\nimport android.content.pm.PackageInfo;\nimport android.content.pm.PackageManager;\nimport android.os.Build;\nimport android.util.Log;\n\nimport org.antennapod.audio.MediaPlayer;\n\nimport de.danoeh.antennapod.core.preferences.UserPreferences;\n\n\/*\n * This class's job is do perform maintenance tasks whenever the app has been updated\n *\/\nclass UpdateManager {\n\n    private UpdateManager(){}\n\n    private static final String TAG = UpdateManager.class.getSimpleName();\n\n    private static final String PREF_NAME = \"app_version\";\n    private static final String KEY_VERSION_CODE = \"version_code\";\n\n    private static int currentVersionCode;\n\n    private static Context context;\n    private static SharedPreferences prefs;\n\n    public static void init(Context context) {\n        UpdateManager.context = context;\n        prefs = context.getSharedPreferences(PREF_NAME, Context.MODE_PRIVATE);\n        PackageManager pm = context.getPackageManager();\n        try {\n            PackageInfo info = pm.getPackageInfo(context.getPackageName(), 0);\n            currentVersionCode = info.versionCode;\n        } catch (PackageManager.NameNotFoundException e) {\n            Log.e(TAG, \"Failed to obtain package info for package name: \" + context.getPackageName(), e);\n            currentVersionCode = 0;\n            return;\n        }\n        final int oldVersionCode = getStoredVersionCode();\n        Log.d(TAG, \"old: \" + oldVersionCode + \", current: \" + currentVersionCode);\n        if(oldVersionCode < currentVersionCode) {\n            onUpgrade(oldVersionCode, currentVersionCode);\n            setCurrentVersionCode();\n        }\n    }\n\n    private static int getStoredVersionCode() {\n        return prefs.getInt(KEY_VERSION_CODE, -1);\n    }\n\n    private static void setCurrentVersionCode() {\n        prefs.edit().putInt(KEY_VERSION_CODE, currentVersionCode).apply();\n    }\n\n    private static void onUpgrade(final int oldVersionCode, final int newVersionCode) {\n        if(oldVersionCode < 1050004) {\n            if(MediaPlayer.isPrestoLibraryInstalled(context) && Build.VERSION.SDK_INT >= 16) {\n                UserPreferences.enableSonic();\n            }\n        }\n    }\n\n}\n","lang_cluster":"Java","length":68,"code_uid":"e6276a0b08a74c53ad91f42e72cffc37"}
{"diff_hunk":"@@ -65,6 +65,7 @@ public final class BaselineErrorProne implements Plugin<Project> {\n                                 .configure(ErrorProneOptions.class, errorProneOptions -> {\n                                     errorProneOptions.check(\"Slf4jLogsafeArgs\", CheckSeverity.OFF);\n                                     errorProneOptions.check(\"PreferSafeLoggableExceptions\", CheckSeverity.OFF);\n+                                    errorProneOptions.check(\"Slf4jConstantLogMessage\", CheckSeverity.OFF);\n                                 }));\n             });\n ","old_code":"\/*\n * (c) Copyright 2017 Palantir Technologies Inc. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage com.palantir.baseline.plugins;\n\nimport com.google.common.base.Splitter;\nimport com.google.common.collect.ImmutableList;\nimport java.io.File;\nimport java.util.AbstractList;\nimport java.util.List;\nimport java.util.Optional;\nimport java.util.stream.Collectors;\nimport net.ltgt.gradle.errorprone.CheckSeverity;\nimport net.ltgt.gradle.errorprone.ErrorProneOptions;\nimport net.ltgt.gradle.errorprone.ErrorPronePlugin;\nimport org.gradle.api.JavaVersion;\nimport org.gradle.api.Plugin;\nimport org.gradle.api.Project;\nimport org.gradle.api.file.FileCollection;\nimport org.gradle.api.plugins.ExtensionAware;\nimport org.gradle.api.tasks.compile.JavaCompile;\nimport org.gradle.api.tasks.javadoc.Javadoc;\nimport org.gradle.api.tasks.testing.Test;\n\npublic final class BaselineErrorProne implements Plugin<Project> {\n\n    private static final String ERROR_PRONE_JAVAC_VERSION = \"9+181-r4173-1\";\n\n    @Override\n    public void apply(Project project) {\n        project.getPluginManager().withPlugin(\"java\", plugin -> {\n            project.getPluginManager().apply(ErrorPronePlugin.class);\n            String version = Optional.ofNullable(getClass().getPackage().getImplementationVersion())\n                    .orElse(\"latest.release\");\n            project.getDependencies().add(\n                    ErrorPronePlugin.CONFIGURATION_NAME,\n                    \"com.palantir.baseline:baseline-error-prone:\" + version);\n\n            project.getTasks().withType(JavaCompile.class).configureEach(javaCompile ->\n                    ((ExtensionAware) javaCompile.getOptions()).getExtensions()\n                            .configure(ErrorProneOptions.class, errorProneOptions -> {\n                                errorProneOptions.setEnabled(true);\n                                errorProneOptions.setDisableWarningsInGeneratedCode(true);\n                                errorProneOptions.check(\"EqualsHashCode\", CheckSeverity.ERROR);\n                                errorProneOptions.check(\"EqualsIncompatibleType\", CheckSeverity.ERROR);\n                                errorProneOptions.check(\"StreamResourceLeak\", CheckSeverity.ERROR);\n                            }));\n\n            project.getPluginManager().withPlugin(\"java-gradle-plugin\", appliedPlugin -> {\n                project.getTasks().withType(JavaCompile.class).configureEach(javaCompile ->\n                        ((ExtensionAware) javaCompile.getOptions()).getExtensions()\n                                .configure(ErrorProneOptions.class, errorProneOptions -> {\n                                    errorProneOptions.check(\"Slf4jLogsafeArgs\", CheckSeverity.OFF);\n                                    errorProneOptions.check(\"PreferSafeLoggableExceptions\", CheckSeverity.OFF);\n                                }));\n            });\n\n            \/\/ In case of java 8 we need to add errorprone javac compiler to bootstrap classpath of tasks that perform\n            \/\/ compilation or code analysis. ErrorProneJavacPluginPlugin handles JavaCompile cases via errorproneJavac\n            \/\/ configuration and we do similar thing for Test and Javadoc type tasks\n            if (!JavaVersion.current().isJava9Compatible()) {\n                project.getDependencies().add(ErrorPronePlugin.JAVAC_CONFIGURATION_NAME,\n                        \"com.google.errorprone:javac:\" + ERROR_PRONE_JAVAC_VERSION);\n                project.getConfigurations()\n                        .named(ErrorPronePlugin.JAVAC_CONFIGURATION_NAME)\n                        .configure(conf -> {\n                            List<File> bootstrapClasspath = Splitter.on(File.pathSeparator)\n                                    .splitToList(System.getProperty(\"sun.boot.class.path\"))\n                                    .stream()\n                                    .map(File::new)\n                                    .collect(Collectors.toList());\n                            FileCollection errorProneFiles = conf.plus(project.files(bootstrapClasspath));\n                            project.getTasks().withType(Test.class)\n                                    .configureEach(test -> test.setBootstrapClasspath(errorProneFiles));\n                            project.getTasks().withType(Javadoc.class)\n                                    .configureEach(javadoc -> javadoc.getOptions()\n                                            .setBootClasspath(new LazyConfigurationList(errorProneFiles)));\n                        });\n            }\n        });\n    }\n\n    private static final class LazyConfigurationList extends AbstractList<File> {\n        private final FileCollection files;\n        private List<File> fileList;\n\n        private LazyConfigurationList(FileCollection files) {\n            this.files = files;\n        }\n\n        @Override\n        public File get(int index) {\n            if (fileList == null) {\n                fileList = ImmutableList.copyOf(files.getFiles());\n            }\n            return fileList.get(index);\n        }\n\n        @Override\n        public int size() {\n            if (fileList == null) {\n                fileList = ImmutableList.copyOf(files.getFiles());\n            }\n            return fileList.size();\n        }\n    }\n\n}\n","lang_cluster":"Java","length":121,"code_uid":"9f0f7d7b57714f7fb19f24f77215cb7e"}
{"diff_hunk":"@@ -69,7 +69,6 @@ public class HttpAccess {\n      * Initializes HttpAccess. Should be called from the application.\n      *\/\n     public static void init(Context app) {\n-        assert DEFAULT == null : \"HttpAccess.init should be called once per process\";\n         DEFAULT = new HttpAccess(app, null \/* user agent will be calculated at request time *\/);\n     }\n ","old_code":"\/*\n * Copyright (c) 2011-present, salesforce.com, inc.\n * All rights reserved.\n * Redistribution and use of this software in source and binary forms, with or\n * without modification, are permitted provided that the following conditions\n * are met:\n * - Redistributions of source code must retain the above copyright notice, this\n * list of conditions and the following disclaimer.\n * - Redistributions in binary form must reproduce the above copyright notice,\n * this list of conditions and the following disclaimer in the documentation\n * and\/or other materials provided with the distribution.\n * - Neither the name of salesforce.com, inc. nor the names of its contributors\n * may be used to endorse or promote products derived from this software without\n * specific prior written permission of salesforce.com, inc.\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n * POSSIBILITY OF SUCH DAMAGE.\n *\/\npackage com.salesforce.androidsdk.auth;\n\nimport android.content.Context;\nimport android.net.ConnectivityManager;\nimport android.net.NetworkInfo;\n\nimport com.salesforce.androidsdk.app.SalesforceSDKManager;\n\nimport java.io.IOException;\nimport java.util.Collections;\nimport java.util.concurrent.TimeUnit;\n\nimport okhttp3.ConnectionSpec;\nimport okhttp3.Interceptor;\nimport okhttp3.OkHttpClient;\nimport okhttp3.Request;\nimport okhttp3.Response;\nimport okhttp3.TlsVersion;\n\n\/**\n * Generic HTTP Access layer - used internally by {@link com.salesforce.androidsdk.rest.RestClient}\n * and {@link OAuth2}. This class watches network changes as well.\n *\/\npublic class HttpAccess {\n\n    \/\/ Timeouts.\n    public static final int CONNECT_TIMEOUT = 60;\n    public static final int READ_TIMEOUT = 20;\n\n    \/\/ User agent header name.\n\tprivate static final String USER_AGENT = \"User-Agent\";\n\n    private String userAgent;\n    private OkHttpClient okHttpClient;\n\n    \/\/ Connection manager.\n    private final ConnectivityManager conMgr;\n\n    \/\/ Singleton instance.\n    public static HttpAccess DEFAULT;\n\n    \/**\n     * Initializes HttpAccess. Should be called from the application.\n     *\/\n    public static void init(Context app) {\n        assert DEFAULT == null : \"HttpAccess.init should be called once per process\";\n        DEFAULT = new HttpAccess(app, null \/* user agent will be calculated at request time *\/);\n    }\n\n    \/**\n     * Parameterized constructor.\n     *\n     * @param app Reference to the application.\n     * @param userAgent The user agent to be used with requests.\n     *\/\n    public HttpAccess(Context app, String userAgent) {\n        this.userAgent = userAgent;\n\n        \/\/ Only null in tests.\n        if (app == null) {\n            conMgr = null;\n        } else {\n\n            \/\/ Gets the connectivity manager and current network type.\n            conMgr = (ConnectivityManager) app.getSystemService(Context.CONNECTIVITY_SERVICE);\n        }\n\n    }\n\n    \/**\n     *\n     * @return okHttpClient.Builder with appropriate connection spec and user agent interceptor\n     *\/\n    public OkHttpClient.Builder getOkHttpClientBuilder() {\n        ConnectionSpec connectionSpec = new ConnectionSpec.Builder(ConnectionSpec.MODERN_TLS)\n                .tlsVersions(TlsVersion.TLS_1_1, TlsVersion.TLS_1_2)\n                .build();\n        OkHttpClient.Builder builder = new OkHttpClient.Builder()\n                .connectionSpecs(Collections.singletonList(connectionSpec))\n                .connectTimeout(CONNECT_TIMEOUT, TimeUnit.SECONDS)\n                .readTimeout(READ_TIMEOUT, TimeUnit.SECONDS)\n                .addNetworkInterceptor(new UserAgentInterceptor());\n        return builder;\n    }\n\n    \/**\n     *\n     * @return okHttpClient tied to this HttpAccess - builds one if needed\n     *\/\n    public synchronized OkHttpClient getOkHttpClient() {\n        if (okHttpClient == null) {\n            okHttpClient = getOkHttpClientBuilder().build();\n        }\n        return okHttpClient;\n    }\n\n    \/**\n     * Returns the status of network connectivity.\n     *\n     * @return True - if network connectivity is available, False - otherwise.\n     *\/\n    public synchronized boolean hasNetwork() {\n        boolean isConnected = true;\n        if (conMgr != null) {\n            final NetworkInfo activeInfo = conMgr.getActiveNetworkInfo();\n            if (activeInfo == null || !activeInfo.isConnected()) {\n                isConnected = false;\n            }\n        }\n        return isConnected;\n    }\n\n    \/**\n     * Returns the current user agent.\n     *\n     * @return User agent.\n     *\/\n    public String getUserAgent() {\n    \treturn userAgent;\n    }\n\n    \/**\n     * Exception thrown if the device is offline, during an attempted HTTP call.\n     *\/\n    public static class NoNetworkException extends IOException {\n\n        private static final long serialVersionUID = 1L;\n\n        public NoNetworkException(String msg) {\n            super(msg);\n        }\n    }\n\n    \/**\n     * Interceptor that adds user agent header\n     *\/\n    public static class UserAgentInterceptor implements Interceptor {\n\n        private String userAgent;\n\n        public UserAgentInterceptor() {\n            \/\/ User this constructor to have the user agent computed for each call\n        }\n\n        public UserAgentInterceptor(String userAgent) {\n            this.userAgent = userAgent;\n        }\n\n        @Override\n        public Response intercept(Chain chain) throws IOException {\n            Request originalRequest = chain.request();\n            Request requestWithUserAgent = originalRequest.newBuilder()\n                    .header(HttpAccess.USER_AGENT, userAgent == null ? SalesforceSDKManager.getInstance().getUserAgent() : userAgent)\n                    .build();\n            return chain.proceed(requestWithUserAgent);\n        }\n    }\n}\n","lang_cluster":"Java","length":184,"code_uid":"c48f3ca50a024c4d8585fcef18b87cf5"}
{"diff_hunk":"@@ -92,6 +92,11 @@ public class ClickhouseSQLDialect extends GenericSQLDialect {\n         return true;\n     }\n \n+    @Override\n+    public boolean supportsGroupBy() {\n+        return false;\n+    }\n+\n     @Override\n     public String getColumnTypeModifiers(@NotNull DBPDataSource dataSource, @NotNull DBSTypedObject column, @NotNull String typeName, @NotNull DBPDataKind dataKind) {\n         if (typeName.equals(\"String\")) {","old_code":"\/*\n * DBeaver - Universal Database Manager\n * Copyright (C) 2010-2022 DBeaver Corp and others\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage org.jkiss.dbeaver.ext.clickhouse.model;\n\nimport org.jkiss.code.NotNull;\nimport org.jkiss.dbeaver.ext.generic.model.GenericSQLDialect;\nimport org.jkiss.dbeaver.model.DBPDataKind;\nimport org.jkiss.dbeaver.model.DBPDataSource;\nimport org.jkiss.dbeaver.model.exec.jdbc.JDBCDatabaseMetaData;\nimport org.jkiss.dbeaver.model.exec.jdbc.JDBCSession;\nimport org.jkiss.dbeaver.model.impl.jdbc.JDBCDataSource;\nimport org.jkiss.dbeaver.model.struct.DBSTypedObject;\n\nimport java.util.Arrays;\n\npublic class ClickhouseSQLDialect extends GenericSQLDialect {\n\n    private static final String[] CLICKHOUSE_FUNCTIONS = {\n        \"quantile\",\n        \"quantileExact\",\n        \"uniq\",\n        \"concat\",\n        \"replaceOne\",\n        \"replaceAll\",\n        \"toStartOfFifteenMinutes\",\n        \"toStartOfFiveMinute\",\n        \"toStartOfInterval\",\n        \"toTimezone\",\n        \"formatDateTime\",\n        \"now\",\n        \"multiIf\",\n        \"geoToS2\",\n        \"s2ToGeo\",\n        \"greatCircleDistance\",\n        \"greatCircleAngle\",\n        \"plus\",\n        \"minus\",\n        \"multiply\",\n        \"divide\",\n        \"arrayConcat\",\n        \"hasAll\",\n        \"hasAny\",\n        \"indexOf\",\n        \"mapKeys\",\n        \"mapValues\",\n        \"UUIDNumToString\",\n        \"UUIDStringToNum\",\n        \"visitParamHas\",\n        \"IPv4StringToNum\",\n        \"randConstant\",\n        \"javaHash\",\n        \"bitmapBuild\",\n        \"bitCount\",\n        \"splitByChar\",\n        \"splitByWhitespace\",\n        \"toLowCardinality\",\n        \"formatRow\"\n    };\n\n    public ClickhouseSQLDialect() {\n        super(\"Clickhouse SQL\", \"clickhouse\");\n    }\n\n    @Override\n    public boolean supportsOrderByIndex() {\n        return false;\n    }\n\n    public void initDriverSettings(JDBCSession session, JDBCDataSource dataSource, JDBCDatabaseMetaData metaData) {\n        super.initDriverSettings(session, dataSource, metaData);\n        removeSQLKeyword(\"DEFAULT\");\n        removeSQLKeyword(\"SYSTEM\");\n        addFunctions(Arrays.asList(CLICKHOUSE_FUNCTIONS));\n    }\n\n    @Override\n    public boolean supportsAliasInSelect() {\n        return true;\n    }\n\n    @Override\n    public String getColumnTypeModifiers(@NotNull DBPDataSource dataSource, @NotNull DBSTypedObject column, @NotNull String typeName, @NotNull DBPDataKind dataKind) {\n        if (typeName.equals(\"String\")) {\n            return null;\n        }\n        return super.getColumnTypeModifiers(dataSource, column, typeName, dataKind);\n    }\n\n    @Override\n    public boolean supportsNestedComments() {\n        return true;\n    }\n}\n","lang_cluster":"Java","length":107,"code_uid":"c06b39534e724ad39484db587db76495"}
{"diff_hunk":"@@ -43,10 +43,19 @@ public class LibrarySidebarToggleGroupSkin extends\n     protected ToggleButton convertToToggleButton(ShortcutCategoryDTO category) {\n         final ToggleButton categoryButton = createSidebarToggleButton(category.getName());\n         \/\/ TODO: store category ID in shortcut.info?\n-        categoryButton.setId(\n-                String.format(\"applications-%s\", SidebarToggleGroupBaseSkin.getToggleButtonId(category.getId())));\n+        categoryButton.setId(LibrarySidebarToggleGroupSkin.getToggleButtonId(category.getId()));\n         categoryButton.setOnMouseClicked(event -> getControl().setSelectedElement(category));\n \n         return categoryButton;\n     }\n+\n+    \/**\n+     * Creates a button ID which can be used e.g. to assign icons via CSS based on the category ID\n+     *\n+     * @param categoryId The category ID which should be used\n+     * @return The created button ID\n+     *\/\n+    public static String getToggleButtonId(String categoryId) {\n+        return String.format(\"library-%s\", SidebarToggleGroupBaseSkin.getToggleButtonId(categoryId));\n+    }\n }","old_code":"package org.phoenicis.javafx.components.library.skin;\n\nimport javafx.scene.control.ToggleButton;\nimport org.phoenicis.javafx.components.library.control.LibrarySidebarToggleGroup;\nimport org.phoenicis.javafx.components.common.skin.SidebarToggleGroupBaseSkin;\nimport org.phoenicis.library.dto.ShortcutCategoryDTO;\n\nimport java.util.Optional;\n\nimport static org.phoenicis.configuration.localisation.Localisation.tr;\n\n\/**\n * A {@link SidebarToggleGroupBaseSkin} implementation class used inside the {@link LibrarySidebar}\n *\/\npublic class LibrarySidebarToggleGroupSkin extends\n        SidebarToggleGroupBaseSkin<ShortcutCategoryDTO, LibrarySidebarToggleGroup, LibrarySidebarToggleGroupSkin> {\n    \/**\n     * Constructor\n     *\n     * @param control The control belonging to the skin\n     *\/\n    public LibrarySidebarToggleGroupSkin(LibrarySidebarToggleGroup control) {\n        super(control);\n    }\n\n    \/**\n     * {@inheritDoc}\n     *\/\n    @Override\n    protected Optional<ToggleButton> createAllButton() {\n        final ToggleButton allCategoryButton = createSidebarToggleButton(tr(\"All\"));\n\n        allCategoryButton.setId(\"all-button\");\n        allCategoryButton.setOnMouseClicked(event -> getControl().setNothingSelected());\n\n        return Optional.of(allCategoryButton);\n    }\n\n    \/**\n     * {@inheritDoc}\n     *\/\n    @Override\n    protected ToggleButton convertToToggleButton(ShortcutCategoryDTO category) {\n        final ToggleButton categoryButton = createSidebarToggleButton(category.getName());\n        \/\/ TODO: store category ID in shortcut.info?\n        categoryButton.setId(\n                String.format(\"applications-%s\", SidebarToggleGroupBaseSkin.getToggleButtonId(category.getId())));\n        categoryButton.setOnMouseClicked(event -> getControl().setSelectedElement(category));\n\n        return categoryButton;\n    }\n}\n","lang_cluster":"Java","length":52,"code_uid":"9d86adb551e04a36b3f243b953c7e270"}
{"diff_hunk":"@@ -40,4 +40,10 @@ public class FlinkConfigOptions {\n           .intType()\n           .defaultValue(100)\n           .withDescription(\"Sets max infer parallelism for source operator.\");\n+\n+  public static final ConfigOption<Integer> SOURCE_READER_FETCH_BATCH_SIZE = ConfigOptions\n+      .key(\"source.iceberg.reader.fetch-batch-size\")\n+      .intType()\n+      .defaultValue(2048)\n+      .withDescription(\"The target batch size for split reader fetch.\");\n }","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.flink;\n\n\nimport org.apache.flink.configuration.ConfigOption;\nimport org.apache.flink.configuration.ConfigOptions;\n\npublic class FlinkConfigOptions {\n\n  private FlinkConfigOptions() {\n  }\n\n  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\")\n          .booleanType()\n          .defaultValue(true)\n          .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n              \"If is true, source parallelism is inferred according to splits number.\\n\");\n\n  public static final ConfigOption<Integer> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX =\n      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\")\n          .intType()\n          .defaultValue(100)\n          .withDescription(\"Sets max infer parallelism for source operator.\");\n}\n","lang_cluster":"Java","length":43,"code_uid":"6fb3d13a71a8410ba65e282fb41d41ac"}
{"diff_hunk":"@@ -76,6 +76,11 @@ public class WriteResult implements Serializable {\n       return this;\n     }\n \n+    public Builder add(Iterable<WriteResult> results) {\n+      results.forEach(this::add);\n+      return this;\n+    }\n+\n     public Builder addDataFiles(DataFile... files) {\n       Collections.addAll(dataFiles, files);\n       return this;","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.io;\n\nimport java.io.Serializable;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Set;\nimport org.apache.iceberg.DataFile;\nimport org.apache.iceberg.DeleteFile;\nimport org.apache.iceberg.relocated.com.google.common.collect.Iterables;\nimport org.apache.iceberg.relocated.com.google.common.collect.Lists;\nimport org.apache.iceberg.util.CharSequenceSet;\n\npublic class WriteResult implements Serializable {\n  private DataFile[] dataFiles;\n  private DeleteFile[] deleteFiles;\n  private CharSequence[] referencedDataFiles;\n\n  private WriteResult(List<DataFile> dataFiles,\n                      List<DeleteFile> deleteFiles,\n                      Set<CharSequence> referencedDataFiles) {\n    this.dataFiles = dataFiles.toArray(new DataFile[0]);\n    this.deleteFiles = deleteFiles.toArray(new DeleteFile[0]);\n    this.referencedDataFiles = referencedDataFiles.toArray(new CharSequence[0]);\n  }\n\n  public DataFile[] dataFiles() {\n    return dataFiles;\n  }\n\n  public DeleteFile[] deleteFiles() {\n    return deleteFiles;\n  }\n\n  public CharSequence[] referencedDataFiles() {\n    return referencedDataFiles;\n  }\n\n  public static Builder builder() {\n    return new Builder();\n  }\n\n  public static class Builder {\n    private final List<DataFile> dataFiles;\n    private final List<DeleteFile> deleteFiles;\n    private final Set<CharSequence> referencedDataFiles;\n\n    private Builder() {\n      this.dataFiles = Lists.newArrayList();\n      this.deleteFiles = Lists.newArrayList();\n      this.referencedDataFiles = CharSequenceSet.empty();\n    }\n\n    public Builder add(WriteResult result) {\n      addDataFiles(result.dataFiles);\n      addDeleteFiles(result.deleteFiles);\n\n      return this;\n    }\n\n    public Builder addDataFiles(DataFile... files) {\n      Collections.addAll(dataFiles, files);\n      return this;\n    }\n\n    public Builder addDataFiles(Iterable<DataFile> files) {\n      Iterables.addAll(dataFiles, files);\n      return this;\n    }\n\n    public Builder addDeleteFiles(DeleteFile... files) {\n      Collections.addAll(deleteFiles, files);\n      return this;\n    }\n\n    public Builder addDeleteFiles(Iterable<DeleteFile> files) {\n      Iterables.addAll(deleteFiles, files);\n      return this;\n    }\n\n    public Builder addReferencedDataFiles(CharSequence... files) {\n      Collections.addAll(referencedDataFiles, files);\n      return this;\n    }\n\n    public Builder addReferencedDataFiles(Iterable<CharSequence> files) {\n      Iterables.addAll(referencedDataFiles, files);\n      return this;\n    }\n\n    public WriteResult build() {\n      return new WriteResult(dataFiles, deleteFiles, referencedDataFiles);\n    }\n  }\n}\n","lang_cluster":"Java","length":113,"code_uid":"4d3b8e8a0d224c39b042a71dbd9a884c"}
{"diff_hunk":"@@ -28,10 +28,8 @@ public class MainnetProtocolSchedule {\n \n   public static final BigInteger DEFAULT_CHAIN_ID = BigInteger.ONE;\n \n-  public static ProtocolSchedule create() {\n-    return fromConfig(\n-        GenesisConfigFile.mainnet().getConfigOptions(), PrivacyParameters.DEFAULT, false);\n-  }\n+  public static final ProtocolSchedule DEFAULT =\n+      fromConfig(GenesisConfigFile.getMainnetConfigOptions(), PrivacyParameters.DEFAULT, false);\n \n   \/**\n    * Create a Mainnet protocol schedule from a config object","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.ethereum.mainnet;\n\nimport org.hyperledger.besu.config.GenesisConfigFile;\nimport org.hyperledger.besu.config.GenesisConfigOptions;\nimport org.hyperledger.besu.ethereum.core.PrivacyParameters;\nimport org.hyperledger.besu.ethereum.difficulty.fixed.FixedDifficultyCalculators;\nimport org.hyperledger.besu.ethereum.difficulty.fixed.FixedDifficultyProtocolSchedule;\n\nimport java.math.BigInteger;\nimport java.util.function.Function;\n\n\/** Provides {@link ProtocolSpec} lookups for mainnet hard forks. *\/\npublic class MainnetProtocolSchedule {\n\n  public static final BigInteger DEFAULT_CHAIN_ID = BigInteger.ONE;\n\n  public static ProtocolSchedule create() {\n    return fromConfig(\n        GenesisConfigFile.mainnet().getConfigOptions(), PrivacyParameters.DEFAULT, false);\n  }\n\n  \/**\n   * Create a Mainnet protocol schedule from a config object\n   *\n   * @param config {@link GenesisConfigOptions} containing the config options for the milestone\n   *     starting points\n   * @param privacyParameters the parameters set for private transactions\n   * @param isRevertReasonEnabled whether storing the revert reason is for failed transactions\n   * @return A configured mainnet protocol schedule\n   *\/\n  public static ProtocolSchedule fromConfig(\n      final GenesisConfigOptions config,\n      final PrivacyParameters privacyParameters,\n      final boolean isRevertReasonEnabled) {\n    if (FixedDifficultyCalculators.isFixedDifficultyInConfig(config)) {\n      return FixedDifficultyProtocolSchedule.create(\n          config, privacyParameters, isRevertReasonEnabled);\n    }\n    return new ProtocolScheduleBuilder(\n            config,\n            DEFAULT_CHAIN_ID,\n            Function.identity(),\n            privacyParameters,\n            isRevertReasonEnabled,\n            config.isQuorum())\n        .createProtocolSchedule();\n  }\n\n  \/**\n   * Create a Mainnet protocol schedule from a config object\n   *\n   * @param config {@link GenesisConfigOptions} containing the config options for the milestone\n   *     starting points\n   * @param isRevertReasonEnabled whether storing the revert reason is for failed transactions\n   * @return A configured mainnet protocol schedule\n   *\/\n  public static ProtocolSchedule fromConfig(\n      final GenesisConfigOptions config, final boolean isRevertReasonEnabled) {\n    return fromConfig(config, PrivacyParameters.DEFAULT, isRevertReasonEnabled);\n  }\n\n  \/**\n   * Create a Mainnet protocol schedule from a config object\n   *\n   * @param config {@link GenesisConfigOptions} containing the config options for the milestone\n   *     starting points\n   * @return A configured mainnet protocol schedule\n   *\/\n  public static ProtocolSchedule fromConfig(final GenesisConfigOptions config) {\n    return fromConfig(config, PrivacyParameters.DEFAULT, false);\n  }\n}\n","lang_cluster":"Java","length":86,"code_uid":"f8cf226c4ebe440ca570b25ccb507d54"}
{"diff_hunk":"@@ -41,6 +41,21 @@ public class GoDiscoveryContext extends DiscoveryContext implements GoContext {\n           .put(Field.Kind.TYPE_DOUBLE, \"0.0\")\n           .build();\n \n+  @Override\n+  protected String arrayTypeName(String elementName) {\n+    return String.format(\"%sArray\", elementName);\n+  }\n+\n+  @Override\n+  protected String mapTypeName(String keyName, String valueName) {\n+    return String.format(\"%sTo%sMap\", keyName, valueName);\n+  }\n+\n+  @Override\n+  protected String objectTypeName(String typeName) {\n+    return upperCamelToLowerCamel(typeName);\n+  }\n+\n   public String typeDefaultValue(Type type, Field field) {\n     if (field.getCardinality() == Field.Cardinality.CARDINALITY_REPEATED) {\n       return typeName(type, field) + \"{}\";","old_code":"\/* Copyright 2016 Google Inc\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage com.google.api.codegen.go;\n\nimport com.google.api.codegen.ApiaryConfig;\nimport com.google.api.codegen.DiscoveryContext;\nimport com.google.api.codegen.DiscoveryImporter;\nimport com.google.api.Service;\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.collect.ImmutableTable;\nimport com.google.protobuf.Field;\nimport com.google.protobuf.Method;\nimport com.google.protobuf.Type;\n\npublic class GoDiscoveryContext extends DiscoveryContext implements GoContext {\n  public GoDiscoveryContext(Service service, ApiaryConfig apiaryConfig) {\n    super(service, apiaryConfig);\n  }\n\n  private static final ImmutableMap<Field.Kind, String> DEFAULT_VALUES =\n      ImmutableMap.<Field.Kind, String>builder()\n          .put(Field.Kind.TYPE_BOOL, \"false\")\n          .put(Field.Kind.TYPE_INT32, \"int64(0)\")\n          .put(Field.Kind.TYPE_UINT32, \"uint64(0)\")\n          .put(Field.Kind.TYPE_INT64, \"int64(0)\")\n          .put(Field.Kind.TYPE_UINT64, \"uint64(0)\")\n          .put(Field.Kind.TYPE_FLOAT, \"float32(0.0)\")\n          .put(Field.Kind.TYPE_DOUBLE, \"0.0\")\n          .build();\n\n  public String typeDefaultValue(Type type, Field field) {\n    if (field.getCardinality() == Field.Cardinality.CARDINALITY_REPEATED) {\n      return typeName(type, field) + \"{}\";\n    }\n    if (DEFAULT_VALUES.containsKey(field.getKind())) {\n      return DEFAULT_VALUES.get(field.getKind());\n    }\n    if (field.getKind() == Field.Kind.TYPE_STRING || field.getKind() == Field.Kind.TYPE_ENUM) {\n      return getDefaultString(type, field);\n    }\n    throw new IllegalArgumentException(\n        String.format(\"not implemented: typeDefaultValue(%s, %s)\", type, field));\n  }\n\n  @Override\n  public String lineEnding(String value) {\n    return value;\n  }\n\n  private static final ImmutableMap<Field.Kind, String> PRIMITIVE_TYPE =\n      ImmutableMap.<Field.Kind, String>builder()\n          .put(Field.Kind.TYPE_BOOL, \"bool\")\n          .put(Field.Kind.TYPE_INT32, \"int64\")\n          .put(Field.Kind.TYPE_UINT32, \"uint64\")\n          .put(Field.Kind.TYPE_INT64, \"int64\")\n          .put(Field.Kind.TYPE_UINT64, \"uint64\")\n          .put(Field.Kind.TYPE_FLOAT, \"float32\")\n          .put(Field.Kind.TYPE_DOUBLE, \"float64\")\n          .put(Field.Kind.TYPE_STRING, \"string\")\n          .put(Field.Kind.TYPE_ENUM, \"string\")\n          .build();\n\n  \/**\n   * Returns the Go representation of a type's field's type.\n   *\/\n  private String typeName(Type type, Field field) {\n    String fieldName = field.getName();\n    String fieldTypeName = field.getTypeUrl();\n    String arrayPrefix = \"\";\n\n    if (field.getCardinality() == Field.Cardinality.CARDINALITY_REPEATED) {\n      Type items = this.getApiaryConfig().getType(fieldTypeName);\n      if (isMapField(type, fieldName)) {\n        return String.format(\n            \"map[%s]%s\",\n            typeName(items, this.getField(items, \"key\")),\n            typeName(items, this.getField(items, \"value\")));\n      }\n      Field elements = this.getField(items, \"elements\");\n      if (elements != null) {\n        return \"[]\" + typeName(items, elements);\n      }\n      arrayPrefix = \"[]\";\n    }\n    if (field.getKind() == Field.Kind.TYPE_MESSAGE) {\n      return arrayPrefix + fieldTypeName;\n    }\n    if (PRIMITIVE_TYPE.containsKey(field.getKind())) {\n      return arrayPrefix + PRIMITIVE_TYPE.get(field.getKind());\n    }\n    throw new IllegalArgumentException(\n        String.format(\"cannot find suitable type for %s %s\", type.getName(), field.getName()));\n  }\n\n  @Override\n  \/**\n   * Most languages ignore return type \"Empty\". However, Go cannot since the client library will\n   * return an empty struct, and we have to assign it to something. Consequently, the only time the\n   * response is truly \"empty\" for Go is when DiscoveryImporter says EMPTY_TYPE_NAME, which\n   * signifies complete absence of return value.\n   *\/\n  public boolean isResponseEmpty(Method method) {\n    return method.getResponseTypeUrl().equals(DiscoveryImporter.EMPTY_TYPE_NAME);\n  }\n\n  @Override\n  public boolean isPageStreaming(Method method) {\n    if (isResponseEmpty(method) || hasRequestField(method)) {\n      return false;\n    }\n    boolean hasNextPageToken = false;\n    for (Field field : getApiaryConfig().getType(method.getResponseTypeUrl()).getFieldsList()) {\n      if (field.getName().equals(\"nextPageToken\")) {\n        hasNextPageToken = true;\n        break;\n      }\n    }\n\n    boolean hasPageToken = false;\n    for (Field field : getApiaryConfig().getType(method.getRequestTypeUrl()).getFieldsList()) {\n      if (field.getName().equals(\"pageToken\")) {\n        hasPageToken = true;\n        break;\n      }\n    }\n    return hasPageToken && hasNextPageToken;\n  }\n\n  private static final ImmutableTable<String, String, String> API_VERSION_RENAME =\n      ImmutableTable.<String, String, String>builder()\n          .put(\"clouduseraccounts\", \"beta\", \"v0.beta\")\n          .build();\n\n  \/**\n   * We need this because in some cases there is a mismatch between discovery doc and import path\n   * version numbers. API_VERSION_RENAME is a table of API name and versions as found in discovery\n   * doc to the renamed versions as found in the import path.\n   *\n   * TODO(pongad): Find a more sustainable solution to this.\n   *\/\n  public String getApiVersion() {\n    String rename = API_VERSION_RENAME.get(getApi().getName(), getApi().getVersion());\n    return rename == null ? getApi().getVersion() : rename;\n  }\n\n  \/*\n   * Returns an empty or singleton list of auth scopes for the method. If the method has no scope,\n   * returns an empty list; otherwise returns the first scope. We return an empty list instead of\n   * null to denote absence of scope since the snippet cannot handle null values. If the scope\n   * exists, it is stripped to its last path-element and converted to camel case, eg\n   * \"https:\/\/www.googleapis.com\/auth\/cloud-platform\" becomes \"CloudPlatform\".\n   *\/\n  public ImmutableList<String> getAuthScopes(Method method) {\n    if (!getApiaryConfig().getAuthScopes().containsKey(method.getName())) {\n      return ImmutableList.<String>of();\n    }\n    String scope = getApiaryConfig().getAuthScopes().get(method.getName()).get(0);\n    int slash = scope.lastIndexOf('\/');\n    if (slash < 0) {\n      throw new IllegalArgumentException(\n          String.format(\"malformed scope, cannot find slash: %s\", scope));\n    }\n    scope = scope.substring(slash + 1);\n    scope = scope.replace('.', '_');\n    scope = scope.replace('-', '_');\n    scope = lowerUnderscoreToUpperCamel(scope);\n    return ImmutableList.<String>of(scope);\n  }\n}\n","lang_cluster":"Java","length":182,"code_uid":"ffe0c5d44f184b13b06229337d033bef"}
{"diff_hunk":"@@ -34,7 +34,7 @@ module Selenium\n           driver.manage.timeouts.implicit_wait = 6\n \n           driver.find_element(id: 'adder').click\n-          driver.find_element(id: 'box0')\n+          expect { driver.find_element(id: 'box0') }.not_to raise_error(WebDriver::Error::NoSuchElementError)\n         end\n \n         it 'should still fail to find an element with implicit waits enabled' do","old_code":"# frozen_string_literal: true\n\n# Licensed to the Software Freedom Conservancy (SFC) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The SFC licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nrequire_relative 'spec_helper'\n\nmodule Selenium\n  module WebDriver\n    describe Timeouts do\n      context 'implicit waits' do\n        before do\n          driver.manage.timeouts.implicit_wait = 0\n          driver.navigate.to url_for('dynamic.html')\n        end\n\n        after { driver.manage.timeouts.implicit_wait = 0 }\n\n        it 'should implicitly wait for a single element', except: {browser: :safari_preview} do\n          driver.manage.timeouts.implicit_wait = 6\n\n          driver.find_element(id: 'adder').click\n          driver.find_element(id: 'box0')\n        end\n\n        it 'should still fail to find an element with implicit waits enabled' do\n          driver.manage.timeouts.implicit_wait = 0.5\n          expect { driver.find_element(id: 'box0') }.to raise_error(WebDriver::Error::NoSuchElementError)\n        end\n\n        it 'should return after first attempt to find one after disabling implicit waits' do\n          driver.manage.timeouts.implicit_wait = 3\n          driver.manage.timeouts.implicit_wait = 0\n\n          expect { driver.find_element(id: 'box0') }.to raise_error(WebDriver::Error::NoSuchElementError)\n        end\n\n        it 'should implicitly wait until at least one element is found when searching for many' do\n          add = driver.find_element(id: 'adder')\n\n          driver.manage.timeouts.implicit_wait = 6\n          add.click\n          add.click\n\n          expect(driver.find_elements(class_name: 'redbox')).not_to be_empty\n        end\n\n        it 'should still fail to find elements when implicit waits are enabled' do\n          driver.manage.timeouts.implicit_wait = 0.5\n          expect(driver.find_elements(class_name: 'redbox')).to be_empty\n        end\n\n        it 'should return after first attempt to find many after disabling implicit waits', except: {browser: :firefox, platform: :windows} do\n          add = driver.find_element(id: 'adder')\n\n          driver.manage.timeouts.implicit_wait = 3\n          driver.manage.timeouts.implicit_wait = 0\n          add.click\n\n          expect(driver.find_elements(class_name: 'redbox')).to be_empty\n        end\n      end\n\n      context 'page loads' do\n        # w3c default is 300,000\n        after { driver.manage.timeouts.page_load = 300000 }\n\n        it 'should be able to set the page load timeout' do\n          expect { driver.manage.timeouts.page_load = 2 }.not_to raise_exception\n        end\n      end\n    end\n  end # WebDriver\nend # Selenium\n","lang_cluster":"Java","length":88,"code_uid":"cb2f447bb070411b9220a33bf432874a"}
{"diff_hunk":"@@ -78,6 +78,32 @@ public final class TestObjects {\n           .build()\n   ).stream().map(ApplyTimestampAndDuration::apply).collect(toList());\n \n+\n+  \/\/ this object simulates\n+  public static final List<Span> TRACEWITHSAMEIDANDSAMEPARENTID = asList(\n+    Span.builder().traceId(WEB_SPAN_ID+1).id(WEB_SPAN_ID).name(\"get\")\n+      .addAnnotation(Annotation.create(TODAY * 1000, SERVER_RECV, WEB_ENDPOINT))\n+      .addAnnotation(Annotation.create((TODAY + 350) * 1000, SERVER_SEND, WEB_ENDPOINT))\n+      .build(),\n+    Span.builder().traceId(WEB_SPAN_ID+1).parentId(WEB_SPAN_ID).id(WEB_SPAN_ID).name(\"get\")\n+      .addAnnotation(Annotation.create((TODAY + 50) * 1000, CLIENT_SEND, WEB_ENDPOINT))\n+      .addAnnotation(Annotation.create((TODAY + 100) * 1000, SERVER_RECV, APP_ENDPOINT))\n+      .addAnnotation(Annotation.create((TODAY + 250) * 1000, SERVER_SEND, APP_ENDPOINT))\n+      .addAnnotation(Annotation.create((TODAY + 300) * 1000, CLIENT_RECV, WEB_ENDPOINT))\n+      .addBinaryAnnotation(BinaryAnnotation.address(CLIENT_ADDR, WEB_ENDPOINT))\n+      .addBinaryAnnotation(BinaryAnnotation.address(SERVER_ADDR, APP_ENDPOINT))\n+      .build(),\n+    Span.builder().traceId(WEB_SPAN_ID+1).parentId(WEB_SPAN_ID).id(WEB_SPAN_ID).name(\"query\")\n+      .addAnnotation(Annotation.create((TODAY + 150) * 1000, CLIENT_SEND, APP_ENDPOINT))\n+      .addAnnotation(Annotation.create((TODAY + 200) * 1000, CLIENT_RECV, APP_ENDPOINT))\n+      .addAnnotation(Annotation.create((TODAY + 190) * 1000, \"\u2ee9\", NO_IP_ENDPOINT))\n+      .addBinaryAnnotation(BinaryAnnotation.address(CLIENT_ADDR, APP_ENDPOINT))\n+      .addBinaryAnnotation(BinaryAnnotation.address(SERVER_ADDR, DB_ENDPOINT))\n+      .addBinaryAnnotation(BinaryAnnotation.create(ERROR, \"\\uD83D\\uDCA9\", NO_IP_ENDPOINT))\n+      .build()\n+  ).stream().map(ApplyTimestampAndDuration::apply).collect(toList());\n+\n+\n   public static final List<DependencyLink> LINKS = asList(\n       DependencyLink.builder().parent(\"web\").child(\"app\").callCount(1).build(),\n       DependencyLink.builder().parent(\"app\").child(\"db\").callCount(1).build()","old_code":"\/**\n * Copyright 2015-2016 The OpenZipkin Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except\n * in compliance with the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License\n * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n * or implied. See the License for the specific language governing permissions and limitations under\n * the License.\n *\/\npackage zipkin;\n\nimport java.util.List;\nimport java.util.Random;\nimport java.util.concurrent.TimeUnit;\nimport zipkin.internal.ApplyTimestampAndDuration;\nimport zipkin.internal.Dependencies;\n\nimport static java.util.Arrays.asList;\nimport static java.util.stream.Collectors.toList;\nimport static zipkin.Constants.CLIENT_ADDR;\nimport static zipkin.Constants.CLIENT_RECV;\nimport static zipkin.Constants.CLIENT_SEND;\nimport static zipkin.Constants.ERROR;\nimport static zipkin.Constants.SERVER_ADDR;\nimport static zipkin.Constants.SERVER_RECV;\nimport static zipkin.Constants.SERVER_SEND;\nimport static zipkin.internal.Util.UTF_8;\nimport static zipkin.internal.Util.midnightUTC;\n\npublic final class TestObjects {\n\n  \/** Notably, the cassandra implementation has day granularity *\/\n  public static final long DAY = TimeUnit.MILLISECONDS.convert(1, TimeUnit.DAYS);\n\n  \/\/ Use real time, as most span-stores have TTL logic which looks back several days.\n  public static final long TODAY = midnightUTC(System.currentTimeMillis());\n\n  public static final Endpoint WEB_ENDPOINT = Endpoint.builder()\n      .serviceName(\"web\")\n      .ipv4(124 << 24 | 13 << 16 | 90 << 8 | 3)\n      \/\/ Cheat so we don't have to catch an exception here\n      .ipv6(sun.net.util.IPAddressUtil.textToNumericFormatV6(\"2001:db8::c001\"))\n      .port((short) 80).build();\n  public static final Endpoint APP_ENDPOINT =\n      Endpoint.builder().serviceName(\"app\").ipv4(172 << 24 | 17 << 16 | 2).port(8080).build();\n  public static final Endpoint DB_ENDPOINT =\n      Endpoint.builder().serviceName(\"db\").ipv4(172 << 24 | 17 << 16 | 2).port(3306).build();\n  public static final Endpoint NO_IP_ENDPOINT = Endpoint.builder().serviceName(\"no_ip\").build();\n\n  static final long WEB_SPAN_ID = -692101025335252320L;\n  static final long APP_SPAN_ID = -7842865617155193778L;\n  static final long DB_SPAN_ID = 8207293009014896295L;\n\n  public static final List<Span> TRACE = asList(\n      Span.builder().traceId(WEB_SPAN_ID).id(WEB_SPAN_ID).name(\"get\")\n          .addAnnotation(Annotation.create(TODAY * 1000, SERVER_RECV, WEB_ENDPOINT))\n          .addAnnotation(Annotation.create((TODAY + 350) * 1000, SERVER_SEND, WEB_ENDPOINT))\n          .build(),\n      Span.builder().traceId(WEB_SPAN_ID).parentId(WEB_SPAN_ID).id(APP_SPAN_ID).name(\"get\")\n          .addAnnotation(Annotation.create((TODAY + 50) * 1000, CLIENT_SEND, WEB_ENDPOINT))\n          .addAnnotation(Annotation.create((TODAY + 100) * 1000, SERVER_RECV, APP_ENDPOINT))\n          .addAnnotation(Annotation.create((TODAY + 250) * 1000, SERVER_SEND, APP_ENDPOINT))\n          .addAnnotation(Annotation.create((TODAY + 300) * 1000, CLIENT_RECV, WEB_ENDPOINT))\n          .addBinaryAnnotation(BinaryAnnotation.address(CLIENT_ADDR, WEB_ENDPOINT))\n          .addBinaryAnnotation(BinaryAnnotation.address(SERVER_ADDR, APP_ENDPOINT))\n          .build(),\n      Span.builder().traceId(WEB_SPAN_ID).parentId(APP_SPAN_ID).id(DB_SPAN_ID).name(\"query\")\n          .addAnnotation(Annotation.create((TODAY + 150) * 1000, CLIENT_SEND, APP_ENDPOINT))\n          .addAnnotation(Annotation.create((TODAY + 200) * 1000, CLIENT_RECV, APP_ENDPOINT))\n          .addAnnotation(Annotation.create((TODAY + 190) * 1000, \"\u2ee9\", NO_IP_ENDPOINT))\n          .addBinaryAnnotation(BinaryAnnotation.address(CLIENT_ADDR, APP_ENDPOINT))\n          .addBinaryAnnotation(BinaryAnnotation.address(SERVER_ADDR, DB_ENDPOINT))\n          .addBinaryAnnotation(BinaryAnnotation.create(ERROR, \"\\uD83D\\uDCA9\", NO_IP_ENDPOINT))\n          .build()\n  ).stream().map(ApplyTimestampAndDuration::apply).collect(toList());\n\n  public static final List<DependencyLink> LINKS = asList(\n      DependencyLink.builder().parent(\"web\").child(\"app\").callCount(1).build(),\n      DependencyLink.builder().parent(\"app\").child(\"db\").callCount(1).build()\n  );\n  public static final Dependencies DEPENDENCIES = Dependencies.create(TODAY, TODAY + 1000, LINKS);\n\n  static final Span.Builder spanBuilder = spanBuilder();\n\n  \/** Reuse a builder as it is significantly slows tests to create 100000 of these! *\/\n  static Span.Builder spanBuilder() {\n    Endpoint e = Endpoint.builder().serviceName(\"service\").ipv4(127 << 24 | 1).port(8080).build();\n    Annotation sr = Annotation.create(System.currentTimeMillis() * 1000, SERVER_RECV, e);\n    Annotation ss = Annotation.create(sr.timestamp + 1000, SERVER_SEND, e);\n    BinaryAnnotation ba = BinaryAnnotation.create(TraceKeys.HTTP_METHOD, \"GET\", e);\n    return Span.builder().name(\"get\").addAnnotation(sr).addAnnotation(ss).addBinaryAnnotation(ba);\n  }\n\n  \/**\n   * Zipkin trace ids are random 64bit numbers. This creates a relatively large input to avoid\n   * flaking out due to PRNG nuance.\n   *\/\n  public static final Span[] LOTS_OF_SPANS =\n      new Random().longs(100_000).mapToObj(t -> span(t)).toArray(Span[]::new);\n\n  public static Span span(long traceId) {\n    return spanBuilder.traceId(traceId).id(traceId).build();\n  }\n}\n","lang_cluster":"Java","length":108,"code_uid":"95233c7fd4954e9f96ad3298975f23d0"}
{"diff_hunk":"@@ -16,17 +16,13 @@\n \n package org.springframework.security.oauth2.server.resource.authentication;\n \n-import java.util.Arrays;\n import java.util.Collection;\n-import java.util.Collections;\n-import java.util.stream.Collectors;\n \n import org.springframework.core.convert.converter.Converter;\n import org.springframework.security.authentication.AbstractAuthenticationToken;\n import org.springframework.security.core.GrantedAuthority;\n-import org.springframework.security.core.authority.SimpleGrantedAuthority;\n import org.springframework.security.oauth2.jwt.Jwt;\n-import org.springframework.util.StringUtils;\n+import org.springframework.util.Assert;\n \n \/**\n  * @author Rob Winch","old_code":"\/*\n * Copyright 2002-2018 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage org.springframework.security.oauth2.server.resource.authentication;\n\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.stream.Collectors;\n\nimport org.springframework.core.convert.converter.Converter;\nimport org.springframework.security.authentication.AbstractAuthenticationToken;\nimport org.springframework.security.core.GrantedAuthority;\nimport org.springframework.security.core.authority.SimpleGrantedAuthority;\nimport org.springframework.security.oauth2.jwt.Jwt;\nimport org.springframework.util.StringUtils;\n\n\/**\n * @author Rob Winch\n * @author Josh Cummings\n * @since 5.1\n *\/\npublic class JwtAuthenticationConverter implements Converter<Jwt, AbstractAuthenticationToken> {\n\tprivate static final String SCOPE_AUTHORITY_PREFIX = \"SCOPE_\";\n\n\tprivate static final Collection<String> WELL_KNOWN_SCOPE_ATTRIBUTE_NAMES =\n\t\t\tArrays.asList(\"scope\", \"scp\");\n\n\n\tpublic final AbstractAuthenticationToken convert(Jwt jwt) {\n\t\tCollection<GrantedAuthority> authorities = extractAuthorities(jwt);\n\t\treturn new JwtAuthenticationToken(jwt, authorities);\n\t}\n\n\tprotected Collection<GrantedAuthority> extractAuthorities(Jwt jwt) {\n\t\treturn this.getScopes(jwt)\n\t\t\t\t\t\t.stream()\n\t\t\t\t\t\t.map(authority -> SCOPE_AUTHORITY_PREFIX + authority)\n\t\t\t\t\t\t.map(SimpleGrantedAuthority::new)\n\t\t\t\t\t\t.collect(Collectors.toList());\n\t}\n\n\tprivate Collection<String> getScopes(Jwt jwt) {\n\t\tfor ( String attributeName : WELL_KNOWN_SCOPE_ATTRIBUTE_NAMES ) {\n\t\t\tObject scopes = jwt.getClaims().get(attributeName);\n\t\t\tif (scopes instanceof String) {\n\t\t\t\tif (StringUtils.hasText((String) scopes)) {\n\t\t\t\t\treturn Arrays.asList(((String) scopes).split(\" \"));\n\t\t\t\t} else {\n\t\t\t\t\treturn Collections.emptyList();\n\t\t\t\t}\n\t\t\t} else if (scopes instanceof Collection) {\n\t\t\t\treturn (Collection<String>) scopes;\n\t\t\t}\n\t\t}\n\n\t\treturn Collections.emptyList();\n\t}\n}\n","lang_cluster":"Java","length":72,"code_uid":"08a976aa79a3432589c5eabadd1bfb90"}
{"diff_hunk":"@@ -20,4 +20,5 @@ import azkaban.executor.ExecutorManagerException;\n public interface ContainerizedImpl {\n   void createContainer(final int executionId) throws ExecutorManagerException;\n   void deleteContainer(final int executionId) throws ExecutorManagerException;\n+  void logPodDetails(final int execId) throws ExecutorManagerException ;\n }","old_code":"\/*\n * Copyright 2020 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n *\/\npackage azkaban.executor.container;\n\nimport azkaban.executor.ExecutorManagerException;\n\npublic interface ContainerizedImpl {\n  void createContainer(final int executionId) throws ExecutorManagerException;\n  void deleteContainer(final int executionId) throws ExecutorManagerException;\n}\n","lang_cluster":"Java","length":23,"code_uid":"94bc2d67d1a14fde98883f905db13b8e"}
{"diff_hunk":"@@ -42,7 +42,7 @@ public class Cast extends NoOp {\n     } else if (targetDataType instanceof RealType) {\n       casted = castToDouble(value);\n     } else {\n-      throw new UnsupportedOperationException(\"only support cast to Long, Double and String\");\n+      casted = value;\n     }\n     row.set(pos, targetDataType, casted);\n   }","old_code":"\/*\n *\n * Copyright 2017 PingCAP, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n *\/\n\npackage com.pingcap.tikv.operation.transformer;\n\nimport com.pingcap.tikv.row.Row;\nimport com.pingcap.tikv.types.*;\nimport java.math.BigDecimal;\n\npublic class Cast extends NoOp {\n  public Cast(DataType type) {\n    super(type);\n  }\n\n  @Override\n  public void set(Object value, Row row, int pos) {\n    Object casted;\n    if (value == null) {\n      row.set(pos, targetDataType, null);\n      return;\n    }\n    if (targetDataType instanceof IntegerType) {\n      casted = castToLong(value);\n    } else if (targetDataType instanceof BytesType) {\n      casted = castToString(value);\n    } else if (targetDataType instanceof DecimalType) {\n      casted = castToDecimal(value);\n    } else if (targetDataType instanceof RealType) {\n      casted = castToDouble(value);\n    } else {\n      throw new UnsupportedOperationException(\"only support cast to Long, Double and String\");\n    }\n    row.set(pos, targetDataType, casted);\n  }\n\n  public Double castToDouble(Object obj) {\n    if (obj instanceof Number) {\n      Number num = (Number) obj;\n      return num.doubleValue();\n    }\n    throw new UnsupportedOperationException(\"can not cast un-number to double \");\n  }\n\n  public BigDecimal castToDecimal(Object obj) {\n    if (obj instanceof Number) {\n      Number num = (Number) obj;\n      return new BigDecimal(num.doubleValue());\n    } else if (obj instanceof BigDecimal) {\n      return (BigDecimal) obj;\n    }\n    throw new UnsupportedOperationException(\n        \"can not cast to BigDecimal: \" + obj == null ? \"null\" : obj.getClass().getSimpleName());\n  }\n\n  public Long castToLong(Object obj) {\n    if (obj instanceof Number) {\n      Number num = (Number) obj;\n      return num.longValue();\n    }\n    throw new UnsupportedOperationException(\"can not cast un-number to long \");\n  }\n\n  public String castToString(Object obj) {\n    return obj.toString();\n  }\n}\n","lang_cluster":"Java","length":80,"code_uid":"49433d5bd0b146c9adcaed1a3b258b82"}
{"diff_hunk":"@@ -42,19 +42,26 @@ import org.apache.spark.sql.catalyst.util.MapData;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.unsafe.types.UTF8String;\n \n-\n-class SparkOrcValueReaders {\n+public class SparkOrcValueReaders {\n   private SparkOrcValueReaders() {\n   }\n \n-  static OrcValueReader<UTF8String> utf8String() {\n+  public static OrcValueReader<UTF8String> utf8String() {\n     return StringReader.INSTANCE;\n   }\n \n-  static OrcValueReader<?> timestampTzs() {\n+  public static OrcValueReader<Long> timestampTzs() {\n     return TimestampTzReader.INSTANCE;\n   }\n \n+  public static OrcValueReader<Decimal> decimals(int precision, int scale) {\n+    if (precision <= Decimal.MAX_LONG_DIGITS()) {\n+      return new SparkOrcValueReaders.Decimal18Reader(precision, scale);\n+    } else {\n+      return new SparkOrcValueReaders.Decimal38Reader(precision, scale);\n+    }\n+  }\n+\n   static OrcValueReader<?> struct(\n       List<OrcValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n     return new StructReader(readers, struct, idToConstant);","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\/\n\npackage org.apache.iceberg.spark.data;\n\nimport java.math.BigDecimal;\nimport java.util.List;\nimport java.util.Map;\nimport org.apache.iceberg.orc.OrcValueReader;\nimport org.apache.iceberg.orc.OrcValueReaders;\nimport org.apache.iceberg.relocated.com.google.common.collect.Lists;\nimport org.apache.iceberg.types.Types;\nimport org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\nimport org.apache.orc.storage.ql.exec.vector.ColumnVector;\nimport org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\nimport org.apache.orc.storage.ql.exec.vector.ListColumnVector;\nimport org.apache.orc.storage.ql.exec.vector.MapColumnVector;\nimport org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\nimport org.apache.orc.storage.serde2.io.HiveDecimalWritable;\nimport org.apache.spark.sql.catalyst.InternalRow;\nimport org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\nimport org.apache.spark.sql.catalyst.util.ArrayBasedMapData;\nimport org.apache.spark.sql.catalyst.util.ArrayData;\nimport org.apache.spark.sql.catalyst.util.GenericArrayData;\nimport org.apache.spark.sql.catalyst.util.MapData;\nimport org.apache.spark.sql.types.Decimal;\nimport org.apache.spark.unsafe.types.UTF8String;\n\n\nclass SparkOrcValueReaders {\n  private SparkOrcValueReaders() {\n  }\n\n  static OrcValueReader<UTF8String> utf8String() {\n    return StringReader.INSTANCE;\n  }\n\n  static OrcValueReader<?> timestampTzs() {\n    return TimestampTzReader.INSTANCE;\n  }\n\n  static OrcValueReader<?> struct(\n      List<OrcValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n    return new StructReader(readers, struct, idToConstant);\n  }\n\n  static OrcValueReader<?> array(OrcValueReader<?> elementReader) {\n    return new ArrayReader(elementReader);\n  }\n\n  static OrcValueReader<?> map(OrcValueReader<?> keyReader, OrcValueReader<?> valueReader) {\n    return new MapReader(keyReader, valueReader);\n  }\n\n  private static class ArrayReader implements OrcValueReader<ArrayData> {\n    private final OrcValueReader<?> elementReader;\n\n    private ArrayReader(OrcValueReader<?> elementReader) {\n      this.elementReader = elementReader;\n    }\n\n    @Override\n    public ArrayData nonNullRead(ColumnVector vector, int row) {\n      ListColumnVector listVector = (ListColumnVector) vector;\n      int offset = (int) listVector.offsets[row];\n      int length = (int) listVector.lengths[row];\n      List<Object> elements = Lists.newArrayListWithExpectedSize(length);\n      for (int c = 0; c < length; ++c) {\n        elements.add(elementReader.read(listVector.child, offset + c));\n      }\n      return new GenericArrayData(elements.toArray());\n    }\n  }\n\n  private static class MapReader implements OrcValueReader<MapData> {\n    private final OrcValueReader<?> keyReader;\n    private final OrcValueReader<?> valueReader;\n\n    private MapReader(OrcValueReader<?> keyReader, OrcValueReader<?> valueReader) {\n      this.keyReader = keyReader;\n      this.valueReader = valueReader;\n    }\n\n    @Override\n    public MapData nonNullRead(ColumnVector vector, int row) {\n      MapColumnVector mapVector = (MapColumnVector) vector;\n      int offset = (int) mapVector.offsets[row];\n      long length = mapVector.lengths[row];\n      List<Object> keys = Lists.newArrayListWithExpectedSize((int) length);\n      List<Object> values = Lists.newArrayListWithExpectedSize((int) length);\n      for (int c = 0; c < length; c++) {\n        keys.add(keyReader.read(mapVector.keys, offset + c));\n        values.add(valueReader.read(mapVector.values, offset + c));\n      }\n\n      return new ArrayBasedMapData(\n          new GenericArrayData(keys.toArray()),\n          new GenericArrayData(values.toArray()));\n    }\n  }\n\n  static class StructReader extends OrcValueReaders.StructReader<InternalRow> {\n    private final int numFields;\n\n    protected StructReader(List<OrcValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n      super(readers, struct, idToConstant);\n      this.numFields = struct.fields().size();\n    }\n\n    @Override\n    protected InternalRow create() {\n      return new GenericInternalRow(numFields);\n    }\n\n    @Override\n    protected void set(InternalRow struct, int pos, Object value) {\n      if (value != null) {\n        struct.update(pos, value);\n      } else {\n        struct.setNullAt(pos);\n      }\n    }\n  }\n\n  private static class StringReader implements OrcValueReader<UTF8String> {\n    private static final StringReader INSTANCE = new StringReader();\n\n    private StringReader() {\n    }\n\n    @Override\n    public UTF8String nonNullRead(ColumnVector vector, int row) {\n      BytesColumnVector bytesVector = (BytesColumnVector) vector;\n      return UTF8String.fromBytes(bytesVector.vector[row], bytesVector.start[row], bytesVector.length[row]);\n    }\n  }\n\n  private static class TimestampTzReader implements OrcValueReader<Long> {\n    private static final TimestampTzReader INSTANCE = new TimestampTzReader();\n\n    private TimestampTzReader() {\n    }\n\n    @Override\n    public Long nonNullRead(ColumnVector vector, int row) {\n      TimestampColumnVector timestampVector = (TimestampColumnVector) vector;\n      return (timestampVector.time[row] \/ 1000) * 1_000_000 + timestampVector.nanos[row] \/ 1000;\n    }\n  }\n\n  static class Decimal18Reader implements OrcValueReader<Decimal> {\n    \/\/TODO: these are being unused. check for bug\n    private final int precision;\n    private final int scale;\n\n    Decimal18Reader(int precision, int scale) {\n      this.precision = precision;\n      this.scale = scale;\n    }\n\n    @Override\n    public Decimal nonNullRead(ColumnVector vector, int row) {\n      HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n    }\n  }\n\n  static class Decimal38Reader implements OrcValueReader<Decimal> {\n    private final int precision;\n    private final int scale;\n\n    Decimal38Reader(int precision, int scale) {\n      this.precision = precision;\n      this.scale = scale;\n    }\n\n    @Override\n    public Decimal nonNullRead(ColumnVector vector, int row) {\n      BigDecimal value = ((DecimalColumnVector) vector).vector[row]\n          .getHiveDecimal().bigDecimalValue();\n      return new Decimal().set(new scala.math.BigDecimal(value), precision, scale);\n    }\n  }\n}\n","lang_cluster":"Java","length":200,"code_uid":"85bab1250484474ab081d9bd786dd815"}
{"diff_hunk":"@@ -14,10 +14,15 @@\n  *\/\n package com.google.api.codegen.transformer.nodejs;\n \n+import com.google.api.codegen.config.MethodConfig;\n import com.google.api.codegen.transformer.ApiMethodParamTransformer;\n import com.google.api.codegen.transformer.MethodTransformerContext;\n+import com.google.api.codegen.transformer.SurfaceNamer;\n+import com.google.api.codegen.util.Name;\n import com.google.api.codegen.viewmodel.DynamicLangDefaultableParamView;\n import com.google.api.codegen.viewmodel.ParamDocView;\n+import com.google.api.codegen.viewmodel.SimpleParamDocView;\n+import com.google.api.tools.framework.model.Field;\n import com.google.common.collect.ImmutableList;\n import java.util.List;\n ","old_code":"\/* Copyright 2017 Google Inc\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\npackage com.google.api.codegen.transformer.nodejs;\n\nimport com.google.api.codegen.transformer.ApiMethodParamTransformer;\nimport com.google.api.codegen.transformer.MethodTransformerContext;\nimport com.google.api.codegen.viewmodel.DynamicLangDefaultableParamView;\nimport com.google.api.codegen.viewmodel.ParamDocView;\nimport com.google.common.collect.ImmutableList;\nimport java.util.List;\n\npublic class NodeJSApiMethodParamTransformer implements ApiMethodParamTransformer {\n  @Override\n  public List<DynamicLangDefaultableParamView> generateMethodParams(\n      MethodTransformerContext context) {\n    \/\/ TODO(eoogbe): implement this method when migrating to MVVM\n    return ImmutableList.<DynamicLangDefaultableParamView>of();\n  }\n\n  @Override\n  public List<ParamDocView> generateParamDocs(MethodTransformerContext context) {\n    \/\/ TODO(eoogbe): implement this method when migrating to MVVM\n    return ImmutableList.<ParamDocView>of();\n  }\n}\n","lang_cluster":"Java","length":37,"code_uid":"b61c0df641144285a86146bf39caf4ac"}
{"diff_hunk":"@@ -133,7 +133,9 @@ public class RestServerVerticle extends AbstractVerticle {\n     serverOptions.setIdleTimeout(TransportConfig.getConnectionIdleTimeoutInSeconds());\n     serverOptions.setCompressionSupported(TransportConfig.getCompressed());\n     serverOptions.setMaxHeaderSize(TransportConfig.getMaxHeaderSize());\n-\n+    if (endpointObject.isHttp2Enabled()) {\n+      serverOptions.setUseAlpn(true);\n+    }\n     if (endpointObject.isSslEnabled()) {\n       SSLOptionFactory factory =\n           SSLOptionFactory.createSSLOptionFactory(SSL_KEY, null);","old_code":"\/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\npackage org.apache.servicecomb.transport.rest.vertx;\n\nimport java.util.List;\n\nimport org.apache.servicecomb.core.Endpoint;\nimport org.apache.servicecomb.core.transport.AbstractTransport;\nimport org.apache.servicecomb.foundation.common.net.URIEndpointObject;\nimport org.apache.servicecomb.foundation.common.utils.SPIServiceUtils;\nimport org.apache.servicecomb.foundation.ssl.SSLCustom;\nimport org.apache.servicecomb.foundation.ssl.SSLOption;\nimport org.apache.servicecomb.foundation.ssl.SSLOptionFactory;\nimport org.apache.servicecomb.foundation.vertx.VertxTLSBuilder;\nimport org.apache.servicecomb.transport.rest.vertx.accesslog.AccessLogConfiguration;\nimport org.apache.servicecomb.transport.rest.vertx.accesslog.impl.AccessLogHandler;\nimport org.apache.servicecomb.transport.rest.vertx.accesslog.parser.impl.DefaultAccessLogPatternParser;\nimport org.apache.servicecomb.transport.rest.vertx.trace.TracePrepareHandler;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport io.vertx.core.AbstractVerticle;\nimport io.vertx.core.Context;\nimport io.vertx.core.Future;\nimport io.vertx.core.Vertx;\nimport io.vertx.core.http.HttpServer;\nimport io.vertx.core.http.HttpServerOptions;\nimport io.vertx.ext.web.Router;\n\npublic class RestServerVerticle extends AbstractVerticle {\n  private static final Logger LOGGER = LoggerFactory.getLogger(RestServerVerticle.class);\n\n  private static final String SSL_KEY = \"rest.provider\";\n\n  private Endpoint endpoint;\n\n  private URIEndpointObject endpointObject;\n\n  @Override\n  public void init(Vertx vertx, Context context) {\n    super.init(vertx, context);\n    this.endpoint = (Endpoint) context.config().getValue(AbstractTransport.ENDPOINT_KEY);\n    this.endpointObject = (URIEndpointObject) endpoint.getAddress();\n  }\n\n  @Override\n  public void start(Future<Void> startFuture) throws Exception {\n    try {\n      super.start();\n      \/\/ \u5982\u679c\u672c\u5730\u672a\u914d\u7f6e\u5730\u5740\uff0c\u5219\u8868\u793a\u4e0d\u5fc5\u76d1\u542c\uff0c\u53ea\u9700\u8981\u4f5c\u4e3a\u5ba2\u6237\u7aef\u4f7f\u7528\u5373\u53ef\n      if (endpointObject == null) {\n        LOGGER.warn(\"rest listen address is not configured, will not start.\");\n        startFuture.complete();\n        return;\n      }\n      Router mainRouter = Router.router(vertx);\n      mountTracePrepareHandler(mainRouter);\n      mountAccessLogHandler(mainRouter);\n      initDispatcher(mainRouter);\n      HttpServer httpServer = createHttpServer();\n      httpServer.requestHandler(mainRouter::accept);\n      startListen(httpServer, startFuture);\n    } catch (Throwable e) {\n      \/\/ vert.x got some states that not print error and execute call back in VertexUtils.blockDeploy, we add a log our self.\n      LOGGER.error(\"\", e);\n      throw e;\n    }\n  }\n\n  private void mountTracePrepareHandler(Router mainRouter) {\n    mainRouter.route().handler(new TracePrepareHandler());\n  }\n\n  private void mountAccessLogHandler(Router mainRouter) {\n    if (AccessLogConfiguration.INSTANCE.getAccessLogEnabled()) {\n      String pattern = AccessLogConfiguration.INSTANCE.getAccesslogPattern();\n      LOGGER.info(\"access log enabled, pattern = {}\", pattern);\n      mainRouter.route()\n          .handler(new AccessLogHandler(\n              pattern,\n              new DefaultAccessLogPatternParser()));\n    }\n  }\n\n  private void initDispatcher(Router mainRouter) {\n    List<VertxHttpDispatcher> dispatchers = SPIServiceUtils.getSortedService(VertxHttpDispatcher.class);\n    for (VertxHttpDispatcher dispatcher : dispatchers) {\n      dispatcher.init(mainRouter);\n    }\n  }\n\n  private void startListen(HttpServer server, Future<Void> startFuture) {\n    server.listen(endpointObject.getPort(), endpointObject.getHostOrIp(), ar -> {\n      if (ar.succeeded()) {\n        LOGGER.info(\"rest listen success. address={}:{}\",\n            endpointObject.getHostOrIp(),\n            ar.result().actualPort());\n        startFuture.complete();\n        return;\n      }\n\n      String msg = String.format(\"rest listen failed, address=%s:%d\",\n          endpointObject.getHostOrIp(),\n          endpointObject.getPort());\n      LOGGER.error(msg, ar.cause());\n      startFuture.fail(ar.cause());\n    });\n  }\n\n  private HttpServer createHttpServer() {\n    HttpServerOptions serverOptions = createDefaultHttpServerOptions();\n    return vertx.createHttpServer(serverOptions);\n  }\n\n  private HttpServerOptions createDefaultHttpServerOptions() {\n    HttpServerOptions serverOptions = new HttpServerOptions();\n    serverOptions.setUsePooledBuffers(true);\n    serverOptions.setIdleTimeout(TransportConfig.getConnectionIdleTimeoutInSeconds());\n    serverOptions.setCompressionSupported(TransportConfig.getCompressed());\n    serverOptions.setMaxHeaderSize(TransportConfig.getMaxHeaderSize());\n\n    if (endpointObject.isSslEnabled()) {\n      SSLOptionFactory factory =\n          SSLOptionFactory.createSSLOptionFactory(SSL_KEY, null);\n      SSLOption sslOption;\n      if (factory == null) {\n        sslOption = SSLOption.buildFromYaml(SSL_KEY);\n      } else {\n        sslOption = factory.createSSLOption();\n      }\n      SSLCustom sslCustom = SSLCustom.createSSLCustom(sslOption.getSslCustomClass());\n      VertxTLSBuilder.buildNetServerOptions(sslOption, sslCustom, serverOptions);\n    }\n\n    return serverOptions;\n  }\n}\n","lang_cluster":"Java","length":152,"code_uid":"bf2248708e9c401b8849f20247a0cc03"}
{"diff_hunk":"@@ -70,6 +70,11 @@ public final class CorrectForClockSkew {\n     if (skew != null) {\n       \/\/ the current span's skew may be a different endpoint than skewFromParent, adjust again.\n       node.value(adjustTimestamps(node.value(), skew));\n+    } else {\n+      if (skewFromParent != null && isLocalSpan(node.value())) {\n+        \/\/Propagate skewFromParent to local spans\n+         skew = skewFromParent;\n+      }\n     }\n     \/\/ propagate skew to any children\n     for (Node<Span> child : node.children()) {","old_code":"\/**\n * Copyright 2015-2017 The OpenZipkin Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except\n * in compliance with the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License\n * is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n * or implied. See the License for the specific language governing permissions and limitations under\n * the License.\n *\/\npackage zipkin.internal;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Iterator;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport zipkin.Annotation;\nimport zipkin.BinaryAnnotation;\nimport zipkin.Constants;\nimport zipkin.Endpoint;\nimport zipkin.Span;\n\n\/**\n * Adjusts spans whose children happen before their parents, based on core annotation values.\n *\/\npublic final class CorrectForClockSkew {\n\n  static class ClockSkew {\n    final Endpoint endpoint;\n    final long skew;\n\n    public ClockSkew(Endpoint endpoint, long skew) {\n      this.endpoint = endpoint;\n      this.skew = skew;\n    }\n  }\n\n  public static List<Span> apply(List<Span> spans) {\n    for (Span s : spans) {\n      if (s.parentId == null) {\n        Node<Span> tree = Node.constructTree(spans);\n        adjust(tree, null);\n        List<Span> result = new ArrayList<>(spans.size());\n        for (Iterator<Node<Span>> i = tree.traverse(); i.hasNext();) {\n          result.add(i.next().value());\n        }\n        return result;\n      }\n    }\n    return spans;\n  }\n\n  \/**\n   * Recursively adjust the timestamps on the span tree. Root span is the reference point, all\n   * children's timestamps gets adjusted based on that span's timestamps.\n   *\/\n  static void adjust(Node<Span> node, @Nullable ClockSkew skewFromParent) {\n    \/\/ adjust skew for the endpoint brought over from the parent span\n    if (skewFromParent != null) {\n      node.value(adjustTimestamps(node.value(), skewFromParent));\n    }\n\n    \/\/ Is there any skew in the current span?\n    ClockSkew skew = getClockSkew(node.value());\n    if (skew != null) {\n      \/\/ the current span's skew may be a different endpoint than skewFromParent, adjust again.\n      node.value(adjustTimestamps(node.value(), skew));\n    }\n    \/\/ propagate skew to any children\n    for (Node<Span> child : node.children()) {\n      adjust(child, skew);\n    }\n  }\n\n  \/** If any annotation has an IP with skew associated, adjust accordingly. *\/\n  static Span adjustTimestamps(Span span, ClockSkew skew) {\n    List<Annotation> annotations = null;\n    Long annotationTimestamp = null;\n    for (int i = 0, length = span.annotations.size(); i < length; i++) {\n      Annotation a = span.annotations.get(i);\n      if (a.endpoint == null) continue;\n      if (ipsMatch(skew.endpoint, a.endpoint)) {\n        if (annotations == null) annotations = new ArrayList<>(span.annotations);\n        if (span.timestamp!= null && a.timestamp == span.timestamp) {\n          annotationTimestamp = a.timestamp;\n        }\n        annotations.set(i, a.toBuilder().timestamp(a.timestamp - skew.skew).build());\n      }\n    }\n    if (annotations != null) {\n      Span.Builder builder = span.toBuilder().annotations(annotations);\n      if (annotationTimestamp != null) {\n        builder.timestamp(annotationTimestamp - skew.skew);\n      }\n      return builder.build();\n    }\n    \/\/ Search for a local span on the skewed endpoint\n    for (int i = 0, length = span.binaryAnnotations.size(); i < length; i++) {\n      BinaryAnnotation b = span.binaryAnnotations.get(i);\n      if (b.endpoint == null) continue;\n      if (b.key.equals(Constants.LOCAL_COMPONENT) && ipsMatch(skew.endpoint, b.endpoint)) {\n        return span.toBuilder().timestamp(span.timestamp - skew.skew).build();\n      }\n    }\n    return span;\n  }\n\n  static boolean ipsMatch(Endpoint skew, Endpoint that) {\n    return (skew.ipv6 != null && Arrays.equals(skew.ipv6, that.ipv6))\n        || (skew.ipv4 != 0 && skew.ipv4 == that.ipv4);\n  }\n\n  \/** Use client\/server annotations to determine if there's clock skew. *\/\n  @Nullable\n  static ClockSkew getClockSkew(Span span) {\n    Map<String, Annotation> annotations = asMap(span.annotations);\n\n    Long clientSend = getTimestamp(annotations, Constants.CLIENT_SEND);\n    Long clientRecv = getTimestamp(annotations, Constants.CLIENT_RECV);\n    Long serverRecv = getTimestamp(annotations, Constants.SERVER_RECV);\n    Long serverSend = getTimestamp(annotations, Constants.SERVER_SEND);\n\n    if (clientSend == null || clientRecv == null || serverRecv == null || serverSend == null) {\n      return null;\n    }\n\n    Endpoint server = annotations.get(Constants.SERVER_RECV).endpoint;\n    server = server == null ? annotations.get(Constants.SERVER_SEND).endpoint : server;\n    if (server == null) return null;\n\n    long clientDuration = clientRecv - clientSend;\n    long serverDuration = serverSend - serverRecv;\n\n    \/\/ There is only clock skew if CS is after SR or CR is before SS\n    boolean csAhead = clientSend < serverRecv;\n    boolean crAhead = clientRecv > serverSend;\n    if (serverDuration > clientDuration || (csAhead && crAhead)) {\n      return null;\n    }\n    long latency = (clientDuration - serverDuration) \/ 2;\n    long skew = serverRecv - latency - clientSend;\n    if (skew != 0L) {\n      return new ClockSkew(server, skew);\n    }\n    return null;\n  }\n\n  \/** Get the annotations as a map with value to annotation bindings. *\/\n  static Map<String, Annotation> asMap(List<Annotation> annotations) {\n    Map<String, Annotation> result = new LinkedHashMap<>(annotations.size());\n    for (Annotation a : annotations) {\n      result.put(a.value, a);\n    }\n    return result;\n  }\n\n  @Nullable\n  static Long getTimestamp(Map<String, Annotation> annotations, String value) {\n    Annotation result = annotations.get(value);\n    return result != null ? result.timestamp : null;\n  }\n\n  private CorrectForClockSkew() {\n  }\n}\n","lang_cluster":"Java","length":170,"code_uid":"c849d9e78a394d3bae4c30f8e6a60026"}
{"diff_hunk":"@@ -24,21 +24,9 @@ public class ExperimentalEIPs {\n   \/\/ To make it easier for tests to reset the value to default\n   public static final long EIP1559_BASEFEE_DEFAULT_VALUE = 1000000000L;\n \n-  @Option(\n-      hidden = true,\n-      names = {\"--Xeip1559-basefee-max-change-denominator\"},\n-      arity = \"1\")\n-  public static Long basefeeMaxChangeDenominator = 8L;\n-\n   @Option(\n       hidden = true,\n       names = {\"--Xeip1559-initial-base-fee\"},\n       arity = \"1\")\n   public static Long initialBasefee = EIP1559_BASEFEE_DEFAULT_VALUE;\n-\n-  @Option(\n-      hidden = true,\n-      names = {\"--Xeip1559-slack-coefficient\"},\n-      arity = \"1\")\n-  public static Long slackCoefficient = 2L;\n }","old_code":"\/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n *\/\npackage org.hyperledger.besu.config.experimental;\n\nimport picocli.CommandLine.Option;\n\n\/**\n * Flags defined in this class must be used with caution, and strictly reserved to experimental\n * EIPs.\n *\/\npublic class ExperimentalEIPs {\n  \/\/ To make it easier for tests to reset the value to default\n  public static final long EIP1559_BASEFEE_DEFAULT_VALUE = 1000000000L;\n\n  @Option(\n      hidden = true,\n      names = {\"--Xeip1559-basefee-max-change-denominator\"},\n      arity = \"1\")\n  public static Long basefeeMaxChangeDenominator = 8L;\n\n  @Option(\n      hidden = true,\n      names = {\"--Xeip1559-initial-base-fee\"},\n      arity = \"1\")\n  public static Long initialBasefee = EIP1559_BASEFEE_DEFAULT_VALUE;\n\n  @Option(\n      hidden = true,\n      names = {\"--Xeip1559-slack-coefficient\"},\n      arity = \"1\")\n  public static Long slackCoefficient = 2L;\n}\n","lang_cluster":"Java","length":44,"code_uid":"a4a0899db9a84af2a85e5400993b416a"}
{"diff_hunk":"@@ -46,8 +46,26 @@ class ServiceImpl final : public MyGame::Example::MonsterStorage::Service {\n                                   const flatbuffers::BufferRef<Stat> *request,\n                                    ::grpc::ServerWriter< flatbuffers::BufferRef<Monster>>* writer)\n                                   override {\n-    assert(false);  \/\/ We're not actually using this RPC.\n-    return grpc::Status::CANCELLED;\n+       fbb_.Clear();\n+       std::cout << \"Hello, \" << request->GetRoot()->id()->str()\n+                 << request->GetRoot()->val()\n+                 << request->GetRoot()->count()\n+                 << std::endl;\n+       std::cout << \"Streaming test.\\n\";\n+\n+       for (int i=0; i<10; i++) {\n+         fbb_.Clear();\n+         auto monster_offset =\n+           CreateMonster(fbb_, 0, 0, 0, fbb_.CreateString(\"Fred No.\" + std::to_string(i)));\n+         fbb_.Finish(monster_offset);\n+\n+         flatbuffers::BufferRef<Monster> result(\n+           fbb_.GetBufferPointer(), fbb_.GetSize());\n+\n+         writer->Write(result);\n+       }\n+\n+       return grpc::Status::OK;\n   }\n \n  private:","old_code":"\/*\n * Copyright 2014 Google Inc. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n#include <thread>\n\n#include <grpc++\/grpc++.h>\n\n#include \"monster_test_generated.h\"\n#include \"monster_test.grpc.fb.h\"\n\nusing namespace MyGame::Example;\n\n\/\/ The callback implementation of our server, that derives from the generated\n\/\/ code. It implements all rpcs specified in the FlatBuffers schema.\nclass ServiceImpl final : public MyGame::Example::MonsterStorage::Service {\n  virtual ::grpc::Status Store(::grpc::ServerContext* context,\n                               const flatbuffers::BufferRef<Monster> *request,\n                               flatbuffers::BufferRef<Stat> *response)\n                               override {\n    \/\/ Create a response from the incoming request name.\n    fbb_.Clear();\n    auto stat_offset = CreateStat(fbb_, fbb_.CreateString(\"Hello, \" +\n                                        request->GetRoot()->name()->str()));\n    fbb_.Finish(stat_offset);\n    \/\/ Since we keep reusing the same FlatBufferBuilder, the memory it owns\n    \/\/ remains valid until the next call (this BufferRef doesn't own the\n    \/\/ memory it points to).\n    *response = flatbuffers::BufferRef<Stat>(fbb_.GetBufferPointer(),\n                                             fbb_.GetSize());\n    return grpc::Status::OK;\n  }\n  virtual ::grpc::Status Retrieve(::grpc::ServerContext *context,\n                                  const flatbuffers::BufferRef<Stat> *request,\n                                   ::grpc::ServerWriter< flatbuffers::BufferRef<Monster>>* writer)\n                                  override {\n    assert(false);  \/\/ We're not actually using this RPC.\n    return grpc::Status::CANCELLED;\n  }\n\n private:\n  flatbuffers::FlatBufferBuilder fbb_;\n};\n\n\/\/ Track the server instance, so we can terminate it later.\ngrpc::Server *server_instance = nullptr;\n\/\/ Mutex to protec this variable.\nstd::mutex wait_for_server;\nstd::condition_variable server_instance_cv;\n\n\/\/ This function implements the server thread.\nvoid RunServer() {\n  auto server_address = \"0.0.0.0:50051\";\n  \/\/ Callback interface we implemented above.\n  ServiceImpl service;\n  grpc::ServerBuilder builder;\n  builder.AddListeningPort(server_address, grpc::InsecureServerCredentials());\n  builder.RegisterService(&service);\n\n  \/\/ Start the server. Lock to change the variable we're changing.\n  wait_for_server.lock();\n  server_instance = builder.BuildAndStart().release();\n  wait_for_server.unlock();\n  server_instance_cv.notify_one();\n\n  std::cout << \"Server listening on \" << server_address << std::endl;\n  \/\/ This will block the thread and serve requests.\n  server_instance->Wait();\n}\n\nint main(int \/*argc*\/, const char * \/*argv*\/[]) {\n  \/\/ Launch server.\n  std::thread server_thread(RunServer);\n\n  \/\/ wait for server to spin up.\n  std::unique_lock<std::mutex> lock(wait_for_server);\n  while (!server_instance) server_instance_cv.wait(lock);\n\n  \/\/ Now connect the client.\n  auto channel = grpc::CreateChannel(\"localhost:50051\",\n                                     grpc::InsecureChannelCredentials());\n  auto stub = MyGame::Example::MonsterStorage::NewStub(channel);\n\n  grpc::ClientContext context;\n\n  \/\/ Build a request with the name set.\n  flatbuffers::FlatBufferBuilder fbb;\n  auto monster_offset = CreateMonster(fbb, 0, 0, 0, fbb.CreateString(\"Fred\"));\n  fbb.Finish(monster_offset);\n  auto request = flatbuffers::BufferRef<Monster>(fbb.GetBufferPointer(),\n                                                 fbb.GetSize());\n  flatbuffers::BufferRef<Stat> response;\n\n  \/\/ The actual RPC.\n  auto status = stub->Store(&context, request, &response);\n\n  if (status.ok()) {\n    auto resp = response.GetRoot()->id();\n    std::cout << \"RPC response: \" << resp->str() << std::endl;\n  } else {\n    std::cout << \"RPC failed\" << std::endl;\n  }\n\n  server_instance->Shutdown();\n\n  server_thread.join();\n\n  delete server_instance;\n\n  return 0;\n}\n\n","lang_cluster":"Java","length":124,"code_uid":"90dd2993209c480898cad4b4ca49e559"}
{"diff_hunk":"@@ -5,18 +5,14 @@\n \n package net.sourceforge.pmd.lang.vf.ast;\n \n-public class ASTAttribute extends AbstractVFNode {\n+public class ASTAttribute extends AbstractVfNode {\n \n     private String name;\n \n-    public ASTAttribute(int id) {\n+    ASTAttribute(int id) {\n         super(id);\n     }\n \n-    public ASTAttribute(VfParser p, int id) {\n-        super(p, id);\n-    }\n-\n     \/**\n      * @return Returns the name.\n      *\/","old_code":"\/**\n * BSD-style license; for more info see http:\/\/pmd.sourceforge.net\/license.html\n *\/\n\/* Generated By:JJTree: Do not edit this line. ASTAttribute.java *\/\n\npackage net.sourceforge.pmd.lang.vf.ast;\n\npublic class ASTAttribute extends AbstractVFNode {\n\n    private String name;\n\n    public ASTAttribute(int id) {\n        super(id);\n    }\n\n    public ASTAttribute(VfParser p, int id) {\n        super(p, id);\n    }\n\n    \/**\n     * @return Returns the name.\n     *\/\n    public String getName() {\n        return name;\n    }\n\n    \/**\n     * @param name\n     *            The name to set.\n     *\/\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    \/**\n     * @return boolean - true if the element has a namespace-prefix, false\n     *         otherwise\n     *\/\n    public boolean isHasNamespacePrefix() {\n        return name.indexOf(':') >= 0;\n    }\n\n    \/**\n     * @return String - the part of the name that is before the (first) colon\n     *         (\":\")\n     *\/\n    public String getNamespacePrefix() {\n        int colonIndex = name.indexOf(':');\n        return colonIndex >= 0 ? name.substring(0, colonIndex) : \"\";\n    }\n\n    \/**\n     * @return String - The part of the name that is after the first colon\n     *         (\":\"). If the name does not contain a colon, the full name is\n     *         returned.\n     *\/\n    public String getLocalName() {\n        int colonIndex = name.indexOf(':');\n        return colonIndex >= 0 ? name.substring(colonIndex + 1) : name;\n    }\n\n    \/**\n     * Accept the visitor. *\n     *\/\n    @Override\n    public Object jjtAccept(VfParserVisitor visitor, Object data) {\n        return visitor.visit(this, data);\n    }\n}\n","lang_cluster":"Java","length":69,"code_uid":"fb13ce4432274cf2b8ffd43daaa81e81"}
{"diff_hunk":"@@ -80,6 +80,12 @@ const start = (passthroughArgs, buildConfig = config.defaultBuildConfig, options\n     if (user_data_dir) {\n       \/\/ clear the data directory before doing a network test\n       fs.removeSync(user_data_dir.replace('\\\\', ''))\n+      if (fs.existsSync(networkLogFile)) {\n+        fs.unlinkSync(networkLogFile)\n+      }\n+      if (fs.existsSync('network-audit-results.json')) {\n+        fs.unlinkSync('network-audit-results.json')\n+      }\n     }\n   }\n ","old_code":"const path = require('path')\nconst fs = require('fs-extra')\nconst ip = require('ip')\nconst URL = require('url').URL\nconst config = require('..\/lib\/config')\nconst util = require('..\/lib\/util')\nconst whitelistedUrlPrefixes = require('.\/whitelistedUrlPrefixes')\n\nconst start = (passthroughArgs, buildConfig = config.defaultBuildConfig, options) => {\n  config.buildConfig = buildConfig\n  config.update(options)\n\n  let braveArgs = [\n    '--enable-logging',\n    '--v=' + options.v,\n  ]\n  if (options.vmodule) {\n    braveArgs.push('--vmodule=' + options.vmodule);\n  }\n  if (options.no_sandbox) {\n    braveArgs.push('--no-sandbox')\n  }\n  if (options.disable_brave_extension) {\n    braveArgs.push('--disable-brave-extension')\n  }\n  if (options.disable_brave_rewards_extension) {\n    braveArgs.push('--disable-brave-rewards-extension')\n  }\n  if (options.disable_pdfjs_extension) {\n    braveArgs.push('--disable-pdfjs-extension')\n  }\n  if (options.disable_webtorrent_extension) {\n    braveArgs.push('--disable-webtorrent-extension')\n  }\n  if (options.ui_mode) {\n    braveArgs.push(`--ui-mode=${options.ui_mode}`)\n  }\n  if (!options.enable_brave_update) {\n    \/\/ This only has meaning with MacOS and official build.\n    braveArgs.push('--disable-brave-update')\n  }\n  if (options.single_process) {\n    braveArgs.push('--single-process')\n  }\n  if (options.show_component_extensions) {\n    braveArgs.push('--show-component-extension-options')\n  }\n  if (options.rewards) {\n    braveArgs.push(`--rewards=${options.rewards}`)\n  }\n  if (options.brave_ads_testing) {\n    braveArgs.push('--brave-ads-testing')\n  }\n  if (options.brave_ads_debug) {\n    braveArgs.push('--brave-ads-debug')\n  }\n  if (options.brave_ads_production) {\n    braveArgs.push('--brave-ads-production')\n  }\n  if (options.brave_ads_staging) {\n    braveArgs.push('--brave-ads-staging')\n  }\n  braveArgs = braveArgs.concat(passthroughArgs)\n\n  let user_data_dir\n  if (options.user_data_dir_name) {\n    if (process.platform === 'darwin') {\n      user_data_dir = path.join(process.env.HOME, 'Library', 'Application\\\\ Support', 'BraveSoftware', options.user_data_dir_name)\n    } else if (process.platform === 'win32') {\n      user_data_dir = path.join(process.env.LocalAppData, 'BraveSoftware', options.user_data_dir_name)\n    } else {\n      user_data_dir = path.join(process.env.HOME, '.config', 'BraveSoftware', options.user_data_dir_name)\n    }\n    braveArgs.push('--user-data-dir=' + user_data_dir);\n  }\n  const networkLogFile = path.resolve(path.join(__dirname, '..', 'network_log.json'))\n  if (options.network_log) {\n    braveArgs.push(`--log-net-log=${networkLogFile}`)\n    braveArgs.push(`--net-log-capture-mode=IncludeSocketBytes`)\n    if (user_data_dir) {\n      \/\/ clear the data directory before doing a network test\n      fs.removeSync(user_data_dir.replace('\\\\', ''))\n    }\n  }\n\n  let cmdOptions = {\n    stdio: 'inherit',\n    timeout: options.network_log ? 120000 : undefined,\n    continueOnFail: options.network_log ? true : false,\n    shell: true\n  }\n\n  if (options.network_log) {\n    console.log('Network audit started. Logging requests for the next 2min or until you quit Brave...')\n  }\n\n  let outputPath = options.output_path\n  if (!outputPath) {\n    if (process.platform === 'darwin') {\n      outputPath = path.join(config.outputDir, config.macAppName() + '.app', 'Contents', 'MacOS', config.macAppName())\n    } else if (process.platform === 'win32') {\n      outputPath = path.join(config.outputDir, 'brave.exe')\n    } else {\n      outputPath = path.join(config.outputDir, 'brave')\n    }\n  }\n  util.run(outputPath, braveArgs, cmdOptions)\n\n  if (options.network_log) {\n    let exitCode = 0\n    \/\/ Read the network log\n    const jsonOutput = fs.readJsonSync(networkLogFile)\n    const URL_REQUEST_TYPE = jsonOutput.constants.logSourceType.URL_REQUEST\n    const URL_REQUEST_FAKE_RESPONSE_HEADERS_CREATED = jsonOutput.constants.logEventTypes.URL_REQUEST_FAKE_RESPONSE_HEADERS_CREATED\n    const urlRequests = jsonOutput.events.filter((event) => {\n      if (event.type === URL_REQUEST_FAKE_RESPONSE_HEADERS_CREATED) {\n        \/\/ showing these helps determine which URL requests which don't\n        \/\/ actually hit the network\n        return true\n      }\n      if (event.source.type === URL_REQUEST_TYPE) {\n        if (!event.params) {\n          return false\n        }\n        const url = event.params.url\n        if (!url) {\n          return false\n        }\n        if (url.startsWith('http') && url.includes('.')) {\n          const found = whitelistedUrlPrefixes.find((prefix) => {\n            return url.startsWith(prefix)\n          })\n          if (!found) {\n            \/\/ Check if the URL is a private IP\n            try {\n              const hostname = new URL(url).hostname\n              if (ip.isPrivate(hostname)) {\n                \/\/ Warn but don't fail the audit\n                console.log('NETWORK AUDIT WARN:', url)\n                return true\n              }\n            } catch (e) {}\n            \/\/ This is not a whitelisted URL! log it and exit with non-zero\n            console.log('NETWORK AUDIT FAIL:', url)\n            exitCode = 1\n          }\n          return true\n        }\n      }\n      return false\n    })\n    fs.writeJsonSync('network-audit-results.json', urlRequests)\n    if (exitCode > 0) {\n      console.log(`network-audit failed. import ${networkLogFile} in chrome:\/\/net-internals for more details.`)\n    } else {\n      console.log('network audit passed.')\n    }\n    process.exit(exitCode)\n  }\n}\n\nmodule.exports = start\n","lang_cluster":"Javascript","length":162,"code_uid":"ca5b19de0d0e418b903ab96a45b49a1a"}
{"diff_hunk":"@@ -10,6 +10,17 @@ export function extend(obj, props) {\n \treturn obj;\n }\n \n+\/** Invoke or update a ref, depending on whether it is a function or object ref.\n+ *  @param {object|function} [ref=null]\n+ *  @param {any} [value]\n+ *\/\n+export function applyRef(ref, value) {\n+\tif (ref!=null) {\n+\t\tif (typeof ref=='function') ref(value);\n+\t\telse ref.current = value;\n+\t}\n+}\n+\n \/**\n  * Call a function asynchronously, as soon as possible. Makes\n  * use of HTML Promise to schedule the callback if available,","old_code":"\/**\n * Copy all properties from `props` onto `obj`.\n * @param {object} obj Object onto which properties should be copied.\n * @param {object} props Object from which to copy properties.\n * @returns {object}\n * @private\n *\/\nexport function extend(obj, props) {\n\tfor (let i in props) obj[i] = props[i];\n\treturn obj;\n}\n\n\/**\n * Call a function asynchronously, as soon as possible. Makes\n * use of HTML Promise to schedule the callback if available,\n * otherwise falling back to `setTimeout` (mainly for IE<11).\n * @type {(callback: function) => void}\n *\/\nexport const defer = typeof Promise=='function' ? Promise.resolve().then.bind(Promise.resolve()) : setTimeout;\n","lang_cluster":"Javascript","length":19,"code_uid":"d764fe754f964bcd830b6255191da91e"}
{"diff_hunk":"@@ -48,11 +48,36 @@ Blockly.WidgetDiv.owner_ = null;\n \n \/**\n  * Optional cleanup function set by whichever object uses the widget.\n+ * This is called as soon as a dispose is desired. If the dispose should\n+ * be animated, the animation should start on the call of dispose_.\n  * @type {Function}\n  * @private\n  *\/\n Blockly.WidgetDiv.dispose_ = null;\n \n+\/**\n+ * Optional function called at the end of a dispose animation.\n+ * Set by whichever object is using the widget.\n+ * @type {Function}\n+ * @private\n+ *\/\n+Blockly.WidgetDiv.disposeAnimateFinished_ = null;\n+\n+\/**\n+ * Timer ID for the dispose animation.\n+ * @type {number}\n+ * @private\n+ *\/\n+Blockly.WidgetDiv.disposeAnimationTimer_ = null;\n+\n+\/**\n+ * Length of time in seconds for the dispose animation.\n+ * @type {number}\n+ * @private\n+ *\/\n+Blockly.WidgetDiv.disposeAnimateTimerLength_ = 0;\n+\n+\n \/**\n  * Create the widget div and inject it onto the page.\n  *\/","old_code":"\/**\n * @license\n * Visual Blocks Editor\n *\n * Copyright 2013 Google Inc.\n * https:\/\/developers.google.com\/blockly\/\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * @fileoverview A div that floats on top of Blockly.  This singleton contains\n *     temporary HTML UI widgets that the user is currently interacting with.\n *     E.g. text input areas, colour pickers, context menus.\n * @author fraser@google.com (Neil Fraser)\n *\/\n'use strict';\n\ngoog.provide('Blockly.WidgetDiv');\n\ngoog.require('Blockly.Css');\ngoog.require('goog.dom');\ngoog.require('goog.style');\n\n\n\/**\n * The HTML container.  Set once by Blockly.WidgetDiv.createDom.\n * @type {Element}\n *\/\nBlockly.WidgetDiv.DIV = null;\n\n\/**\n * The object currently using this container.\n * @type {Object}\n * @private\n *\/\nBlockly.WidgetDiv.owner_ = null;\n\n\/**\n * Optional cleanup function set by whichever object uses the widget.\n * @type {Function}\n * @private\n *\/\nBlockly.WidgetDiv.dispose_ = null;\n\n\/**\n * Create the widget div and inject it onto the page.\n *\/\nBlockly.WidgetDiv.createDom = function() {\n  if (Blockly.WidgetDiv.DIV) {\n    return;  \/\/ Already created.\n  }\n  \/\/ Create an HTML container for popup overlays (e.g. editor widgets).\n  Blockly.WidgetDiv.DIV = goog.dom.createDom('div', 'blocklyWidgetDiv');\n  document.body.appendChild(Blockly.WidgetDiv.DIV);\n};\n\n\/**\n * Initialize and display the widget div.  Close the old one if needed.\n * @param {!Object} newOwner The object that will be using this container.\n * @param {boolean} rtl Right-to-left (true) or left-to-right (false).\n * @param {Function} dispose Optional cleanup function to be run when the widget\n *   is closed.\n *\/\nBlockly.WidgetDiv.show = function(newOwner, rtl, dispose) {\n  Blockly.WidgetDiv.hide();\n  Blockly.WidgetDiv.owner_ = newOwner;\n  Blockly.WidgetDiv.dispose_ = dispose;\n  \/\/ Temporarily move the widget to the top of the screen so that it does not\n  \/\/ cause a scrollbar jump in Firefox when displayed.\n  var xy = goog.style.getViewportPageOffset(document);\n  Blockly.WidgetDiv.DIV.style.top = xy.y + 'px';\n  Blockly.WidgetDiv.DIV.style.direction = rtl ? 'rtl' : 'ltr';\n  Blockly.WidgetDiv.DIV.style.display = 'block';\n  Blockly.Events.setGroup(true);\n};\n\n\/**\n * Destroy the widget and hide the div.\n *\/\nBlockly.WidgetDiv.hide = function() {\n  if (Blockly.WidgetDiv.owner_) {\n    Blockly.WidgetDiv.owner_ = null;\n    Blockly.WidgetDiv.DIV.style.display = 'none';\n    Blockly.WidgetDiv.DIV.style.left = '';\n    Blockly.WidgetDiv.DIV.style.top = '';\n    Blockly.WidgetDiv.DIV.style.height = '';\n    Blockly.WidgetDiv.dispose_ && Blockly.WidgetDiv.dispose_();\n    Blockly.WidgetDiv.dispose_ = null;\n    goog.dom.removeChildren(Blockly.WidgetDiv.DIV);\n    Blockly.Events.setGroup(false);\n  }\n};\n\n\/**\n * Is the container visible?\n * @return {boolean} True if visible.\n *\/\nBlockly.WidgetDiv.isVisible = function() {\n  return !!Blockly.WidgetDiv.owner_;\n};\n\n\/**\n * Destroy the widget and hide the div if it is being used by the specified\n *   object.\n * @param {!Object} oldOwner The object that was using this container.\n *\/\nBlockly.WidgetDiv.hideIfOwner = function(oldOwner) {\n  if (Blockly.WidgetDiv.owner_ == oldOwner) {\n    Blockly.WidgetDiv.hide();\n  }\n};\n\n\/**\n * Position the widget at a given location.  Prevent the widget from going\n * offscreen top or left (right in RTL).\n * @param {number} anchorX Horizontal location (window coorditates, not body).\n * @param {number} anchorY Vertical location (window coorditates, not body).\n * @param {!goog.math.Size} windowSize Height\/width of window.\n * @param {!goog.math.Coordinate} scrollOffset X\/y of window scrollbars.\n * @param {boolean} rtl True if RTL, false if LTR.\n *\/\nBlockly.WidgetDiv.position = function(anchorX, anchorY, windowSize,\n                                      scrollOffset, rtl) {\n  \/\/ Don't let the widget go above the top edge of the window.\n  if (anchorY < scrollOffset.y) {\n    anchorY = scrollOffset.y;\n  }\n  if (rtl) {\n    \/\/ Don't let the widget go right of the right edge of the window.\n    if (anchorX > windowSize.width + scrollOffset.x) {\n      anchorX = windowSize.width + scrollOffset.x;\n    }\n  } else {\n    \/\/ Don't let the widget go left of the left edge of the window.\n    if (anchorX < scrollOffset.x) {\n      anchorX = scrollOffset.x;\n    }\n  }\n  Blockly.WidgetDiv.DIV.style.left = anchorX + 'px';\n  Blockly.WidgetDiv.DIV.style.top = anchorY + 'px';\n  Blockly.WidgetDiv.DIV.style.height =\n      (windowSize.height - anchorY + scrollOffset.y) + 'px';\n};\n","lang_cluster":"Javascript","length":154,"code_uid":"913ea1847c6a4cc9b83eec281718669c"}
{"diff_hunk":"@@ -15,7 +15,7 @@ export default AbstractEditController.extend({\n   showUpdateButton: true,\n \n   database: inject.service(),\n-  editController: inject.controller('patients\/edit'),\n+  editController: null,\n   filesystem: inject.service(),\n \n   photoFileNotSet: computed('model.photoFile', function() {","old_code":"import AbstractEditController from 'hospitalrun\/controllers\/abstract-edit-controller';\nimport Ember from 'ember';\nimport { translationMacro as t } from 'ember-i18n';\n\nconst { computed, get, inject, isEmpty, RSVP, set } = Ember;\n\nexport default AbstractEditController.extend({\n  addAction: 'addPhoto',\n  editTitle: t('patients.titles.editPhoto'),\n  fileRequiredMessage: t('patients.messages.photoFileRequired'),\n  modelName: 'photo',\n  newTitle: t('patients.titles.addPhoto'),\n  newModel: false,\n  showFileRequired: false,\n  showUpdateButton: true,\n\n  database: inject.service(),\n  editController: inject.controller('patients\/edit'),\n  filesystem: inject.service(),\n\n  photoFileNotSet: computed('model.photoFile', function() {\n    let model = get(this, 'model');\n    let isNew = get(model, 'isNew');\n    let photoFile = get(model, 'photoFile');\n    return (isNew && isEmpty(photoFile));\n  }),\n\n  title: computed('model.isNew', function() {\n    let isNew = get(this, 'model.isNew');\n    if (isNew) {\n      return get(this, 'newTitle');\n    } else {\n      return get(this, 'editTitle');\n    }\n  }),\n\n  updateButtonAction: computed('photoFileNotSet', function() {\n    let photoFileNotSet = get(this, 'photoFileNotSet');\n    if (photoFileNotSet) {\n      return 'showFileRequired';\n    } else {\n      set(this, 'showFileRequired', false);\n      return 'update';\n    }\n  }),\n\n  updateButtonClass: computed('photoFileNotSet', function() {\n    let photoFileNotSet = get(this, 'photoFileNotSet');\n    if (photoFileNotSet) {\n      return 'disabled-btn';\n    }\n  }),\n\n  afterUpdate(model) {\n    let isNew = get(this, 'newModel');\n    let editController = get(this, 'editController');\n    if (isNew) {\n      let photoFile = get(model, 'photoFile');\n      let saveToDir = get(model, 'saveToDir');\n      let fileSystem = get(this, 'filesystem');\n      let modelName = get(this, 'modelName');\n      let pouchDbId = get(this, 'database').getPouchId(get(model, 'id'), modelName);\n      fileSystem.addFile(photoFile, saveToDir, pouchDbId).then((fileEntry) => {\n        model.setProperties({\n          localFile: true,\n          fileName: fileEntry.fullPath,\n          url: fileEntry.toURL()\n        });\n        model.save().then(() => {\n          editController.send(get(this, 'addAction'), model);\n        }).catch((err) => {\n          throw err;\n        });\n      });\n    } else {\n      this.send('closeModal');\n    }\n  },\n\n  beforeUpdate() {\n    let model = get(this, 'model');\n    let photoFile = get(model, 'photoFile');\n    let isImage = get(model, 'isImage');\n    let isNew = get(model, 'isNew');\n    set(this, 'newModel', isNew);\n    if (isNew) {\n      model.setProperties({\n        files: [Ember.Object.create({\n          content_type: photoFile.type,\n          data: photoFile,\n          name: 'file'\n        })],\n        isImage\n      });\n    }\n    return RSVP.resolve();\n  },\n\n  actions: {\n    cancel() {\n      this.send('closeModal');\n    },\n\n    showFileRequired() {\n      set(this, 'showFileRequired', true);\n    }\n  }\n});\n","lang_cluster":"Javascript","length":108,"code_uid":"2c10729314cf4c209eeb8e60235503c8"}
{"diff_hunk":"@@ -20,23 +20,45 @@\n \n import Realm from 'realm';\n \n-class Todo extends Realm.Object {}\n-Todo.schema = {\n-    name: 'Todo',\n+class Task extends Realm.Object {}\n+Task.schema = {\n+    name: 'Task',\n     properties: {\n-        done: {type: 'bool', default: false},\n+        completed: {type: 'bool', default: false},\n         text: 'string',\n     },\n };\n \n-class TodoList extends Realm.Object {}\n-TodoList.schema = {\n-    name: 'TodoList',\n+class TaskList extends Realm.Object {}\n+TaskList.schema = {\n+    name: 'TaskList',\n     properties: {\n-        name: 'string',\n-        creationDate: 'date',\n-        items: {type: 'list', objectType: 'Todo'},\n+        id: 'string',\n+        text: 'string',\n+        completed: {type: 'bool', default: false},\n+        items: {type: 'list', objectType: 'Task'},\n+    },\n+    primaryKey: 'id'\n+};\n+\n+\n+class TaskListList extends Realm.Object {}\n+TaskListList.schema = {\n+    name: 'TaskListList',\n+    properties: {\n+        id: 'int',\n+        items: {type: 'list', objectType: 'TaskList'},\n     },\n+    primaryKey: 'id'\n };\n \n-export default new Realm({schema: [Todo, TodoList]});\n+var adminToken = \"ewoJImlkZW50aXR5IjogImFkbWluIiwKCSJhY2Nlc3MiOiBbInVwbG9hZCIsICJkb3dubG9hZCIsICJtYW5hZ2UiXQp9Cg==:I1mEfddNdQo4wS\/7dVgEIEm9pv7pLEyyx3zmGgWMCxkEHOxz8DxLjm\/yLD8OD7iE9XcCHy4xs6SOu2ZbbRysezssyY+3z\/anrz7u\/EBDvyfHPRakaw3rTpD6RcTb4SaAeLBagtDP7YqycneZrGd3oUXofvJdWg4YpDOvr\/+1zwntt3DGg5D1ghKzSWcF7FnosswjiqGk91t2hbZbPYNEPEJKwkCPj0exOZqQ73627rD9Jzbr3CcHpp0V4U1BEhVfgZyo9dchWYT9Qw0rkqjqc5ylvR73Ihpa9hN9+wKxhU1K6nNzsbhrD0sMfOOlxsuduWHrhiT8vkTL\/WFOO4vC1Q==\";\n+var adminUser = Realm.Sync.User.adminUser(adminToken);\n+export default new Realm({\n+                         schema: [Task, TaskList, TaskListList],\n+                         sync: {\n+                            user: adminUser,\n+                            url: \"realm:\/\/192.168.157.64:9080\/a46feb0c3959646acb7540971c411269\/realmtasks\"\n+                         },\n+                         path: \"tasks.realm\"\n+                         });","old_code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/\n\/\/ Copyright 2016 Realm Inc.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n'use strict';\n\nimport Realm from 'realm';\n\nclass Todo extends Realm.Object {}\nTodo.schema = {\n    name: 'Todo',\n    properties: {\n        done: {type: 'bool', default: false},\n        text: 'string',\n    },\n};\n\nclass TodoList extends Realm.Object {}\nTodoList.schema = {\n    name: 'TodoList',\n    properties: {\n        name: 'string',\n        creationDate: 'date',\n        items: {type: 'list', objectType: 'Todo'},\n    },\n};\n\nexport default new Realm({schema: [Todo, TodoList]});\n","lang_cluster":"Javascript","length":42,"code_uid":"4918df354eae435289457efc313b2b53"}
{"diff_hunk":"@@ -57,11 +57,10 @@ module Selenium\n               is_relative = Regexp.last_match(1).strip == '1'\n             when \/^Path=(.+)$\/\n               path = Regexp.last_match(1).strip\n+              p = path_for(name, is_relative, path)\n+              @profile_paths[name] = p if p\n             end\n           end\n-\n-          p = path_for(name, is_relative, path)\n-          @profile_paths[name] = p if p\n         end\n \n         def path_for(name, is_relative, path)","old_code":"# encoding: utf-8\n#\n# Licensed to the Software Freedom Conservancy (SFC) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The SFC licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nmodule Selenium\n  module WebDriver\n    module Firefox\n      # @api private\n      class ProfilesIni\n        def initialize\n          @ini_path = File.join(Util.app_data_path, 'profiles.ini')\n          @profile_paths = {}\n\n          parse if File.exist?(@ini_path)\n        end\n\n        def [](name)\n          path = @profile_paths[name]\n          path && Profile.new(path)\n        end\n\n        def refresh\n          @profile_paths.clear\n          parse\n        end\n\n        private\n\n        def parse\n          string      = File.read @ini_path\n          name        = nil\n          is_relative = nil\n          path        = nil\n\n          string.split(\"\\n\").each do |line|\n            case line\n            when \/^\\[Profile\/\n              name, path = nil if path_for(name, is_relative, path)\n            when \/^Name=(.+)$\/\n              name = Regexp.last_match(1).strip\n            when \/^IsRelative=(.+)$\/\n              is_relative = Regexp.last_match(1).strip == '1'\n            when \/^Path=(.+)$\/\n              path = Regexp.last_match(1).strip\n            end\n          end\n\n          p = path_for(name, is_relative, path)\n          @profile_paths[name] = p if p\n        end\n\n        def path_for(name, is_relative, path)\n          return unless [name, path].any?\n          is_relative ? File.join(Util.app_data_path, path) : path\n        end\n      end # ProfilesIni\n    end # Firefox\n  end # WebDriver\nend # Selenium\n","lang_cluster":"Javascript","length":74,"code_uid":"165e80eeecf64188812c6454ed7b597c"}
{"diff_hunk":"@@ -43,18 +43,25 @@ var TESTS = {\n     QueryTests: require('.\/query-tests'),\n     MigrationTests: require('.\/migration-tests'),\n     EncryptionTests: require('.\/encryption-tests'),\n-    ObjectIDTests: require('.\/object-id-tests'),\n     AliasTests: require('.\/alias-tests'),\n     BsonTests: require('.\/bson-tests'),\n     \/\/ Garbagecollectiontests: require('.\/garbage-collection'),\n };\n \n+\/\/TODO: remove when MongoDB Realm test server can be hosted on Mac or other options exists\n+if (isNodeProcess) {\n+    TESTS.ObjectIDTests = require('.\/object-id-tests');\n+}\n+\n \/\/ If sync is enabled, run the sync tests\n if (global.enableSyncTests) {\n-    TESTS.AppTests = require('.\/app-tests');\n+    \/\/TODO: remove when MongoDB Realm test server can be hosted on Mac or other options exists\n+    if (isNodeProcess) {\n+        TESTS.AppTests = require('.\/app-tests');\n     \/\/ TESTS.OpenBehaviorTests = require('.\/open-behavior-tests'); \/\/ FIXME: figure out how to enable them\n-    TESTS.UserTests = require('.\/user-tests');\n-    TESTS.SessionTests = require('.\/session-tests');\n+        TESTS.UserTests = require('.\/user-tests');\n+        TESTS.SessionTests = require('.\/session-tests');\n+    }\n }\n \n \/\/ If on node, run the async tests","old_code":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\/\/\n\/\/ Copyright 2016 Realm Inc.\n\/\/\n\/\/ Licensed under the Apache License, Version 2.0 (the \"License\");\n\/\/ you may not use this file except in compliance with the License.\n\/\/ You may obtain a copy of the License at\n\/\/\n\/\/ http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing, software\n\/\/ distributed under the License is distributed on an \"AS IS\" BASIS,\n\/\/ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\/\/ See the License for the specific language governing permissions and\n\/\/ limitations under the License.\n\/\/\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n'use strict';\n\nconst Realm = require('realm');\n\nif (typeof Realm.App !== 'undefined' && Realm.App !== null) {\n    global.WARNING = \"global is not available in React Native. Use it only in tests\";\n    global.enableSyncTests = process.env.REALM_DISABLE_SYNC_TESTS ? false : true;\n}\n\nconst isNodeProcess = typeof process === 'object' && process + '' === '[object process]';\nconst isElectronProcess = typeof process === 'object' && process.versions && process.versions.electron;\nconst require_method = require;\nfunction node_require(module) { return require_method(module); }\n\nif (isNodeProcess && process.platform === 'win32') {\n    global.enableSyncTests = false;\n}\n\nvar TESTS = {\n    ListTests: require('.\/list-tests'),\n    LinkingObjectsTests: require('.\/linkingobjects-tests'),\n    ObjectTests: require('.\/object-tests'),\n    RealmTests: require('.\/realm-tests'),\n    ResultsTests: require('.\/results-tests'),\n    QueryTests: require('.\/query-tests'),\n    MigrationTests: require('.\/migration-tests'),\n    EncryptionTests: require('.\/encryption-tests'),\n    ObjectIDTests: require('.\/object-id-tests'),\n    AliasTests: require('.\/alias-tests'),\n    BsonTests: require('.\/bson-tests'),\n    \/\/ Garbagecollectiontests: require('.\/garbage-collection'),\n};\n\n\/\/ If sync is enabled, run the sync tests\nif (global.enableSyncTests) {\n    TESTS.AppTests = require('.\/app-tests');\n    \/\/ TESTS.OpenBehaviorTests = require('.\/open-behavior-tests'); \/\/ FIXME: figure out how to enable them\n    TESTS.UserTests = require('.\/user-tests');\n    TESTS.SessionTests = require('.\/session-tests');\n}\n\n\/\/ If on node, run the async tests\nif (isNodeProcess && process.platform !== 'win32') {\n    TESTS.AsyncTests = node_require('.\/async-tests');\n}\n\nif (global.enableSyncTests) {\n    \/\/ Ensure that the sync manager is initialized as initializing it\n    \/\/ after calling clearTestState() doesn't work\n    \/\/ Realm.Sync.User.all;\n}\n\nvar SPECIAL_METHODS = {\n    beforeEach: true,\n    afterEach: true,\n};\n\nexports.getTestNames = function() {\n    var testNames = {};\n\n    for (var suiteName in TESTS) {\n        var testSuite = TESTS[suiteName];\n\n        testNames[suiteName] = Object.keys(testSuite).filter(function(testName) {\n            return !(testName in SPECIAL_METHODS) && typeof testSuite[testName] == 'function';\n        });\n    }\n\n    return testNames;\n};\n\nexports.registerTests = function(tests) {\n    for (var suiteName in tests) {\n        TESTS[suiteName] = tests[suiteName];\n    }\n};\n\nexports.prepare = function (done) {\n    done();\n};\n\nexports.runTest = function(suiteName, testName) {\n    const testSuite = TESTS[suiteName];\n    const testMethod = testSuite && testSuite[testName];\n\n    if (testMethod) {\n        Realm.clearTestState();\n        console.warn(\"Starting test \" + testName);\n        var result = testMethod.call(testSuite);\n\n        \/\/make sure v8 GC can collect garbage after each test and does not fail\n        if (isNodeProcess || isElectronProcess) {\n            if (result instanceof Promise) {\n                result.finally(() => global.gc());\n                return result;\n            }\n            else {\n                global.gc();\n            }\n        }\n\n        return result;\n    }\n\n    if (!testSuite || !(testName in SPECIAL_METHODS)) {\n        throw new Error(`Missing test: ${suiteName}.${testName}`);\n    }\n}\n","lang_cluster":"Javascript","length":126,"code_uid":"c61b2945f840468d990040db667f709d"}
{"diff_hunk":"@@ -116,7 +116,7 @@ define(['layoutManager', 'browser', 'actionsheet', 'css!.\/emby-select', 'registe\n             inputId++;\n         }\n \n-        if (!browser.firefox) {\n+        if (browser) {\n             this.classList.add('emby-select-withcolor');\n \n \t\t\tif (layoutManager.tv) {","old_code":"define(['layoutManager', 'browser', 'actionsheet', 'css!.\/emby-select', 'registerElement'], function (layoutManager, browser, actionsheet) {\n    'use strict';\n\n    var EmbySelectPrototype = Object.create(HTMLSelectElement.prototype);\n\n    function enableNativeMenu() {\n\n        if (browser.edgeUwp || browser.xboxOne) {\n            return true;\n        }\n\n        \/\/ Doesn't seem to work at all\n        if (browser.tizen || browser.orsay || browser.web0s) {\n            return false;\n        }\n\n        \/\/ Take advantage of the native input methods\n        if (browser.tv) {\n            return true;\n        }\n\n        if (layoutManager.tv) {\n            return false;\n        }\n\n        return true;\n    }\n\n    function triggerChange(select) {\n        var evt = document.createEvent(\"HTMLEvents\");\n        evt.initEvent(\"change\", false, true);\n        select.dispatchEvent(evt);\n    }\n\n    function setValue(select, value) {\n\n        select.value = value;\n    }\n\n    function showActionSheet(select) {\n\n        var labelElem = getLabel(select);\n        var title = labelElem ? (labelElem.textContent || labelElem.innerText) : null;\n\n        actionsheet.show({\n            items: select.options,\n            positionTo: select,\n            title: title\n\n        }).then(function (value) {\n            setValue(select, value);\n            triggerChange(select);\n        });\n    }\n\n    function getLabel(select) {\n        var elem = select.previousSibling;\n        while (elem && elem.tagName !== 'LABEL') {\n            elem = elem.previousSibling;\n        }\n        return elem;\n    }\n\n    function onFocus(e) {\n        var label = getLabel(this);\n        if (label) {\n            label.classList.add('selectLabelFocused');\n        }\n    }\n\n    function onBlur(e) {\n        var label = getLabel(this);\n        if (label) {\n            label.classList.remove('selectLabelFocused');\n        }\n    }\n\n    function onMouseDown(e) {\n\n        \/\/ e.button=0 for primary (left) mouse button click\n        if (!e.button && !enableNativeMenu()) {\n            e.preventDefault();\n            showActionSheet(this);\n        }\n    }\n\n    function onKeyDown(e) {\n\n        switch (e.keyCode) {\n\n            case 13:\n                if (!enableNativeMenu()) {\n                    e.preventDefault();\n                    showActionSheet(this);\n                }\n                return;\n            case 37:\n            case 38:\n            case 39:\n            case 40:\n                if (layoutManager.tv) {\n                    e.preventDefault();\n                }\n                return;\n            default:\n                break;\n        }\n    }\n\n    var inputId = 0;\n\n    EmbySelectPrototype.createdCallback = function () {\n\n        if (!this.id) {\n            this.id = 'embyselect' + inputId;\n            inputId++;\n        }\n\n        if (!browser.firefox) {\n            this.classList.add('emby-select-withcolor');\n\n\t\t\tif (layoutManager.tv) {\n                this.classList.add('emby-select-tv-withcolor');\n\t\t\t}\n        }\n\n        if (layoutManager.tv) {\n            this.classList.add('emby-select-focusscale');\n        }\n\n        this.addEventListener('mousedown', onMouseDown);\n        this.addEventListener('keydown', onKeyDown);\n\n        this.addEventListener('focus', onFocus);\n        this.addEventListener('blur', onBlur);\n    };\n\n    EmbySelectPrototype.attachedCallback = function () {\n\n        if (this.classList.contains('emby-select')) {\n            return;\n        }\n\n        this.classList.add('emby-select');\n\n        var label = this.ownerDocument.createElement('label');\n        label.innerHTML = this.getAttribute('label') || '';\n        label.classList.add('selectLabel');\n        label.htmlFor = this.id;\n        this.parentNode.insertBefore(label, this);\n\n        if (this.classList.contains('emby-select-withcolor')) {\n            this.parentNode.insertAdjacentHTML('beforeend', '<div class=\"selectArrowContainer\"><div style=\"visibility:hidden;\">0<\/div><i class=\"selectArrow md-icon\">&#xE313;<\/i><\/div>');\n        }\n    };\n\n    EmbySelectPrototype.setLabel = function (text) {\n\n        var label = this.parentNode.querySelector('label');\n\n        label.innerHTML = text;\n    };\n\n    document.registerElement('emby-select', {\n        prototype: EmbySelectPrototype,\n        extends: 'select'\n    });\n});","lang_cluster":"Javascript","length":168,"code_uid":"e677fe30734449708fba4cf560515b71"}
{"diff_hunk":"@@ -3,17 +3,16 @@ import { Adapter } from 'ember-pouch';\n import PouchAdapterUtils from \"hospitalrun\/mixins\/pouch-adapter-utils\";\n \n export default Adapter.extend(PouchAdapterUtils, {\n-    pouchDBService: Ember.inject.service('pouchdb'),\n-    \n-    mainDB: Ember.computed.alias('pouchDBService.mainDB'),\n-    db:  Ember.computed.alias('pouchDBService.mainDB'),\n-    \n+    database: Ember.inject.service(),\n+    mainDB: Ember.computed.alias('database.mainDB'),\n+    db:  Ember.computed.alias('database.mainDB'),\n+\n     _specialQueries: [\n         'containsValue',\n         'mapReduce',\n         'searchIndex'\n     ],\n-  \n+\n     _executeContainsSearch: function(store, type, query) {\n          return new Ember.RSVP.Promise(function(resolve, reject){\n             var searchUrl = '\/search\/hrdb\/'+type.typeKey+'\/_search';","old_code":"import Ember from \"ember\";\nimport { Adapter } from 'ember-pouch';\nimport PouchAdapterUtils from \"hospitalrun\/mixins\/pouch-adapter-utils\";\n\nexport default Adapter.extend(PouchAdapterUtils, {\n    pouchDBService: Ember.inject.service('pouchdb'),\n    \n    mainDB: Ember.computed.alias('pouchDBService.mainDB'),\n    db:  Ember.computed.alias('pouchDBService.mainDB'),\n    \n    _specialQueries: [\n        'containsValue',\n        'mapReduce',\n        'searchIndex'\n    ],\n  \n    _executeContainsSearch: function(store, type, query) {\n         return new Ember.RSVP.Promise(function(resolve, reject){\n            var searchUrl = '\/search\/hrdb\/'+type.typeKey+'\/_search';\n            if (query.containsValue && query.containsValue.value) {\n                var queryString = '';\n                query.containsValue.keys.forEach(function(key) {\n                    if (!Ember.isEmpty(queryString)) {\n                        queryString += ' OR ';\n                    }\n                    queryString += key+':'+query.containsValue.value;\n                });\n                Ember.$.ajax(searchUrl, {\n                    dataType: 'json',\n                    data: {\n                        q:queryString\n                    },\n                    success: function (results) {\n                        if (results && results.hits && results.hits.hits) {\n                            var resultDocs = Ember.A(results.hits.hits).map(function(hit) {\n                                var mappedResult = hit._source;\n                                mappedResult.id = mappedResult._id;\n                                return mappedResult;\n                            });\n                            var response = {\n                                rows: resultDocs\n                            };\n                            this._handleQueryResponse(response, store, type).then(resolve,reject);\n                        } else if (results.rows) {\n                            this._handleQueryResponse(results, store, type).then(resolve,reject);\n                        } else {\n                            reject('Search results are not valid');\n                        }\n                    }.bind(this)\n                });\n            } else {\n                reject('invalid query');\n            }\n        }.bind(this));\n    },\n    \n    _handleQueryResponse: function(response, store, type) {\n        var pouchDBService = this.get('pouchDBService');\n        return new Ember.RSVP.Promise(function(resolve, reject){\n            if (response.rows.length > 0) {\n                var ids = response.rows.map(function(row) {\n                    return pouchDBService.getEmberId(row.id);\n                });\n                this.findRecord(store, type, ids).then(function(findResponse) {\n                    var primaryRecordName = type.modelName.camelize().pluralize(),\n                        sortedValues = [];\n                    \/\/Sort response in order of ids\n                    ids.forEach(function(id) {\n                        var resolvedRecord = findResponse[primaryRecordName].findBy('id',id);\n                        sortedValues.push(resolvedRecord);\n                    });\n                    findResponse[primaryRecordName] = sortedValues;\n                    resolve(findResponse);\n                }.bind(this), reject);\n            } else {\n                var emptyResponse = {};\n                emptyResponse[type.modelName] = [];\n                resolve(emptyResponse);\n            }\n        }.bind(this));\n    },\n    \n    \/**\n     * Look for nulls and maxvalues in start key because those keys can't be handled by the sort\/list function\n     *\/\n    _doesStartKeyContainSpecialCharacters: function(startkey) {\n        var haveSpecialCharacters = false,\n            maxValue = this.get('maxValue');\n        if (!Ember.isEmpty(startkey) && Ember.isArray(startkey)) {\n            startkey.forEach(function(keyvalue) {\n                if (keyvalue === null || keyvalue === maxValue) {\n                    haveSpecialCharacters = true;\n                }\n            });\n        }\n        return haveSpecialCharacters;\n    },\n    \n    findQuery: function(store, type, query, options) {\n        var specialQuery = false;\n        for (var i=0;i< this._specialQueries.length; i++) {\n            if (Ember.get(query,this._specialQueries[i])) {\n                specialQuery = true;\n                break;\n            }\n        }\n        if (!specialQuery) {\n            if (query.options) {\n                this._init(store, type);\n                var recordTypeName = this.getRecordTypeName(type);\n                return this.get('db').rel.find(recordTypeName, query.options);\n            } else {\n                return this._super(store, type, query, options);\n            }\n        } else {\n            var mapReduce = null,                \n                queryParams = {};\n            if (query.searchIndex) {\n                queryParams = query.searchIndex;\n            }\n            if (query.options) {\n                queryParams = Ember.copy(query.options);\n                if (query.sortKey || query.filterBy) {\n                    if (query.sortDesc) {\n                        queryParams.sortDesc = query.sortDesc;\n                    }\n                    if (query.sortKey) {\n                        queryParams.sortKey = query.sortKey;\n                    }\n                    if (!this._doesStartKeyContainSpecialCharacters(queryParams.startkey)) {\n                        queryParams.sortLimit = queryParams.limit;\n                        delete queryParams.limit;\n                        queryParams.sortStartKey = JSON.stringify(queryParams.startkey);\n                        delete queryParams.startkey;\n                    } else if (queryParams.startkey) {\n                        queryParams.startkey = JSON.stringify(queryParams.startkey);\n                    }\n                    if (query.filterBy) {\n                        queryParams.filterBy = JSON.stringify(query.filterBy);\n                    }\n                    if (queryParams.endkey) {\n                        queryParams.endkey = JSON.stringify(queryParams.endkey);\n                    }\n                    query.useList = true;\n                }\n            }\n            queryParams.reduce  = false;\n            queryParams.include_docs = false;\n            \n            if (query.mapReduce) {\n                mapReduce = query.mapReduce;\n            } else if (query.containsValue) {\n                return this._executeContainsSearch(store, type, query);\n            }\n            return new Ember.RSVP.Promise(function(resolve, reject){\n                var db = this.get('db');\n                    try {\n                        if (mapReduce) {\n                            if (query.useList) {\n                                queryParams.include_docs = true;\n                                var listParams = {\n                                    query: queryParams\n                                };\n                                db.list(mapReduce+'\/sort\/'+mapReduce, listParams, function(err, response) {\n                                    if (err) {\n                                        this._pouchError(reject)(err);\n                                    } else {\n                                        this._handleQueryResponse(response.json, store, type).then(resolve, reject);\n                                    }\n                                }.bind(this));\n                            } else {\n                                db.query(mapReduce, queryParams, function(err, response) {\n                                    if (err) {\n                                        this._pouchError(reject)(err);\n                                    } else {\n                                        this._handleQueryResponse(response, store, type).then(resolve, reject);\n                                    }\n                                }.bind(this));\n                            }\n                        } else {\n                            db.allDocs(queryParams, function(err, response) {\n                                if (err) {\n                                    this._pouchError(reject)(err);\n                                } else {\n                                    this._handleQueryResponse(response, store, type).then(resolve, reject);\n                                }\n                            }.bind(this));\n                        }\n                    } catch (err){\n                        this._pouchError(reject)(err);\n                    }                \n            }.bind(this), \"findQuery in application-pouchdb-adapter\");\n        }\n    }\n});","lang_cluster":"Javascript","length":195,"code_uid":"6db7336ad6e84691a04d5ea14b523e5c"}
{"diff_hunk":"@@ -9,7 +9,7 @@ const build = (buildConfig = config.defaultBuildConfig, options) => {\n     util.updateBranding()\n   }\n \n-  util.buildMuon()\n+  util.buildMuon('brave')\n }\n \n module.exports = build","old_code":"const config = require('..\/lib\/config')\nconst util = require('..\/lib\/util')\n\nconst build = (buildConfig = config.defaultBuildConfig, options) => {\n  config.buildConfig = buildConfig\n  config.update(options)\n\n  if (!options.no_branding_update) {\n    util.updateBranding()\n  }\n\n  util.buildMuon()\n}\n\nmodule.exports = build\n","lang_cluster":"Javascript","length":15,"code_uid":"25a036a7ea4f4d9fad818dcbe6c889f1"}
{"diff_hunk":"@@ -4,6 +4,7 @@ const config = require('.\/config')\n const fs = require('fs-extra')\n \n const runGClient = (args, options = {}) => {\n+  if (options.verbose) args.push('--verbose')\n   options.cwd = options.cwd || config.rootDir\n   options = mergeWithDefault(options)\n   options.env.GCLIENT_FILE = config.gClientFile","old_code":"const path = require('path')\nconst spawnSync = require('child_process').spawnSync\nconst config = require('.\/config')\nconst fs = require('fs-extra')\n\nconst runGClient = (args, options = {}) => {\n  options.cwd = options.cwd || config.rootDir\n  options = mergeWithDefault(options)\n  options.env.GCLIENT_FILE = config.gClientFile\n  util.run('gclient', args, options)\n}\n\nconst mergeWithDefault = (options) => {\n  return Object.assign({}, config.defaultOptions, options)\n}\n\nconst util = {\n  run: (cmd, args = [], options = {}) => {\n    console.log(cmd, args.join(' '))\n    const continueOnFail = options.continueOnFail\n    delete options.continueOnFail\n\n    const prog = spawnSync(cmd, args, options)\n    if (prog.status !== 0) {\n      if (!continueOnFail) {\n        console.log(prog.stdout && prog.stdout.toString())\n        console.error(prog.stderr && prog.stderr.toString())\n        process.exit(1)\n      }\n    }\n    return prog\n  },\n\n  buildGClientConfig: () => {\n    function replacer(key, value) {\n      return value;\n    }\n\n    let solutions = config.projectNames.filter((projectName) => config.projects[projectName].ref).map((projectName) => {\n      let project = config.projects[projectName]\n      return {\n        managed: \"%False%\",\n        name: project.gclientName,\n        url: project.url,\n        custom_deps: project.custom_deps\n      }\n    })\n\n    const out = 'solutions = ' + JSON.stringify(solutions, replacer, 2)\n      .replace(\/\"%None%\"\/g, \"None\")\n      .replace(\/\"%False%\"\/g, \"False\")\n    fs.writeFileSync(config.defaultGClientFile, out)\n  },\n\n  updateBranding: () => {\n    console.log('update branding...')\n    const chromeComponentsDir = path.join(config.srcDir, 'components')\n    const chromeAppDir = path.join(config.srcDir, 'chrome', 'app')\n    const braveAppDir = path.join(config.projects['brave-core'].dir, 'app')\n    fs.copySync(path.join(braveAppDir, 'brave_strings.grd'), path.join(chromeAppDir, 'brave_strings.grd'))\n    fs.copySync(path.join(braveAppDir, 'settings_brave_strings.grdp'), path.join(chromeAppDir, 'settings_brave_strings.grdp'))\n    fs.copySync(path.join(braveAppDir, 'components_brave_strings.grd'), path.join(config.srcDir, 'components', 'components_brave_strings.grd'))\n    fs.copySync(path.join(braveAppDir, 'theme', 'brave'), path.join(chromeAppDir, 'theme', 'brave'))\n    fs.copySync(path.join(braveAppDir, 'theme', 'default_100_percent', 'brave'), path.join(chromeAppDir, 'theme', 'default_100_percent', 'brave'))\n    fs.copySync(path.join(braveAppDir, 'theme', 'default_200_percent', 'brave'), path.join(chromeAppDir, 'theme', 'default_200_percent', 'brave'))\n    fs.copySync(path.join(braveAppDir, 'vector_icons', 'brave'), path.join(chromeAppDir, 'vector_icons', 'brave'))\n    \/\/ Copy XTB files for app\/brave_strings.grd => chromium_strings.grd\n    fs.copySync(path.join(braveAppDir, 'resources'), path.join(chromeAppDir, 'resources'))\n    \/\/ Copy XTB files for brave\/app\/components_brave_strings.grd => components\/components_chromium_strings.grd\n    fs.copySync(path.join(braveAppDir, 'strings'), path.join(chromeComponentsDir, 'strings'))\n  },\n\n  buildMuon: (options = config.defaultOptions) => {\n    console.log('building brave...')\n\n    const args = util.buildArgsToString(config.buildArgs())\n    util.run('gn', ['gen', config.outputDir, '--args=\"' + args + '\"'], options)\n    util.run('ninja', ['-C', config.outputDir, 'brave'], options)\n  },\n\n  submoduleSync: (options = { cwd: config.rootDir }) => {\n    options = mergeWithDefault(options)\n    util.run('git', ['submodule', 'sync'], options)\n    util.run('git', ['submodule', 'update', '--init', '--recursive'], options)\n    util.run('git', ['clean', '-fxd'], Object.assign(options, {cwd: config.depotToolsDir}))\n    util.run('git', ['reset', '--hard', 'HEAD'], Object.assign(options, {cwd: config.depotToolsDir}))\n  },\n\n  gclientSync: (options = {}) => {\n    runGClient(['sync', '--force', '--nohooks', '--with_branch_heads'], options)\n  },\n\n  gclientRunhooks: (options = {}) => {\n    runGClient(['runhooks'], options)\n  },\n\n  fetch: (options = {}) => {\n    options = mergeWithDefault(options)\n    util.run('git', ['fetch', '--all', '--tags'], options)\n  },\n\n  setVersion: (version, options = {}) => {\n    util.run('git', ['clean', '-f'], options)\n    util.run('git', ['reset', '--hard', version], options)\n  },\n\n  setDepVersion: (dir, version) => {\n    const options = { cwd: dir }\n    util.fetch(options)\n    util.setVersion(version, options)\n  },\n\n  buildArgsToString: (buildArgs) => {\n    let args = ''\n    for (let arg in buildArgs) {\n      let val = buildArgs[arg]\n      if (typeof val === 'string') {\n        val = '\"' + val + '\"'\n      } else {\n        val = JSON.stringify(val)\n      }\n      args += arg + '=' + val + ' '\n    }\n    return args.replace(\/\"\/g,'\\\\\"')\n  }\n}\n\nmodule.exports = util\n","lang_cluster":"Javascript","length":128,"code_uid":"8c776d26e891433c8268ac46c71a706e"}
{"diff_hunk":"@@ -1,4 +1,4 @@\n-define(['browser', 'css!.\/emby-collapse', 'registerElement', 'emby-button'], function (browser) {\n+define(['browser', 'css!elements\/emby-collapse\/emby-collapse', 'registerElement', 'emby-button'], function (browser) {\n     'use strict';\n \n     var EmbyButtonPrototype = Object.create(HTMLDivElement.prototype);","old_code":"define(['browser', 'css!.\/emby-collapse', 'registerElement', 'emby-button'], function (browser) {\n    'use strict';\n\n    var EmbyButtonPrototype = Object.create(HTMLDivElement.prototype);\n\n    function slideDownToShow(button, elem) {\n\n        elem.classList.remove('hide');\n        elem.classList.add('expanded');\n        elem.style.height = 'auto';\n        var height = elem.offsetHeight + 'px';\n        elem.style.height = '0';\n\n        \/\/ trigger reflow\n        var newHeight = elem.offsetHeight;\n        elem.style.height = height;\n\n        setTimeout(function () {\n            if (elem.classList.contains('expanded')) {\n                elem.classList.remove('hide');\n            } else {\n                elem.classList.add('hide');\n            }\n            elem.style.height = 'auto';\n        }, 300);\n\n        var icon = button.querySelector('i');\n        \/\/icon.innerHTML = 'expand_less';\n        icon.classList.add('emby-collapse-expandIconExpanded');\n    }\n\n    function slideUpToHide(button, elem) {\n\n        elem.style.height = elem.offsetHeight + 'px';\n        \/\/ trigger reflow\n        var newHeight = elem.offsetHeight;\n\n        elem.classList.remove('expanded');\n        elem.style.height = '0';\n\n        setTimeout(function () {\n            if (elem.classList.contains('expanded')) {\n                elem.classList.remove('hide');\n            } else {\n                elem.classList.add('hide');\n            }\n        }, 300);\n\n        var icon = button.querySelector('i');\n        \/\/icon.innerHTML = 'expand_more';\n        icon.classList.remove('emby-collapse-expandIconExpanded');\n    }\n\n    function onButtonClick(e) {\n\n        var button = this;\n        var collapseContent = button.parentNode.querySelector('.collapseContent');\n\n        if (collapseContent.expanded) {\n            collapseContent.expanded = false;\n            slideUpToHide(button, collapseContent);\n        } else {\n            collapseContent.expanded = true;\n            slideDownToShow(button, collapseContent);\n        }\n    }\n\n    EmbyButtonPrototype.attachedCallback = function () {\n\n        if (this.classList.contains('emby-collapse')) {\n            return;\n        }\n\n        this.classList.add('emby-collapse');\n\n        var collapseContent = this.querySelector('.collapseContent');\n        if (collapseContent) {\n            collapseContent.classList.add('hide');\n        }\n\n        var title = this.getAttribute('title');\n\n        var html = '<button is=\"emby-button\" type=\"button\" on-click=\"toggleExpand\" id=\"expandButton\" class=\"emby-collapsible-button iconRight\"><h3 class=\"emby-collapsible-title\" title=\"' + title + '\">' + title + '<\/h3><i class=\"material-icons emby-collapse-expandIcon expand_more\"><\/i><\/button>';\n\n        this.insertAdjacentHTML('afterbegin', html);\n\n        var button = this.querySelector('.emby-collapsible-button');\n\n        button.addEventListener('click', onButtonClick);\n\n        if (this.getAttribute('data-expanded') === 'true') {\n            onButtonClick.call(button);\n        }\n    };\n\n    document.registerElement('emby-collapse', {\n        prototype: EmbyButtonPrototype,\n        extends: 'div'\n    });\n});\n","lang_cluster":"Javascript","length":100,"code_uid":"86fab455809443178dfd70073619609b"}
{"diff_hunk":"@@ -1,18 +1,15 @@\n+\"use strict\";\n+const express = require(\"express\");\n+const mongoose = require(\"..\/..\/..\/lib\");\n \n-'use strict';\n-const express = require('express');\n-const mongoose = require('..\/..\/..\/lib');\n-\n-const uri = 'mongodb:\/\/localhost\/mongoose-shared-connection';\n+const uri = \"mongodb:\/\/localhost\/mongoose-shared-connection\";\n global.db = mongoose.createConnection(uri);\n \n-const routes = require('.\/routes');\n+const routes = require(\".\/routes\");\n \n const app = express();\n-app.get('\/', routes.home);\n-app.get('\/insert', routes.insert);\n-app.get('\/name', routes.modelName);\n+app.get(\"\/\", routes.home);\n+app.get(\"\/insert\", routes.insert);\n+app.get(\"\/name\", routes.modelName);\n \n-app.listen(8000, function() {\n-  console.log('listening on http:\/\/localhost:8000');\n-});\n+app.listen(8000, console.log(\"listening on http:\/\/localhost:8000\"));","old_code":"\n'use strict';\nconst express = require('express');\nconst mongoose = require('..\/..\/..\/lib');\n\nconst uri = 'mongodb:\/\/localhost\/mongoose-shared-connection';\nglobal.db = mongoose.createConnection(uri);\n\nconst routes = require('.\/routes');\n\nconst app = express();\napp.get('\/', routes.home);\napp.get('\/insert', routes.insert);\napp.get('\/name', routes.modelName);\n\napp.listen(8000, function() {\n  console.log('listening on http:\/\/localhost:8000');\n});\n","lang_cluster":"Javascript","length":18,"code_uid":"8529ce554c1c464a8e48312374ac6344"}
{"diff_hunk":"@@ -126,14 +126,18 @@ module.exports = function(config, auth, storage) {\n   })\n \n   \/\/ Search\n-  app.get('\/-\/search\/:anything', function(req, res, next) {\n+  app.get('\/-\/search\/:anything', can('access'), function(req, res, next) {\n     var results = Search.query(req.params.anything)\n     var packages = []\n \n     var getData = function(i) {\n       storage.get_package(results[i].ref, function(err, entry) {\n         if (!err && entry) {\n-          packages.push(entry.versions[entry['dist-tags'].latest])\n+          auth.allow_access(entry.name, req.remote_user, function(err, allowed) { \/\/ TODO: This may cause performance issue?\n+            if (err || !allowed) return\n+\n+            packages.push(entry.versions[entry['dist-tags'].latest])\n+          })\n         }\n \n         if (i >= results.length - 1) {","old_code":"var async         = require('async')\nvar bodyParser    = require('body-parser')\nvar Cookies       = require('cookies')\nvar express       = require('express')\nvar fs            = require('fs')\nvar Handlebars    = require('handlebars')\nvar renderReadme  = require('render-readme')\nvar Search        = require('.\/search')\nvar Middleware    = require('.\/middleware')\nvar match         = Middleware.match\nvar validate_name = Middleware.validate_name\nvar validate_pkg  = Middleware.validate_package\n\nmodule.exports = function(config, auth, storage) {\n  var app = express.Router()\n  var can = Middleware.allow(auth)\n\n  \/\/ validate all of these params as a package name\n  \/\/ this might be too harsh, so ask if it causes trouble\n  app.param('package',  validate_pkg)\n  app.param('filename', validate_name)\n  app.param('version',  validate_name)\n  app.param('anything', match(\/.*\/))\n\n  app.use(Cookies.express())\n  app.use(bodyParser.urlencoded({ extended: false }))\n  app.use(auth.cookie_middleware())\n  app.use(function(req, res, next) {\n    \/\/ disable loading in frames (clickjacking, etc.)\n    res.header('X-Frame-Options', 'deny')\n    next()\n  })\n\n  Search.configureStorage(storage)\n\n  Handlebars.registerPartial('entry', fs.readFileSync(require.resolve('.\/GUI\/entry.hbs'), 'utf8'))\n\n  if(config.web && config.web.template) {\n    var template = Handlebars.compile(fs.readFileSync(config.web.template, 'utf8'));\n  }\n  else {\n    var template = Handlebars.compile(fs.readFileSync(require.resolve('.\/GUI\/index.hbs'), 'utf8'))\n  }\n  app.get('\/', function(req, res, next) {\n    var base = config.url_prefix\n             ? config.url_prefix.replace(\/\\\/$\/, '')\n             : req.protocol + ':\/\/' + req.get('host')\n    res.setHeader('Content-Type', 'text\/html')\n\n    storage.get_local(function(err, packages) {\n      if (err) throw err \/\/ that function shouldn't produce any\n      async.filterSeries(packages, function(package, cb) {\n        auth.allow_access(package.name, req.remote_user, function(err, allowed) {\n          setImmediate(function () {\n            if (err) {\n              cb(null, false);\n            } else {\n              cb(err, allowed)\n            }\n          })\n        })\n      }, function(err, packages) {\n        if (err) throw err\n        packages.sort(function(p1, p2) {\n          if (p1.name < p2.name) {\n            return -1;\n          }\n          else {\n            return 1;\n          }\n        });\n\n        next(template({\n          name:       config.web && config.web.title ? config.web.title : 'Verdaccio',\n          tagline:    config.web && config.web.tagline ? config.web.tagline : '',\n          packages:   packages,\n          baseUrl:    base,\n          username:   req.remote_user.name,\n        }))\n      })\n    })\n  })\n\n  \/\/ Static\n  app.get('\/-\/static\/:filename', function(req, res, next) {\n    var file = __dirname + '\/static\/' + req.params.filename\n    res.sendFile(file, function(err) {\n      if (!err) return\n      if (err.status === 404) {\n        next()\n      } else {\n        next(err)\n      }\n    })\n  })\n\n  app.get('\/-\/logo', function(req, res, next) {\n    res.sendFile( config.web && config.web.logo\n                ? config.web.logo\n                : __dirname + '\/static\/logo-sm.png' )\n  })\n\n  app.post('\/-\/login', function(req, res, next) {\n    auth.authenticate(req.body.user, req.body.pass, function(err, user) {\n      if (!err) {\n        req.remote_user = user\n        \/\/res.cookies.set('token', auth.issue_token(req.remote_user))\n\n        var str = req.body.user + ':' + req.body.pass\n        res.cookies.set('token', auth.aes_encrypt(str).toString('base64'))\n      }\n\n      var base = config.url_prefix\n               ? config.url_prefix.replace(\/\\\/$\/, '')\n               : req.protocol + ':\/\/' + req.get('host')\n      res.redirect(base)\n    })\n  })\n\n  app.post('\/-\/logout', function(req, res, next) {\n    var base = config.url_prefix\n             ? config.url_prefix.replace(\/\\\/$\/, '')\n             : req.protocol + ':\/\/' + req.get('host')\n    res.cookies.set('token', '')\n    res.redirect(base)\n  })\n\n  \/\/ Search\n  app.get('\/-\/search\/:anything', function(req, res, next) {\n    var results = Search.query(req.params.anything)\n    var packages = []\n\n    var getData = function(i) {\n      storage.get_package(results[i].ref, function(err, entry) {\n        if (!err && entry) {\n          packages.push(entry.versions[entry['dist-tags'].latest])\n        }\n\n        if (i >= results.length - 1) {\n          next(packages)\n        } else {\n          getData(i + 1)\n        }\n      })\n    }\n\n    if (results.length) {\n      getData(0)\n    } else {\n      next([])\n    }\n  })\n\n  app.get('\/-\/readme(\/@:scope?)?\/:package\/:version?', can('access'), function(req, res, next) {\n    var packageName = req.params.package;\n    if (req.params.scope) packageName = \"@\"+ req.params.scope + \"\/\" + packageName;\n    storage.get_package(packageName, {req: req}, function(err, info) {\n      if (err) return next(err)\n      next( renderReadme(info.readme || 'ERROR: No README data found!') )\n    })\n  })\n  return app\n}\n","lang_cluster":"Javascript","length":163,"code_uid":"d9694270e2934034a40579dcc95addfc"}
{"diff_hunk":"@@ -30,17 +30,14 @@ define(['layoutManager', 'cardBuilder', 'apphost', 'imageLoader', 'loading', 'sc\n             recordingItems.classList.add('vertical-wrap');\n         }\n \n-        var supportsImageAnalysis = appHost.supports('imageanalysis');\n-        var cardLayout = appHost.preferVisualCards || supportsImageAnalysis;\n-        cardLayout = false;\n         recordingItems.innerHTML = cardBuilder.getCardsHtml(Object.assign({\n             items: recordings,\n             shape: enableScrollX() ? 'autooverflow' : 'auto',\n             showTitle: true,\n             showParentTitle: true,\n             coverImage: true,\n-            cardLayout: cardLayout,\n-            centerText: !cardLayout,\n+            cardLayout: appHost.preferVisualCards,\n+            centerText: !appHost.preferVisualCards,\n             allowBottomPadding: !enableScrollX(),\n             preferThumb: 'auto'\n         }, cardOptions || {}));","old_code":"define(['layoutManager', 'cardBuilder', 'apphost', 'imageLoader', 'loading', 'scripts\/livetvcomponents', 'emby-button', 'emby-itemscontainer'], function (layoutManager, cardBuilder, appHost, imageLoader, loading) {\n    'use strict';\n\n    function enableScrollX() {\n        return !layoutManager.desktop;\n    }\n\n    function renderRecordings(elem, recordings, cardOptions) {\n        if (recordings.length) {\n            elem.classList.remove('hide');\n        } else {\n            elem.classList.add('hide');\n        }\n\n        var recordingItems = elem.querySelector('.recordingItems');\n\n        if (enableScrollX()) {\n            recordingItems.classList.add('scrollX');\n\n            if (layoutManager.tv) {\n                recordingItems.classList.add('smoothScrollX');\n            }\n\n            recordingItems.classList.add('hiddenScrollX');\n            recordingItems.classList.remove('vertical-wrap');\n        } else {\n            recordingItems.classList.remove('scrollX');\n            recordingItems.classList.remove('smoothScrollX');\n            recordingItems.classList.remove('hiddenScrollX');\n            recordingItems.classList.add('vertical-wrap');\n        }\n\n        var supportsImageAnalysis = appHost.supports('imageanalysis');\n        var cardLayout = appHost.preferVisualCards || supportsImageAnalysis;\n        cardLayout = false;\n        recordingItems.innerHTML = cardBuilder.getCardsHtml(Object.assign({\n            items: recordings,\n            shape: enableScrollX() ? 'autooverflow' : 'auto',\n            showTitle: true,\n            showParentTitle: true,\n            coverImage: true,\n            cardLayout: cardLayout,\n            centerText: !cardLayout,\n            allowBottomPadding: !enableScrollX(),\n            preferThumb: 'auto'\n        }, cardOptions || {}));\n        imageLoader.lazyChildren(recordingItems);\n    }\n\n    function getBackdropShape() {\n        return enableScrollX() ? 'overflowBackdrop' : 'backdrop';\n    }\n\n    function renderActiveRecordings(context, promise) {\n        promise.then(function (result) {\n            renderRecordings(context.querySelector('#activeRecordings'), result.Items, {\n                shape: enableScrollX() ? 'autooverflow' : 'auto',\n                defaultShape: getBackdropShape(),\n                showParentTitle: false,\n                showParentTitleOrTitle: true,\n                showTitle: false,\n                showAirTime: true,\n                showAirEndTime: true,\n                showChannelName: true,\n                coverImage: true,\n                overlayText: false,\n                overlayMoreButton: true\n            });\n        });\n    }\n\n    function renderTimers(context, timers, options) {\n        LiveTvHelpers.getTimersHtml(timers, options).then(function (html) {\n            var elem = context;\n\n            if (html) {\n                elem.classList.remove('hide');\n            } else {\n                elem.classList.add('hide');\n            }\n\n            elem.querySelector('.recordingItems').innerHTML = html;\n            imageLoader.lazyChildren(elem);\n        });\n    }\n\n    function renderUpcomingRecordings(context, promise) {\n        promise.then(function (result) {\n            renderTimers(context.querySelector('#upcomingRecordings'), result.Items);\n            loading.hide();\n        });\n    }\n\n    return function (view, params, tabContent) {\n        var activeRecordingsPromise;\n        var upcomingRecordingsPromise;\n        var self = this;\n        tabContent.querySelector('#upcomingRecordings .recordingItems').addEventListener('timercancelled', function () {\n            self.preRender();\n            self.renderTab();\n        });\n\n        self.preRender = function () {\n            activeRecordingsPromise = ApiClient.getLiveTvRecordings({\n                UserId: Dashboard.getCurrentUserId(),\n                IsInProgress: true,\n                Fields: 'CanDelete,PrimaryImageAspectRatio,BasicSyncInfo',\n                EnableTotalRecordCount: false,\n                EnableImageTypes: 'Primary,Thumb,Backdrop'\n            });\n            upcomingRecordingsPromise = ApiClient.getLiveTvTimers({\n                IsActive: false,\n                IsScheduled: true\n            });\n        };\n\n        self.renderTab = function () {\n            loading.show();\n            renderActiveRecordings(tabContent, activeRecordingsPromise);\n            renderUpcomingRecordings(tabContent, upcomingRecordingsPromise);\n        };\n    };\n});\n","lang_cluster":"Javascript","length":123,"code_uid":"9fe7dbf01e2d44de9899a1cec4ce644b"}
{"diff_hunk":"@@ -43,7 +43,8 @@ module.exports = function(config, auth, storage) {\n     const base = Utils.combineBaseUrl(Utils.getWebProtocol(req), req.get('host'), config.url_prefix);\n     let webPage = template\n       .replace(\/ToReplaceByVerdaccio\/g, base)\n-      .replace(\/ToReplaceByTitle\/g, _.get(config, 'web.title') ? config.web.title : WEB_TITLE);\n+      .replace(\/ToReplaceByTitle\/g, _.get(config, 'web.title') ? config.web.title : WEB_TITLE)\n+      .replace(\/ToReplaceByScope\/g, _.get(config, 'web.scope') ? config.web.scope : '');\n \n     res.setHeader('Content-Type', 'text\/html');\n ","old_code":"import express from 'express';\nimport _ from 'lodash';\nimport fs from 'fs';\nimport Search from '..\/..\/lib\/search';\nimport * as Utils from '..\/..\/lib\/utils';\nimport {WEB_TITLE} from '..\/..\/lib\/constants';\n\nconst {securityIframe} = require('..\/middleware');\n\/* eslint new-cap:off *\/\nconst router = express.Router();\nconst env = require('..\/..\/config\/env');\nconst template = fs.readFileSync(`${env.DIST_PATH}\/index.html`).toString();\nconst spliceURL = require('..\/..\/utils\/string').spliceURL;\n\nmodule.exports = function(config, auth, storage) {\n  Search.configureStorage(storage);\n\n  router.use(auth.webUIJWTmiddleware());\n  router.use(securityIframe);\n\n  \/\/ Static\n  router.get('\/-\/static\/:filename', function(req, res, next) {\n    const file = `${env.APP_ROOT}\/static\/${req.params.filename}`;\n    res.sendFile(file, function(err) {\n      if (!err) {\n        return;\n      }\n      if (err.status === 404) {\n        next();\n      } else {\n        next(err);\n      }\n    });\n  });\n\n  router.get('\/-\/verdaccio\/logo', function(req, res) {\n    const installPath = _.get(config, 'url_prefix', '');\n\n    res.send(_.get(config, 'web.logo') || spliceURL(installPath, '\/-\/static\/logo.png'));\n  });\n\n  router.get('\/', function(req, res) {\n    const base = Utils.combineBaseUrl(Utils.getWebProtocol(req), req.get('host'), config.url_prefix);\n    let webPage = template\n      .replace(\/ToReplaceByVerdaccio\/g, base)\n      .replace(\/ToReplaceByTitle\/g, _.get(config, 'web.title') ? config.web.title : WEB_TITLE);\n\n    res.setHeader('Content-Type', 'text\/html');\n\n    res.send(webPage);\n  });\n\n  return router;\n};\n","lang_cluster":"Javascript","length":54,"code_uid":"34ba99408113441f95acdc9d5a095ad4"}
{"diff_hunk":"@@ -3,11 +3,12 @@\n const Aspect = require('.\/operation').Aspect;\n const OperationBase = require('.\/operation').OperationBase;\n const resolveReadPreference = require('..\/utils').resolveReadPreference;\n+const serverLacksFeature = require('..\/utils').serverLacksFeature;\n const ReadConcern = require('..\/read_concern');\n const WriteConcern = require('..\/write_concern');\n const maxWireVersion = require('..\/core\/utils').maxWireVersion;\n const commandSupportsReadConcern = require('..\/core\/sessions').commandSupportsReadConcern;\n-const MongoError = require('..\/error').MongoError;\n+const MongoError = require('..\/core').MongoError;\n \n const SUPPORTS_WRITE_CONCERN_AND_COLLATION = 5;\n ","old_code":"'use strict';\n\nconst Aspect = require('.\/operation').Aspect;\nconst OperationBase = require('.\/operation').OperationBase;\nconst resolveReadPreference = require('..\/utils').resolveReadPreference;\nconst ReadConcern = require('..\/read_concern');\nconst WriteConcern = require('..\/write_concern');\nconst maxWireVersion = require('..\/core\/utils').maxWireVersion;\nconst commandSupportsReadConcern = require('..\/core\/sessions').commandSupportsReadConcern;\nconst MongoError = require('..\/error').MongoError;\n\nconst SUPPORTS_WRITE_CONCERN_AND_COLLATION = 5;\n\nclass CommandOperationV2 extends OperationBase {\n  constructor(parent, options, operationOptions) {\n    super(options);\n\n    this.ns = parent.s.namespace.withCollection('$cmd');\n    this.readPreference = resolveReadPreference(parent, this.options);\n    this.readConcern = resolveReadConcern(parent, this.options);\n    this.writeConcern = resolveWriteConcern(parent, this.options);\n    this.explain = false;\n\n    if (operationOptions && typeof operationOptions.fullResponse === 'boolean') {\n      this.fullResponse = true;\n    }\n\n    \/\/ TODO: A lot of our code depends on having the read preference in the options. This should\n    \/\/       go away, but also requires massive test rewrites.\n    this.options.readPreference = this.readPreference;\n\n    \/\/ TODO(NODE-2056): make logger another \"inheritable\" property\n    if (parent.s.logger) {\n      this.logger = parent.s.logger;\n    } else if (parent.s.db && parent.s.db.logger) {\n      this.logger = parent.s.db.logger;\n    }\n  }\n\n  executeCommand(server, cmd, callback) {\n    \/\/ TODO: consider making this a non-enumerable property\n    this.server = server;\n\n    const options = this.options;\n    const serverWireVersion = maxWireVersion(server);\n    const inTransaction = this.session && this.session.inTransaction();\n\n    if (this.readConcern && commandSupportsReadConcern(cmd) && !inTransaction) {\n      Object.assign(cmd, { readConcern: this.readConcern });\n    }\n\n    if (options.collation && serverWireVersion < SUPPORTS_WRITE_CONCERN_AND_COLLATION) {\n      callback(\n        new MongoError(\n          `Server ${server.name}, which reports wire version ${serverWireVersion}, does not support collation`\n        )\n      );\n      return;\n    }\n\n    if (serverWireVersion >= SUPPORTS_WRITE_CONCERN_AND_COLLATION) {\n      if (this.writeConcern && this.hasAspect(Aspect.WRITE_OPERATION)) {\n        Object.assign(cmd, { writeConcern: this.writeConcern });\n      }\n\n      if (options.collation && typeof options.collation === 'object') {\n        Object.assign(cmd, { collation: options.collation });\n      }\n    }\n\n    if (typeof options.maxTimeMS === 'number') {\n      cmd.maxTimeMS = options.maxTimeMS;\n    }\n\n    if (typeof options.comment === 'string') {\n      cmd.comment = options.comment;\n    }\n\n    if (this.logger && this.logger.isDebug()) {\n      this.logger.debug(`executing command ${JSON.stringify(cmd)} against ${this.ns}`);\n    }\n\n    server.command(this.ns.toString(), cmd, this.options, (err, result) => {\n      if (err) {\n        callback(err, null);\n        return;\n      }\n\n      if (this.fullResponse) {\n        callback(null, result);\n        return;\n      }\n\n      callback(null, result.result);\n    });\n  }\n}\n\nfunction resolveWriteConcern(parent, options) {\n  return WriteConcern.fromOptions(options) || parent.writeConcern;\n}\n\nfunction resolveReadConcern(parent, options) {\n  return ReadConcern.fromOptions(options) || parent.readConcern;\n}\n\nmodule.exports = CommandOperationV2;\n","lang_cluster":"Javascript","length":107,"code_uid":"347577f674d9421ab7e65bcd0f185027"}
{"diff_hunk":"@@ -7,20 +7,32 @@\n  * @private\n  *\/\n function getIncompleteReason(checkData, messages) {\n+\tfunction getDefaultMsg(messages) {\n+\t\tif (messages.incomplete && messages.incomplete.default) {\n+\t\t\t\/\/ fall back to the default message if no reason specified\n+\t\t\treturn messages.incomplete.default;\n+\t\t} else {\n+\t\t\t\/\/ TODO: localize it\n+\t\t\treturn 'aXe couldn\\'t tell the reason. Time to break out the element inspector!';\n+\t\t}\n+\t}\n \tif (checkData && checkData.missingData) {\n-\t\tvar missingReason;\n \t\ttry {\n-\t\t\tmissingReason = checkData.missingData[0].reason;\n-\t\t} finally {\n+\t\t\tvar msg = messages.incomplete[checkData.missingData[0].reason];\n+\t\t\tif (!msg) {\n+\t\t\t\tthrow new Error();\n+\t\t\t}\n+\t\t\treturn msg;\n+\t\t} catch (e) {\n \t\t\tif (typeof checkData.missingData === 'string')  {\n-\t\t\t\tmissingReason = checkData.missingData;\n+\t\t\t\t\/\/ return a string with the appropriate reason\n+\t\t\t\treturn messages.incomplete[checkData.missingData];\n+\t\t\t} else {\n+\t\t\t\treturn getDefaultMsg(messages);\n \t\t\t}\n \t\t}\n-\t\t\/\/ return a function with the appropriate message\n-\t\treturn messages.incomplete[missingReason];\n \t} else {\n-\t\t\/\/ fall back to the default message if no reason specified\n-\t\treturn messages.incomplete.default;\n+\t\treturn getDefaultMsg(messages);\n \t}\n }\n \/**","old_code":"\n\/**\n * Construct incomplete message from check.data\n * @param  {Object} checkData Check result with reason specified\n * @param  {Object} messages Source data object with message options\n * @return  {String}\n * @private\n *\/\nfunction getIncompleteReason(checkData, messages) {\n\tif (checkData && checkData.missingData) {\n\t\tvar missingReason;\n\t\ttry {\n\t\t\tmissingReason = checkData.missingData[0].reason;\n\t\t} finally {\n\t\t\tif (typeof checkData.missingData === 'string')  {\n\t\t\t\tmissingReason = checkData.missingData;\n\t\t\t}\n\t\t}\n\t\t\/\/ return a function with the appropriate message\n\t\treturn messages.incomplete[missingReason];\n\t} else {\n\t\t\/\/ fall back to the default message if no reason specified\n\t\treturn messages.incomplete.default;\n\t}\n}\n\/**\n * Extend checksData with the correct result message\n * @param  {Object} checksData The check result data\n * @param  {Boolean} shouldBeTrue Result of pass\/fail check run\n * @return {Function}\n * @private\n *\/\nfunction extender(checksData, shouldBeTrue) {\n\t'use strict';\n\treturn function (check) {\n\t\tvar sourceData = checksData[check.id] || {};\n\t\tvar messages = sourceData.messages || {};\n\t\tvar data = Object.assign({}, sourceData);\n\t\tdelete data.messages;\n\t\tif (check.result === undefined) {\n\t\t\tif (typeof messages.incomplete === 'object') {\n\t\t\t\tdata.message = function() { return getIncompleteReason(check.data, messages); };\n\t\t\t} else {\n\t\t\t\t\/\/ fall back to string message\n\t\t\t\tdata.message = messages.incomplete;\n\t\t\t}\n\t\t} else {\n\t\t\tdata.message = check.result === shouldBeTrue ? messages.pass : messages.fail;\n\t\t}\n\t\taxe.utils.extendMetaData(check, data);\n\t};\n}\n\n\/**\n * Publish metadata from axe._audit.data\n * @param  {RuleResult} result Result to publish to\n * @private\n *\/\naxe.utils.publishMetaData = function (ruleResult) {\n\t'use strict';\n\n\tvar checksData = axe._audit.data.checks || {};\n\tvar rulesData = axe._audit.data.rules || {};\n\tvar rule = axe.utils.findBy(axe._audit.rules, 'id', ruleResult.id) || {};\n\n\truleResult.tags = axe.utils.clone(rule.tags || []);\n\n\tvar shouldBeTrue = extender(checksData, true);\n\tvar shouldBeFalse = extender(checksData, false);\n\truleResult.nodes.forEach(function (detail) {\n\t\tdetail.any.forEach(shouldBeTrue);\n\t\tdetail.all.forEach(shouldBeTrue);\n\t\tdetail.none.forEach(shouldBeFalse);\n\t});\n\taxe.utils.extendMetaData(ruleResult, axe.utils.clone(rulesData[ruleResult.id] || {}));\n};\n","lang_cluster":"Javascript","length":76,"code_uid":"e0d71fd1be11458b80c71e2c12549be2"}
{"diff_hunk":"@@ -82,9 +82,9 @@ class Button extends Component {\n \t\t\t\taria-controls={ ariaControls }\n \t\t\t\trole={ 'a' === SemanticButton ? 'button' : undefined }\n \t\t\t>\n-\t\t\t\t{ icon && icon }\n+\t\t\t\t{ icon }\n \t\t\t\t<span className=\"mdc-button__label\">{ children }<\/span>\n-\t\t\t\t{ trailingIcon && trailingIcon }\n+\t\t\t\t{ trailingIcon }\n \t\t\t<\/SemanticButton>\n \t\t);\n \t}","old_code":"\/**\n * Button component.\n *\n * Site Kit by Google, Copyright 2019 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * External dependencies\n *\/\nimport PropTypes from 'prop-types';\nimport classnames from 'classnames';\n\/**\n * WordPress dependencies\n *\/\nimport { Component, createRef } from '@wordpress\/element';\n\/**\n * Internal dependencies\n *\/\nimport { MDCRipple } from '..\/material-components';\n\nclass Button extends Component {\n\tconstructor( props ) {\n\t\tsuper( props );\n\t\tthis.buttonRef = createRef();\n\t}\n\n\tcomponentDidMount() {\n\t\tnew MDCRipple( this.buttonRef.current );\n\t}\n\n\trender() {\n\t\tconst {\n\t\t\tonClick,\n\t\t\tchildren,\n\t\t\thref,\n\t\t\ttext,\n\t\t\tclassName,\n\t\t\tdanger,\n\t\t\tdisabled,\n\t\t\ttarget,\n\t\t\tid,\n\t\t\ticon,\n\t\t\ttrailingIcon,\n\t\t\tariaHaspopup,\n\t\t\tariaExpanded,\n\t\t\tariaControls,\n\t\t} = this.props;\n\n\t\t\/\/ Use a button if disabled, even if a href is provided to ensure expected behavior.\n\t\tconst SemanticButton = ( href && ! disabled ) ? 'a' : 'button';\n\n\t\treturn (\n\t\t\t<SemanticButton\n\t\t\t\tclassName={ classnames(\n\t\t\t\t\t'mdc-button',\n\t\t\t\t\tclassName,\n\t\t\t\t\t{\n\t\t\t\t\t\t'mdc-button--raised': ! text,\n\t\t\t\t\t\t'mdc-button--danger': danger,\n\t\t\t\t\t}\n\t\t\t\t) }\n\t\t\t\tonClick={ onClick }\n\t\t\t\thref={ disabled ? undefined : href }\n\t\t\t\tref={ this.buttonRef }\n\t\t\t\tdisabled={ !! disabled }\n\t\t\t\ttarget={ target || '_self' }\n\t\t\t\tid={ id }\n\t\t\t\taria-haspopup={ ariaHaspopup }\n\t\t\t\taria-expanded={ ariaExpanded }\n\t\t\t\taria-controls={ ariaControls }\n\t\t\t\trole={ 'a' === SemanticButton ? 'button' : undefined }\n\t\t\t>\n\t\t\t\t{ icon && icon }\n\t\t\t\t<span className=\"mdc-button__label\">{ children }<\/span>\n\t\t\t\t{ trailingIcon && trailingIcon }\n\t\t\t<\/SemanticButton>\n\t\t);\n\t}\n}\n\nButton.propTypes = {\n\tonClick: PropTypes.func,\n\tchildren: PropTypes.string.isRequired,\n\thref: PropTypes.string,\n\ttext: PropTypes.bool,\n\tclassName: PropTypes.string,\n\tdanger: PropTypes.bool,\n\tdisabled: PropTypes.bool,\n\ticon: PropTypes.element,\n\ttrailingIcon: PropTypes.element,\n\tariaHaspopup: PropTypes.string,\n\tariaExpanded: PropTypes.bool,\n\tariaControls: PropTypes.string,\n};\n\nButton.defaultProps = {\n\tonClick: null,\n\thref: null,\n\ttext: false,\n\tclassName: '',\n\tdanger: false,\n\tdisabled: false,\n\ticon: null,\n\ttrailingIcon: null,\n\tariaHaspopup: '',\n\tariaExpanded: false,\n\tariaControls: '',\n};\n\nexport default Button;\n","lang_cluster":"Javascript","length":122,"code_uid":"96c1004c1eb44830b3dfd0d89445ca7d"}
{"diff_hunk":"@@ -1,11 +1,11 @@\n module.exports = {\n-\thash: '964177e31d959b03bc63f11216c61e3e', files: {\n-\t\t'highlight.js\/atom-one-dark-reasonable.css': { data: require('.\/highlight.js\/atom-one-dark-reasonable.css.base64.js'), mime: 'text\/css', encoding: 'base64' },\n-\t\t'highlight.js\/atom-one-light.css': { data: require('.\/highlight.js\/atom-one-light.css.base64.js'), mime: 'text\/css', encoding: 'base64' },\n-\t\t'katex\/fonts\/KaTeX_Main-Regular.woff2': { data: require('.\/katex\/fonts\/KaTeX_Main-Regular.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n-\t\t'katex\/fonts\/KaTeX_Math-Italic.woff2': { data: require('.\/katex\/fonts\/KaTeX_Math-Italic.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n-\t\t'katex\/fonts\/KaTeX_Size1-Regular.woff2': { data: require('.\/katex\/fonts\/KaTeX_Size1-Regular.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n-\t\t'katex\/fonts\/KaTeX_Size2-Regular.woff2': { data: require('.\/katex\/fonts\/KaTeX_Size2-Regular.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n-\t\t'katex\/katex.css': { data: require('.\/katex\/katex.css.base64.js'), mime: 'text\/css', encoding: 'base64' },\n-\t},\n-};\n+hash:\"964177e31d959b03bc63f11216c61e3e\", files: {\n+'highlight.js\/atom-one-dark-reasonable.css': { data: require('.\/highlight.js\/atom-one-dark-reasonable.css.base64.js'), mime: 'text\/css', encoding: 'base64' },\n+'highlight.js\/atom-one-light.css': { data: require('.\/highlight.js\/atom-one-light.css.base64.js'), mime: 'text\/css', encoding: 'base64' },\n+'katex\/fonts\/KaTeX_Main-Regular.woff2': { data: require('.\/katex\/fonts\/KaTeX_Main-Regular.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n+'katex\/fonts\/KaTeX_Math-Italic.woff2': { data: require('.\/katex\/fonts\/KaTeX_Math-Italic.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n+'katex\/fonts\/KaTeX_Size1-Regular.woff2': { data: require('.\/katex\/fonts\/KaTeX_Size1-Regular.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n+'katex\/fonts\/KaTeX_Size2-Regular.woff2': { data: require('.\/katex\/fonts\/KaTeX_Size2-Regular.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n+'katex\/katex.css': { data: require('.\/katex\/katex.css.base64.js'), mime: 'text\/css', encoding: 'base64' },\n+}\n+};","old_code":"module.exports = {\n\thash: '964177e31d959b03bc63f11216c61e3e', files: {\n\t\t'highlight.js\/atom-one-dark-reasonable.css': { data: require('.\/highlight.js\/atom-one-dark-reasonable.css.base64.js'), mime: 'text\/css', encoding: 'base64' },\n\t\t'highlight.js\/atom-one-light.css': { data: require('.\/highlight.js\/atom-one-light.css.base64.js'), mime: 'text\/css', encoding: 'base64' },\n\t\t'katex\/fonts\/KaTeX_Main-Regular.woff2': { data: require('.\/katex\/fonts\/KaTeX_Main-Regular.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n\t\t'katex\/fonts\/KaTeX_Math-Italic.woff2': { data: require('.\/katex\/fonts\/KaTeX_Math-Italic.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n\t\t'katex\/fonts\/KaTeX_Size1-Regular.woff2': { data: require('.\/katex\/fonts\/KaTeX_Size1-Regular.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n\t\t'katex\/fonts\/KaTeX_Size2-Regular.woff2': { data: require('.\/katex\/fonts\/KaTeX_Size2-Regular.woff2.base64.js'), mime: 'application\/octet-stream', encoding: 'base64' },\n\t\t'katex\/katex.css': { data: require('.\/katex\/katex.css.base64.js'), mime: 'text\/css', encoding: 'base64' },\n\t},\n};\n","lang_cluster":"Javascript","length":11,"code_uid":"c9df91a74d5a4212b1b206ad207df18b"}
{"diff_hunk":"@@ -114,7 +114,10 @@ const baseResolvers = {\n \t\t\treturn;\n \t\t}\n \n-\t\tyield fetchGetURLChannelsStore.actions.fetchGetURLChannels( accountID, clientID );\n+\t\tconst { error } = yield fetchGetURLChannelsStore.actions.fetchGetURLChannels( accountID, clientID );\n+\t\tif ( error ) {\n+\t\t\tyield errorStoreActions.receiveError( error, 'getURLChannels', [ accountID, clientID ] );\n+\t\t}\n \t},\n };\n ","old_code":"\/**\n * `modules\/adsense` data store: URL channels.\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * External dependencies\n *\/\nimport invariant from 'invariant';\n\n\/**\n * Internal dependencies\n *\/\nimport API from 'googlesitekit-api';\nimport Data from 'googlesitekit-data';\nimport { STORE_NAME } from '.\/constants';\nimport { createFetchStore } from '..\/..\/..\/googlesitekit\/data\/create-fetch-store';\nimport { actions as errorStoreActions } from '..\/..\/..\/googlesitekit\/data\/create-error-store';\n\nconst fetchGetURLChannelsStore = createFetchStore( {\n\tbaseName: 'getURLChannels',\n\tcontrolCallback: ( { accountID, clientID } ) => {\n\t\treturn API.get( 'modules', 'adsense', 'urlchannels', { accountID, clientID }, {\n\t\t\tuseCache: false,\n\t\t} );\n\t},\n\treducerCallback: ( state, urlchannels, { accountID, clientID } ) => {\n\t\treturn {\n\t\t\t...state,\n\t\t\turlchannels: {\n\t\t\t\t...state.urlchannels,\n\t\t\t\t[ `${ accountID }::${ clientID }` ]: [ ...urlchannels ],\n\t\t\t},\n\t\t};\n\t},\n\targsToParams: ( accountID, clientID ) => {\n\t\treturn { accountID, clientID };\n\t},\n\tvalidateParams: ( { accountID, clientID } = {} ) => {\n\t\tinvariant( accountID, 'accountID is required.' );\n\t\tinvariant( clientID, 'clientID is required.' );\n\t},\n} );\n\n\/\/ Actions\nconst RESET_URLCHANNELS = 'RESET_URLCHANNELS';\n\nconst baseInitialState = {\n\turlchannels: {},\n};\n\nconst baseActions = {\n\t*resetURLChannels() {\n\t\tconst { dispatch } = yield Data.commonActions.getRegistry();\n\n\t\tyield {\n\t\t\tpayload: {},\n\t\t\ttype: RESET_URLCHANNELS,\n\t\t};\n\n\t\tyield errorStoreActions.clearErrors( 'getURLChannels' );\n\n\t\treturn dispatch( STORE_NAME )\n\t\t\t.invalidateResolutionForStoreSelector( 'getURLChannels' );\n\t},\n};\n\nconst baseReducer = ( state, { type } ) => {\n\tswitch ( type ) {\n\t\tcase RESET_URLCHANNELS: {\n\t\t\tconst {\n\t\t\t\tsiteStatus,\n\t\t\t\tsiteSetupComplete,\n\t\t\t} = state.savedSettings || {};\n\t\t\treturn {\n\t\t\t\t...state,\n\t\t\t\turlchannels: initialState.urlchannels,\n\t\t\t\tsettings: {\n\t\t\t\t\t...( state.settings || {} ),\n\t\t\t\t\tsiteStatus,\n\t\t\t\t\tsiteSetupComplete,\n\t\t\t\t},\n\t\t\t};\n\t\t}\n\n\t\tdefault: {\n\t\t\treturn state;\n\t\t}\n\t}\n};\n\nconst baseResolvers = {\n\t*getURLChannels( accountID, clientID ) {\n\t\tif ( undefined === accountID || undefined === clientID ) {\n\t\t\treturn;\n\t\t}\n\n\t\tconst registry = yield Data.commonActions.getRegistry();\n\t\tconst existingURLChannels = registry.select( STORE_NAME ).getURLChannels( accountID, clientID );\n\t\tif ( existingURLChannels ) {\n\t\t\treturn;\n\t\t}\n\n\t\tyield fetchGetURLChannelsStore.actions.fetchGetURLChannels( accountID, clientID );\n\t},\n};\n\nconst baseSelectors = {\n\t\/**\n\t * Gets all Google AdSense URL channels for this account and client.\n\t *\n\t * @since 1.9.0\n\t *\n\t * @param {Object} state     Data store's state.\n\t * @param {string} accountID The AdSense Account ID to fetch URL channels for.\n\t * @param {string} clientID  The AdSense Client ID to fetch URL channels for.\n\t * @return {(Array.<Object>|undefined)} An array of AdSense URL channels; `undefined` if not loaded.\n\t *\/\n\tgetURLChannels( state, accountID, clientID ) {\n\t\tif ( undefined === accountID || undefined === clientID ) {\n\t\t\treturn undefined;\n\t\t}\n\n\t\treturn state.urlchannels[ `${ accountID }::${ clientID }` ];\n\t},\n};\n\nconst store = Data.combineStores(\n\tfetchGetURLChannelsStore,\n\t{\n\t\tinitialState: baseInitialState,\n\t\tactions: baseActions,\n\t\treducer: baseReducer,\n\t\tresolvers: baseResolvers,\n\t\tselectors: baseSelectors,\n\t}\n);\n\nexport const initialState = store.initialState;\nexport const actions = store.actions;\nexport const controls = store.controls;\nexport const reducer = store.reducer;\nexport const resolvers = store.resolvers;\nexport const selectors = store.selectors;\n\nexport default store;\n","lang_cluster":"Javascript","length":159,"code_uid":"6eef444a93c34ef398b5a0fe9f29c8e7"}
{"diff_hunk":"@@ -1,13 +1,24 @@\n \/* global aria, axe, dom *\/\n-function findDomNode(node, functor) {\n-\tif (functor(node)) {\n-\t\treturn node;\n+const idRefsRegex = \/^idrefs?$\/;\n+\n+function cacheIdRefs(node, refAttrs) {\n+\tif (node.hasAttribute) {\n+\t\tif (node.nodeName.toUpperCase() === 'LABEL' && node.hasAttribute('for')) {\n+\t\t\taxe._cache.idRefs[node.getAttribute('for')] = 1;\n+\t\t}\n+\n+\t\trefAttrs\n+\t\t\t.filter(attr => node.hasAttribute(attr))\n+\t\t\t.forEach(attr => {\n+\t\t\t\tconst attrValue = node.getAttribute(attr);\n+\t\t\t\taxe.utils.tokenList(attrValue).forEach(id => {\n+\t\t\t\t\taxe._cache.idRefs[id] = 1;\n+\t\t\t\t});\n+\t\t\t});\n \t}\n+\n \tfor (let i = 0; i < node.children.length; i++) {\n-\t\tconst out = findDomNode(node.children[i], functor);\n-\t\tif (out) {\n-\t\t\treturn out;\n-\t\t}\n+\t\tcacheIdRefs(node.children[i], refAttrs);\n \t}\n }\n ","old_code":"\/* global aria, axe, dom *\/\nfunction findDomNode(node, functor) {\n\tif (functor(node)) {\n\t\treturn node;\n\t}\n\tfor (let i = 0; i < node.children.length; i++) {\n\t\tconst out = findDomNode(node.children[i], functor);\n\t\tif (out) {\n\t\t\treturn out;\n\t\t}\n\t}\n}\n\n\/**\n * Check that a DOM node is a reference in the accessibility tree\n * @param {Element} node\n * @returns {Boolean}\n *\/\naria.isAccessibleRef = function isAccessibleRef(node) {\n\tnode = node.actualNode || node;\n\tlet root = dom.getRootNode(node);\n\troot = root.documentElement || root; \/\/ account for shadow roots\n\tconst id = node.id;\n\n\t\/\/ Get all idref(s) attributes on the lookup table\n\tconst refAttrs = Object.keys(aria.lookupTable.attributes).filter(attr => {\n\t\tconst { type } = aria.lookupTable.attributes[attr];\n\t\treturn \/^idrefs?$\/.test(type);\n\t});\n\n\t\/\/ Find the first element that IDREF(S) the node\n\tlet refElm = findDomNode(root, elm => {\n\t\tif (elm.nodeType !== 1) {\n\t\t\t\/\/ Elements only\n\t\t\treturn;\n\t\t}\n\t\tif (\n\t\t\telm.nodeName.toUpperCase() === 'LABEL' &&\n\t\t\telm.getAttribute('for') === id\n\t\t) {\n\t\t\treturn true;\n\t\t}\n\t\t\/\/ See if there are any aria attributes that reference the node\n\t\treturn refAttrs\n\t\t\t.filter(attr => elm.hasAttribute(attr))\n\t\t\t.some(attr => {\n\t\t\t\tconst attrValue = elm.getAttribute(attr);\n\t\t\t\tif (aria.lookupTable.attributes[attr].type === 'idref') {\n\t\t\t\t\treturn attrValue === id;\n\t\t\t\t}\n\t\t\t\treturn axe.utils.tokenList(attrValue).includes(id);\n\t\t\t});\n\t});\n\treturn typeof refElm !== 'undefined';\n};\n","lang_cluster":"Javascript","length":55,"code_uid":"ce570ba19ac34556898e7581cfe30761"}
{"diff_hunk":"@@ -17,6 +17,13 @@ const getHasListItem = (hasListItem, tagName, isListItemRole) => {\n \treturn hasListItem || (tagName === 'LI' && isListItemRole) || isListItemRole;\n };\n \n+const getIsHidden = actualNode => {\n+\treturn (\n+\t\twindow.getComputedStyle(actualNode, null).getPropertyValue('display') ===\n+\t\t'none'\n+\t);\n+};\n+\n let base = {\n \tbadNodes: [],\n \tisEmpty: true,","old_code":"const ALLOWED_TAGS = [\n\t'STYLE',\n\t'META',\n\t'LINK',\n\t'MAP',\n\t'AREA',\n\t'SCRIPT',\n\t'DATALIST',\n\t'TEMPLATE'\n];\n\nconst getIsListItemRole = (role, tagName) => {\n\treturn role === 'listitem' || (tagName === 'LI' && !role);\n};\n\nconst getHasListItem = (hasListItem, tagName, isListItemRole) => {\n\treturn hasListItem || (tagName === 'LI' && isListItemRole) || isListItemRole;\n};\n\nlet base = {\n\tbadNodes: [],\n\tisEmpty: true,\n\thasNonEmptyTextNode: false,\n\thasListItem: false,\n\tliItemsWithRole: 0\n};\n\nlet out = virtualNode.children.reduce((out, { actualNode }) => {\n\t\/*eslint \n\t\tmax-statements: [\"error\", 20]\n\t\tcomplexity: [\"error\", 11] \n\t\t*\/\n\tconst tagName = actualNode.nodeName.toUpperCase();\n\n\tif (actualNode.nodeType === 1) {\n\t\tif (!ALLOWED_TAGS.includes(tagName)) {\n\t\t\tconst role = (actualNode.getAttribute('role') || '').toLowerCase();\n\t\t\tconst isListItemRole = getIsListItemRole(role, tagName);\n\n\t\t\tout.hasListItem = getHasListItem(\n\t\t\t\tout.hasListItem,\n\t\t\t\ttagName,\n\t\t\t\tisListItemRole\n\t\t\t);\n\n\t\t\tif (isListItemRole) {\n\t\t\t\tout.isEmpty = false;\n\t\t\t}\n\t\t\tif (tagName === 'LI' && !isListItemRole) {\n\t\t\t\tout.liItemsWithRole++;\n\t\t\t}\n\t\t\tif (tagName !== 'LI' && !isListItemRole) {\n\t\t\t\tout.badNodes.push(actualNode);\n\t\t\t}\n\t\t}\n\t}\n\tif (actualNode.nodeType === 3) {\n\t\tif (actualNode.nodeValue.trim() !== '') {\n\t\t\tout.hasNonEmptyTextNode = true;\n\t\t}\n\t}\n\n\treturn out;\n}, base);\n\nconst virtualNodeChildrenOfTypeLi = virtualNode.children.filter(\n\t({ actualNode }) => {\n\t\treturn actualNode.nodeName.toUpperCase() === 'LI';\n\t}\n);\n\nconst allLiItemsHaveRole =\n\tout.liItemsWithRole > 0 &&\n\tvirtualNodeChildrenOfTypeLi.length === out.liItemsWithRole;\n\nif (out.badNodes.length) {\n\tthis.relatedNodes(out.badNodes);\n}\n\nconst isInvalidListItem = !(\n\tout.hasListItem ||\n\t(out.isEmpty && !allLiItemsHaveRole)\n);\nreturn isInvalidListItem || !!out.badNodes.length || out.hasNonEmptyTextNode;\n","lang_cluster":"Javascript","length":84,"code_uid":"a71a1dbe52464328aff151e2bdce3eeb"}
{"diff_hunk":"@@ -10,6 +10,8 @@ export function assign(obj, props) {\n \treturn \/** @type {O & P} *\/ (obj);\n }\n \n+export const IS_NON_DIMENSIONAL = \/^(-|f[lo].*[^se]$|g.{5,}[^ps]$|z|o[pr]|(W.{5})?[lL]i.*(t|mp)$|an|(bo|s).{4}Im|sca|m.{6}[ds]|ta|c.*[st]$|wido|ini)\/;\n+\n \/**\n  * Check if two objects have a different shape\n  * @param {object} a","old_code":"\/**\n * Assign properties from `props` to `obj`\n * @template O, P The obj and props types\n * @param {O} obj The object to copy properties to\n * @param {P} props The object to copy properties from\n * @returns {O & P}\n *\/\nexport function assign(obj, props) {\n\tfor (let i in props) obj[i] = props[i];\n\treturn \/** @type {O & P} *\/ (obj);\n}\n\n\/**\n * Check if two objects have a different shape\n * @param {object} a\n * @param {object} b\n * @returns {boolean}\n *\/\nexport function shallowDiffers(a, b) {\n\tfor (let i in a) if (i !== '__source' && !(i in b)) return true;\n\tfor (let i in b) if (i !== '__source' && a[i] !== b[i]) return true;\n\treturn false;\n}\n","lang_cluster":"Javascript","length":23,"code_uid":"d085b74f8e184a7e99448b8d60aa7692"}
{"diff_hunk":"@@ -5,7 +5,7 @@ setResolver(resolver);\n \n \/* jshint ignore:start *\/\n mocha.setup({\n-    timeout: 15000,\n+    timeout: 25000,\n     slow: 500\n });\n \/* jshint ignore:end *\/","old_code":"import resolver from '.\/helpers\/resolver';\nimport {setResolver} from 'ember-mocha';\n\nsetResolver(resolver);\n\n\/* jshint ignore:start *\/\nmocha.setup({\n    timeout: 15000,\n    slow: 500\n});\n\/* jshint ignore:end *\/\n","lang_cluster":"Javascript","length":11,"code_uid":"5283a875a56a48d4b88d3856a261961c"}
{"diff_hunk":"@@ -1,4 +1,21 @@\n+\/**\n+ * Utilities for the Tag Manager module.\n+ *\n+ * Site Kit by Google, Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\/\n+\n+export * from '.\/validation';\n export { default as getContainers } from '.\/get-containers';\n-export { default as isValidAccountID } from '.\/is-valid-account-id';\n-export { default as isValidContainerID } from '.\/is-valid-container-id';\n export { default as tagMatchers } from '.\/tag-matchers';","old_code":"export { default as getContainers } from '.\/get-containers';\nexport { default as isValidAccountID } from '.\/is-valid-account-id';\nexport { default as isValidContainerID } from '.\/is-valid-container-id';\nexport { default as tagMatchers } from '.\/tag-matchers';\n","lang_cluster":"Javascript","length":4,"code_uid":"21e7c30a0c6747ee820f8ff59ba1831c"}
{"diff_hunk":"@@ -27,7 +27,7 @@ const main = async url => {\n \t\t\/\/ Inject and run axe-core\n \t\tconst handle = await page.evaluateHandle(`\n \t\t\t\/\/ Inject axe source code\n-\t\t\t${axeCore.source}\n+\t\t\tvar axe = ${axeCore.source}\n \t\t\t\/\/ Run axe\n \t\t\taxe.run()\n \t\t`);","old_code":"const puppeteer = require('puppeteer');\nconst axeCore = require('axe-core');\nconst { parse: parseURL } = require('url');\nconst assert = require('assert');\n\n\/\/ Cheap URL validation\nconst isValidURL = input => {\n\tconst u = parseURL(input);\n\treturn u.protocol && u.host;\n};\n\n\/\/ node axe-puppeteer.js <url>\nconst url = process.argv[2];\nassert(isValidURL(url), 'Invalid URL');\n\nconst main = async url => {\n\tlet browser;\n\tlet results;\n\ttry {\n\t\t\/\/ Setup Puppeteer\n\t\tbrowser = await puppeteer.launch();\n\n\t\t\/\/ Get new page\n\t\tconst page = await browser.newPage();\n\t\tawait page.goto(url);\n\n\t\t\/\/ Inject and run axe-core\n\t\tconst handle = await page.evaluateHandle(`\n\t\t\t\/\/ Inject axe source code\n\t\t\t${axeCore.source}\n\t\t\t\/\/ Run axe\n\t\t\taxe.run()\n\t\t`);\n\n\t\t\/\/ Get the results from `axe.run()`.\n\t\tresults = await handle.jsonValue();\n\t\t\/\/ Destroy the handle & return axe results.\n\t\tawait handle.dispose();\n\t} catch (err) {\n\t\t\/\/ Ensure we close the puppeteer connection when possible\n\t\tif (browser) {\n\t\t\tawait browser.close();\n\t\t}\n\n\t\t\/\/ Re-throw\n\t\tthrow err;\n\t}\n\n\tawait browser.close();\n\treturn results;\n};\n\nmain(url)\n\t.then(results => {\n\t\tconsole.log(results);\n\t})\n\t.catch(err => {\n\t\tconsole.error('Error running axe-core:', err.message);\n\t\tprocess.exit(1);\n\t});\n","lang_cluster":"Javascript","length":60,"code_uid":"fd333b21e09d476b9ec69221878bc777"}
{"diff_hunk":"@@ -34,7 +34,9 @@ import { isValidStringsOrObjects } from '..\/..\/..\/util\/report-validation';\n export function isValidMetrics( metrics ) {\n \treturn isValidStringsOrObjects( metrics, ( metric ) => {\n \t\tconst validExpression = metric.hasOwnProperty( 'expression' ) && typeof metric.expression === 'string';\n-\t\tconst validAlias = metric.hasOwnProperty( 'alias' ) && typeof metric.alias === 'string';\n+\n+\t\t\/\/ 'alias' is optional; if provided, it must be a string.\n+\t\tconst validAlias = ! metric.hasOwnProperty( 'alias' ) || typeof metric.alias === 'string';\n \t\treturn validExpression && validAlias;\n \t} );\n }","old_code":"\/**\n * Reporting API validation utilities.\n *\n * Site Kit by Google, Copyright 2020 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * Internal dependencies\n *\/\nimport { isValidStringsOrObjects } from '..\/..\/..\/util\/report-validation';\n\n\/**\n * Verifies that provided metrics match allowed values. Metrics can be a string,\n * an array of string, an array of objects or mix of strings and objects. Objects\n * must have \"expression\" and \"alias\" properties in order to be considered as valid.\n *\n * @since 1.13.0\n *\n * @param {string|string[]|Object|Object[]} metrics The metrics to check.\n * @return {boolean} TRUE if metrics are valid, otherwise FALSE.\n *\/\nexport function isValidMetrics( metrics ) {\n\treturn isValidStringsOrObjects( metrics, ( metric ) => {\n\t\tconst validExpression = metric.hasOwnProperty( 'expression' ) && typeof metric.expression === 'string';\n\t\tconst validAlias = metric.hasOwnProperty( 'alias' ) && typeof metric.alias === 'string';\n\t\treturn validExpression && validAlias;\n\t} );\n}\n\n\/**\n * Verifies provided dimensions to make sure it matches allowed values. It can be a string,\n * array of strings, an object with \"name\" field, array of such objects or an array of strings\n * and objects.\n *\n * @since 1.13.0\n *\n * @param {string|string[]|Object|Object[]} dimensions The dimensions to check.\n * @return {boolean} TRUE if dimensions are valid, otherwise FALSE.\n *\/\nexport function isValidDimensions( dimensions ) {\n\treturn isValidStringsOrObjects( dimensions, ( dimension ) => {\n\t\treturn dimension.hasOwnProperty( 'name' ) && typeof dimension.name === 'string';\n\t} );\n}\n","lang_cluster":"Javascript","length":56,"code_uid":"7ee2e007d84741fda7e1973c3aaba8a7"}
{"diff_hunk":"@@ -16,9 +16,16 @@ module.exports = function(environment) {\n     APP: {\n       \/\/ Here you can pass flags\/options to your application instance\n       \/\/ when it is created\n+    },\n+\n+    'simple-auth': {\n+\n+    },\n+    'ember-cli-mirage': {\n+      enabled: false\n     }\n   };\n-    \n+\n ENV.APP.manifest = {\n   enabled: true,\n   appcacheFile: \"\/manifest.appcache\",","old_code":"\/* jshint node: true *\/\n\nmodule.exports = function(environment) {\n  var ENV = {\n    modulePrefix: 'hospitalrun',\n    environment: environment,\n    baseURL: '\/',\n    locationType: 'hash', \/\/Auto incompatible with google login right now\n    EmberENV: {\n      FEATURES: {\n        \/\/ Here you can enable experimental features on an ember canary build\n        \/\/ e.g. 'with-controller': true\n      }\n    },\n\n    APP: {\n      \/\/ Here you can pass flags\/options to your application instance\n      \/\/ when it is created\n    }\n  };\n    \nENV.APP.manifest = {\n  enabled: true,\n  appcacheFile: \"\/manifest.appcache\",\n  excludePaths: ['index.html', 'someother.html'],\n  includePaths: ['\/'],\n  network: ['api\/'],\n  showCreateDate: true\n};    \n\n  if (environment === 'development') {\n    \/\/ ENV.APP.LOG_RESOLVER = true;\n    \/\/ ENV.APP.LOG_ACTIVE_GENERATION = true;\n    \/\/ ENV.APP.LOG_TRANSITIONS = true;\n    \/\/ ENV.APP.LOG_TRANSITIONS_INTERNAL = true;\n    \/\/ ENV.APP.LOG_VIEW_LOOKUPS = true;\n  }\n\n  if (environment === 'test') {\n    \/\/ Testem prefers this...\n    ENV.baseURL = '\/';\n    ENV.locationType = 'none';\n\n    \/\/ keep test console output quieter\n    ENV.APP.LOG_ACTIVE_GENERATION = false;\n    ENV.APP.LOG_VIEW_LOOKUPS = false;\n\n    ENV.APP.rootElement = '#ember-testing';\n  }\n\n  if (environment === 'production') {\n\n  }\n    \nENV.manifest = {\n  enabled: true,\n  appcacheFile: \"\/manifest.appcache\",\n  excludePaths: ['index.html', 'tests', 'dymo','robots.txt','testem.js'],\n  \/\/includePaths: ['\/'],\n  showCreateDate: true\n};\n\n  return ENV;\n};\n","lang_cluster":"Javascript","length":64,"code_uid":"b003eeff71b945dba7b113adc94726f5"}
{"diff_hunk":"@@ -9,8 +9,8 @@ const expect = require('chai').expect,\n   MongoError = require('..\/..\/..\/lib\/core\/error').MongoError,\n   ReadPreference = require('..\/..\/..\/lib\/core\/topologies\/read_preference');\n \n-const rsWithPrimaryPath = f('%s\/..\/spec\/max-staleness\/ReplicaSetWithPrimary', __dirname);\n-const rsWithoutPrimaryPath = f('%s\/..\/spec\/max-staleness\/ReplicaSetNoPrimary', __dirname);\n+const rsWithPrimaryPath = f('%s\/..\/..\/spec\/max-staleness\/ReplicaSetWithPrimary', __dirname);\n+const rsWithoutPrimaryPath = f('%s\/..\/..\/spec\/max-staleness\/ReplicaSetNoPrimary', __dirname);\n \n describe('Max Staleness', function() {\n   describe('ReplicaSet without primary', function() {","old_code":"'use strict';\n\nconst expect = require('chai').expect,\n  p = require('path'),\n  f = require('util').format,\n  fs = require('fs'),\n  Server = require('..\/..\/..\/lib\/core\/topologies\/server'),\n  ReplSetState = require('..\/..\/..\/lib\/core\/topologies\/replset_state'),\n  MongoError = require('..\/..\/..\/lib\/core\/error').MongoError,\n  ReadPreference = require('..\/..\/..\/lib\/core\/topologies\/read_preference');\n\nconst rsWithPrimaryPath = f('%s\/..\/spec\/max-staleness\/ReplicaSetWithPrimary', __dirname);\nconst rsWithoutPrimaryPath = f('%s\/..\/spec\/max-staleness\/ReplicaSetNoPrimary', __dirname);\n\ndescribe('Max Staleness', function() {\n  describe('ReplicaSet without primary', function() {\n    fs\n      .readdirSync(rsWithoutPrimaryPath)\n      .filter(x => x.indexOf('.json') !== -1)\n      .forEach(x => {\n        it(p.basename(x, '.json'), function(done) {\n          executeEntry(f('%s\/%s', rsWithoutPrimaryPath, x), done);\n        });\n      });\n  });\n\n  describe('ReplicaSet with primary', function() {\n    fs\n      .readdirSync(rsWithPrimaryPath)\n      .filter(x => x.indexOf('.json') !== -1)\n      .filter(x => x.indexOf('LongHeartbeat2.jwson') === -1)\n      .forEach(x => {\n        it(p.basename(x, '.json'), function(done) {\n          executeEntry(f('%s\/%s', rsWithPrimaryPath, x), done);\n        });\n      });\n  });\n});\n\nfunction convert(mode) {\n  if (mode === undefined) return 'primary';\n  if (mode.toLowerCase() === 'primarypreferred') return 'primaryPreferred';\n  if (mode.toLowerCase() === 'secondarypreferred') return 'secondaryPreferred';\n  return mode.toLowerCase();\n}\n\nfunction executeEntry(path, callback) {\n  \/\/ Read and parse the json file\n  var file = require(path);\n\n  \/\/ Let's pick out the parts of the selection specification\n  var error = file.error;\n  var heartbeatFrequencyMS = file.heartbeatFrequencyMS || 10000;\n  var inLatencyWindow = file.in_latency_window;\n  var readPreference = file.read_preference;\n  var topologyDescription = file.topology_description;\n\n  try {\n    \/\/ Create a Replset and populate it with dummy topology servers\n    var replset = new ReplSetState({\n      heartbeatFrequencyMS: heartbeatFrequencyMS\n    });\n\n    replset.topologyType = topologyDescription.type;\n    \/\/ For each server add them to the state\n    topologyDescription.servers.forEach(function(s) {\n      var server = new Server({\n        host: s.address.split(':')[0],\n        port: parseInt(s.address.split(':')[1], 10)\n      });\n\n      \/\/ Add additional information\n      if (s.avg_rtt_ms) server.lastIsMasterMS = s.avg_rtt_ms;\n      if (s.lastUpdateTime) server.lastUpdateTime = s.lastUpdateTime;\n      \/\/ Set the last write\n      if (s.lastWrite) {\n        server.lastWriteDate = s.lastWrite.lastWriteDate.$numberLong;\n      }\n\n      server.ismaster = {};\n      if (s.tags) server.ismaster.tags = s.tags;\n      if (s.maxWireVersion) server.ismaster.maxWireVersion = s.maxWireVersion;\n      \/\/ Ensure the server looks connected\n      server.isConnected = function() {\n        return true;\n      };\n\n      if (s.type === 'RSSecondary') {\n        server.ismaster.secondary = true;\n        replset.secondaries.push(server);\n      } else if (s.type === 'RSPrimary') {\n        server.ismaster.ismaster = true;\n        replset.primary = server;\n      } else if (s.type === 'RSArbiter') {\n        server.ismaster.arbiterOnly = true;\n        replset.arbiters.push(server);\n      }\n    });\n\n    \/\/ Calculate staleness\n    replset.updateSecondariesMaxStaleness(heartbeatFrequencyMS);\n\n    \/\/ Create read preference\n    var rp = new ReadPreference(convert(readPreference.mode), readPreference.tag_sets, {\n      maxStalenessSeconds: readPreference.maxStalenessSeconds\n    });\n\n    \/\/ Perform a pickServer\n    var server = replset.pickServer(rp);\n    var foundWindow = null;\n\n    \/\/ We expect an error\n    if (error) {\n      expect(server).to.be.an.instanceof(MongoError);\n      return callback(null, null);\n    }\n\n    \/\/ server should be in the latency window\n    for (var i = 0; i < inLatencyWindow.length; i++) {\n      var w = inLatencyWindow[i];\n\n      if (server.name === w.address) {\n        foundWindow = w;\n        break;\n      }\n    }\n\n    if (\n      ['ReplicaSetNoPrimary', 'Primary', 'ReplicaSetWithPrimary'].indexOf(\n        topologyDescription.type\n      ) !== -1 &&\n      inLatencyWindow.length === 0\n    ) {\n      if (server instanceof MongoError) {\n        expect(server.message).to.equal('maxStalenessSeconds must be set to at least 90 seconds');\n      } else {\n        expect(server).to.be.null;\n      }\n    } else {\n      expect(foundWindow).to.not.be.null;\n    }\n  } catch (err) {\n    if (file.error) return callback(null, null);\n    return callback(err, null);\n  }\n\n  callback(null, null);\n}\n","lang_cluster":"Javascript","length":148,"code_uid":"bf4dfbf3a69b41f0845903411fa78f86"}
{"diff_hunk":"@@ -18,6 +18,7 @@ bitcore.encoding.BufferWriter = require('.\/lib\/encoding\/bufferwriter');\n bitcore.encoding.Varint = require('.\/lib\/encoding\/varint');\n \n \/\/ main bitcoin library\n+bitcore.Unit = require('.\/lib\/unit');\n bitcore.Address = require('.\/lib\/address');\n bitcore.BIP32 = require('.\/lib\/bip32');\n bitcore.Block = require('.\/lib\/block');","old_code":"var bitcore = module.exports;\n\n\n\/\/ crypto \nbitcore.crypto = {};\nbitcore.crypto.BN = require('.\/lib\/crypto\/bn');\nbitcore.crypto.ECDSA = require('.\/lib\/crypto\/ecdsa');\nbitcore.crypto.Hash = require('.\/lib\/crypto\/hash');\nbitcore.crypto.Random = require('.\/lib\/crypto\/random');\nbitcore.crypto.Point = require('.\/lib\/crypto\/point');\n\n\/\/ encoding\nbitcore.encoding = {};\nbitcore.encoding.Base58 = require('.\/lib\/encoding\/base58');\nbitcore.encoding.Base58Check = require('.\/lib\/encoding\/base58check');\nbitcore.encoding.BufferReader = require('.\/lib\/encoding\/bufferreader');\nbitcore.encoding.BufferWriter = require('.\/lib\/encoding\/bufferwriter');\nbitcore.encoding.Varint = require('.\/lib\/encoding\/varint');\n\n\/\/ main bitcoin library\nbitcore.Address = require('.\/lib\/address');\nbitcore.BIP32 = require('.\/lib\/bip32');\nbitcore.Block = require('.\/lib\/block');\nbitcore.Blockheader = require('.\/lib\/blockheader');\nbitcore.Networks = require('.\/lib\/networks');\nbitcore.Opcode = require('.\/lib\/opcode');\nbitcore.PrivateKey = require('.\/lib\/privatekey');\nbitcore.PublicKey = require('.\/lib\/publickey');\nbitcore.Script = require('.\/lib\/script');\nbitcore.Signature = require('.\/lib\/signature');\nbitcore.Transaction = require('.\/lib\/transaction');\nbitcore.Txin = require('.\/lib\/txin');\nbitcore.Txout = require('.\/lib\/txout');\n\n\n\/\/dependencies, subject to change\nbitcore.deps = {};\nbitcore.deps.bnjs = require('bn.js');\nbitcore.deps.bs58 = require('bs58');\nbitcore.deps.Buffer = Buffer;\nbitcore.deps.elliptic = require('elliptic');\n\n\/\/bitcore.scriptexec = require('lib\/scriptexec');\n\/\/bitcore.tx = require('lib\/tx');\n\/\/bitcore.txpartial = require('lib\/txpartial');\n\n\/\/bitcore.bip70 = require('lib\/bip70');\n","lang_cluster":"Javascript","length":47,"code_uid":"b58452104a6c47b48e94ad87eee51924"}
{"diff_hunk":"@@ -41,14 +41,48 @@ describe( 'SettingsApp', () => {\n \t\tregistry.dispatch( CORE_USER ).receiveGetAuthentication( { needsReauthentication: false } );\n \t\tregistry.dispatch( CORE_USER ).receiveConnectURL( 'test-url' );\n \n-\t\tprovideModules( registry, [ {\n-\t\t\tslug: 'analytics',\n-\t\t\tname: 'Analytics',\n-\t\t\tactive: true,\n-\t\t\tconnected: true,\n-\t\t\tsetupComplete: true,\n-\t\t\tSettingsEditComponent: () => <div data-testid=\"edit-component\">edit<\/div>,\n-\t\t} ] );\n+\t\tprovideModules( registry, [\n+\t\t\t{\n+\t\t\t\tslug: 'analytics',\n+\t\t\t\tname: 'Analytics',\n+\t\t\t\tactive: true,\n+\t\t\t\tconnected: true,\n+\t\t\t\tsetupComplete: true,\n+\t\t\t\tSettingsEditComponent: () => <div data-testid=\"edit-component\">edit<\/div>,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tslug: 'optimize',\n+\t\t\t\tname: 'Optimize',\n+\t\t\t\tactive: true,\n+\t\t\t\tconnected: true,\n+\t\t\t\tsetupComplete: true,\n+\t\t\t\tSettingsEditComponent: () => <div data-testid=\"edit-component\">edit<\/div>,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tslug: 'tagmanager',\n+\t\t\t\tname: 'Tag Manager',\n+\t\t\t\tactive: true,\n+\t\t\t\tconnected: true,\n+\t\t\t\tsetupComplete: true,\n+\t\t\t\tSettingsEditComponent: () => <div data-testid=\"edit-component\">edit<\/div>,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tslug: 'adsense',\n+\t\t\t\tname: 'AdSense',\n+\t\t\t\tactive: true,\n+\t\t\t\tconnected: true,\n+\t\t\t\tsetupComplete: true,\n+\t\t\t\tSettingsEditComponent: () => <div data-testid=\"edit-component\">edit<\/div>,\n+\t\t\t},\n+\t\t\t{\n+\t\t\t\tslug: 'pagespeed-insights',\n+\t\t\t\tname: 'PageSpeed Insights',\n+\t\t\t\tactive: true,\n+\t\t\t\tconnected: true,\n+\t\t\t\tsetupComplete: true,\n+\t\t\t\tSettingsEditComponent: () => <div data-testid=\"edit-component\">edit<\/div>,\n+\t\t\t},\n+\t\t] );\n \n \t\tglobal._googlesitekitLegacyData.modules.analytics = {\n \t\t\t...global._googlesitekitLegacyData.modules.analytics,","old_code":"\/**\n * SettingsApp component tests.\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * Internal dependencies\n *\/\nimport SettingsApp from '.\/SettingsApp';\nimport { render, fireEvent, createTestRegistry, provideModules, waitFor } from '..\/..\/..\/..\/tests\/js\/test-utils';\nimport { CORE_USER } from '..\/..\/googlesitekit\/datastore\/user\/constants';\nimport * as fixtures from '..\/..\/modules\/analytics\/datastore\/__fixtures__';\n\ndescribe( 'SettingsApp', () => {\n\tlet registry;\n\n\tconst features = [ 'storeErrorNotifications' ];\n\tconst validResponse = {\n\t\taccountID: 'pub-12345678',\n\t\tclientID: 'ca-pub-12345678',\n\t\tuseSnippet: true,\n\t};\n\n\tbeforeEach( () => {\n\t\tglobal.location.hash = '';\n\n\t\tregistry = createTestRegistry();\n\t\tregistry.dispatch( CORE_USER ).receiveGetAuthentication( { needsReauthentication: false } );\n\t\tregistry.dispatch( CORE_USER ).receiveConnectURL( 'test-url' );\n\n\t\tprovideModules( registry, [ {\n\t\t\tslug: 'analytics',\n\t\t\tname: 'Analytics',\n\t\t\tactive: true,\n\t\t\tconnected: true,\n\t\t\tsetupComplete: true,\n\t\t\tSettingsEditComponent: () => <div data-testid=\"edit-component\">edit<\/div>,\n\t\t} ] );\n\n\t\tglobal._googlesitekitLegacyData.modules.analytics = {\n\t\t\t...global._googlesitekitLegacyData.modules.analytics,\n\t\t\tactive: true,\n\t\t\tsetupComplete: true,\n\t\t};\n\t} );\n\n\tit( 'should change location hash & DOM correctly when module accordion clicked and opened', async () => {\n\t\tfetchMock.getOnce(\n\t\t\t\/^\\\/google-site-kit\\\/v1\\\/modules\\\/analytics\\\/data\\\/settings\/,\n\t\t\t{ body: validResponse, status: 200 }\n\t\t);\n\n\t\tconst { getByRole, findByRole } = render( <SettingsApp \/>, { features, registry } );\n\n\t\tfireEvent.click( getByRole( 'tab', { name: \/analytics\/i } ) );\n\t\texpect( global.location.hash ).toEqual( '#settings\/analytics\/view' );\n\n\t\tconst analyticsPanel = await findByRole( 'tabpanel', { hidden: false } );\n\t\texpect( analyticsPanel ).toHaveAttribute( 'id', 'googlesitekit-settings-module__content--analytics' );\n\t} );\n\n\tit( 'should change location hash & DOM correctly when module accordion clicked and closed', async () => {\n\t\tconst { getByRole, findByRole, queryByRole } = render( <SettingsApp \/>, { features, registry } );\n\n\t\tfireEvent.click( getByRole( 'tab', { name: \/analytics\/i } ) );\n\n\t\tconst analyticsPanel = await findByRole( 'tabpanel' );\n\t\texpect( analyticsPanel ).toBeInTheDocument();\n\n\t\texpect( global.location.hash ).toEqual( '#settings\/analytics\/view' );\n\n\t\tfireEvent.click( getByRole( 'tab', { name: \/analytics\/i } ) );\n\t\t\/\/ A closed state should not be reflected in the hash as it is implied as default.\n\t\texpect( global.location.hash ).toEqual( '#settings' );\n\n\t\tawait waitFor( () => {\n\t\t\texpect( queryByRole( 'tabpanel' ) ).toBeNull();\n\t\t} );\n\t} );\n\n\tit( 'should change location hash & DOM correctly when module is being edited', async () => {\n\t\tfetchMock.getOnce(\n\t\t\t\/^\\\/google-site-kit\\\/v1\\\/modules\\\/analytics\\\/data\\\/accounts-properties-profiles\/,\n\t\t\t{ body: fixtures.accountsPropertiesProfiles, status: 200 }\n\t\t);\n\n\t\tconst { getByRole, findByRole, queryByTestID } = render( <SettingsApp \/>, { features, registry } );\n\n\t\tfireEvent.click( getByRole( 'tab', { name: \/analytics\/i } ) );\n\n\t\tconst analyticsPanel = await findByRole( 'tabpanel' );\n\t\texpect( analyticsPanel ).toBeInTheDocument();\n\n\t\tfireEvent.click( getByRole( 'button', { name: \/edit\/i } ) );\n\n\t\texpect( global.location.hash ).toEqual( '#settings\/analytics\/edit' );\n\n\t\tawait waitFor( () => {\n\t\t\texpect( queryByTestID( 'edit-component' ) ).toBeInTheDocument();\n\t\t} );\n\t} );\n\n\tit( 'should change location hash & DOM correctly when module is no longer being edited', async () => {\n\t\tfetchMock.getOnce(\n\t\t\t\/^\\\/google-site-kit\\\/v1\\\/modules\\\/analytics\\\/data\\\/accounts-properties-profiles\/,\n\t\t\t{ body: fixtures.accountsPropertiesProfiles, status: 200 }\n\t\t);\n\n\t\tconst { getByRole, findByRole } = render( <SettingsApp \/>, { features, registry } );\n\n\t\tfireEvent.click( getByRole( 'tab', { name: \/analytics\/i } ) );\n\n\t\tconst analyticsPanel = await findByRole( 'tabpanel' );\n\t\texpect( analyticsPanel ).toBeInTheDocument();\n\n\t\tfireEvent.click( getByRole( 'button', { name: \/edit\/i } ) );\n\t\texpect( global.location.hash ).toEqual( '#settings\/analytics\/edit' );\n\n\t\tfireEvent.click( getByRole( 'button', { name: \/cancel\/i } ) );\n\t\texpect( global.location.hash ).toEqual( '#settings\/analytics\/view' );\n\n\t\tconst finalAnalyticsPanel = await findByRole( 'tabpanel' );\n\t\texpect( finalAnalyticsPanel ).toBeInTheDocument();\n\t} );\n\n\tit( 'should change location hash & DOM correctly when tab is clicked and changed', async () => {\n\t\tconst { findByText, getAllByRole } = render( <SettingsApp \/>, { features, registry } );\n\n\t\tfireEvent.click( getAllByRole( 'tab' )[ 1 ] );\n\t\texpect( global.location.hash ).toEqual( '#connect' );\n\n\t\tconst allConnected = await findByText( \/congrats, you\u2019ve connected all services\/i );\n\t\texpect( allConnected ).toBeInTheDocument();\n\n\t\tfireEvent.click( getAllByRole( 'tab' )[ 2 ] );\n\t\texpect( global.location.hash ).toEqual( '#admin' );\n\n\t\tconst pluginStatus = await findByText( \/plugin status\/i );\n\t\texpect( pluginStatus ).toBeInTheDocument();\n\t} );\n} );\n","lang_cluster":"Javascript","length":154,"code_uid":"567f14e4e7fe4d25bd06101974fc7c91"}
{"diff_hunk":"@@ -2,10 +2,36 @@\n \n const expect = require('chai').expect;\n const { getSymbolFrom } = require('..\/tools\/utils');\n-const MongoNetworkError = require('..\/..\/src\/error').MongoNetworkError;\n+const { MongoNetworkError } = require('..\/..\/src\/index');\n+const {\n+  PoolClosedError: MongoPoolClosedError,\n+  WaitQueueTimeoutError: MongoWaitQueueTimeoutError\n+} = require('..\/..\/src\/cmap\/errors');\n \n-describe('MongoErrors', function () {\n-  describe('MongoNetworkError', function () {\n+describe('MongoErrors', () => {\n+  \/\/ import errors as object\n+  let errorClasses = Object.fromEntries(\n+    Object.entries(require('..\/..\/src\/index')).filter(([key]) => key.endsWith('Error'))\n+  );\n+  errorClasses = { ...errorClasses, MongoPoolClosedError, MongoWaitQueueTimeoutError };\n+\n+  for (const errorName in errorClasses) {\n+    describe(errorName, () => {\n+      it(`name should be read-only`, () => {\n+        \/\/ Dynamically create error class with message\n+        let error = new errorClasses[errorName]('generated by test');\n+        \/\/ expect name property to be class name\n+        expect(error).to.have.property('name', errorName);\n+\n+        try {\n+          error.name = 'renamed by test';\n+          expect(error).to.have.property('name', 'generated by test');\n+        } catch (err) {}\n+      });\n+    });\n+  }\n+\n+  describe('when MongoNetworkError is constructed', () => {\n     it('should only define beforeHandshake symbol if boolean option passed in', function () {\n       const errorWithOptionTrue = new MongoNetworkError('', { beforeHandshake: true });\n       expect(getSymbolFrom(errorWithOptionTrue, 'beforeHandshake', false)).to.be.a('symbol');","old_code":"'use strict';\n\nconst expect = require('chai').expect;\nconst { getSymbolFrom } = require('..\/tools\/utils');\nconst MongoNetworkError = require('..\/..\/src\/error').MongoNetworkError;\n\ndescribe('MongoErrors', function () {\n  describe('MongoNetworkError', function () {\n    it('should only define beforeHandshake symbol if boolean option passed in', function () {\n      const errorWithOptionTrue = new MongoNetworkError('', { beforeHandshake: true });\n      expect(getSymbolFrom(errorWithOptionTrue, 'beforeHandshake', false)).to.be.a('symbol');\n\n      const errorWithOptionFalse = new MongoNetworkError('', { beforeHandshake: false });\n      expect(getSymbolFrom(errorWithOptionFalse, 'beforeHandshake', false)).to.be.a('symbol');\n\n      const errorWithBadOption = new MongoNetworkError('', { beforeHandshake: 'not boolean' });\n      expect(getSymbolFrom(errorWithBadOption, 'beforeHandshake', false)).to.be.an('undefined');\n\n      const errorWithoutOption = new MongoNetworkError('');\n      expect(getSymbolFrom(errorWithoutOption, 'beforeHandshake', false)).to.be.an('undefined');\n    });\n  });\n});\n","lang_cluster":"Javascript","length":23,"code_uid":"c64f4fc6243e4ff2a4e3b9d41d3f90f8"}
{"diff_hunk":"@@ -15,6 +15,7 @@ export default AbstractModel.extend(DOBDays, PatientName, {\n   bloodType: DS.attr('string'),\n   clinic: DS.attr('string'),\n   country: DS.attr('string'),\n+  createOutpatientVisit: DS.attr('boolean'),\n   dateOfBirth: DS.attr('date'),\n   economicClassification: DS.attr('string'),\n   email: DS.attr('string'),","old_code":"import AbstractModel from 'hospitalrun\/models\/abstract';\nimport DOBDays from 'hospitalrun\/mixins\/dob-days';\nimport EmailValidation from 'hospitalrun\/utils\/email-validation';\nimport Ember from 'ember';\nimport DS from 'ember-data';\nimport PatientName from 'hospitalrun\/mixins\/patient-name';\n\nexport default AbstractModel.extend(DOBDays, PatientName, {\n  admitted: DS.attr('boolean', { defaultValue: false }),\n  additionalContacts: DS.attr(),\n  address: DS.attr('string'),\n  address2: DS.attr('string'),\n  address3: DS.attr('string'),\n  address4: DS.attr('string'),\n  bloodType: DS.attr('string'),\n  clinic: DS.attr('string'),\n  country: DS.attr('string'),\n  dateOfBirth: DS.attr('date'),\n  economicClassification: DS.attr('string'),\n  email: DS.attr('string'),\n  expenses: DS.attr(),\n  externalPatientId: DS.attr('string'),\n  familySupport1: DS.attr('string'),\n  familySupport2: DS.attr('string'),\n  familySupport3: DS.attr('string'),\n  familySupport4: DS.attr('string'),\n  familySupport5: DS.attr('string'),\n  friendlyId: DS.attr('string'),\n  familyInfo: DS.attr(),\n  firstName: DS.attr('string'),\n  sex: DS.attr('string'),\n  occupation: DS.attr('string'),\n  history: DS.attr('string'),\n  insurance: DS.attr('string'),\n  lastName: DS.attr('string'),\n  livingArrangement: DS.attr('string'),\n  middleName: DS.attr('string'),\n  notes: DS.attr('string'),\n  otherIncome: DS.attr('string'),\n  payments: DS.hasMany('payment', {\n    async: true\n  }),\n  patientType: DS.attr('string'),\n  parent: DS.attr('string'),\n  paymentProfile: DS.belongsTo('price-profile', {\n    async: false\n  }),\n  phone: DS.attr('string'),\n  placeOfBirth: DS.attr('string'),\n  referredDate: DS.attr('date'),\n  referredBy: DS.attr('string'),\n  religion: DS.attr('string'),\n  socialActionTaken: DS.attr('string'),\n  socialRecommendation: DS.attr('string'),\n  status: DS.attr('string'),\n\n  age: function() {\n    var dob = this.get('dateOfBirth');\n    return this.convertDOBToText(dob);\n  }.property('dateOfBirth'),\n\n  displayAddress: function() {\n    var addressFields = this.getProperties('address', 'address2', 'address3', 'address4'),\n      displayAddress = '';\n    for (var prop in addressFields) {\n      if (!Ember.isEmpty(addressFields[prop])) {\n        if (!Ember.isEmpty(displayAddress)) {\n          displayAddress += ', ';\n        }\n        displayAddress += addressFields[prop];\n      }\n    }\n    return displayAddress;\n  }.property('address', 'address2', 'address3', 'address4'),\n\n  displayName: function() {\n    return this.getPatientDisplayName(this);\n  }.property('firstName', 'lastName', 'middleName'),\n\n  displayPatientId: function() {\n    return this.getPatientDisplayId(this);\n  }.property('id', 'externalPatientId', 'friendlyId'),\n\n  validations: {\n    email: {\n      format: {\n        with: EmailValidation.emailRegex,\n        allowBlank: true,\n        message: 'please enter a valid email address'\n      }\n    },\n    friendlyId: {\n      presence: true\n    },\n    firstName: {\n      presence: true\n    },\n    lastName: {\n      presence: true\n    }\n  }\n\n});\n","lang_cluster":"Javascript","length":103,"code_uid":"4eaabba3e1c24bf595e9412ff962974b"}
{"diff_hunk":"@@ -6,6 +6,17 @@ let null_dict = null_dictionary || {}\n \/\/ eslint-disable-next-line no-undef\n assert_true(dict.X !== undefined && dict.doSomething !== undefined);  \/\/ Testing successful object creation.\n \n+\n+\/*\n+    It seems that Object.values(dict) is not supported on JSC for non-static fields, it would be interesting to\n+    review this in the future.\n+    verify_object_fields(Object.values(dict).length)\n+ *\/\n+\n+\/\/ eslint-disable-next-line no-undef\n+verify_object_fields(Object.keys(dict).length)\n+\n+\n null_dict.hello(true);  \/\/ Testing method call from object.\n null_dict.alo(true); \/\/ Testing method call from object <again>.\n ","old_code":"\/\/ eslint-disable-next-line no-undef,strict\nlet dict = dictionary || {}\n\/\/ eslint-disable-next-line no-undef\nlet null_dict = null_dictionary || {}\n\n\/\/ eslint-disable-next-line no-undef\nassert_true(dict.X !== undefined && dict.doSomething !== undefined);  \/\/ Testing successful object creation.\n\nnull_dict.hello(true);  \/\/ Testing method call from object.\nnull_dict.alo(true); \/\/ Testing method call from object <again>.\n\n\ndict.doSomething(28850);\n\n\/* Testing accessors. *\/\ndict.X=666;\n\n\/\/ eslint-disable-next-line no-undef\ntest_accessor(dict, 'X', 666);\n\nassert_enumerate(JSON.stringify(dict))\n\n\/*\n    Testing exception mechanism.\n*\/\ntry {\n    dict.X = -1  \/\/ testing wrong value.\n}catch(error){\n    assert_exception(error.message)\n}","lang_cluster":"Javascript","length":30,"code_uid":"dc67801a819a46b9b613334d7a593387"}
{"diff_hunk":"@@ -9,6 +9,10 @@ const databaseNamespace = require('.\/shared').databaseNamespace;\n const isTransactionCommand = require('..\/transactions').isTransactionCommand;\n const applySession = require('..\/sessions').applySession;\n \n+function isClientEncryptionEnabled(server) {\n+  return server && server.s && server.s.options && server.s.options.autoEncrypter;\n+}\n+\n function command(server, ns, cmd, options, callback) {\n   if (typeof options === 'function') (callback = options), (options = {});\n   options = options || {};","old_code":"'use strict';\n\nconst Query = require('..\/connection\/commands').Query;\nconst Msg = require('..\/connection\/msg').Msg;\nconst MongoError = require('..\/error').MongoError;\nconst getReadPreference = require('.\/shared').getReadPreference;\nconst isSharded = require('.\/shared').isSharded;\nconst databaseNamespace = require('.\/shared').databaseNamespace;\nconst isTransactionCommand = require('..\/transactions').isTransactionCommand;\nconst applySession = require('..\/sessions').applySession;\n\nfunction command(server, ns, cmd, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  if (cmd == null) {\n    return callback(new MongoError(`command ${JSON.stringify(cmd)} does not return a cursor`));\n  }\n\n  const bson = server.s.bson;\n  const pool = server.s.pool;\n  const readPreference = getReadPreference(cmd, options);\n  const shouldUseOpMsg = supportsOpMsg(server);\n  const session = options.session;\n\n  let clusterTime = server.clusterTime;\n  let finalCmd = Object.assign({}, cmd);\n  if (hasSessionSupport(server) && session) {\n    if (\n      session.clusterTime &&\n      session.clusterTime.clusterTime.greaterThan(clusterTime.clusterTime)\n    ) {\n      clusterTime = session.clusterTime;\n    }\n\n    const err = applySession(session, finalCmd, options);\n    if (err) {\n      return callback(err);\n    }\n  }\n\n  \/\/ if we have a known cluster time, gossip it\n  if (clusterTime) {\n    finalCmd.$clusterTime = clusterTime;\n  }\n\n  if (\n    isSharded(server) &&\n    !shouldUseOpMsg &&\n    readPreference &&\n    readPreference.preference !== 'primary'\n  ) {\n    finalCmd = {\n      $query: finalCmd,\n      $readPreference: readPreference.toJSON()\n    };\n  }\n\n  const commandOptions = Object.assign(\n    {\n      command: true,\n      numberToSkip: 0,\n      numberToReturn: -1,\n      checkKeys: false\n    },\n    options\n  );\n\n  \/\/ This value is not overridable\n  commandOptions.slaveOk = readPreference.slaveOk();\n\n  const cmdNs = `${databaseNamespace(ns)}.$cmd`;\n  const message = shouldUseOpMsg\n    ? new Msg(bson, cmdNs, finalCmd, commandOptions)\n    : new Query(bson, cmdNs, finalCmd, commandOptions);\n\n  const inTransaction = session && (session.inTransaction() || isTransactionCommand(finalCmd));\n  const commandResponseHandler = inTransaction\n    ? function(err) {\n        if (\n          !cmd.commitTransaction &&\n          err &&\n          err instanceof MongoError &&\n          err.hasErrorLabel('TransientTransactionError')\n        ) {\n          session.transaction.unpinServer();\n        }\n\n        return callback.apply(null, arguments);\n      }\n    : callback;\n\n  try {\n    pool.write(message, commandOptions, commandResponseHandler);\n  } catch (err) {\n    commandResponseHandler(err);\n  }\n}\n\nfunction hasSessionSupport(topology) {\n  if (topology == null) return false;\n  if (topology.description) {\n    return topology.description.maxWireVersion >= 6;\n  }\n\n  return topology.ismaster == null ? false : topology.ismaster.maxWireVersion >= 6;\n}\n\nfunction supportsOpMsg(topologyOrServer) {\n  const description = topologyOrServer.ismaster\n    ? topologyOrServer.ismaster\n    : topologyOrServer.description;\n\n  if (description == null) {\n    return false;\n  }\n\n  return description.maxWireVersion >= 6 && description.__nodejs_mock_server__ == null;\n}\n\nmodule.exports = command;\n","lang_cluster":"Javascript","length":121,"code_uid":"af32e7f889c34e9fb3fae79d6402c519"}
{"diff_hunk":"@@ -1,5 +1,6 @@\n const allowedE2EModules = [\n   'window',\n+  'hyperformula*',\n   'jasmine-co',\n   'jest-matcher-utils',\n   'html-parse-stringify',","old_code":"const allowedE2EModules = [\n  'window',\n  'jasmine-co',\n  'jest-matcher-utils',\n  'html-parse-stringify',\n  'core-js\/*',\n  'regenerator-runtime\/runtime*',\n  '@babel\/runtime\/*',\n  '.\/htmlNormalize',\n  '.\/common',\n  '.\/mouseEvents',\n  '.\/..\/bootstrap',\n  '.\/helpers\/custom-matchers',\n  '.\/asciiTable',\n  '.\/MemoryLeakTest',\n  '..\/MemoryLeakTest',\n];\n\nconst babelPresetConfig = () => ({\n  targets: {\n    chrome: '41',\n    firefox: '34',\n    ie: '9',\n    safari: '9'\n  },\n  modules: false,\n  debug: false,\n  useBuiltIns: 'usage',\n  corejs: 3,\n});\n\nmodule.exports = {\n  presets: [\n    ['@babel\/preset-env', babelPresetConfig()]\n  ],\n  plugins: [\n    ['@babel\/plugin-proposal-object-rest-spread', { useBuiltIns: true }],\n    ['transform-inline-environment-variables'],\n    ['@babel\/plugin-proposal-class-properties']\n  ],\n  env: {\n    \/\/ Environment for unit testing, source code and languages building via webpack (UMD).\n    commonjs: {\n      plugins: [\n        ['@babel\/plugin-transform-runtime', {\n          corejs: false,\n          helpers: true,\n          regenerator: true,\n          useESModules: false,\n        }],\n        ['@babel\/plugin-transform-modules-commonjs', { loose: true }]\n      ]\n    },\n    \/\/ Environment for transpiling files to be compatible with CommonJS.\n    commonjs_dist: {\n      plugins: [\n        ['@babel\/plugin-transform-modules-commonjs', { loose: true }],\n        ['babel-plugin-transform-require-ignore', { extensions: ['.css'] }]\n      ],\n      ignore: [\n        '**\/__tests__\/**',\n        '**\/test\/**',\n        '**\/dist\/**',\n      ]\n    },\n    \/\/ Environment for transpiling files to be compatible with ES Modules.\n    es: {\n      plugins: [\n        ['babel-plugin-transform-require-ignore', { extensions: ['.css'] }],\n        ['.\/.config\/plugin\/babel\/add-import-extension.js', { extension: 'mjs' }]\n      ],\n      ignore: [\n        '**\/__tests__\/**',\n        '**\/test\/**',\n        '**\/dist\/**',\n      ],\n    },\n    \/\/ Environment for transpiling only legacy language files (e.q. import `languages\/pl-PL`)\n    \/\/ which need to be compatible with ES Modules. That format, by default, automatically\n    \/\/ registers the language pack. It's not suitable to use with the modularized version of\n    \/\/ the Handsontable.\n    es_languages: {\n      plugins: [\n        ['babel-plugin-transform-require-ignore', { extensions: ['.css'] }],\n        ['.\/.config\/plugin\/babel\/add-import-extension.js', { extension: 'mjs' }],\n        ['.\/.config\/plugin\/babel\/add-language-registration.js'],\n      ],\n    },\n    \/\/ Environment for building E2E tests (UMD).\n    commonjs_e2e: {\n      plugins: [\n        ['@babel\/plugin-transform-runtime', {\n          corejs: false,\n          helpers: true,\n          regenerator: true,\n          useESModules: false,\n        }],\n        ['@babel\/plugin-transform-modules-commonjs', { loose: true }],\n        ['babel-plugin-forbidden-imports', {\n          allowedModules: allowedE2EModules\n        }]\n      ],\n    },\n  },\n  ignore: [\n    'src\/3rdparty\/walkontable\/dist\/',\n    'src\/3rdparty\/walkontable\/test\/dist\/',\n  ],\n};\n","lang_cluster":"Javascript","length":109,"code_uid":"dda691556d274a98b0331e51c9aa1f1b"}
{"diff_hunk":"@@ -45,12 +45,18 @@ class SearchConsoleDashboardWidgetSiteStats extends Component {\n \n \tsetOptions() {\n \t\tconst { selectedStats, series, vAxes } = this.props;\n+\t\tconst { pageTitle } = global.googlesitekit;\n \n-\t\tconst pageTitle = global.googlesitekit.pageTitle && global.googlesitekit.pageTitle.length ? sprintf( __( 'Search Traffic Summary for %s', 'google-site-kit' ), decodeHtmlEntity( global.googlesitekit.pageTitle ) ) : __( 'Search Traffic Summary', 'google-site-kit' );\n+\t\tlet title = __( 'Search Traffic Summary', 'google-site-kit' );\n+\n+\t\tif ( pageTitle && pageTitle.length ) {\n+\t\t\t\/* translators: %s: page title *\/\n+\t\t\ttitle = sprintf( __( 'Search Traffic Summary for %s', 'google-site-kit' ), decodeHtmlEntity( pageTitle ) );\n+\t\t}\n \n \t\tconst options = {\n \t\t\tchart: {\n-\t\t\t\ttitle: pageTitle,\n+\t\t\t\ttitle,\n \t\t\t},\n \t\t\tcurveType: 'line',\n \t\t\theight: 270,","old_code":"\/**\n * SearchConsoleDashboardWidgetSiteStats component.\n *\n * Site Kit by Google, Copyright 2019 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * External dependencies\n *\/\nimport withData from 'GoogleComponents\/higherorder\/withdata';\nimport { TYPE_MODULES } from 'GoogleComponents\/data';\nimport GoogleChart from 'GoogleComponents\/google-chart.js';\nimport PreviewBlock from 'GoogleComponents\/preview-block';\nimport { decodeHtmlEntity, getTimeInSeconds } from 'GoogleUtil';\n\n\/**\n * Internal dependencies\n *\/\nimport { extractSearchConsoleDashboardData } from '.\/util';\n\n\/**\n * WordPress dependencies\n *\/\nimport { __, sprintf } from '@wordpress\/i18n';\nimport { Component } from '@wordpress\/element';\n\nclass SearchConsoleDashboardWidgetSiteStats extends Component {\n\tconstructor( props ) {\n\t\tsuper( props );\n\n\t\tthis.setOptions = this.setOptions.bind( this );\n\t}\n\n\tsetOptions() {\n\t\tconst { selectedStats, series, vAxes } = this.props;\n\n\t\tconst pageTitle = global.googlesitekit.pageTitle && global.googlesitekit.pageTitle.length ? sprintf( __( 'Search Traffic Summary for %s', 'google-site-kit' ), decodeHtmlEntity( global.googlesitekit.pageTitle ) ) : __( 'Search Traffic Summary', 'google-site-kit' );\n\n\t\tconst options = {\n\t\t\tchart: {\n\t\t\t\ttitle: pageTitle,\n\t\t\t},\n\t\t\tcurveType: 'line',\n\t\t\theight: 270,\n\t\t\twidth: '100%',\n\t\t\tchartArea: {\n\t\t\t\theight: '80%',\n\t\t\t\twidth: '87%',\n\t\t\t},\n\t\t\tlegend: {\n\t\t\t\tposition: 'top',\n\t\t\t\ttextStyle: {\n\t\t\t\t\tcolor: '#616161',\n\t\t\t\t\tfontSize: 12,\n\t\t\t\t},\n\t\t\t},\n\t\t\thAxis: {\n\t\t\t\tformat: 'M\/d\/yy',\n\t\t\t\tgridlines: {\n\t\t\t\t\tcolor: '#fff',\n\t\t\t\t},\n\t\t\t\ttextStyle: {\n\t\t\t\t\tcolor: '#616161',\n\t\t\t\t\tfontSize: 12,\n\t\t\t\t},\n\t\t\t},\n\t\t\tvAxis: {\n\t\t\t\tgridlines: {\n\t\t\t\t\tcolor: '#eee',\n\t\t\t\t},\n\t\t\t\tminorGridlines: {\n\t\t\t\t\tcolor: '#eee',\n\t\t\t\t},\n\t\t\t\ttextStyle: {\n\t\t\t\t\tcolor: '#616161',\n\t\t\t\t\tfontSize: 12,\n\t\t\t\t},\n\t\t\t\ttitleTextStyle: {\n\t\t\t\t\tcolor: '#616161',\n\t\t\t\t\tfontSize: 12,\n\t\t\t\t\titalic: false,\n\t\t\t\t},\n\t\t\t},\n\t\t};\n\n\t\toptions.series = series;\n\t\toptions.vAxes = vAxes;\n\n\t\t\/\/ Clean up chart if more than three stats are selected.\n\t\tif ( 3 <= selectedStats.length ) {\n\t\t\toptions.vAxis.textPosition = 'none';\n\t\t\toptions.vAxis.gridlines.color = '#fff';\n\t\t\toptions.vAxis.minorGridlines.color = '#fff';\n\t\t\toptions.chartArea.width = '98%';\n\t\t}\n\n\t\treturn options;\n\t}\n\n\trender() {\n\t\tconst { data, selectedStats } = this.props;\n\n\t\tif ( ! data || ! data.length ) {\n\t\t\treturn null;\n\t\t}\n\n\t\tconst options = this.setOptions();\n\t\tconst processedData = extractSearchConsoleDashboardData( data );\n\n\t\treturn (\n\t\t\t<section className=\"mdc-layout-grid\">\n\t\t\t\t<div className=\"mdc-layout-grid__inner\">\n\t\t\t\t\t<div className=\"mdc-layout-grid__cell mdc-layout-grid__cell--span-12\">\n\t\t\t\t\t\t<GoogleChart\n\t\t\t\t\t\t\tselectedStats={ selectedStats }\n\t\t\t\t\t\t\tdata={ processedData.dataMap }\n\t\t\t\t\t\t\toptions={ options }\n\t\t\t\t\t\t\tsingleStat={ false }\n\t\t\t\t\t\t\/>\n\t\t\t\t\t<\/div>\n\t\t\t\t<\/div>\n\t\t\t<\/section>\n\t\t);\n\t}\n}\n\nexport default withData(\n\tSearchConsoleDashboardWidgetSiteStats,\n\t[\n\t\t{\n\t\t\ttype: TYPE_MODULES,\n\t\t\tidentifier: 'search-console',\n\t\t\tdatapoint: 'searchanalytics',\n\t\t\tdata: {\n\t\t\t\tdimensions: 'date',\n\t\t\t\tcompareDateRanges: true,\n\t\t\t},\n\t\t\tpriority: 1,\n\t\t\tmaxAge: getTimeInSeconds( 'day' ),\n\t\t\tcontext: 'Single',\n\t\t},\n\t],\n\t<PreviewBlock width=\"100%\" height=\"270px\" padding \/>,\n\t{ createGrid: true }\n);\n","lang_cluster":"Javascript","length":157,"code_uid":"6bf92895f9a44e0ebbe98466e3f881d8"}
{"diff_hunk":"@@ -19,28 +19,23 @@\n \/**\n  * WordPress dependencies\n  *\/\n-import { Component, createPortal } from '@wordpress\/element';\n+import { createPortal, useEffect, useState } from '@wordpress\/element';\n \n-export default class Modal extends Component {\n-\tconstructor( props ) {\n-\t\tsuper( props );\n-\t\tthis.el = document.createElement( 'div' );\n-\t\t\/\/ This class is the outermost wrapper which is present on all Site Kit pages.\n-\t\tthis.root = document.querySelector( '.googlesitekit-plugin' ) || document.body;\n-\t}\n+function Modal( { children } ) {\n+\t\/\/ Using state as we need `el` to not change when the component re-renders\n+\tconst [ el ] = useState( document.createElement( 'div' ) );\n \n-\tcomponentDidMount() {\n-\t\tthis.root.appendChild( this.el );\n-\t}\n+\tuseEffect( () => {\n+\t\tconst root = document.querySelector( '.googlesitekit-plugin' ) || document.body;\n+\t\troot.appendChild( el );\n \n-\tcomponentWillUnmount() {\n-\t\tthis.root.removeChild( this.el );\n-\t}\n+\t\treturn () => root.removeChild( el );\n+\t}, [] );\n \n-\trender() {\n-\t\treturn createPortal(\n-\t\t\tthis.props.children,\n-\t\t\tthis.el,\n-\t\t);\n-\t}\n+\treturn createPortal(\n+\t\tchildren,\n+\t\tel,\n+\t);\n }\n+\n+export default Modal;","old_code":"\/**\n * Modal component.\n *\n * Site Kit by Google, Copyright 2019 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * WordPress dependencies\n *\/\nimport { Component, createPortal } from '@wordpress\/element';\n\nexport default class Modal extends Component {\n\tconstructor( props ) {\n\t\tsuper( props );\n\t\tthis.el = document.createElement( 'div' );\n\t\t\/\/ This class is the outermost wrapper which is present on all Site Kit pages.\n\t\tthis.root = document.querySelector( '.googlesitekit-plugin' ) || document.body;\n\t}\n\n\tcomponentDidMount() {\n\t\tthis.root.appendChild( this.el );\n\t}\n\n\tcomponentWillUnmount() {\n\t\tthis.root.removeChild( this.el );\n\t}\n\n\trender() {\n\t\treturn createPortal(\n\t\t\tthis.props.children,\n\t\t\tthis.el,\n\t\t);\n\t}\n}\n","lang_cluster":"Javascript","length":46,"code_uid":"53166301602744f8a9dbcc6bbf9fc81d"}
{"diff_hunk":"@@ -1,5 +1,5 @@\n const moment = require('moment');\n-const packageBody = require('.\/package.json');\n+const packageBody = require('.\/handsontable\/package.json');\n \n module.exports = {\n   HOT_FILENAME: 'handsontable',","old_code":"const moment = require('moment');\nconst packageBody = require('.\/package.json');\n\nmodule.exports = {\n  HOT_FILENAME: 'handsontable',\n  HOT_VERSION: packageBody.version,\n  HOT_PACKAGE_NAME: packageBody.name,\n  HOT_BUILD_DATE: moment().format('DD\/MM\/YYYY HH:mm:ss'),\n  HOT_RELEASE_DATE: '29\/09\/2021',\n};\n","lang_cluster":"Javascript","length":10,"code_uid":"15f0cea13519441ab946b8ceff700f1f"}
{"diff_hunk":"@@ -1,4 +1,11 @@\n var parent = axe.commons.dom.getComposedParent(node);\n-return (['UL', 'OL'].includes(parent.nodeName.toUpperCase()) ||\n-    (parent.getAttribute('role') || '').toLowerCase() === 'list');\n-  \n+\n+var parentRole = (parent.getAttribute('role') || '').toLowerCase();\n+\n+var isListRole = parentRole === 'list';\n+\n+return (\n+\t(['UL', 'OL'].includes(parent.nodeName.toUpperCase()) &&\n+\t\t(!parentRole || isListRole)) ||\n+\tisListRole\n+);","old_code":"var parent = axe.commons.dom.getComposedParent(node);\nreturn (['UL', 'OL'].includes(parent.nodeName.toUpperCase()) ||\n    (parent.getAttribute('role') || '').toLowerCase() === 'list');\n  ","lang_cluster":"Javascript","length":4,"code_uid":"7a4dcc15526d46f6878ed515249e9427"}
{"diff_hunk":"@@ -1,12 +1,27 @@\n-\/**\n- *\n- * sends auth token to uppy client\n- *\/\n const { URL } = require('url')\n const tokenService = require('..\/helpers\/jwt')\n const { hasMatch, sanitizeHtml } = require('..\/helpers\/utils')\n const oAuthState = require('..\/helpers\/oauth-state')\n-const versionCmp = require('..\/helpers\/version')\n+\n+\/**\n+ *\n+ * @param {string} token uppy auth token\n+ * @param {string} origin url string\n+ *\/\n+const htmlContent = (token, origin) => {\n+  return `\n+    <!DOCTYPE html>\n+    <html>\n+    <head>\n+        <meta charset=\"utf-8\" \/>\n+        <script>\n+          window.opener.postMessage(JSON.stringify({token: \"${token}\"}), \"${sanitizeHtml(origin)}\")\n+          window.close()\n+        <\/script>\n+    <\/head>\n+    <body><\/body>\n+    <\/html>`\n+}\n \n \/**\n  *","old_code":"\/**\n *\n * sends auth token to uppy client\n *\/\nconst { URL } = require('url')\nconst tokenService = require('..\/helpers\/jwt')\nconst { hasMatch, sanitizeHtml } = require('..\/helpers\/utils')\nconst oAuthState = require('..\/helpers\/oauth-state')\nconst versionCmp = require('..\/helpers\/version')\n\n\/**\n *\n * @param {object} req\n * @param {object} res\n * @param {Function} next\n *\/\nmodule.exports = function sendToken (req, res, next) {\n  const uppyAuthToken = req.companion.authToken\n  \/\/ some providers need the token in cookies for thumbnail\/image requests\n  if (req.companion.provider.needsCookieAuth) {\n    tokenService.addToCookies(res, uppyAuthToken, req.companion.options, req.companion.provider.authProvider)\n  }\n\n  const dynamic = (req.session.grant || {}).dynamic || {}\n  const { state } = dynamic\n  if (state) {\n    const origin = oAuthState.getFromState(state, 'origin', req.companion.options.secret)\n    const clientVersion = oAuthState.getFromState(\n      state,\n      'clientVersion',\n      req.companion.options.secret\n    )\n    const allowedClients = req.companion.options.clients\n    \/\/ if no preset clients then allow any client\n    if (!allowedClients || hasMatch(origin, allowedClients) || hasMatch((new URL(origin)).host, allowedClients)) {\n      const allowsStringMessage = versionCmp.gte(clientVersion, '1.0.2')\n      return res.send(allowsStringMessage ? htmlContent(uppyAuthToken, origin) : oldHtmlContent(uppyAuthToken, origin))\n    }\n  }\n  next()\n}\n\n\/**\n *\n * @param {string} token uppy auth token\n * @param {string} origin url string\n *\/\nconst htmlContent = (token, origin) => {\n  return `\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <meta charset=\"utf-8\" \/>\n        <script>\n          window.opener.postMessage(JSON.stringify({token: \"${token}\"}), \"${sanitizeHtml(origin)}\")\n          window.close()\n        <\/script>\n    <\/head>\n    <body><\/body>\n    <\/html>`\n}\n\n\/**\n * @todo remove this function in next major release\n * @param {string} token uppy auth token\n * @param {string} origin url string\n *\/\nconst oldHtmlContent = (token, origin) => {\n  return `\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <meta charset=\"utf-8\" \/>\n        <script>\n          window.opener.postMessage({token: \"${token}\"}, \"${sanitizeHtml(origin)}\")\n          window.close()\n        <\/script>\n    <\/head>\n    <body><\/body>\n    <\/html>`\n}\n","lang_cluster":"Javascript","length":81,"code_uid":"1fb652a4ca334e7c9edde7083b816dbc"}
{"diff_hunk":"@@ -25,9 +25,11 @@ import {\n \tsubscribeUntil,\n \tunsubscribeFromAll,\n } from 'tests\/js\/utils';\n-import { INITIAL_STATE } from '.\/index';\n+import store from '.\/index';\n import { STORE_NAME } from '.\/constants';\n \n+const { INITIAL_STATE } = store;\n+\n describe( 'core\/site site info', () => {\n \tconst baseInfoVar = '_googlesitekitBaseData';\n \tconst baseInfo = {","old_code":"\/**\n * core\/site data store: site info tests.\n *\n * Site Kit by Google, Copyright 2020 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https:\/\/www.apache.org\/licenses\/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\/\n\n\/**\n * Internal dependencies\n *\/\nimport {\n\tcreateTestRegistry,\n\tmuteConsole,\n\tsubscribeUntil,\n\tunsubscribeFromAll,\n} from 'tests\/js\/utils';\nimport { INITIAL_STATE } from '.\/index';\nimport { STORE_NAME } from '.\/constants';\n\ndescribe( 'core\/site site info', () => {\n\tconst baseInfoVar = '_googlesitekitBaseData';\n\tconst baseInfo = {\n\t\tadminURL: 'http:\/\/something.test\/wp-admin',\n\t\tampMode: 'reader',\n\t\thomeURL: 'http:\/\/something.test\/homepage',\n\t\treferenceSiteURL: 'http:\/\/something.test',\n\t};\n\tconst entityInfoVar = '_googlesitekitEntityData';\n\tconst entityInfo = {\n\t\tcurrentEntityURL: 'http:\/\/something.test',\n\t\tcurrentEntityType: 'post',\n\t\tcurrentEntityTitle: 'Something Witty',\n\t\tcurrentEntityID: '4',\n\t};\n\tlet registry;\n\n\tbeforeEach( () => {\n\t\tregistry = createTestRegistry();\n\t} );\n\n\tafterEach( () => {\n\t\tdelete global[ baseInfoVar ];\n\t\tdelete global[ entityInfoVar ];\n\t\tunsubscribeFromAll( registry );\n\t} );\n\n\tdescribe( 'actions', () => {\n\t\tdescribe( 'receiveSiteInfo', () => {\n\t\t\tit( 'requires the siteInfo param', () => {\n\t\t\t\texpect( () => {\n\t\t\t\t\tregistry.dispatch( STORE_NAME ).receiveSiteInfo();\n\t\t\t\t} ).toThrow( 'siteInfo is required.' );\n\t\t\t} );\n\n\t\t\tit( 'receives and sets site info ', async () => {\n\t\t\t\tawait registry.dispatch( STORE_NAME ).receiveSiteInfo( { ...baseInfo, ...entityInfo } );\n\n\t\t\t\texpect(\n\t\t\t\t\tregistry.select( STORE_NAME ).getSiteInfo()\n\t\t\t\t).toMatchObject( { ...baseInfo, ...entityInfo, currentEntityID: 4 } );\n\t\t\t} );\n\t\t} );\n\t} );\n\n\tdescribe( 'selectors', () => {\n\t\tdescribe( 'getSiteInfo', () => {\n\t\t\tit( 'uses a resolver to load site info from a global variable by default, then deletes that global variable after consumption', async () => {\n\t\t\t\tglobal[ baseInfoVar ] = baseInfo;\n\t\t\t\tglobal[ entityInfoVar ] = entityInfo;\n\n\t\t\t\texpect( global[ baseInfoVar ] ).not.toEqual( undefined );\n\t\t\t\texpect( global[ entityInfoVar ] ).not.toEqual( undefined );\n\n\t\t\t\tregistry.select( STORE_NAME ).getSiteInfo();\n\t\t\t\tawait subscribeUntil( registry,\n\t\t\t\t\t() => (\n\t\t\t\t\t\tregistry.select( STORE_NAME ).getSiteInfo() !== INITIAL_STATE\n\t\t\t\t\t),\n\t\t\t\t);\n\n\t\t\t\tconst info = registry.select( STORE_NAME ).getSiteInfo();\n\n\t\t\t\texpect( info ).toEqual( { ...baseInfo, ...entityInfo, currentEntityID: 4 } );\n\n\t\t\t\t\/\/ Data must not be wiped after retrieving, as it could be used by other dependants.\n\t\t\t\texpect( global[ baseInfoVar ] ).not.toEqual( undefined );\n\t\t\t\texpect( global[ entityInfoVar ] ).not.toEqual( undefined );\n\t\t\t} );\n\n\t\t\tit( 'will return initial state (undefined) when no data is available', async () => {\n\t\t\t\texpect( global[ baseInfoVar ] ).toEqual( undefined );\n\t\t\t\texpect( global[ entityInfoVar ] ).toEqual( undefined );\n\n\t\t\t\tmuteConsole( 'error' );\n\t\t\t\tconst info = registry.select( STORE_NAME ).getSiteInfo();\n\n\t\t\t\texpect( info ).toBe( INITIAL_STATE.siteInfo );\n\t\t\t} );\n\t\t} );\n\n\t\tdescribe.each( [\n\t\t\t[ 'getAdminURL' ],\n\t\t\t[ 'getAMPMode' ],\n\t\t\t[ 'getCurrentEntityID' ],\n\t\t\t[ 'getCurrentEntityTitle' ],\n\t\t\t[ 'getCurrentEntityType' ],\n\t\t\t[ 'getCurrentEntityURL' ],\n\t\t\t[ 'getHomeURL' ],\n\t\t\t[ 'getReferenceSiteURL' ],\n\t\t] )( `%i()`, ( selector ) => {\n\t\t\tit( 'uses a resolver to load site info then returns the info when this specific selector is used', async () => {\n\t\t\t\tglobal[ baseInfoVar ] = baseInfo;\n\t\t\t\tglobal[ entityInfoVar ] = entityInfo;\n\n\t\t\t\tregistry.select( STORE_NAME )[ selector ]();\n\t\t\t\tawait subscribeUntil( registry,\n\t\t\t\t\t() => (\n\t\t\t\t\t\tregistry.select( STORE_NAME )[ selector ]() !== undefined\n\t\t\t\t\t),\n\t\t\t\t);\n\n\t\t\t\tconst info = registry.select( STORE_NAME ).getSiteInfo();\n\n\t\t\t\texpect( info ).toEqual( { ...baseInfo, ...entityInfo, currentEntityID: 4 } );\n\t\t\t} );\n\n\t\t\tit( 'will return initial state (undefined) when no data is available', async () => {\n\t\t\t\texpect( global[ baseInfoVar ] ).toEqual( undefined );\n\t\t\t\texpect( global[ entityInfoVar ] ).toEqual( undefined );\n\n\t\t\t\tmuteConsole( 'error' );\n\t\t\t\tconst result = registry.select( STORE_NAME )[ selector ]();\n\n\t\t\t\texpect( result ).toEqual( undefined );\n\t\t\t} );\n\t\t} );\n\n\t\tdescribe( 'isAmp', () => {\n\t\t\tit( 'uses a resolver to load site info, then returns true if AMP mode is set', async () => {\n\t\t\t\tglobal[ baseInfoVar ] = baseInfo;\n\t\t\t\tglobal[ entityInfoVar ] = entityInfo;\n\n\t\t\t\tregistry.select( STORE_NAME ).isAmp();\n\t\t\t\tawait subscribeUntil( registry,\n\t\t\t\t\t() => (\n\t\t\t\t\t\tregistry.select( STORE_NAME ).isAmp() !== undefined\n\t\t\t\t\t),\n\t\t\t\t);\n\n\t\t\t\tconst isAmp = registry.select( STORE_NAME ).isAmp();\n\n\t\t\t\texpect( isAmp ).toEqual( true );\n\t\t\t} );\n\n\t\t\tit( 'uses a resolver to load site info, then returns false if AMP mode is not set', async () => {\n\t\t\t\tglobal[ baseInfoVar ] = {\n\t\t\t\t\t...baseInfo,\n\t\t\t\t\tampMode: null,\n\t\t\t\t};\n\t\t\t\tglobal[ entityInfoVar ] = entityInfo;\n\n\t\t\t\tregistry.select( STORE_NAME ).isAmp();\n\t\t\t\tawait subscribeUntil( registry,\n\t\t\t\t\t() => (\n\t\t\t\t\t\tregistry.select( STORE_NAME ).isAmp() !== undefined\n\t\t\t\t\t),\n\t\t\t\t);\n\n\t\t\t\tconst isAmp = registry.select( STORE_NAME ).isAmp();\n\n\t\t\t\texpect( isAmp ).toEqual( false );\n\t\t\t} );\n\n\t\t\tit( 'will return initial state (undefined) when no data is available', async () => {\n\t\t\t\texpect( global[ baseInfoVar ] ).toEqual( undefined );\n\t\t\t\texpect( global[ entityInfoVar ] ).toEqual( undefined );\n\n\t\t\t\tmuteConsole( 'error' );\n\t\t\t\tconst result = registry.select( STORE_NAME ).isAmp();\n\n\t\t\t\texpect( result ).toEqual( undefined );\n\t\t\t} );\n\t\t} );\n\t} );\n} );\n","lang_cluster":"Javascript","length":196,"code_uid":"5ecb42492f344b539fe6dd68289dd7ca"}
{"diff_hunk":"@@ -32,19 +32,19 @@ const getConfigFromEnv = () => {\n     providerOptions: {\n       google: {\n         key: process.env.COMPANION_GOOGLE_KEY,\n-        secret: process.env.COMPANION_GOOGLE_SECRET\n+        secret: typeof process.env.COMPANION_GOOGLE_SECRET_FILE === 'string' ? fs.readFileSync(process.env.COMPANION_GOOGLE_SECRET_FILE).toString() : process.env.COMPANION_GOOGLE_SECRET\n       },\n       dropbox: {\n         key: process.env.COMPANION_DROPBOX_KEY,\n-        secret: process.env.COMPANION_DROPBOX_SECRET\n+        secret: typeof process.env.COMPANION_DROPBOX_SECRET_FILE === 'string' ? fs.readFileSync(process.env.COMPANION_DROPBOX_SECRET_FILE).toString() : process.env.COMPANION_DROPBOX_SECRET\n       },\n       instagram: {\n         key: process.env.COMPANION_INSTAGRAM_KEY,\n-        secret: process.env.COMPANION_INSTAGRAM_SECRET\n+        secret: typeof process.env.COMPANION_INSTAGRAM_SECRET_FILE === 'string' ? fs.readFileSync(process.env.COMPANION_INSTAGRAM_SECRET_FILE).toString() : process.env.COMPANION_INSTAGRAM_SECRET\n       },\n       s3: {\n         key: process.env.COMPANION_AWS_KEY,\n-        secret: process.env.COMPANION_AWS_SECRET,\n+        secret: typeof process.env.COMPANION_AWS_SECRET_FILE === 'string' ? fs.readFileSync(process.env.COMPANION_AWS_SECRET_FILE).toString() : process.env.COMPANION_AWS_SECRET,\n         bucket: process.env.COMPANION_AWS_BUCKET,\n         endpoint: process.env.COMPANION_AWS_ENDPOINT,\n         region: process.env.COMPANION_AWS_REGION","old_code":"const fs = require('fs')\nconst merge = require('lodash.merge')\nconst stripIndent = require('common-tags\/lib\/stripIndent')\nconst utils = require('..\/server\/helpers\/utils')\nconst logger = require('..\/server\/logger')\nconst crypto = require('crypto')\n\/\/ @ts-ignore\nconst { version } = require('..\/..\/package.json')\n\n\/**\n * Reads all companion configuration set via environment variables\n * and via the config file path\n *\n * @returns {object}\n *\/\nexports.getUppyOptions = () => {\n  return merge({}, getConfigFromEnv(), getConfigFromFile())\n}\n\n\/**\n * Loads the config from environment variables\n *\n * @returns {object}\n *\/\nconst getConfigFromEnv = () => {\n  const uploadUrls = process.env.COMPANION_UPLOAD_URLS\n  const domains = process.env.COMPANION_DOMAINS || process.env.COMPANION_DOMAIN || null\n  const validHosts = domains ? domains.split(',') : []\n\n  return {\n    \/\/ TODO: Rename providerOptions to providers.\n    providerOptions: {\n      google: {\n        key: process.env.COMPANION_GOOGLE_KEY,\n        secret: process.env.COMPANION_GOOGLE_SECRET\n      },\n      dropbox: {\n        key: process.env.COMPANION_DROPBOX_KEY,\n        secret: process.env.COMPANION_DROPBOX_SECRET\n      },\n      instagram: {\n        key: process.env.COMPANION_INSTAGRAM_KEY,\n        secret: process.env.COMPANION_INSTAGRAM_SECRET\n      },\n      s3: {\n        key: process.env.COMPANION_AWS_KEY,\n        secret: process.env.COMPANION_AWS_SECRET,\n        bucket: process.env.COMPANION_AWS_BUCKET,\n        endpoint: process.env.COMPANION_AWS_ENDPOINT,\n        region: process.env.COMPANION_AWS_REGION\n      }\n    },\n    server: {\n      host: process.env.COMPANION_DOMAIN,\n      protocol: process.env.COMPANION_PROTOCOL,\n      path: process.env.COMPANION_PATH,\n      implicitPath: process.env.COMPANION_IMPLICIT_PATH,\n      oauthDomain: process.env.COMPANION_OAUTH_DOMAIN,\n      validHosts: validHosts\n    },\n    filePath: process.env.COMPANION_DATADIR,\n    redisUrl: process.env.COMPANION_REDIS_URL,\n    sendSelfEndpoint: process.env.COMPANION_SELF_ENDPOINT,\n    uploadUrls: uploadUrls ? uploadUrls.split(',') : null,\n    secret: process.env.COMPANION_SECRET || generateSecret(),\n    debug: process.env.NODE_ENV !== 'production',\n    \/\/ TODO: this is a temporary hack to support distributed systems.\n    \/\/ it is not documented, because it should be changed soon.\n    cookieDomain: process.env.COMPANION_COOKIE_DOMAIN,\n    multipleInstances: true\n  }\n}\n\n\/**\n * Auto-generates server secret\n *\n * @returns {string}\n *\/\nconst generateSecret = () => {\n  logger.warn('auto-generating server secret because none was specified', 'startup.secret')\n  return crypto.randomBytes(64).toString('hex')\n}\n\n\/**\n * Loads the config from a file and returns it as an object\n *\n * @returns {object}\n *\/\nconst getConfigFromFile = () => {\n  const path = getConfigPath()\n  if (!path) return {}\n\n  const rawdata = fs.readFileSync(getConfigPath())\n  \/\/ TODO validate the json object fields to match the uppy config schema\n  \/\/ @ts-ignore\n  return JSON.parse(rawdata)\n}\n\n\/**\n * Returns the config path specified via cli arguments\n *\n * @returns {string}\n *\/\nconst getConfigPath = () => {\n  let configPath\n\n  for (let i = process.argv.length - 1; i >= 0; i--) {\n    const isConfigFlag = process.argv[i] === '-c' || process.argv[i] === '--config'\n    const flagHasValue = i + 1 <= process.argv.length\n    if (isConfigFlag && flagHasValue) {\n      configPath = process.argv[i + 1]\n      break\n    }\n  }\n\n  return configPath\n}\n\n\/**\n * validates that the mandatory companion options are set.\n * If it is invalid, it will console an error of unset options and exits the process.\n * If it is valid, nothing happens.\n *\n * @param {object} config\n *\/\nexports.validateConfig = (config) => {\n  const mandatoryOptions = ['secret', 'filePath', 'server.host']\n  \/** @type {string[]} *\/\n  const unspecified = []\n\n  mandatoryOptions.forEach((i) => {\n    const value = i.split('.').reduce((prev, curr) => prev[curr], config)\n\n    if (!value) unspecified.push(`\"${i}\"`)\n  })\n\n  \/\/ vaidate that all required config is specified\n  if (unspecified.length) {\n    console.error('\\x1b[31m', 'Please specify the following options',\n      'to run companion as Standalone:\\n', unspecified.join(',\\n'), '\\x1b[0m')\n    process.exit(1)\n  }\n\n  \/\/ validate that specified filePath is writeable\/readable.\n  \/\/ TODO: consider moving this into the uppy module itself.\n  try {\n    \/\/ @ts-ignore\n    fs.accessSync(`${config.filePath}`, fs.R_OK | fs.W_OK)\n  } catch (err) {\n    console.error('\\x1b[31m', `No access to \"${config.filePath}\".`,\n      'Please ensure the directory exists and with read\/write permissions.', '\\x1b[0m')\n    process.exit(1)\n  }\n}\n\n\/**\n *\n * @param {string} url\n *\/\nexports.hasProtocol = (url) => {\n  return url.startsWith('http:\/\/') || url.startsWith('https:\/\/')\n}\n\nexports.buildHelpfulStartupMessage = (uppyOptions) => {\n  const buildURL = utils.getURLBuilder(uppyOptions)\n  const callbackURLs = []\n  Object.keys(uppyOptions.providerOptions).forEach((providerName) => {\n    \/\/ s3 does not need redirect_uris\n    if (providerName === 's3') {\n      return\n    }\n\n    if (providerName === 'google') {\n      providerName = 'drive'\n    }\n\n    callbackURLs.push(buildURL(`\/connect\/${providerName}\/callback`, true))\n  })\n\n  return stripIndent`\n    Welcome to Companion v${version}\n    ===================================\n\n    Congratulations on setting up Companion! Thanks for joining our cause, you have taken\n    the first step towards the future of file uploading! We\n    hope you are as excited about this as we are!\n\n    While you did an awesome job on getting Companion running, this is just the welcome\n    message, so let's talk about the places that really matter:\n\n    - Be sure to add ${callbackURLs.join(', ')} as your Oauth redirect uris on their corresponding developer interfaces.\n    - The URL ${buildURL('\/metrics', true)} is available for  statistics to keep Companion running smoothly\n    - https:\/\/github.com\/transloadit\/uppy\/issues - report your bugs here\n\n    So quit lollygagging, start uploading and experience the future!\n  `\n}\n","lang_cluster":"Javascript","length":197,"code_uid":"b12825e08e364083a063897f76218aa4"}
{"diff_hunk":"@@ -37,7 +37,12 @@ class WriteConcern {\n   \/**\n    * Construct a WriteConcern given an options object.\n    *\n-   * @param {object} options The options object from which to extract the write concern.\n+   * @param {object} [options] The options object from which to extract the write concern.\n+   * @param {(number|string)} [options.w] **Deprecated** Use `options.writeConcern` instead\n+   * @param {number} [options.wtimeout] **Deprecated** Use `options.writeConcern` instead\n+   * @param {boolean} [options.j] **Deprecated** Use `options.writeConcern` instead\n+   * @param {boolean} [options.fsync] **Deprecated** Use `options.writeConcern` instead\n+   * @param {object|WriteConcern} [options.writeConcern] Specify write concern settings.\n    * @return {WriteConcern}\n    *\/\n   static fromOptions(options) {","old_code":"'use strict';\n\nconst kWriteConcernKeys = new Set(['w', 'wtimeout', 'j', 'fsync']);\n\n\/**\n * The **WriteConcern** class is a class that represents a MongoDB WriteConcern.\n * @class\n * @property {(number|string)} w The write concern\n * @property {number} wtimeout The write concern timeout\n * @property {boolean} j The journal write concern\n * @property {boolean} fsync The file sync write concern\n * @see https:\/\/docs.mongodb.com\/manual\/reference\/write-concern\/index.html\n *\/\nclass WriteConcern {\n  \/**\n   * Constructs a WriteConcern from the write concern properties.\n   * @param {(number|string)} [w] The write concern\n   * @param {number} [wtimeout] The write concern timeout\n   * @param {boolean} [j] The journal write concern\n   * @param {boolean} [fsync] The file sync write concern\n   *\/\n  constructor(w, wtimeout, j, fsync) {\n    if (w != null) {\n      this.w = w;\n    }\n    if (wtimeout != null) {\n      this.wtimeout = wtimeout;\n    }\n    if (j != null) {\n      this.j = j;\n    }\n    if (fsync != null) {\n      this.fsync = fsync;\n    }\n  }\n\n  \/**\n   * Construct a WriteConcern given an options object.\n   *\n   * @param {object} options The options object from which to extract the write concern.\n   * @return {WriteConcern}\n   *\/\n  static fromOptions(options) {\n    if (\n      options == null ||\n      (options.writeConcern == null &&\n        options.w == null &&\n        options.wtimeout == null &&\n        options.j == null &&\n        options.fsync == null)\n    ) {\n      return;\n    }\n\n    if (options.writeConcern) {\n      if (typeof options.writeConcern === 'string') {\n        return new WriteConcern(options.writeConcern);\n      }\n\n      if (!Object.keys(options.writeConcern).some(key => kWriteConcernKeys.has(key))) {\n        return;\n      }\n\n      return new WriteConcern(\n        options.writeConcern.w,\n        options.writeConcern.wtimeout,\n        options.writeConcern.j,\n        options.writeConcern.fsync\n      );\n    }\n\n    return new WriteConcern(options.w, options.wtimeout, options.j, options.fsync);\n  }\n}\n\nmodule.exports = WriteConcern;\n","lang_cluster":"Javascript","length":76,"code_uid":"c88f97cd8cd0402a9148e9c9d8174d94"}
{"diff_hunk":"@@ -13,7 +13,7 @@ describe('has-visible-text', function() {\n   });\n \n   it('should return false if there is no visible text', function() {\n-    var params = checkSetup('<object id=\"target\"><\/object>');\n+    var params = checkSetup('<p id=\"target\"><\/p>');\n     assert.isFalse(\n       axe.testUtils\n         .getCheckEvaluate('has-visible-text')","old_code":"describe('has-visible-text', function() {\n  'use strict';\n\n  var fixture = document.getElementById('fixture');\n  var checkSetup = axe.testUtils.checkSetup;\n\n  var checkContext = axe.testUtils.MockCheckContext();\n\n  afterEach(function() {\n    fixture.innerHTML = '';\n    axe._tree = undefined;\n    checkContext.reset();\n  });\n\n  it('should return false if there is no visible text', function() {\n    var params = checkSetup('<object id=\"target\"><\/object>');\n    assert.isFalse(\n      axe.testUtils\n        .getCheckEvaluate('has-visible-text')\n        .apply(checkContext, params)\n    );\n  });\n\n  it('should return false if there is text, but its hidden', function() {\n    var params = checkSetup(\n      '<object id=\"target\"><span style=\"display:none\">hello!<\/span><\/object>'\n    );\n    assert.isFalse(\n      axe.testUtils\n        .getCheckEvaluate('has-visible-text')\n        .apply(checkContext, params)\n    );\n  });\n\n  it('should return true if there is visible text', function() {\n    var params = checkSetup('<object id=\"target\">hello!<\/object>');\n    assert.isTrue(\n      axe.testUtils\n        .getCheckEvaluate('has-visible-text')\n        .apply(checkContext, params)\n    );\n  });\n\n  describe('SerialVirtualNode', function() {\n    it('should return false if element is not named from contents', function() {\n      var node = new axe.SerialVirtualNode({\n        nodeName: 'article'\n      });\n\n      assert.isFalse(\n        axe.testUtils.getCheckEvaluate('has-visible-text')(null, {}, node)\n      );\n    });\n\n    it('should return incomplete if no other properties are set', function() {\n      var node = new axe.SerialVirtualNode({\n        nodeName: 'button'\n      });\n\n      assert.isUndefined(\n        axe.testUtils.getCheckEvaluate('has-visible-text')(null, {}, node)\n      );\n    });\n\n    it('should return false if there is no visible text', function() {\n      var node = new axe.SerialVirtualNode({\n        nodeName: 'button'\n      });\n      node.children = [];\n\n      assert.isFalse(\n        axe.testUtils.getCheckEvaluate('has-visible-text')(null, {}, node)\n      );\n    });\n\n    it('should return true if there is visible text', function() {\n      var node = new axe.SerialVirtualNode({\n        nodeName: 'object'\n      });\n      var child = new axe.SerialVirtualNode({\n        nodeName: '#text',\n        nodeType: 3,\n        nodeValue: 'hello!'\n      });\n      node.children = [child];\n\n      assert.isTrue(\n        axe.testUtils.getCheckEvaluate('has-visible-text')(null, {}, node)\n      );\n    });\n  });\n});\n","lang_cluster":"Javascript","length":92,"code_uid":"379672b74884412e9af3eb9ae146851c"}
{"diff_hunk":"@@ -15,7 +15,9 @@ const browsers = {\n   chrome: tester((ua, vendor) => \/Chrome\/.test(ua) && \/Google\/.test(vendor)),\n   edge: tester(ua => \/Edge\/.test(ua)),\n   ie: tester(ua => \/Trident\/.test(ua)),\n+  \/\/ eslint-disable-next-line\n   ie8: tester(() => !(document.createTextNode('test').textContent)),\n+  \/\/ eslint-disable-next-line\n   ie9: tester(() => !!(document.documentMode)),\n   mobile: tester(ua => \/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini\/i.test(ua)),\n   safari: tester((ua, vendor) => \/Safari\/.test(ua) && \/Apple Computer\/.test(vendor)),","old_code":"import { objectEach } from '.\/object';\n\nconst tester = (testerFunc) => {\n  const result = {\n    value: false,\n  };\n  result.test = (ua, vendor) => {\n    result.value = testerFunc(ua, vendor);\n  };\n\n  return result;\n};\n\nconst browsers = {\n  chrome: tester((ua, vendor) => \/Chrome\/.test(ua) && \/Google\/.test(vendor)),\n  edge: tester(ua => \/Edge\/.test(ua)),\n  ie: tester(ua => \/Trident\/.test(ua)),\n  ie8: tester(() => !(document.createTextNode('test').textContent)),\n  ie9: tester(() => !!(document.documentMode)),\n  mobile: tester(ua => \/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini\/i.test(ua)),\n  safari: tester((ua, vendor) => \/Safari\/.test(ua) && \/Apple Computer\/.test(vendor)),\n};\n\nexport function setBrowserMeta({ userAgent = navigator.userAgent, vendor = navigator.vendor } = {}) {\n  objectEach(browsers, ({ test }) => void test(userAgent, vendor));\n}\n\nsetBrowserMeta();\n\nexport function isChrome() {\n  return browsers.chrome.value;\n}\n\nexport function isEdge() {\n  return browsers.edge.value;\n}\n\nexport function isIE() {\n  return browsers.ie.value;\n}\n\nexport function isIE8() {\n  return browsers.ie8.value;\n}\n\nexport function isIE9() {\n  return browsers.ie9.value;\n}\n\nexport function isMSBrowser() {\n  return browsers.ie.value || browsers.edge.value;\n}\n\nexport function isMobileBrowser() {\n  return browsers.mobile.value;\n}\n\nexport function isSafari() {\n  return browsers.safari.value;\n}\n","lang_cluster":"Javascript","length":60,"code_uid":"7374567a45d14fc3b004898a996490ee"}
{"diff_hunk":"@@ -95,7 +95,7 @@ define([\"loading\", \"dialogHelper\", \"dom\", \"jQuery\", \"components\/libraryoptionsed\n \n     function getFolderHtml(pathInfo, index) {\n         var html = \"\";\n-        return html += '<div class=\"listItem listItem-border lnkPath\" style=\"padding-left:.5em;\">', html += '<div class=\"' + (pathInfo.NetworkPath ? \"listItemBody two-line\" : \"listItemBody\") + '\">', html += '<div class=\"listItemBodyText\">' + pathInfo.Path + \"<\/div>\", pathInfo.NetworkPath && (html += '<div class=\"listItemBodyText secondary\">' + pathInfo.NetworkPath + \"<\/div>\"), html += \"<\/div>\", html += '<button type=\"button\" is=\"paper-icon-button-light\"\" class=\"listItemButton btnRemovePath\" data-index=\"' + index + '\"><i class=\"md-icon\">remove_circle<\/i><\/button>', html += \"<\/div>\"\n+        return html += '<div class=\"listItem listItem-border lnkPath\" style=\"padding-left:.5em;\">', html += '<div class=\"' + (pathInfo.NetworkPath ? \"listItemBody two-line\" : \"listItemBody\") + '\">', html += '<div class=\"listItemBodyText\">' + pathInfo.Path + \"<\/div>\", pathInfo.NetworkPath && (html += '<div class=\"listItemBodyText secondary\">' + pathInfo.NetworkPath + \"<\/div>\"), html += \"<\/div>\", html += '<button type=\"button\" is=\"paper-icon-button-light\"\" class=\"listItemButton btnRemovePath\" data-index=\"' + index + '\"><i class=\"material-icons\">remove_circle<\/i><\/button>', html += \"<\/div>\"\n     }\n \n     function renderPaths(page) {","old_code":"define([\"loading\", \"dialogHelper\", \"dom\", \"jQuery\", \"components\/libraryoptionseditor\/libraryoptionseditor\", \"emby-toggle\", \"emby-input\", \"emby-select\", \"paper-icon-button-light\", \"listViewStyle\", \"formDialogStyle\", \"emby-button\", \"flexStyles\"], function(loading, dialogHelper, dom, $, libraryoptionseditor) {\n    \"use strict\";\n\n    function onAddLibrary() {\n        if (isCreating) return false;\n\n        if (pathInfos.length == 0) {\n            require([\"alert\"], function(alert) {\n                alert({\n                    text: Globalize.translate(\"PleaseAddAtLeastOneFolder\"),\n                    type: \"error\"\n                })\n            });\n            return false;\n        }\n\n        isCreating = true;\n        loading.show();\n\n        var dlg = dom.parentWithClass(this, \"dlg-librarycreator\");\n        var name = $(\"#txtValue\", dlg).val();\n        var type = $(\"#selectCollectionType\", dlg).val();\n        if (type == \"mixed\") type = null;\n        var libraryOptions = libraryoptionseditor.getLibraryOptions(dlg.querySelector(\".libraryOptions\"));\n        libraryOptions.PathInfos = pathInfos;\n        ApiClient.addVirtualFolder(name, type, currentOptions.refresh, libraryOptions).then(function() {\n            hasChanges = true;\n            isCreating = false;\n            loading.hide();\n            dialogHelper.close(dlg);\n        }, function() {\n            require([\"toast\"], function(toast) {\n                toast(Globalize.translate(\"ErrorAddingMediaPathToVirtualFolder\"));\n            });\n            isCreating = false;\n            loading.hide();\n        });\n        return false;\n    }\n\n    function getCollectionTypeOptionsHtml(collectionTypeOptions) {\n        return collectionTypeOptions.map(function(i) {\n            return '<option value=\"' + i.value + '\">' + i.name + \"<\/option>\";\n        }).join(\"\");\n    }\n\n    function initEditor(page, collectionTypeOptions) {\n        $(\"#selectCollectionType\", page).html(getCollectionTypeOptionsHtml(collectionTypeOptions)).val(\"\").on(\"change\", function() {\n            var value = this.value;\n            var dlg = $(this).parents(\".dialog\")[0];\n            libraryoptionseditor.setContentType(dlg.querySelector(\".libraryOptions\"), value == \"mixed\" ? \"\" : value);\n            if (value) {\n                dlg.querySelector(\".libraryOptions\").classList.remove(\"hide\");\n            } else {\n                dlg.querySelector(\".libraryOptions\").classList.add(\"hide\");\n            }\n\n            if (value != \"mixed\") {\n                var index = this.selectedIndex;\n                if (index != -1) {\n                    var name = this.options[index].innerHTML.replace(\"*\", \"\").replace(\"&amp;\", \"&\");\n                    $(\"#txtValue\", dlg).val(name);\n                    var folderOption = collectionTypeOptions.filter(function(i) {\n                        return i.value == value\n                    })[0];\n                    $(\".collectionTypeFieldDescription\", dlg).html(folderOption.message || \"\")\n                }\n            }\n        });\n\n        page.querySelector(\".btnAddFolder\").addEventListener(\"click\", onAddButtonClick);\n        page.querySelector(\".btnSubmit\").addEventListener(\"click\", onAddLibrary);\n        page.querySelector(\".folderList\").addEventListener(\"click\", onRemoveClick);\n        page.querySelector(\".chkAdvanced\").addEventListener(\"change\", onToggleAdvancedChange);\n    }\n\n    function onToggleAdvancedChange() {\n        var dlg = dom.parentWithClass(this, \"dlg-librarycreator\");\n        libraryoptionseditor.setAdvancedVisible(dlg.querySelector(\".libraryOptions\"), this.checked);\n    }\n\n    function onAddButtonClick() {\n        var page = dom.parentWithClass(this, \"dlg-librarycreator\");\n        require([\"directorybrowser\"], function(directoryBrowser) {\n            var picker = new directoryBrowser;\n            picker.show({\n                enableNetworkSharePath: true,\n                callback: function(path, networkSharePath) {\n                    path && addMediaLocation(page, path, networkSharePath);\n                    picker.close();\n                }\n            })\n        })\n    }\n\n    function getFolderHtml(pathInfo, index) {\n        var html = \"\";\n        return html += '<div class=\"listItem listItem-border lnkPath\" style=\"padding-left:.5em;\">', html += '<div class=\"' + (pathInfo.NetworkPath ? \"listItemBody two-line\" : \"listItemBody\") + '\">', html += '<div class=\"listItemBodyText\">' + pathInfo.Path + \"<\/div>\", pathInfo.NetworkPath && (html += '<div class=\"listItemBodyText secondary\">' + pathInfo.NetworkPath + \"<\/div>\"), html += \"<\/div>\", html += '<button type=\"button\" is=\"paper-icon-button-light\"\" class=\"listItemButton btnRemovePath\" data-index=\"' + index + '\"><i class=\"md-icon\">remove_circle<\/i><\/button>', html += \"<\/div>\"\n    }\n\n    function renderPaths(page) {\n        var foldersHtml = pathInfos.map(getFolderHtml).join(\"\");\n        var folderList = page.querySelector(\".folderList\");\n        folderList.innerHTML = foldersHtml;\n        foldersHtml ? folderList.classList.remove(\"hide\") : folderList.classList.add(\"hide\");\n    }\n\n    function addMediaLocation(page, path, networkSharePath) {\n        var pathLower = path.toLowerCase();\n        var pathFilter = pathInfos.filter(function(p) {\n            return p.Path.toLowerCase() == pathLower;\n        });\n        if (!pathFilter.length) {\n            var pathInfo = {\n                Path: path\n            };\n            networkSharePath && (pathInfo.NetworkPath = networkSharePath);\n            pathInfos.push(pathInfo);\n            renderPaths(page);\n        }\n    }\n\n    function onRemoveClick(e) {\n        var button = dom.parentWithClass(e.target, \"btnRemovePath\");\n        var index = parseInt(button.getAttribute(\"data-index\"));\n        var location = pathInfos[index].Path;\n        var locationLower = location.toLowerCase();\n        pathInfos = pathInfos.filter(function(p) {\n            return p.Path.toLowerCase() != locationLower;\n        });\n        renderPaths(dom.parentWithClass(button, \"dlg-librarycreator\"));\n    }\n\n    function onDialogClosed() {\n        currentResolve(hasChanges);\n    }\n\n    function initLibraryOptions(dlg) {\n        libraryoptionseditor.embed(dlg.querySelector(\".libraryOptions\")).then(function() {\n            $(\"#selectCollectionType\", dlg).trigger(\"change\");\n            onToggleAdvancedChange.call(dlg.querySelector(\".chkAdvanced\"));\n        })\n    }\n\n    function editor() {\n        this.show = function(options) {\n            return new Promise(function(resolve, reject) {\n                currentOptions = options;\n                currentResolve = resolve;\n                hasChanges = false;\n                var xhr = new XMLHttpRequest;\n                xhr.open(\"GET\", \"components\/medialibrarycreator\/medialibrarycreator.template.html\", true);\n                xhr.onload = function(e) {\n                    var template = this.response;\n                    var dlg = dialogHelper.createDialog({\n                        size: \"medium-tall\",\n                        modal: false,\n                        removeOnClose: true,\n                        scrollY: false\n                    });\n                    dlg.classList.add(\"ui-body-a\");\n                    dlg.classList.add(\"background-theme-a\");\n                    dlg.classList.add(\"dlg-librarycreator\");\n                    dlg.classList.add(\"formDialog\");\n                    dlg.innerHTML = Globalize.translateDocument(template);\n                    initEditor(dlg, options.collectionTypeOptions);\n                    dlg.addEventListener(\"close\", onDialogClosed);\n                    dialogHelper.open(dlg);\n                    dlg.querySelector(\".btnCancel\").addEventListener(\"click\", function() {\n                        dialogHelper.close(dlg)\n                    });\n                    pathInfos = [];\n                    renderPaths(dlg);\n                    initLibraryOptions(dlg);\n                };\n                xhr.send();\n            });\n        }\n    }\n\n    var pathInfos = [];\n    var currentResolve;\n    var currentOptions;\n\n    var hasChanges = false;\n    var isCreating = false;\n\n    return editor\n});\n","lang_cluster":"Javascript","length":189,"code_uid":"0456147ae60e47ac90b4f8958f6dc0b4"}
{"diff_hunk":"@@ -7,11 +7,18 @@ const sinon = require('sinon');\n class MockTopology extends EventEmitter {\n   constructor() {\n     super();\n+    this.s = {\n+      promiseLibrary: Promise\n+    };\n   }\n \n   capabilities() {\n     return {};\n   }\n+\n+  hasSessionSupport() {\n+    return false;\n+  }\n }\n \n const test = {};","old_code":"'use strict';\n\nconst EventEmitter = require('events');\nconst expect = require('chai').expect;\nconst sinon = require('sinon');\n\nclass MockTopology extends EventEmitter {\n  constructor() {\n    super();\n  }\n\n  capabilities() {\n    return {};\n  }\n}\n\nconst test = {};\ndescribe('Database', function() {\n  before(() => {\n    \/\/ NOTE: These modules are being used prior to test run. In order to monkey-patch them\n    \/\/       we must remove their cached versions.\n    const resolvedUtils = require.resolve('..\/..\/lib\/utils');\n    const resolvedDb = require.resolve('..\/..\/lib\/db');\n    delete require.cache[resolvedUtils];\n    delete require.cache[resolvedDb];\n    test.utils = require('..\/..\/lib\/utils');\n\n    \/\/ create a sandbox for stub cleanup\n    test.sandbox = sinon.sandbox.create();\n  });\n\n  afterEach(() => test.sandbox.restore());\n\n  it('should ignore a readPreference for dropDatabase', {\n    metadata: { requires: { topology: 'single' } },\n    test: function() {\n      sinon.stub(test.utils, 'executeOperation').callsFake((topology, operation, args) => {\n        const options = args[args.length - 2];\n        expect(options.readPreference).to.equal('primary');\n      });\n\n      const Db = require('..\/..\/lib\/db');\n      const db = new Db('fakeDb', new MockTopology(), { readPreference: 'nearest' });\n      db.dropDatabase();\n    }\n  });\n});\n","lang_cluster":"Javascript","length":47,"code_uid":"dbe0722e6ceb4a07959da92e19c99dba"}
{"diff_hunk":"@@ -61,7 +61,20 @@ export default Controller.extend({\n             this.integration.rollbackAttributes();\n \n             return transition.retry();\n+        },\n+\n+        deleteIntegration() {\n+            this.integration.destroyRecord();\n+        },\n+\n+        confirmIntegrationDeletion() {\n+            this.set('showDeleteIntegrationModal', true);\n+        },\n+\n+        cancelIntegrationDeletion() {\n+            this.set('showDeleteIntegrationModal', false);\n         }\n+\n     },\n \n     save: task(function* () {","old_code":"import Controller from '@ember\/controller';\nimport {alias} from '@ember\/object\/computed';\nimport {computed} from '@ember\/object';\nimport {task, timeout} from 'ember-concurrency';\n\nexport default Controller.extend({\n    integration: alias('model'),\n\n    allWebhooks: computed(function () {\n        return this.store.peekAll('webhook');\n    }),\n\n    filteredWebhooks: computed('allWebhooks.@each.{isNew,isDeleted}', function () {\n        return this.allWebhooks.filter((webhook) => {\n            let matchesIntegration = webhook.belongsTo('integration').id() === this.integration.id;\n\n            return matchesIntegration\n                && !webhook.isNew\n                && !webhook.isDeleted;\n        });\n    }),\n\n    actions: {\n        save() {\n            return this.save.perform();\n        },\n\n        toggleUnsavedChangesModal(transition) {\n            let leaveTransition = this.leaveScreenTransition;\n\n            if (!transition && this.showUnsavedChangesModal) {\n                this.set('leaveScreenTransition', null);\n                this.set('showUnsavedChangesModal', false);\n                return;\n            }\n\n            if (!leaveTransition || transition.targetName === leaveTransition.targetName) {\n                this.set('leaveScreenTransition', transition);\n\n                \/\/ if a save is running, wait for it to finish then transition\n                if (this.save.isRunning) {\n                    return this.save.last.then(() => {\n                        transition.retry();\n                    });\n                }\n\n                \/\/ we genuinely have unsaved data, show the modal\n                this.set('showUnsavedChangesModal', true);\n            }\n        },\n\n        leaveScreen() {\n            let transition = this.leaveScreenTransition;\n\n            if (!transition) {\n                this.notifications.showAlert('Sorry, there was an error in the application. Please let the Ghost team know what happened.', {type: 'error'});\n                return;\n            }\n\n            \/\/ roll back changes on model props\n            this.integration.rollbackAttributes();\n\n            return transition.retry();\n        }\n    },\n\n    save: task(function* () {\n        return yield this.integration.save();\n    }),\n\n    copyContentKey: task(function* () {\n        this._copyInputTextToClipboard('input#content_key');\n        yield timeout(3000);\n    }),\n\n    copyAdminKey: task(function* () {\n        this._copyInputTextToClipboard('input#admin_key');\n        yield timeout(3000);\n    }),\n\n    _copyInputTextToClipboard(selector) {\n        let input = document.querySelector(selector);\n        input.disabled = false;\n        input.focus();\n        input.select();\n        document.execCommand('copy');\n        input.disabled = true;\n    }\n});\n","lang_cluster":"Javascript","length":89,"code_uid":"3597af08f294445aa94b0d6d670b0217"}
{"diff_hunk":"@@ -56,7 +56,7 @@ class TopProductFacade\n     }\n \n     \/**\n-     * @param $domainId\n+     * @param int $domainId\n      * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\Product[] $products\n      *\/\n     public function saveTopProductsForDomain($domainId, array $products)","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\Model\\Product\\TopProduct;\n\nuse Doctrine\\ORM\\EntityManagerInterface;\n\nclass TopProductFacade\n{\n    \/**\n     * @var \\Doctrine\\ORM\\EntityManagerInterface\n     *\/\n    protected $em;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\TopProduct\\TopProductRepository\n     *\/\n    protected $topProductRepository;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\TopProduct\\TopProductFactoryInterface\n     *\/\n    protected $topProductFactory;\n\n    \/**\n     * @param \\Doctrine\\ORM\\EntityManagerInterface $em\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\TopProduct\\TopProductRepository $topProductRepository\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\TopProduct\\TopProductFactoryInterface $topProductFactory\n     *\/\n    public function __construct(\n        EntityManagerInterface $em,\n        TopProductRepository $topProductRepository,\n        TopProductFactoryInterface $topProductFactory\n    ) {\n        $this->em = $em;\n        $this->topProductRepository = $topProductRepository;\n        $this->topProductFactory = $topProductFactory;\n    }\n\n    \/**\n     * @param int $domainId\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Product\\TopProduct\\TopProduct[]\n     *\/\n    public function getAll($domainId)\n    {\n        return $this->topProductRepository->getAll($domainId);\n    }\n\n    \/**\n     * @param int $domainId\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroup $pricingGroup\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Product\\Product[]\n     *\/\n    public function getAllOfferedProducts($domainId, $pricingGroup)\n    {\n        return $this->topProductRepository->getOfferedProductsForTopProductsOnDomain($domainId, $pricingGroup);\n    }\n\n    \/**\n     * @param $domainId\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\Product[] $products\n     *\/\n    public function saveTopProductsForDomain($domainId, array $products)\n    {\n        $oldTopProducts = $this->topProductRepository->getAll($domainId);\n        foreach ($oldTopProducts as $oldTopProduct) {\n            $this->em->remove($oldTopProduct);\n        }\n        $this->em->flush($oldTopProducts);\n\n        $topProducts = [];\n        $position = 1;\n        foreach ($products as $product) {\n            $topProduct = $this->topProductFactory->create($product, $domainId, $position++);\n            $this->em->persist($topProduct);\n            $topProducts[] = $topProduct;\n        }\n        $this->em->flush($topProducts);\n    }\n}\n","lang_cluster":"PHP","length":79,"code_uid":"ec35ed2e04944fdeb0a9ea5563987915"}
{"diff_hunk":"@@ -77,10 +77,12 @@ class MediaAdminController extends Controller\n         $datagrid->setValue('context', null, $context);\n \n         \/\/ retrieve the main category for the tree view\n-        $category = $this->container->get('sonata.classification.manager.category')->getRootCategory($context);\n+        $rootCategory = $this->container->get('sonata.classification.manager.category')->getRootCategory($context);\n+        \/\/ This should be safe as the root category has to exist for a given context but I do not like fatal errors\n+        $rootCategoryId = !empty($rootCategory) ? $rootCategory->getId() : null;\n \n         if (!$filters) {\n-            $datagrid->setValue('category', null, $category);\n+            $datagrid->setValue('category', null, $rootCategoryId);\n         }\n         if ($request->get('category')) {\n             $categoryByContext = $this->container->get('sonata.classification.manager.category')->findOneBy(array(","old_code":"<?php\n\n\/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Controller;\n\nuse Sonata\\AdminBundle\\Controller\\CRUDController as Controller;\nuse Symfony\\Component\\HttpFoundation\\Request;\nuse Symfony\\Component\\HttpFoundation\\Response;\nuse Symfony\\Component\\Security\\Core\\Exception\\AccessDeniedException;\n\nclass MediaAdminController extends Controller\n{\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function createAction(Request $request = null)\n    {\n        if (false === $this->admin->isGranted('CREATE')) {\n            throw new AccessDeniedException();\n        }\n\n        if (!$request->get('provider') && $request->isMethod('get')) {\n            return $this->render('SonataMediaBundle:MediaAdmin:select_provider.html.twig', array(\n                'providers' => $this->get('sonata.media.pool')->getProvidersByContext($request->get('context', $this->get('sonata.media.pool')->getDefaultContext())),\n                'base_template' => $this->getBaseTemplate(),\n                'admin' => $this->admin,\n                'action' => 'create',\n            ));\n        }\n\n        return parent::createAction();\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function render($view, array $parameters = array(), Response $response = null, Request $request = null)\n    {\n        $parameters['media_pool'] = $this->container->get('sonata.media.pool');\n        $parameters['persistent_parameters'] = $this->admin->getPersistentParameters();\n\n        return parent::render($view, $parameters, $response, $request);\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function listAction(Request $request = null)\n    {\n        if (false === $this->admin->isGranted('LIST')) {\n            throw new AccessDeniedException();\n        }\n\n        if ($listMode = $request->get('_list_mode', 'mosaic')) {\n            $this->admin->setListMode($listMode);\n        }\n\n        $datagrid = $this->admin->getDatagrid();\n\n        $filters = $request->get('filter');\n\n        \/\/ set the default context\n        if (!$filters || !array_key_exists('context', $filters)) {\n            $context = $this->admin->getPersistentParameter('context', $this->get('sonata.media.pool')->getDefaultContext());\n        } else {\n            $context = $filters['context']['value'];\n        }\n\n        $datagrid->setValue('context', null, $context);\n\n        \/\/ retrieve the main category for the tree view\n        $category = $this->container->get('sonata.classification.manager.category')->getRootCategory($context);\n\n        if (!$filters) {\n            $datagrid->setValue('category', null, $category);\n        }\n        if ($request->get('category')) {\n            $categoryByContext = $this->container->get('sonata.classification.manager.category')->findOneBy(array(\n                'id' => (int) $request->get('category'),\n                'context' => $context,\n            ));\n\n            if (!empty($categoryByContext)) {\n                $datagrid->setValue('category', null, $categoryByContext);\n            } else {\n                $datagrid->setValue('category', null, $category);\n            }\n        }\n\n        $formView = $datagrid->getForm()->createView();\n\n        \/\/ set the theme for the current Admin Form\n        $this->get('twig')->getExtension('form')->renderer->setTheme($formView, $this->admin->getFilterTheme());\n\n        return $this->render($this->admin->getTemplate('list'), array(\n            'action' => 'list',\n            'form' => $formView,\n            'datagrid' => $datagrid,\n            'root_category' => $category,\n            'csrf_token' => $this->getCsrfToken('sonata.batch'),\n        ));\n    }\n}\n","lang_cluster":"PHP","length":111,"code_uid":"2695e1f453e647a4afe857f1507c46b5"}
{"diff_hunk":"@@ -76,7 +76,7 @@ class CustomerUserRepository\n     {\n         $customerUser = $this->findById($id);\n         if ($customerUser === null) {\n-            throw new \\Shopsys\\FrameworkBundle\\Model\\Customer\\Exception\\CustomerUserNotFoundException($id);\n+            throw new \\Shopsys\\FrameworkBundle\\Model\\Customer\\Exception\\CustomerUserNotFoundException((string)$id);\n         }\n         return $customerUser;\n     }","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\Model\\Customer\\User;\n\nuse Doctrine\\ORM\\EntityManagerInterface;\nuse Shopsys\\FrameworkBundle\\Component\\String\\DatabaseSearching;\nuse Shopsys\\FrameworkBundle\\Form\\Admin\\QuickSearch\\QuickSearchFormData;\nuse Shopsys\\FrameworkBundle\\Model\\Customer\\BillingAddress;\nuse Shopsys\\FrameworkBundle\\Model\\Customer\\Exception\\CustomerUserNotFoundException;\nuse Shopsys\\FrameworkBundle\\Model\\Order\\Order;\nuse Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroup;\n\nclass CustomerUserRepository\n{\n    \/**\n     * @var \\Doctrine\\ORM\\EntityManagerInterface\n     *\/\n    protected $em;\n\n    \/**\n     * @param \\Doctrine\\ORM\\EntityManagerInterface $entityManager\n     *\/\n    public function __construct(EntityManagerInterface $entityManager)\n    {\n        $this->em = $entityManager;\n    }\n\n    \/**\n     * @return \\Doctrine\\ORM\\EntityRepository\n     *\/\n    protected function getCustomerUserRepository()\n    {\n        return $this->em->getRepository(CustomerUser::class);\n    }\n\n    \/**\n     * @param string $email\n     * @param int $domainId\n     *\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUser|null\n     *\/\n    public function findCustomerUserByEmailAndDomain($email, $domainId)\n    {\n        return $this->getCustomerUserRepository()->findOneBy([\n            'email' => mb_strtolower($email),\n            'domainId' => $domainId,\n        ]);\n    }\n\n    \/**\n     * @param string $email\n     * @param int $domainId\n     *\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUser|null\n     *\/\n    public function getCustomerUserByEmailAndDomain($email, $domainId)\n    {\n        $customerUser = $this->findCustomerUserByEmailAndDomain($email, $domainId);\n\n        if ($customerUser === null) {\n            throw new \\Shopsys\\FrameworkBundle\\Model\\Customer\\Exception\\CustomerUserNotFoundByEmailAndDomainException(\n                $email,\n                $domainId\n            );\n        }\n\n        return $customerUser;\n    }\n\n    \/**\n     * @param int $id\n     *\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUser\n     *\/\n    public function getCustomerUserById($id)\n    {\n        $customerUser = $this->findById($id);\n        if ($customerUser === null) {\n            throw new \\Shopsys\\FrameworkBundle\\Model\\Customer\\Exception\\CustomerUserNotFoundException($id);\n        }\n        return $customerUser;\n    }\n\n    \/**\n     * @param int $id\n     *\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUser|null\n     *\/\n    public function findById($id)\n    {\n        return $this->getCustomerUserRepository()->find($id);\n    }\n\n    \/**\n     * @param int $id\n     * @param string $loginToken\n     *\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUser|null\n     *\/\n    public function findByIdAndLoginToken($id, $loginToken)\n    {\n        return $this->getCustomerUserRepository()->findOneBy([\n            'id' => $id,\n            'loginToken' => $loginToken,\n        ]);\n    }\n\n    \/**\n     * @param int $domainId\n     * @param \\Shopsys\\FrameworkBundle\\Form\\Admin\\QuickSearch\\QuickSearchFormData $quickSearchData\n     * @return \\Doctrine\\ORM\\QueryBuilder\n     *\/\n    public function getCustomerUserListQueryBuilderByQuickSearchData(\n        $domainId,\n        QuickSearchFormData $quickSearchData\n    ) {\n        $queryBuilder = $this->em->createQueryBuilder()\n            ->select('\n                u.id,\n                u.email,\n                u.telephone,\n                MAX(pg.name) AS pricingGroup,\n                MAX(ba.city) city,\n                MAX(CASE WHEN ba.companyCustomer = true\n                        THEN ba.companyName\n                        ELSE CONCAT(u.lastName, \\' \\', u.firstName)\n                    END) AS name,\n                COUNT(o.id) ordersCount,\n                SUM(o.totalPriceWithVat) ordersSumPrice,\n                MAX(o.createdAt) lastOrderAt')\n            ->from(CustomerUser::class, 'u')\n            ->where('u.domainId = :selectedDomainId')\n            ->setParameter('selectedDomainId', $domainId)\n            ->join('u.customer', 'c')\n            ->leftJoin(BillingAddress::class, 'ba', 'WITH', 'c.id = ba.customer')\n            ->leftJoin(Order::class, 'o', 'WITH', 'o.customerUser = u.id AND o.deleted = :deleted')\n            ->setParameter('deleted', false)\n            ->leftJoin(PricingGroup::class, 'pg', 'WITH', 'pg.id = u.pricingGroup')\n            ->groupBy('u.id');\n\n        if ($quickSearchData->text !== null && $quickSearchData->text !== '') {\n            $queryBuilder\n                ->andWhere('\n                    (\n                        NORMALIZE(u.lastName) LIKE NORMALIZE(:text)\n                        OR\n                        NORMALIZE(u.email) LIKE NORMALIZE(:text)\n                        OR\n                        NORMALIZE(ba.companyName) LIKE NORMALIZE(:text)\n                        OR\n                        NORMALIZE(u.telephone) LIKE :text\n                    )');\n            $querySearchText = DatabaseSearching::getFullTextLikeSearchString($quickSearchData->text);\n            $queryBuilder->setParameter('text', $querySearchText);\n        }\n\n        return $queryBuilder;\n    }\n\n    \/**\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroup $oldPricingGroup\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroup $newPricingGroup\n     *\/\n    public function replaceCustomerUsersPricingGroup(PricingGroup $oldPricingGroup, PricingGroup $newPricingGroup)\n    {\n        $this->em->createQueryBuilder()\n            ->update(CustomerUser::class, 'u')\n            ->set('u.pricingGroup', ':newPricingGroup')->setParameter('newPricingGroup', $newPricingGroup)\n            ->where('u.pricingGroup = :oldPricingGroup')->setParameter('oldPricingGroup', $oldPricingGroup)\n            ->getQuery()->execute();\n    }\n\n    \/**\n     * @param string $uuid\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUser\n     *\/\n    public function getOneByUuid(string $uuid): CustomerUser\n    {\n        \/** @var \\Shopsys\\FrameworkBundle\\Model\\Customer\\User\\CustomerUser|null $customerUser *\/\n        $customerUser = $this->getCustomerUserRepository()->findOneBy(['uuid' => $uuid]);\n\n        if ($customerUser === null) {\n            throw new CustomerUserNotFoundException('Customer with UUID ' . $uuid . ' does not exist.');\n        }\n\n        return $customerUser;\n    }\n}\n","lang_cluster":"PHP","length":188,"code_uid":"d8a0a00b0cd843caad43bf678dbc3e19"}
{"diff_hunk":"@@ -38,6 +38,28 @@ namespace VuFind\\Hierarchy\\TreeDataSource;\n  *\/\n class PluginManager extends \\VuFind\\ServiceManager\\AbstractPluginManager\n {\n+    \/**\n+     * Default plugin aliases.\n+     *\n+     * @var array\n+     *\/\n+    protected $aliases = [\n+        'solr' => 'VuFind\\Hierarchy\\TreeDataSource\\Solr',\n+        'xmlfile' => 'VuFind\\Hierarchy\\TreeDataSource\\XMLFile',\n+    ];\n+\n+    \/**\n+     * Default plugin factories.\n+     *\n+     * @var array\n+     *\/\n+    protected $factories = [\n+        'VuFind\\Hierarchy\\TreeDataSource\\Solr' =>\n+            'VuFind\\Hierarchy\\TreeDataSource\\Factory::getSolr',\n+        'VuFind\\Hierarchy\\TreeDataSource\\XMLFile' =>\n+            'Zend\\ServiceManager\\Factory\\InvokableFactory',\n+    ];\n+\n     \/**\n      * Return the name of the base class or interface that plug-ins must conform\n      * to.","old_code":"<?php\n\/**\n * Hierarchy tree data source plugin manager\n *\n * PHP version 5\n *\n * Copyright (C) Villanova University 2010.\n *\n * This program is free software; you can redistribute it and\/or modify\n * it under the terms of the GNU General Public License version 2,\n * as published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\n *\n * @category VuFind\n * @package  HierarchyTree_DataSource\n * @author   Demian Katz <demian.katz@villanova.edu>\n * @license  http:\/\/opensource.org\/licenses\/gpl-2.0.php GNU General Public License\n * @link     https:\/\/vufind.org\/wiki\/development:plugins:hierarchy_components Wiki\n *\/\nnamespace VuFind\\Hierarchy\\TreeDataSource;\n\n\/**\n * Hierarchy tree data source plugin manager\n *\n * @category VuFind\n * @package  HierarchyTree_DataSource\n * @author   Demian Katz <demian.katz@villanova.edu>\n * @license  http:\/\/opensource.org\/licenses\/gpl-2.0.php GNU General Public License\n * @link     https:\/\/vufind.org\/wiki\/development:plugins:hierarchy_components Wiki\n *\/\nclass PluginManager extends \\VuFind\\ServiceManager\\AbstractPluginManager\n{\n    \/**\n     * Return the name of the base class or interface that plug-ins must conform\n     * to.\n     *\n     * @return string\n     *\/\n    protected function getExpectedInterface()\n    {\n        return 'VuFind\\Hierarchy\\TreeDataSource\\AbstractBase';\n    }\n}\n","lang_cluster":"PHP","length":51,"code_uid":"847788272cce4426949a64408b30dcdf"}
{"diff_hunk":"@@ -94,8 +94,8 @@ abstract class AbstractSolrTask extends AbstractTask {\n     public function __sleep()\n     {\n         $properties = get_object_vars($this);\n-        \/\/ avoid serialization if the site object\n-        unset($properties['site']);\n+        \/\/ avoid serialization if the site and logger object\n+        unset($properties['site'], $properties['logger']);\n         return array_keys($properties);\n     }\n-}\n+}","old_code":"<?php\nnamespace ApacheSolrForTypo3\\Solr\\Task;\n\n\/***************************************************************\n *  Copyright notice\n *\n *  (c) 2017 Timo Hund <timo.hund@dkd.de>\n *  All rights reserved\n *\n *  This script is part of the TYPO3 project. The TYPO3 project is\n *  free software; you can redistribute it and\/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; either version 3 of the License, or\n *  (at your option) any later version.\n *\n *  The GNU General Public License can be found at\n *  http:\/\/www.gnu.org\/copyleft\/gpl.html.\n *\n *  This script is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *  GNU General Public License for more details.\n *\n *  This copyright notice MUST APPEAR in all copies of the script!\n ***************************************************************\/\n\nuse ApacheSolrForTypo3\\Solr\\Domain\\Site\\SiteRepository;\nuse ApacheSolrForTypo3\\Solr\\Domain\\Site\\Site;\nuse ApacheSolrForTypo3\\Solr\\System\\Logging\\SolrLogManager;\nuse TYPO3\\CMS\\Core\\Utility\\GeneralUtility;\nuse TYPO3\\CMS\\Scheduler\\Task\\AbstractTask;\n\n\/**\n * Abstract scheduler task for solr scheduler tasks, contains the logic to\n * retrieve the site, avoids serialization of site, when scheduler task is saved.\n *\n * @package ApacheSolrForTypo3\\Solr\\Task\n *\/\nabstract class AbstractSolrTask extends AbstractTask {\n    \/**\n     * The site this task is supposed to initialize the index queue for.\n     *\n     * @var Site\n     *\/\n    protected $site;\n\n    \/**\n     * The rootPageId of the site that should be reIndexed\n     *\n     * @var integer\n     *\/\n    protected $rootPageId;\n\n    \/**\n     * @return int\n     *\/\n    public function getRootPageId()\n    {\n        return $this->rootPageId;\n    }\n\n    \/**\n     * @param int $rootPageId\n     *\/\n    public function setRootPageId($rootPageId)\n    {\n        $this->rootPageId = $rootPageId;\n    }\n\n    \/**\n     * @return Site\n     *\/\n    public function getSite()\n    {\n        if (!is_null($this->site)) {\n            return $this->site;\n        }\n\n        try {\n            \/** @var $siteRepository SiteRepository *\/\n            $siteRepository = GeneralUtility::makeInstance(SiteRepository::class);\n            $this->site = $siteRepository->getSiteByRootPageId($this->rootPageId);\n        } catch (\\InvalidArgumentException $e) {\n            $logger = GeneralUtility::makeInstance(SolrLogManager::class, \/** @scrutinizer ignore-type *\/ __CLASS__);\n            $logger->log(SolrLogManager::ERROR, 'Scheduler task tried to get invalid site');\n        }\n\n        return $this->site;\n    }\n\n    \/**\n     * @return array\n     *\/\n    public function __sleep()\n    {\n        $properties = get_object_vars($this);\n        \/\/ avoid serialization if the site object\n        unset($properties['site']);\n        return array_keys($properties);\n    }\n}","lang_cluster":"PHP","length":101,"code_uid":"1d92998c50cb45208da9ba0f6f5c33c3"}
{"diff_hunk":"@@ -4,6 +4,10 @@ declare(strict_types=1);\n \n use Doctrine\\Common\\Annotations\\AnnotationRegistry;\n \n+$symfonyDumpFunctionPath = 'vendor\/symfony\/var-dumper\/Resources\/functions\/dump.php';\n+\n+file_exists(__DIR__ . '\/..\/' . $symfonyDumpFunctionPath) ? require_once __DIR__ . '\/..\/' . $symfonyDumpFunctionPath : require_once __DIR__ . '\/..\/..\/' . $symfonyDumpFunctionPath;\n+\n \/* @var \\Composer\\Autoload\\ClassLoader $loader *\/\n $loader = file_exists(__DIR__ . '\/..\/vendor\/autoload.php') ? require __DIR__ . '\/..\/vendor\/autoload.php' : require __DIR__ . '\/..\/..\/vendor\/autoload.php';\n ","old_code":"<?php\n\ndeclare(strict_types=1);\n\nuse Doctrine\\Common\\Annotations\\AnnotationRegistry;\n\n\/* @var \\Composer\\Autoload\\ClassLoader $loader *\/\n$loader = file_exists(__DIR__ . '\/..\/vendor\/autoload.php') ? require __DIR__ . '\/..\/vendor\/autoload.php' : require __DIR__ . '\/..\/..\/vendor\/autoload.php';\n\nAnnotationRegistry::registerLoader([$loader, 'loadClass']);\n\nreturn $loader;\n","lang_cluster":"PHP","length":12,"code_uid":"b0a79dd2caf64efca36c95f5d9aa46fd"}
{"diff_hunk":"@@ -17,7 +17,7 @@ const SiteNotFound = \"not found\"\n const SiteDirMissing = \"app directory missing\"\n \n \/\/ SiteConfigMissing defines the string used to denote when a site is missing its .ddev\/config.yml file.\n-const SiteConfigMissing = \".ddev\/config.yml missing\"\n+const SiteConfigMissing = \".ddev\/config.yaml missing\"\n \n \/\/ SiteStopped defines the string used to denote when a site is in the stopped state.\n const SiteStopped = \"stopped\"","old_code":"package platform\n\nimport \"fmt\"\nimport (\n\t\"strings\"\n\n\t\"github.com\/fsouza\/go-dockerclient\"\n)\n\n\/\/ SiteRunning defines the string used to denote running sites.\nconst SiteRunning = \"running\"\n\n\/\/ SiteNotFound defines the string used to denote a site where the containers were not found\/do not exist.\nconst SiteNotFound = \"not found\"\n\n\/\/ SiteDirMissing defines the string used to denote when a site is missing its application directory.\nconst SiteDirMissing = \"app directory missing\"\n\n\/\/ SiteConfigMissing defines the string used to denote when a site is missing its .ddev\/config.yml file.\nconst SiteConfigMissing = \".ddev\/config.yml missing\"\n\n\/\/ SiteStopped defines the string used to denote when a site is in the stopped state.\nconst SiteStopped = \"stopped\"\n\n\/\/ App is an interface apps for Drud Local must implement to use shared functionality\ntype App interface {\n\tInit(string) error\n\tDescribe() (map[string]interface{}, error)\n\tGetType() string\n\tAppRoot() string\n\tGetName() string\n\tStart() error\n\tStop() error\n\tDockerEnv()\n\tDockerComposeYAMLPath() string\n\tDown(removeData bool) error\n\tCreateSettingsFile() error\n\tHostName() string\n\tURL() string\n\tImport() error\n\tImportDB(imPath string, extPath string) error\n\tImportFiles(imPath string, extPath string) error\n\tSiteStatus() string\n\tFindContainerByType(containerType string) (docker.APIContainers, error)\n\t\/\/ Returns err, stdout, stderr\n\tExec(service string, cmd ...string) (string, string, error)\n\tExecWithTty(service string, cmd ...string) error\n\tLogs(service string, follow bool, timestamps bool, tail string) error\n}\n\n\/\/ PluginMap maps the name of the plugins to their implementation.\nvar PluginMap = map[string]App{\n\t\"local\": &LocalApp{},\n}\n\n\/\/ GetPluginApp will return an application of the type specified by pluginType\nfunc GetPluginApp(pluginType string) (App, error) {\n\tswitch strings.ToLower(pluginType) {\n\tcase \"local\":\n\t\treturn &LocalApp{}, nil\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"could not find plugin type %s\", pluginType)\n\t}\n}\n","lang_cluster":"PHP","length":64,"code_uid":"6e166d211d934172b1935b0d493277ee"}
{"diff_hunk":"@@ -3,7 +3,7 @@ namespace Psalm\\Tests\\Provider;\n \n use PhpParser;\n \n-class ParserInstanceCacheProvider extends \\Psalm\\Provider\\ParserCacheProvider\n+class ParserInstanceCacheProvider extends \\Psalm\\Internal\\Provider\\ParserCacheProvider\n {\n     \/**\n      * @var array<string, string>","old_code":"<?php\nnamespace Psalm\\Tests\\Provider;\n\nuse PhpParser;\n\nclass ParserInstanceCacheProvider extends \\Psalm\\Provider\\ParserCacheProvider\n{\n    \/**\n     * @var array<string, string>\n     *\/\n    private $file_contents_cache = [];\n\n    \/**\n     * @var array<string, string>\n     *\/\n    private $file_content_hash = [];\n\n    \/**\n     * @var array<string, array<int, PhpParser\\Node\\Stmt>>\n     *\/\n    private $statements_cache = [];\n\n    \/**\n     * @var array<string, float>\n     *\/\n    private $statements_cache_time = [];\n\n    \/**\n     * @var int\n     *\/\n    private $last_good_run = 0;\n\n    public function __construct()\n    {\n    }\n\n    public function loadStatementsFromCache($file_path, $file_modified_time, $file_content_hash)\n    {\n        if (isset($this->statements_cache[$file_path])\n            && $this->statements_cache_time[$file_path] >= $file_modified_time\n            && $this->file_content_hash[$file_path] === $file_content_hash\n        ) {\n            return $this->statements_cache[$file_path];\n        }\n\n        return null;\n    }\n\n    \/**\n     * @param  string   $file_content_hash\n     * @param  string   $file_path\n     * @param mixed $file_modified_time\n     *\n     * @return array<int, PhpParser\\Node\\Stmt>|null\n     *\/\n    public function loadExistingStatementsFromCache($file_path)\n    {\n        if (isset($this->statements_cache[$file_path])) {\n            return $this->statements_cache[$file_path];\n        }\n\n        return null;\n    }\n\n    \/**\n     * @param  string                           $file_path\n     * @param  string                           $file_content_hash\n     * @param  array<int, PhpParser\\Node\\Stmt>  $stmts\n     * @param  bool                             $touch_only\n     *\n     * @return void\n     *\/\n    public function saveStatementsToCache($file_path, $file_content_hash, array $stmts, $touch_only)\n    {\n        $this->statements_cache[$file_path] = $stmts;\n        $this->statements_cache_time[$file_path] = microtime(true);\n        $this->file_content_hash[$file_path] = $file_content_hash;\n    }\n\n    \/**\n     * @param  string   $file_path\n     *\n     * @return string|null\n     *\/\n    public function loadExistingFileContentsFromCache($file_path)\n    {\n        if (isset($this->file_contents_cache[$file_path])) {\n            return $this->file_contents_cache[$file_path];\n        }\n\n        return null;\n    }\n\n    \/**\n     * @param  string  $file_path\n     * @param  string  $file_contents\n     *\n     * @return void\n     *\/\n    public function cacheFileContents($file_path, $file_contents)\n    {\n        $this->file_contents_cache[$file_path] = $file_contents;\n    }\n\n    \/**\n     * @return int\n     *\/\n    public function getLastGoodRun()\n    {\n        return $this->last_good_run;\n    }\n\n    \/**\n     * @param float $start_time\n     *\n     * @return void\n     *\/\n    public function processSuccessfulRun($start_time)\n    {\n        $this->last_good_run = (int) $start_time;\n    }\n\n    \/**\n     * @return bool\n     *\/\n    public function canDiffFiles()\n    {\n        return $this->last_good_run > 0;\n    }\n}\n","lang_cluster":"PHP","length":130,"code_uid":"12fc8660d0d24ca08a426b8e92b66ecc"}
{"diff_hunk":"@@ -4,13 +4,30 @@ declare(strict_types=1);\n \n namespace Bolt\\DataFixtures;\n \n+use Bolt\\Configuration\\Areas;\n+use Bolt\\Configuration\\Config;\n use Doctrine\\Bundle\\FixturesBundle\\Fixture;\n+use Symfony\\Component\\Finder\\Finder;\n+use Tightenco\\Collect\\Support\\Collection;\n+use Webmozart\\PathUtil\\Path;\n \n abstract class BaseFixture extends Fixture\n {\n     private $referencesIndex = [];\n     private $taxonomyIndex = [];\n \n+    \/** @var Config *\/\n+    protected $config;\n+\n+    \/** @var Areas *\/\n+    protected $areas;\n+\n+    public function __construct(Config $config, Areas $areas)\n+    {\n+        $this->config = $config;\n+        $this->areas = $areas;\n+    }\n+\n     protected function getRandomReference(string $entityName)\n     {\n         if (isset($this->referencesIndex[$entityName]) === false) {","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Bolt\\DataFixtures;\n\nuse Doctrine\\Bundle\\FixturesBundle\\Fixture;\n\nabstract class BaseFixture extends Fixture\n{\n    private $referencesIndex = [];\n    private $taxonomyIndex = [];\n\n    protected function getRandomReference(string $entityName)\n    {\n        if (isset($this->referencesIndex[$entityName]) === false) {\n            $this->referencesIndex[$entityName] = [];\n\n            foreach (array_keys($this->referenceRepository->getReferences()) as $key) {\n                if (mb_strpos($key, $entityName.'_') === 0) {\n                    $this->referencesIndex[$entityName][] = $key;\n                }\n            }\n        }\n        if (empty($this->referencesIndex[$entityName])) {\n            throw new \\Exception(sprintf('Cannot find any references for Entity \"%s\"', $entityName));\n        }\n        $randomReferenceKey = array_rand($this->referencesIndex[$entityName], 1);\n\n        return $this->getReference($this->referencesIndex[$entityName][$randomReferenceKey]);\n    }\n\n    protected function getRandomTaxonomies(string $type, int $amount): array\n    {\n        if (empty($this->taxonomyIndex)) {\n            foreach (array_keys($this->referenceRepository->getReferences()) as $key) {\n                if (mb_strpos($key, 'taxonomy_') === 0) {\n                    $tuples = explode('_', $key);\n                    $this->taxonomyIndex[$tuples[1]][] = $key;\n                }\n            }\n        }\n\n        if (empty($this->taxonomyIndex[$type])) {\n            return [];\n        }\n\n        $taxonomies = [];\n\n        foreach ((array) array_rand($this->taxonomyIndex[$type], $amount) as $key) {\n            $taxonomies[] = $this->getReference($this->taxonomyIndex[$type][$key]);\n        }\n\n        return $taxonomies;\n    }\n}\n","lang_cluster":"PHP","length":56,"code_uid":"529b55896b1343b69b58407a121d8e44"}
{"diff_hunk":"@@ -52,7 +52,8 @@ if (TYPO3_MODE == 'BE') {\n         '',\n         [\n             \/\/ An array holding the controller-action-combinations that are accessible\n-            'Administration' => 'index,setSite,setCore,noSiteAvailable'\n+            'Administration' => 'index,setSite,setCore,noSiteAvailable',\n+            'Backend\\\\Web\\\\Info\\\\ApacheSolrDocument' => 'index'\n         ],\n         [\n             'access' => 'admin',","old_code":"<?php\nif (!defined('TYPO3_MODE')) {\n    die('Access denied.');\n}\n\n# ----- # ----- # ----- # ----- # ----- # ----- # ----- # ----- # ----- #\n\n\/\/ add search plugin to content element wizard\nif (TYPO3_MODE == 'BE') {\n    $TBE_MODULES_EXT['xMOD_db_new_content_el']['addElClasses']['ApacheSolrForTypo3\\\\Solr\\\\Backend\\\\ContentElementWizardIconProvider'] =\n        \\TYPO3\\CMS\\Core\\Utility\\ExtensionManagementUtility::extPath($_EXTKEY) . 'Classes\/Backend\/ContentElementWizardIconProvider.php';\n}\n# ----- # ----- # ----- # ----- # ----- # ----- # ----- # ----- # ----- #\n\n$extIconPath = 'EXT:solr\/Resources\/Public\/Images\/Icons\/';\nif (TYPO3_MODE === 'BE') {\n    $modulePrefix = 'extensions-solr-module';\n    $bitmapProvider = \\TYPO3\\CMS\\Core\\Imaging\\IconProvider\\BitmapIconProvider::class;\n    $svgProvider = \\TYPO3\\CMS\\Core\\Imaging\\IconProvider\\SvgIconProvider::class;\n\n        \/\/ register all module icons with extensions-solr-module-modulename\n    $iconRegistry = \\TYPO3\\CMS\\Core\\Utility\\GeneralUtility::makeInstance(\\TYPO3\\CMS\\Core\\Imaging\\IconRegistry::class);\n    $iconRegistry->registerIcon($modulePrefix . '-administration', $svgProvider,\n        ['source' => $extIconPath . 'ModuleAdministration.svg']);\n    $iconRegistry->registerIcon($modulePrefix . '-overview', $bitmapProvider,\n        ['source' => $extIconPath . 'Search.png']);\n    $iconRegistry->registerIcon($modulePrefix . '-indexqueue', $bitmapProvider,\n        ['source' => $extIconPath . 'IndexQueue.png']);\n    $iconRegistry->registerIcon($modulePrefix . '-indexmaintenance', $bitmapProvider,\n        ['source' => $extIconPath . 'IndexMaintenance.png']);\n    $iconRegistry->registerIcon($modulePrefix . '-indexfields', $bitmapProvider,\n        ['source' => $extIconPath . 'IndexFields.png']);\n    $iconRegistry->registerIcon($modulePrefix . '-stopwords', $bitmapProvider,\n        ['source' => $extIconPath . 'StopWords.png']);\n    $iconRegistry->registerIcon($modulePrefix . '-synonyms', $bitmapProvider,\n        ['source' => $extIconPath . 'Synonyms.png']);\n    $iconRegistry->registerIcon($modulePrefix . '-searchstatistics', $bitmapProvider,\n        ['source' => $extIconPath . 'SearchStatistics.png']);\n    $iconRegistry->registerIcon($modulePrefix . '-initsolrconnections', $svgProvider,\n        ['source' => $extIconPath . 'InitSolrConnections.svg']);\n\n    \/\/ register plugin icon\n    $iconRegistry->registerIcon('extensions-solr-plugin-contentelement', $svgProvider,\n        ['source' => $extIconPath . 'ContentElement.svg']);\n}\n\nif (TYPO3_MODE == 'BE') {\n    \\TYPO3\\CMS\\Extbase\\Utility\\ExtensionUtility::registerModule(\n        'ApacheSolrForTypo3.' . $_EXTKEY,\n        'tools',\n        'administration',\n        '',\n        [\n            \/\/ An array holding the controller-action-combinations that are accessible\n            'Administration' => 'index,setSite,setCore,noSiteAvailable'\n        ],\n        [\n            'access' => 'admin',\n            'icon' => 'EXT:solr\/Resources\/Public\/Images\/Icons\/ModuleAdministration.svg',\n            'labels' => 'LLL:EXT:' . $_EXTKEY . '\/Resources\/Private\/Language\/locallang.xlf',\n        ]\n    );\n\n    ApacheSolrForTypo3\\Solr\\Backend\\SolrModule\\AdministrationModuleManager::registerModule(\n        'ApacheSolrForTypo3.' . $_EXTKEY,\n        'Overview',\n        ['index']\n    );\n\n    ApacheSolrForTypo3\\Solr\\Backend\\SolrModule\\AdministrationModuleManager::registerModule(\n        'ApacheSolrForTypo3.' . $_EXTKEY,\n        'IndexQueue',\n        ['index,initializeIndexQueue,resetLogErrors,clearIndexQueue']\n    );\n\n    ApacheSolrForTypo3\\Solr\\Backend\\SolrModule\\AdministrationModuleManager::registerModule(\n        'ApacheSolrForTypo3.' . $_EXTKEY,\n        'IndexMaintenance',\n        ['index,cleanUpIndex,emptyIndex,reloadIndexConfiguration']\n    );\n\n    ApacheSolrForTypo3\\Solr\\Backend\\SolrModule\\AdministrationModuleManager::registerModule(\n        'ApacheSolrForTypo3.' . $_EXTKEY,\n        'IndexFields',\n        ['index']\n    );\n\n    ApacheSolrForTypo3\\Solr\\Backend\\SolrModule\\AdministrationModuleManager::registerModule(\n        'ApacheSolrForTypo3.' . $_EXTKEY,\n        'SearchStatistics',\n        ['index']\n    );\n\n    ApacheSolrForTypo3\\Solr\\Backend\\SolrModule\\AdministrationModuleManager::registerModule(\n        'ApacheSolrForTypo3.' . $_EXTKEY,\n        'StopWords',\n        ['index,saveStopWords']\n    );\n\n    ApacheSolrForTypo3\\Solr\\Backend\\SolrModule\\AdministrationModuleManager::registerModule(\n        'ApacheSolrForTypo3.' . $_EXTKEY,\n        'Synonyms',\n        ['index,addSynonyms,deleteSynonyms']\n    );\n\n    \/\/ registering reports\n    $GLOBALS['TYPO3_CONF_VARS']['SC_OPTIONS']['reports']['tx_reports']['status']['providers']['solr'] = [\n        \\ApacheSolrForTypo3\\Solr\\Report\\SchemaStatus::class,\n        \\ApacheSolrForTypo3\\Solr\\Report\\SolrConfigStatus::class,\n        \\ApacheSolrForTypo3\\Solr\\Report\\SolrConfigurationStatus::class,\n        \\ApacheSolrForTypo3\\Solr\\Report\\SolrStatus::class,\n        \\ApacheSolrForTypo3\\Solr\\Report\\SolrVersionStatus::class,\n        \\ApacheSolrForTypo3\\Solr\\Report\\AccessFilterPluginInstalledStatus::class,\n        \\ApacheSolrForTypo3\\Solr\\Report\\AllowUrlFOpenStatus::class,\n        \\ApacheSolrForTypo3\\Solr\\Report\\FilterVarStatus::class\n    ];\n\n    \/\/ Index Inspector\n    \\TYPO3\\CMS\\Core\\Utility\\ExtensionManagementUtility::insertModuleFunction(\n        'web_info',\n        \\ApacheSolrForTypo3\\Solr\\Backend\\IndexInspector\\IndexInspector::class,\n        null,\n        'LLL:EXT:solr\/Resources\/Private\/Language\/locallang.xlf:module_indexinspector'\n    );\n\n    \/\/ register Clear Cache Menu hook\n    $GLOBALS['TYPO3_CONF_VARS']['SC_OPTIONS']['additionalBackendItems']['cacheActions']['clearSolrConnectionCache'] = \\ApacheSolrForTypo3\\Solr\\ConnectionManager::class;\n}\nif ((TYPO3_MODE === 'BE') || (TYPO3_MODE === 'FE' && isset($_POST['TSFE_EDIT']))) {\n    \/\/ the order of registering the garbage collector and the record monitor is important!\n    \/\/ for certain scenarios items must be removed by GC first, and then be re-added to to Index Queue\n\n    \/\/ hooking into TCE Main to monitor record updates that may require deleting documents from the index\n    $GLOBALS['TYPO3_CONF_VARS']['SC_OPTIONS']['t3lib\/class.t3lib_tcemain.php']['processCmdmapClass'][] = \\ApacheSolrForTypo3\\Solr\\GarbageCollector::class;\n    $GLOBALS['TYPO3_CONF_VARS']['SC_OPTIONS']['t3lib\/class.t3lib_tcemain.php']['processDatamapClass'][] = \\ApacheSolrForTypo3\\Solr\\GarbageCollector::class;\n\n    \/\/ hooking into TCE Main to monitor record updates that may require reindexing by the index queue\n    $GLOBALS['TYPO3_CONF_VARS']['SC_OPTIONS']['t3lib\/class.t3lib_tcemain.php']['processCmdmapClass'][] = \\ApacheSolrForTypo3\\Solr\\IndexQueue\\RecordMonitor::class;\n    $GLOBALS['TYPO3_CONF_VARS']['SC_OPTIONS']['t3lib\/class.t3lib_tcemain.php']['processDatamapClass'][] = \\ApacheSolrForTypo3\\Solr\\IndexQueue\\RecordMonitor::class;\n}\n\n# ----- # ----- # ----- # ----- # ----- # ----- # ----- # ----- # ----- #\n\n\/\/ register click menu item to initialize the Solr connections for a single site\n\/\/ visible for admin users only\n\\TYPO3\\CMS\\Core\\Utility\\ExtensionManagementUtility::addUserTSConfig('\n[adminUser = 1]\noptions.contextMenu.table.pages.items.850 = ITEM\noptions.contextMenu.table.pages.items.850 {\n\tname = Tx_Solr_initializeSolrConnections\n\tlabel = Initialize Solr Connections\n\ticonName = extensions-solr-module-initsolrconnections\n\tdisplayCondition = getRecord|is_siteroot = 1\n\tcallbackAction = initializeSolrConnections\n}\n\noptions.contextMenu.table.pages.items.851 = DIVIDER\n[global]\n');\n\n\\TYPO3\\CMS\\Core\\Utility\\ExtensionManagementUtility::registerExtDirectComponent(\n    'TYPO3.Solr.ContextMenuActionController',\n    \\ApacheSolrForTypo3\\Solr\\ContextMenuActionController::class,\n    'web',\n    'admin'\n);\n\n\/\/ include JS in backend\n$GLOBALS['TYPO3_CONF_VARS']['typo3\/backend.php']['additionalBackendItems']['Solr.ContextMenuInitializeSolrConnectionsAction'] = \\TYPO3\\CMS\\Core\\Utility\\ExtensionManagementUtility::extPath('solr') . 'Classes\/BackendItem\/ContextMenuActionJavascriptRegistration.php';\n\n# ----- # ----- # ----- # ----- # ----- # ----- # ----- # ----- # ----- #\n\n\/\/ replace the built-in search content element\n\\TYPO3\\CMS\\Core\\Utility\\ExtensionManagementUtility::addPiFlexFormValue(\n    '*',\n    'FILE:EXT:' . $_EXTKEY . '\/Configuration\/FlexForms\/Results.xml',\n    'search'\n);\n\n$TCA['tt_content']['types']['search']['showitem'] =\n    '--palette--;LLL:EXT:cms\/locallang_ttc.xml:palette.general;general,\n\t--palette--;LLL:EXT:cms\/locallang_ttc.xml:palette.header;header,\n\t--div--;LLL:EXT:cms\/locallang_ttc.xml:tabs.plugin,\n\t\tpi_flexform;;;;1-1-1,\n\t--div--;LLL:EXT:cms\/locallang_ttc.xml:tabs.access,\n\t\t--palette--;LLL:EXT:cms\/locallang_ttc.xml:palette.visibility;visibility,\n\t\t--palette--;LLL:EXT:cms\/locallang_ttc.xml:palette.access;access,\n\t--div--;LLL:EXT:cms\/locallang_ttc.xml:tabs.appearance,\n\t\t--palette--;LLL:EXT:cms\/locallang_ttc.xml:palette.frames;frames,\n\t--div--;LLL:EXT:cms\/locallang_ttc.xml:tabs.behaviour,\n\t--div--;LLL:EXT:cms\/locallang_ttc.xml:tabs.extended';\n","lang_cluster":"PHP","length":191,"code_uid":"ac494e58c8e1462490e4badb06f348e3"}
{"diff_hunk":"@@ -5,12 +5,16 @@ declare(strict_types=1);\n namespace Tests\\App\\Functional\\Controller;\n \n use App\\DataFixtures\\Demo\\ProductDataFixture;\n+use Faker\\Provider\\Text;\n use Shopsys\\FrameworkBundle\\Component\\Domain\\Domain;\n-use Tests\\App\\Test\\TransactionFunctionalTestCase;\n+use Tests\\App\\Test\\FunctionalTestCase;\n+use Zalas\\Injector\\PHPUnit\\Symfony\\TestCase\\SymfonyTestContainer;\n \n-class ProductRenameRedirectPreviousUrlTest extends TransactionFunctionalTestCase\n+class ProductRenameRedirectPreviousUrlTest extends FunctionalTestCase\n {\n-    private const TESTED_PRODUCT_ID = 1;\n+    use SymfonyTestContainer;\n+\n+    private const TESTED_PRODUCT_ID = 100;\n \n     \/**\n      * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductDataFactoryInterface","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Tests\\App\\Functional\\Controller;\n\nuse App\\DataFixtures\\Demo\\ProductDataFixture;\nuse Shopsys\\FrameworkBundle\\Component\\Domain\\Domain;\nuse Tests\\App\\Test\\TransactionFunctionalTestCase;\n\nclass ProductRenameRedirectPreviousUrlTest extends TransactionFunctionalTestCase\n{\n    private const TESTED_PRODUCT_ID = 1;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductDataFactoryInterface\n     * @inject\n     *\/\n    private $productDataFactory;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade\n     * @inject\n     *\/\n    private $productFacade;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Component\\Router\\FriendlyUrl\\FriendlyUrlFacade\n     * @inject\n     *\/\n    private $friendlyUrlFacade;\n\n    public function testPreviousUrlRedirect(): void\n    {\n        $product = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . self::TESTED_PRODUCT_ID);\n\n        $previousFriendlyUrlSlug = $this->friendlyUrlFacade->findMainFriendlyUrl(Domain::FIRST_DOMAIN_ID, 'front_product_detail', self::TESTED_PRODUCT_ID)->getSlug();\n\n        \/** @var \\App\\Model\\Product\\Product $product *\/\n        $productData = $this->productDataFactory->createFromProduct($product);\n        $productData->name[$this->domain->getDomainConfigById(Domain::FIRST_DOMAIN_ID)->getLocale()] = 'rename';\n\n        $this->productFacade->edit(self::TESTED_PRODUCT_ID, $productData);\n\n        $client = $this->getClient();\n        $client->request('GET', '\/' . $previousFriendlyUrlSlug);\n\n        \/\/ Should be 301 (moved permanently), because old product urls should be permanently redirected\n        $this->assertEquals(301, $client->getResponse()->getStatusCode());\n    }\n}\n","lang_cluster":"PHP","length":51,"code_uid":"c97085e0ea86487a998d881f4a2951e3"}
{"diff_hunk":"@@ -73,6 +73,10 @@ class CarouselLoop extends Image\n     {\n         \/** @var \\Carousel\\Model\\Carousel $carousel *\/\n         foreach ($loopResult->getResultDataCollection() as $carousel) {\n+            if (!file_exists($carousel->getUploadDir() . DS . $carousel->getFile())) {\n+                continue;\n+            }\n+\n             $loopResultRow = new LoopResultRow($carousel);\n \n             $event = new ImageEvent();","old_code":"<?php\n\/*************************************************************************************\/\n\/*      This file is part of the Thelia package.                                     *\/\n\/*                                                                                   *\/\n\/*      Copyright (c) OpenStudio                                                     *\/\n\/*      email : dev@thelia.net                                                       *\/\n\/*      web : http:\/\/www.thelia.net                                                  *\/\n\/*                                                                                   *\/\n\/*      For the full copyright and license information, please view the LICENSE.txt  *\/\n\/*      file that was distributed with this source code.                             *\/\n\/*************************************************************************************\/\n\nnamespace Carousel\\Loop;\n\nuse Carousel\\Model\\CarouselQuery;\nuse Propel\\Runtime\\ActiveQuery\\Criteria;\nuse Thelia\\Core\\Event\\Image\\ImageEvent;\nuse Thelia\\Core\\Event\\TheliaEvents;\nuse Thelia\\Core\\Template\\Element\\LoopResult;\nuse Thelia\\Core\\Template\\Element\\LoopResultRow;\nuse Thelia\\Core\\Template\\Loop\\Argument\\Argument;\nuse Thelia\\Core\\Template\\Loop\\Argument\\ArgumentCollection;\nuse Thelia\\Core\\Template\\Loop\\Image;\nuse Thelia\\Type\\EnumListType;\nuse Thelia\\Type\\EnumType;\nuse Thelia\\Type\\TypeCollection;\n\n\/**\n * Class CarouselLoop\n * @package Carousel\\Loop\n * @author manuel raynaud <mraynaud@openstudio.fr>\n *\/\nclass CarouselLoop extends Image\n{\n\n\n    \/**\n     * @inheritdoc\n     *\/\n    protected function getArgDefinitions()\n    {\n        return new ArgumentCollection(\n            Argument::createIntTypeArgument('width'),\n            Argument::createIntTypeArgument('height'),\n            Argument::createIntTypeArgument('rotation', 0),\n            Argument::createAnyTypeArgument('background_color'),\n            Argument::createIntTypeArgument('quality'),\n            new Argument(\n                'resize_mode',\n                new TypeCollection(\n                    new EnumType(array('crop', 'borders', 'none'))\n                ),\n                'none'\n            ),\n            new Argument(\n                'order',\n                new TypeCollection(\n                    new EnumListType(array('alpha', 'alpha-reverse', 'manual', 'manual-reverse', 'random'))\n                ),\n                'manual'\n            ),\n            Argument::createAnyTypeArgument('effects'),\n            Argument::createBooleanTypeArgument('allow_zoom', false)\n        );\n    }\n\n    \/**\n     * @param LoopResult $loopResult\n     *\n     * @return LoopResult\n     *\/\n    public function parseResults(LoopResult $loopResult)\n    {\n        \/** @var \\Carousel\\Model\\Carousel $carousel *\/\n        foreach ($loopResult->getResultDataCollection() as $carousel) {\n            $loopResultRow = new LoopResultRow($carousel);\n\n            $event = new ImageEvent();\n            $event->setSourceFilepath($carousel->getUploadDir() . DS . $carousel->getFile())\n                ->setCacheSubdirectory('carousel');\n\n            switch ($this->getResizeMode()) {\n                case 'crop':\n                    $resize_mode = \\Thelia\\Action\\Image::EXACT_RATIO_WITH_CROP;\n                    break;\n\n                case 'borders':\n                    $resize_mode = \\Thelia\\Action\\Image::EXACT_RATIO_WITH_BORDERS;\n                    break;\n\n                case 'none':\n                default:\n                    $resize_mode = \\Thelia\\Action\\Image::KEEP_IMAGE_RATIO;\n\n            }\n\n            \/\/ Prepare tranformations\n            $width = $this->getWidth();\n            $height = $this->getHeight();\n            $rotation = $this->getRotation();\n            $background_color = $this->getBackgroundColor();\n            $quality = $this->getQuality();\n            $effects = $this->getEffects();\n\n            if (!is_null($width)) {\n                $event->setWidth($width);\n            }\n            if (!is_null($height)) {\n                $event->setHeight($height);\n            }\n            $event->setResizeMode($resize_mode);\n            if (!is_null($rotation)) {\n                $event->setRotation($rotation);\n            }\n            if (!is_null($background_color)) {\n                $event->setBackgroundColor($background_color);\n            }\n            if (!is_null($quality)) {\n                $event->setQuality($quality);\n            }\n            if (!is_null($effects)) {\n                $event->setEffects($effects);\n            }\n\n            $event->setAllowZoom($this->getAllowZoom());\n\n            \/\/ Dispatch image processing event\n            $this->dispatcher->dispatch(TheliaEvents::IMAGE_PROCESS, $event);\n\n            $loopResultRow\n                ->set('ID', $carousel->getId())\n                ->set(\"LOCALE\", $this->locale)\n                ->set(\"IMAGE_URL\", $event->getFileUrl())\n                ->set(\"ORIGINAL_IMAGE_URL\", $event->getOriginalFileUrl())\n                ->set(\"IMAGE_PATH\", $event->getCacheFilepath())\n                ->set(\"ORIGINAL_IMAGE_PATH\", $event->getSourceFilepath())\n                ->set(\"TITLE\", $carousel->getVirtualColumn('i18n_TITLE'))\n                ->set(\"CHAPO\", $carousel->getVirtualColumn('i18n_CHAPO'))\n                ->set(\"DESCRIPTION\", $carousel->getVirtualColumn('i18n_DESCRIPTION'))\n                ->set(\"POSTSCRIPTUM\", $carousel->getVirtualColumn('i18n_POSTSCRIPTUM'))\n                ->set(\"ALT\", $carousel->getVirtualColumn('i18n_ALT'))\n                ->set(\"URL\", $carousel->getUrl())\n                ->set('POSITION', $carousel->getPosition())\n            ;\n\n            $loopResult->addRow($loopResultRow);\n        }\n\n        return $loopResult;\n    }\n\n    \/**\n     * this method returns a Propel ModelCriteria\n     *\n     * @return \\Propel\\Runtime\\ActiveQuery\\ModelCriteria\n     *\/\n    public function buildModelCriteria()\n    {\n        $search = CarouselQuery::create();\n\n        $this->configureI18nProcessing($search, [ 'ALT', 'TITLE', 'CHAPO', 'DESCRIPTION', 'POSTSCRIPTUM' ]);\n\n        $orders  = $this->getOrder();\n\n        \/\/ Results ordering\n        foreach ($orders as $order) {\n            switch ($order) {\n                case \"alpha\":\n                    $search->addAscendingOrderByColumn('i18n_TITLE');\n                    break;\n                case \"alpha-reverse\":\n                    $search->addDescendingOrderByColumn('i18n_TITLE');\n                    break;\n                case \"manual-reverse\":\n                    $search->orderByPosition(Criteria::DESC);\n                    break;\n                case \"manual\":\n                    $search->orderByPosition(Criteria::ASC);\n                    break;\n                case \"random\":\n                    $search->clearOrderByColumns();\n                    $search->addAscendingOrderByColumn('RAND()');\n                    break(2);\n                    break;\n            }\n        }\n\n        return $search;\n    }\n}\n","lang_cluster":"PHP","length":190,"code_uid":"0ffe957e635a49dcb7d699d1a1e57893"}
{"diff_hunk":"@@ -35,7 +35,7 @@ return [\n     'digits'               => ':attribute \u5fc5\u987b\u662f :digits \u4f4d\u7684\u6570\u5b57\u3002',\n     'digits_between'       => ':attribute \u5fc5\u987b\u662f\u4ecb\u4e8e :min \u548c :max \u4f4d\u7684\u6570\u5b57\u3002',\n     'dimensions'           => ':attribute \u56fe\u7247\u5c3a\u5bf8\u4e0d\u6b63\u786e\u3002',\n-    'distinct'             => ':attribute \u5df2\u7d93\u5b58\u5728\u3002',\n+    'distinct'             => ':attribute \u5df2\u7ecf\u5b58\u5728\u3002',\n     'email'                => ':attribute \u4e0d\u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u90ae\u7bb1\u3002',\n     'exists'               => ':attribute \u4e0d\u5b58\u5728\u3002',\n     'file'                 => ':attribute \u5fc5\u987b\u662f\u6587\u4ef6\u3002',","old_code":"<?php\n\nreturn [\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Validation Language Lines\n    |--------------------------------------------------------------------------\n    |\n    | The following language lines contain the default error messages used by\n    | the validator class. Some of these rules have multiple versions such\n    | such as the size rules. Feel free to tweak each of these messages.\n    |\n    *\/\n\n    'accepted'             => ':attribute \u5fc5\u987b\u63a5\u53d7\u3002',\n    'active_url'           => ':attribute \u4e0d\u662f\u4e00\u4e2a\u6709\u6548\u7684\u7f51\u5740\u3002',\n    'after'                => ':attribute \u5fc5\u987b\u662f\u4e00\u4e2a\u5728 :date \u4e4b\u540e\u7684\u65e5\u671f\u3002',\n    'alpha'                => ':attribute \u53ea\u80fd\u7531\u5b57\u6bcd\u7ec4\u6210\u3002',\n    'alpha_dash'           => ':attribute \u53ea\u80fd\u7531\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u659c\u6760\u7ec4\u6210\u3002',\n    'alpha_num'            => ':attribute \u53ea\u80fd\u7531\u5b57\u6bcd\u548c\u6570\u5b57\u7ec4\u6210\u3002',\n    'array'                => ':attribute \u5fc5\u987b\u662f\u4e00\u4e2a\u6570\u7ec4\u3002',\n    'before'               => ':attribute \u5fc5\u987b\u662f\u4e00\u4e2a\u5728 :date \u4e4b\u524d\u7684\u65e5\u671f\u3002',\n    'between'              => [\n        'numeric' => ':attribute \u5fc5\u987b\u4ecb\u4e8e :min - :max \u4e4b\u95f4\u3002',\n        'file'    => ':attribute \u5fc5\u987b\u4ecb\u4e8e :min - :max kb \u4e4b\u95f4\u3002',\n        'string'  => ':attribute \u5fc5\u987b\u4ecb\u4e8e :min - :max \u4e2a\u5b57\u7b26\u4e4b\u95f4\u3002',\n        'array'   => ':attribute \u5fc5\u987b\u53ea\u6709 :min - :max \u4e2a\u5355\u5143\u3002',\n    ],\n    'boolean'              => ':attribute \u5fc5\u987b\u4e3a\u5e03\u5c14\u503c\u3002',\n    'confirmed'            => ':attribute \u4e24\u6b21\u8f93\u5165\u4e0d\u4e00\u81f4\u3002',\n    'date'                 => ':attribute \u4e0d\u662f\u4e00\u4e2a\u6709\u6548\u7684\u65e5\u671f\u3002',\n    'date_format'          => ':attribute \u7684\u683c\u5f0f\u5fc5\u987b\u4e3a :format\u3002',\n    'different'            => ':attribute \u548c :other \u5fc5\u987b\u4e0d\u540c\u3002',\n    'digits'               => ':attribute \u5fc5\u987b\u662f :digits \u4f4d\u7684\u6570\u5b57\u3002',\n    'digits_between'       => ':attribute \u5fc5\u987b\u662f\u4ecb\u4e8e :min \u548c :max \u4f4d\u7684\u6570\u5b57\u3002',\n    'dimensions'           => ':attribute \u56fe\u7247\u5c3a\u5bf8\u4e0d\u6b63\u786e\u3002',\n    'distinct'             => ':attribute \u5df2\u7d93\u5b58\u5728\u3002',\n    'email'                => ':attribute \u4e0d\u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u90ae\u7bb1\u3002',\n    'exists'               => ':attribute \u4e0d\u5b58\u5728\u3002',\n    'file'                 => ':attribute \u5fc5\u987b\u662f\u6587\u4ef6\u3002',\n    'filled'               => ':attribute \u4e0d\u80fd\u4e3a\u7a7a\u3002',\n    'image'                => ':attribute \u5fc5\u987b\u662f\u56fe\u7247\u3002',\n    'in'                   => '\u5df2\u9009\u7684\u5c5e\u6027 :attribute \u975e\u6cd5\u3002',\n    'in_array'             => ':attribute \u6ca1\u6709\u5728 :other \u4e2d\u3002',\n    'integer'              => ':attribute \u5fc5\u987b\u662f\u6574\u6570\u3002',\n    'ip'                   => ':attribute \u5fc5\u987b\u662f\u6709\u6548\u7684 IP \u5730\u5740\u3002',\n    'json'                 => ':attribute \u5fc5\u987b\u662f\u6b63\u786e\u7684 JSON \u683c\u5f0f\u3002',\n    'max'                  => [\n        'numeric' => ':attribute \u4e0d\u80fd\u5927\u4e8e :max\u3002',\n        'file'    => ':attribute \u4e0d\u80fd\u5927\u4e8e :max kb\u3002',\n        'string'  => ':attribute \u4e0d\u80fd\u5927\u4e8e :max \u4e2a\u5b57\u7b26\u3002',\n        'array'   => ':attribute \u6700\u591a\u53ea\u6709 :max \u4e2a\u5355\u5143\u3002',\n    ],\n    'mimes'                => ':attribute \u5fc5\u987b\u662f\u4e00\u4e2a :values \u7c7b\u578b\u7684\u6587\u4ef6\u3002',\n    'min'                  => [\n        'numeric' => ':attribute \u5fc5\u987b\u5927\u4e8e\u7b49\u4e8e :min\u3002',\n        'file'    => ':attribute \u5927\u5c0f\u4e0d\u80fd\u5c0f\u4e8e :min kb\u3002',\n        'string'  => ':attribute \u81f3\u5c11\u4e3a :min \u4e2a\u5b57\u7b26\u3002',\n        'array'   => ':attribute \u81f3\u5c11\u6709 :min \u4e2a\u5355\u5143\u3002',\n    ],\n    'not_in'               => '\u5df2\u9009\u7684\u5c5e\u6027 :attribute \u975e\u6cd5\u3002',\n    'numeric'              => ':attribute \u5fc5\u987b\u662f\u4e00\u4e2a\u6570\u5b57\u3002',\n    'present'              => ':attribute \u5fc5\u987b\u5b58\u5728\u3002',\n    'regex'                => ':attribute \u683c\u5f0f\u4e0d\u6b63\u786e\u3002',\n    'required'             => ':attribute \u4e0d\u80fd\u4e3a\u7a7a\u3002',\n    'required_if'          => '\u5f53 :other \u4e3a :value \u65f6 :attribute \u4e0d\u80fd\u4e3a\u7a7a\u3002',\n    'required_unless'      => '\u5f53 :other \u4e0d\u4e3a :value \u65f6 :attribute \u4e0d\u80fd\u4e3a\u7a7a\u3002',\n    'required_with'        => '\u5f53 :values \u5b58\u5728\u65f6 :attribute \u4e0d\u80fd\u4e3a\u7a7a\u3002',\n    'required_with_all'    => '\u5f53 :values \u5b58\u5728\u65f6 :attribute \u4e0d\u80fd\u4e3a\u7a7a\u3002',\n    'required_without'     => '\u5f53 :values \u4e0d\u5b58\u5728\u65f6 :attribute \u4e0d\u80fd\u4e3a\u7a7a\u3002',\n    'required_without_all' => '\u5f53 :values \u90fd\u4e0d\u5b58\u5728\u65f6 :attribute \u4e0d\u80fd\u4e3a\u7a7a\u3002',\n    'same'                 => ':attribute \u548c :other \u5fc5\u987b\u76f8\u540c\u3002',\n    'size'                 => [\n        'numeric' => ':attribute \u5927\u5c0f\u5fc5\u987b\u4e3a :size\u3002',\n        'file'    => ':attribute \u5927\u5c0f\u5fc5\u987b\u4e3a :size kb\u3002',\n        'string'  => ':attribute \u5fc5\u987b\u662f :size \u4e2a\u5b57\u7b26\u3002',\n        'array'   => ':attribute \u5fc5\u987b\u4e3a :size \u4e2a\u5355\u5143\u3002',\n    ],\n    'string'               => ':attribute \u5fc5\u987b\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002',\n    'timezone'             => ':attribute \u5fc5\u987b\u662f\u4e00\u4e2a\u5408\u6cd5\u7684\u65f6\u533a\u503c\u3002',\n    'unique'               => ':attribute \u5df2\u7ecf\u5b58\u5728\u3002',\n    'url'                  => ':attribute \u683c\u5f0f\u4e0d\u6b63\u786e\u3002',\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Custom Validation Language Lines\n    |--------------------------------------------------------------------------\n    |\n    | Here you may specify custom validation messages for attributes using the\n    | convention 'attribute.rule' to name the lines. This makes it quick to\n    | specify a specific custom language line for a given attribute rule.\n    |\n    *\/\n\n    'custom'               => [\n        'attribute-name' => [\n            'rule-name' => 'custom-message',\n        ],\n    ],\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Custom Validation Attributes\n    |--------------------------------------------------------------------------\n    |\n    | The following language lines are used to swap attribute place-holders\n    | with something more reader friendly such as E-Mail Address instead\n    | of 'email'. This simply helps us make messages a little cleaner.\n    |\n    *\/\n\n    'attributes'           => [\n        'name'                  => '\u540d\u79f0',\n        'username'              => '\u7528\u6237\u540d',\n        'email'                 => '\u90ae\u7bb1',\n        'first_name'            => '\u540d',\n        'last_name'             => '\u59d3',\n        'password'              => '\u5bc6\u7801',\n        'password_confirmation' => '\u786e\u8ba4\u5bc6\u7801',\n        'city'                  => '\u57ce\u5e02',\n        'country'               => '\u56fd\u5bb6',\n        'address'               => '\u5730\u5740',\n        'phone'                 => '\u7535\u8bdd',\n        'mobile'                => '\u624b\u673a',\n        'age'                   => '\u5e74\u9f84',\n        'sex'                   => '\u6027\u522b',\n        'gender'                => '\u6027\u522b',\n        'day'                   => '\u5929',\n        'month'                 => '\u6708',\n        'year'                  => '\u5e74',\n        'hour'                  => '\u65f6',\n        'minute'                => '\u5206',\n        'second'                => '\u79d2',\n        'title'                 => '\u6807\u9898',\n        'content'               => '\u5185\u5bb9',\n        'description'           => '\u63cf\u8ff0',\n        'excerpt'               => '\u6458\u8981',\n        'date'                  => '\u65e5\u671f',\n        'time'                  => '\u65f6\u95f4',\n        'available'             => '\u53ef\u7528\u7684',\n        'size'                  => '\u5927\u5c0f',\n    ],\n\n];\n","lang_cluster":"PHP","length":145,"code_uid":"9e574b20b97e4fe2bd99be537f8889c1"}
{"diff_hunk":"@@ -64,4 +64,12 @@ class ProductVisibility\n     {\n         return $this->visible;\n     }\n+\n+    \/**\n+     * @return \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroup\n+     *\/\n+    public function getPricingGroup(): PricingGroup\n+    {\n+        return $this->pricingGroup;\n+    }\n }","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\Model\\Product;\n\nuse Doctrine\\ORM\\Mapping as ORM;\nuse Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroup;\n\n\/**\n * @ORM\\Table(name=\"product_visibilities\")\n * @ORM\\Entity\n *\/\nclass ProductVisibility\n{\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\Product\n     *\n     * @ORM\\Id\n     * @ORM\\ManyToOne(targetEntity=\"Shopsys\\FrameworkBundle\\Model\\Product\\Product\")\n     * @ORM\\JoinColumn(nullable=false, name=\"product_id\", referencedColumnName=\"id\", onDelete=\"CASCADE\")\n     *\/\n    protected $product;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroup\n     *\n     * @ORM\\Id\n     * @ORM\\ManyToOne(targetEntity=\"Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroup\")\n     * @ORM\\JoinColumn(nullable=false, name=\"pricing_group_id\", referencedColumnName=\"id\", onDelete=\"CASCADE\")\n     *\/\n    protected $pricingGroup;\n\n    \/**\n     * @var int\n     *\n     * @ORM\\Id\n     * @ORM\\Column(type=\"integer\")\n     *\/\n    protected $domainId;\n\n    \/**\n     * @var bool\n     *\n     * @ORM\\Column(type=\"boolean\")\n     *\/\n    protected $visible;\n\n    \/**\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\Product $product\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroup $pricingGroup\n     * @param int $domainId\n     *\/\n    public function __construct(\n        Product $product,\n        PricingGroup $pricingGroup,\n        $domainId\n    ) {\n        $this->product = $product;\n        $this->pricingGroup = $pricingGroup;\n        $this->domainId = $domainId;\n        $this->visible = false;\n    }\n\n    public function isVisible()\n    {\n        return $this->visible;\n    }\n}\n","lang_cluster":"PHP","length":67,"code_uid":"3f804ead300d4a708a5fa197e068b703"}
{"diff_hunk":"@@ -14,6 +14,7 @@ use Ergonode\\SharedKernel\\Domain\\AbstractCode;\n class AttributeCode extends AbstractCode\n {\n     public const PATTERN = '\/^([a-zA-Z0-9_]+)$\/';\n+    public const FORBIDDEN = ['id'];\n \n     public function __construct(string $value)\n     {","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types=1);\n\nnamespace Ergonode\\Attribute\\Domain\\ValueObject;\n\nuse Ergonode\\SharedKernel\\Domain\\AbstractCode;\n\nclass AttributeCode extends AbstractCode\n{\n    public const PATTERN = '\/^([a-zA-Z0-9_]+)$\/';\n\n    public function __construct(string $value)\n    {\n        parent::__construct(strtolower($value));\n    }\n\n    public static function isValid(string $value): bool\n    {\n        $value = strtolower($value);\n\n        return parent::isValid($value)\n            && preg_match(self::PATTERN, $value);\n    }\n}\n","lang_cluster":"PHP","length":30,"code_uid":"5981167a484f44a29edcc2403b52ae8c"}
{"diff_hunk":"@@ -23,10 +23,11 @@ function roots_scripts() {\n   if (is_single() && comments_open() && get_option('thread_comments')) {\n     wp_enqueue_script('comment-reply');\n   }\n-\n-  wp_register_script('roots_plugins', get_template_directory_uri() . '\/js\/plugins.js', false, null, false);\n+  \n+  \/\/ Not included by default since this code is used for debugging javas in the console. Uncomment next line and wp_enqueue_script below to use it.\n+  \/\/wp_register_script('roots_plugins', get_template_directory_uri() . '\/js\/plugins.js', false, null, false);\n   wp_register_script('roots_main', get_template_directory_uri() . '\/js\/main.js', false, null, false);\n-  wp_enqueue_script('roots_plugins');\n+  \/\/wp_enqueue_script('roots_plugins');\n   wp_enqueue_script('roots_main');\n }\n ","old_code":"<?php\n\nfunction roots_scripts() {\n  \/\/ Not included by default since Bootstrap's reset supersedes h5bp's. Include if you aren't using Bootstrap. \n  \/\/wp_enqueue_style('roots_style', get_template_directory_uri() . '\/css\/style.css', false, null);\n  wp_enqueue_style('roots_bootstrap_style', get_template_directory_uri() . '\/css\/bootstrap.css', false, null);\n\n  if (BOOTSTRAP_RESPONSIVE) {\n    wp_enqueue_style('roots_bootstrap_responsive_style', get_template_directory_uri() . '\/css\/bootstrap-responsive.css', array('roots_bootstrap_style'), null);\n  }\n\n  wp_enqueue_style('roots_app_style', get_template_directory_uri() . '\/css\/app.css', false, null);\n\n  if (is_child_theme()) {\n    wp_enqueue_style('roots_child_style', get_stylesheet_uri());\n  }\n\n  if (!is_admin()) {\n    wp_deregister_script('jquery');\n    wp_register_script('jquery', '', '', '', false);\n  }\n\n  if (is_single() && comments_open() && get_option('thread_comments')) {\n    wp_enqueue_script('comment-reply');\n  }\n\n  wp_register_script('roots_plugins', get_template_directory_uri() . '\/js\/plugins.js', false, null, false);\n  wp_register_script('roots_main', get_template_directory_uri() . '\/js\/main.js', false, null, false);\n  wp_enqueue_script('roots_plugins');\n  wp_enqueue_script('roots_main');\n}\n\nadd_action('wp_enqueue_scripts', 'roots_scripts', 100);\n","lang_cluster":"PHP","length":33,"code_uid":"146a308a37da48939825c7cbf9cab5a0"}
{"diff_hunk":"@@ -17,6 +17,7 @@\n \/\/ along with BOINC.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n \n \/\/ web interfaces for viewing and controlling batches\n+\/\/ DEPRECATED: replaced by submit.php\n \n ini_set('display_errors', 'stdout');\n error_reporting(E_ALL);","old_code":"<?php\n\/\/ This file is part of BOINC.\n\/\/ http:\/\/boinc.berkeley.edu\n\/\/ Copyright (C) 2011 University of California\n\/\/\n\/\/ BOINC is free software; you can redistribute it and\/or modify it\n\/\/ under the terms of the GNU Lesser General Public License\n\/\/ as published by the Free Software Foundation,\n\/\/ either version 3 of the License, or (at your option) any later version.\n\/\/\n\/\/ BOINC is distributed in the hope that it will be useful,\n\/\/ but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\/\/ See the GNU Lesser General Public License for more details.\n\/\/\n\/\/ You should have received a copy of the GNU Lesser General Public License\n\/\/ along with BOINC.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n\/\/ web interfaces for viewing and controlling batches\n\nini_set('display_errors', 'stdout');\nerror_reporting(E_ALL);\n\nrequire_once(\"..\/inc\/util.inc\");\nrequire_once(\"..\/inc\/boinc_db.inc\");\nrequire_once(\"..\/inc\/result.inc\");\nrequire_once(\"..\/inc\/submit_db.inc\");\n\nfunction show_batch($user) {\n    $batch_id = get_int('batch_id');\n    $batch = BoincBatch::lookup_id($batch_id);\n    if (!$batch || $batch->user_id != $user->id) {\n        error_page(\"no batch\");\n    }\n    page_head(\"Batch $batch->id\");\n    $results = BoincResult::enum(\"batch=$batch->id order by workunitid\");\n    result_table_start(true, true, null);\n    foreach ($results as $result) {\n        show_result_row($result, true, true, true);\n    }\n    end_table();\n    page_tail();\n}\n\nfunction show_batches($user) {\n    $batches = BoincBatch::enum(\"user_id=$user->id\");\n    page_head(\"Batches\");\n    start_table();\n    table_header(\"Batch ID\", \"Submitted\", \"# jobs\");\n    foreach ($batches as $batch) {\n        echo \"<tr>\n            <td><a href=submit_status.php?action=show_batch&batch_id=$batch->id>$batch->id<\/a><\/td>\n            <td>\".time_str($batch->create_time).\"<\/td>\n            <td>$batch->njobs<\/td>\n            <\/tr>\n        \";\n    }\n    end_table();\n    page_tail();\n}\n\n$user = get_logged_in_user();\n\n$action = get_str('action', true);\nswitch ($action) {\ncase '': show_batches($user); break;\ncase 'show_batch': show_batch($user);\n}\n?>\n","lang_cluster":"PHP","length":69,"code_uid":"8e9d49e5c24d47cfbccc44217b448af5"}
{"diff_hunk":"@@ -27,7 +27,7 @@ final class CategoryManager implements CategoryManagerInterface\n      *\/\n     private $categoryManager;\n \n-    public function __construct(ManagerInterface $categoryManager)\n+    public function __construct(?ManagerInterface $categoryManager = null)\n     {\n         $this->categoryManager = $categoryManager;\n     }","old_code":"<?php\n\ndeclare(strict_types=1);\n\n\/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Model;\n\nuse Sonata\\ClassificationBundle\\Model\\CategoryInterface;\nuse Sonata\\ClassificationBundle\\Model\\CategoryManagerInterface as ManagerInterface;\n\n\/**\n * @author Joao Albuquerque <albuquerque.joao.filipe@gmail.com>\n * @author Christian Gripp <mail@core23.de>\n *\/\nfinal class CategoryManager implements CategoryManagerInterface\n{\n    \/**\n     * @var ManagerInterface\n     *\/\n    private $categoryManager;\n\n    public function __construct(ManagerInterface $categoryManager)\n    {\n        $this->categoryManager = $categoryManager;\n    }\n\n    public function getRootCategory($context): CategoryInterface\n    {\n        return $this->categoryManager->getRootCategory($context);\n    }\n\n    public function getRootCategories($loadChildren = true): iterable\n    {\n        return $this->categoryManager->getRootCategories($loadChildren);\n    }\n\n    public function find($categoryId): ?CategoryInterface\n    {\n        return $this->categoryManager->find($categoryId);\n    }\n\n    public function findBy(array $criteria): iterable\n    {\n        return $this->categoryManager->findBy($criteria);\n    }\n\n    public function findOneBy(array $criteria): ?CategoryInterface\n    {\n        return $this->categoryManager->findOneBy($criteria);\n    }\n\n    public function create(): CategoryInterface\n    {\n        return $this->categoryManager->create();\n    }\n\n    public function save($category): void\n    {\n        $this->categoryManager->save($category);\n    }\n}\n","lang_cluster":"PHP","length":69,"code_uid":"987ebb1e337f41a2b080f1e9e08ba30c"}
{"diff_hunk":"@@ -62,6 +62,10 @@ class SimpleResizer implements ResizerInterface\n         if ($settings['height'] == null) {\n             $settings['height'] = (int) ($settings['width'] * $size->getHeight() \/ $size->getWidth());\n         }\n+        \n+        if ($settings['width'] == null) {\n+            $settings['width'] = (int) ($settings['height'] * $size->getWidth() \/ $size->getHeight());\n+        }\n \n         return $this->computeBox($media, $settings);\n     }","old_code":"<?php\n\n\/*\n * This file is part of the Sonata project.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Resizer;\n\nuse Imagine\\Image\\ImagineInterface;\nuse Imagine\\Image\\Box;\nuse Gaufrette\\File;\nuse Sonata\\MediaBundle\\Model\\MediaInterface;\nuse Imagine\\Image\\ImageInterface;\nuse Imagine\\Exception\\InvalidArgumentException;\n\nclass SimpleResizer implements ResizerInterface\n{\n    protected $adapter;\n\n    protected $mode;\n\n    \/**\n     * @param \\Imagine\\Image\\ImagineInterface $adapter\n     * @param string                          $mode\n     *\/\n    public function __construct(ImagineInterface $adapter, $mode)\n    {\n        $this->adapter = $adapter;\n        $this->mode    = $mode;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function resize(MediaInterface $media, File $in, File $out, $format, array $settings)\n    {\n        if (!isset($settings['width'])) {\n            throw new \\RuntimeException(sprintf('Width parameter is missing in context \"%s\" for provider \"%s\"', $media->getContext(), $media->getProviderName()));\n        }\n\n        $image = $this->adapter->load($in->getContent());\n\n        $content = $image\n            ->thumbnail($this->getBox($media, $settings), $this->mode)\n            ->get($format, array('quality' => $settings['quality']));\n\n        $out->setContent($content);\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getBox(MediaInterface $media, array $settings)\n    {\n        $size = $media->getBox();\n\n        if ($settings['height'] == null) {\n            $settings['height'] = (int) ($settings['width'] * $size->getHeight() \/ $size->getWidth());\n        }\n\n        return $this->computeBox($media, $settings);\n    }\n\n    \/**\n     * @throws \\Imagine\\Exception\\InvalidArgumentException\n     *\n     * @param \\Sonata\\MediaBundle\\Model\\MediaInterface $media\n     * @param array                                    $settings\n     *\n     * @return \\Imagine\\Image\\Box\n     *\/\n    private function computeBox(MediaInterface $media, array $settings)\n    {\n        if ($this->mode !== ImageInterface::THUMBNAIL_INSET && $this->mode !== ImageInterface::THUMBNAIL_OUTBOUND) {\n            throw new InvalidArgumentException('Invalid mode specified');\n        }\n\n        $size = $media->getBox();\n\n        $ratios = array(\n            $settings['width'] \/ $size->getWidth(),\n            $settings['height'] \/ $size->getHeight()\n        );\n\n        if ($this->mode === ImageInterface::THUMBNAIL_INSET) {\n            $ratio = min($ratios);\n        } else {\n            $ratio = max($ratios);\n        }\n\n        return $size->scale($ratio);\n    }\n}","lang_cluster":"PHP","length":98,"code_uid":"4bb056819918467b8d143002e3f4d0d2"}
{"diff_hunk":"@@ -2,6 +2,8 @@\n \n namespace Shopsys\\ShopBundle\\Command;\n \n+use Shopsys\\ShopBundle\\Component\\Image\\DirectoryStructureCreator as ImageDirectoryStructureCreator;\n+use Shopsys\\ShopBundle\\Component\\UploadedFile\\DirectoryStructureCreator as UploadedFileDirectoryStructureCreator;\n use Symfony\\Bundle\\FrameworkBundle\\Command\\ContainerAwareCommand;\n use Symfony\\Component\\Console\\Input\\InputInterface;\n use Symfony\\Component\\Console\\Output\\OutputInterface;","old_code":"<?php\n\nnamespace Shopsys\\ShopBundle\\Command;\n\nuse Symfony\\Bundle\\FrameworkBundle\\Command\\ContainerAwareCommand;\nuse Symfony\\Component\\Console\\Input\\InputInterface;\nuse Symfony\\Component\\Console\\Output\\OutputInterface;\n\nclass CreateApplicationDirectoriesCommand extends ContainerAwareCommand\n{\n    protected function configure()\n    {\n        $this\n            ->setName('shopsys:create-directories')\n            ->setDescription('Create application directories for locks, docs, content, images, uploaded files, etc.');\n    }\n\n    \/**\n     * @param \\Symfony\\Component\\Console\\Input\\InputInterface $input\n     * @param \\Symfony\\Component\\Console\\Output\\OutputInterface $output\n     *\/\n    protected function execute(InputInterface $input, OutputInterface $output)\n    {\n        $this->createMiscellaneousDirectories($output);\n        $this->createImageDirectories($output);\n        $this->createUploadedFileDirectories($output);\n    }\n\n    private function createMiscellaneousDirectories(OutputInterface $output)\n    {\n        $rootDirectory = $this->getContainer()->getParameter('shopsys.root_dir');\n        $webDirectory = $this->getContainer()->getParameter('shopsys.web_dir');\n\n        $directories = [\n            $rootDirectory . '\/build\/stats',\n            $rootDirectory . '\/docs\/generated',\n            $rootDirectory . '\/var\/cache',\n            $rootDirectory . '\/var\/lock',\n            $rootDirectory . '\/var\/logs',\n            $rootDirectory . '\/var\/errorPages',\n            $webDirectory . '\/assets\/admin\/styles',\n            $webDirectory . '\/assets\/frontend\/styles',\n            $webDirectory . '\/assets\/scripts',\n            $webDirectory . '\/content\/feeds',\n            $webDirectory . '\/content\/sitemaps',\n            $webDirectory . '\/content\/wysiwyg',\n        ];\n\n        $filesystem = $this->getContainer()->get('filesystem');\n        \/* @var $filesystem \\Symfony\\Component\\Filesystem\\Filesystem *\/\n\n        $filesystem->mkdir($directories);\n\n        $output->writeln('<fg=green>Miscellaneous application directories were successfully created.<\/fg=green>');\n    }\n\n    \/**\n     * @param \\Symfony\\Component\\Console\\Output\\OutputInterface $output\n     *\/\n    private function createImageDirectories(OutputInterface $output)\n    {\n        $imageDirectoryStructureCreator = $this->getContainer()\n            ->get('shopsys.shop.component.image.directory_structure_creator');\n        \/* @var $imageDirectoryStructureCreator \\Shopsys\\ShopBundle\\Component\\Image\\DirectoryStructureCreator *\/\n        $imageDirectoryStructureCreator->makeImageDirectories();\n\n        $output->writeln('<fg=green>Directories for images were successfully created.<\/fg=green>');\n    }\n\n    \/**\n     * @param \\Symfony\\Component\\Console\\Output\\OutputInterface $output\n     *\/\n    private function createUploadedFileDirectories(OutputInterface $output)\n    {\n        $uploadedFileDirectoryStructureCreator = $this->getContainer()\n            ->get('shopsys.shop.component.uploaded_file.directory_structure_creator');\n        \/* @var $uploadedFileDirectoryStructureCreator \\Shopsys\\ShopBundle\\Component\\UploadedFile\\DirectoryStructureCreator *\/\n        $uploadedFileDirectoryStructureCreator->makeUploadedFileDirectories();\n\n        $output->writeln('<fg=green>Directories for UploadedFile entities were successfully created.<\/fg=green>');\n    }\n}\n","lang_cluster":"PHP","length":82,"code_uid":"6d1d43c1528a4f9f9904608fec833ce6"}
{"diff_hunk":"@@ -174,6 +174,7 @@ return [\n         'password_confirmation' => 'confirmaci\u00f3n de la contrase\u00f1a',\n         'phone'                 => 'tel\u00e9fono',\n         'price'                 => 'precio',\n+        'role'                  => 'rol',\n         'second'                => 'segundo',\n         'sex'                   => 'sexo',\n         'subject'               => 'asunto',","old_code":"<?php\n\nreturn [\n    \/*\n    |--------------------------------------------------------------------------\n    | Validation Language Lines\n    |--------------------------------------------------------------------------\n    |\n    | The following language lines contain the default error messages used by\n    | the validator class. Some of these rules have multiple versions such\n    | as the size rules. Feel free to tweak each of these messages.\n    |\n    *\/\n\n    'accepted'             => ':attribute debe ser aceptado.',\n    'active_url'           => ':attribute no es una URL v\u00e1lida.',\n    'after'                => ':attribute debe ser una fecha posterior a :date.',\n    'after_or_equal'       => ':attribute debe ser una fecha posterior o igual a :date.',\n    'alpha'                => ':attribute s\u00f3lo debe contener letras.',\n    'alpha_dash'           => ':attribute s\u00f3lo debe contener letras, n\u00fameros, guiones y guiones bajos.',\n    'alpha_num'            => ':attribute s\u00f3lo debe contener letras y n\u00fameros.',\n    'array'                => ':attribute debe ser un conjunto.',\n    'before'               => ':attribute debe ser una fecha anterior a :date.',\n    'before_or_equal'      => ':attribute debe ser una fecha anterior o igual a :date.',\n    'between'              => [\n        'numeric' => ':attribute tiene que estar entre :min - :max.',\n        'file'    => ':attribute debe pesar entre :min - :max kilobytes.',\n        'string'  => ':attribute tiene que tener entre :min - :max caracteres.',\n        'array'   => ':attribute tiene que tener entre :min - :max elementos.',\n    ],\n    'boolean'              => 'El campo :attribute debe tener un valor verdadero o falso.',\n    'confirmed'            => 'La confirmaci\u00f3n de :attribute no coincide.',\n    'date'                 => ':attribute no es una fecha v\u00e1lida.',\n    'date_equals'          => ':attribute debe ser una fecha igual a :date.',\n    'date_format'          => ':attribute no corresponde al formato :format.',\n    'different'            => ':attribute y :other deben ser diferentes.',\n    'digits'               => ':attribute debe tener :digits d\u00edgitos.',\n    'digits_between'       => ':attribute debe tener entre :min y :max d\u00edgitos.',\n    'dimensions'           => 'Las dimensiones de la imagen :attribute no son v\u00e1lidas.',\n    'distinct'             => 'El campo :attribute contiene un valor duplicado.',\n    'email'                => ':attribute no es un correo v\u00e1lido.',\n    'ends_with'            => 'El campo :attribute debe finalizar con uno de los siguientes valores: :values',\n    'exists'               => ':attribute es inv\u00e1lido.',\n    'file'                 => 'El campo :attribute debe ser un archivo.',\n    'filled'               => 'El campo :attribute es obligatorio.',\n    'gt'                   => [\n        'numeric' => 'El campo :attribute debe ser mayor que :value.',\n        'file'    => 'El campo :attribute debe tener m\u00e1s de :value kilobytes.',\n        'string'  => 'El campo :attribute debe tener m\u00e1s de :value caracteres.',\n        'array'   => 'El campo :attribute debe tener m\u00e1s de :value elementos.',\n    ],\n    'gte'                  => [\n        'numeric' => 'El campo :attribute debe ser como m\u00ednimo :value.',\n        'file'    => 'El campo :attribute debe tener como m\u00ednimo :value kilobytes.',\n        'string'  => 'El campo :attribute debe tener como m\u00ednimo :value caracteres.',\n        'array'   => 'El campo :attribute debe tener como m\u00ednimo :value elementos.',\n    ],\n    'image'                => ':attribute debe ser una imagen.',\n    'in'                   => ':attribute es inv\u00e1lido.',\n    'in_array'             => 'El campo :attribute no existe en :other.',\n    'integer'              => ':attribute debe ser un n\u00famero entero.',\n    'ip'                   => ':attribute debe ser una direcci\u00f3n IP v\u00e1lida.',\n    'ipv4'                 => ':attribute debe ser una direcci\u00f3n IPv4 v\u00e1lida.',\n    'ipv6'                 => ':attribute debe ser una direcci\u00f3n IPv6 v\u00e1lida.',\n    'json'                 => 'El campo :attribute debe ser una cadena JSON v\u00e1lida.',\n    'lt'                   => [\n        'numeric' => 'El campo :attribute debe ser menor que :value.',\n        'file'    => 'El campo :attribute debe tener menos de :value kilobytes.',\n        'string'  => 'El campo :attribute debe tener menos de :value caracteres.',\n        'array'   => 'El campo :attribute debe tener menos de :value elementos.',\n    ],\n    'lte'                  => [\n        'numeric' => 'El campo :attribute debe ser como m\u00e1ximo :value.',\n        'file'    => 'El campo :attribute debe tener como m\u00e1ximo :value kilobytes.',\n        'string'  => 'El campo :attribute debe tener como m\u00e1ximo :value caracteres.',\n        'array'   => 'El campo :attribute debe tener como m\u00e1ximo :value elementos.',\n    ],\n    'max'                  => [\n        'numeric' => ':attribute no debe ser mayor que :max.',\n        'file'    => ':attribute no debe ser mayor que :max kilobytes.',\n        'string'  => ':attribute no debe ser mayor que :max caracteres.',\n        'array'   => ':attribute no debe tener m\u00e1s de :max elementos.',\n    ],\n    'mimes'                => ':attribute debe ser un archivo con formato: :values.',\n    'mimetypes'            => ':attribute debe ser un archivo con formato: :values.',\n    'min'                  => [\n        'numeric' => 'El tama\u00f1o de :attribute debe ser de al menos :min.',\n        'file'    => 'El tama\u00f1o de :attribute debe ser de al menos :min kilobytes.',\n        'string'  => ':attribute debe contener al menos :min caracteres.',\n        'array'   => ':attribute debe tener al menos :min elementos.',\n    ],\n    'multiple_of'          => 'El campo :attribute debe ser m\u00faltiplo de :value',\n    'not_in'               => ':attribute es inv\u00e1lido.',\n    'not_regex'            => 'El formato del campo :attribute no es v\u00e1lido.',\n    'numeric'              => ':attribute debe ser num\u00e9rico.',\n    'password'             => 'La contrase\u00f1a es incorrecta.',\n    'present'              => 'El campo :attribute debe estar presente.',\n    'regex'                => 'El formato de :attribute es inv\u00e1lido.',\n    'required'             => 'El campo :attribute es obligatorio.',\n    'required_if'          => 'El campo :attribute es obligatorio cuando :other es :value.',\n    'required_unless'      => 'El campo :attribute es obligatorio a menos que :other est\u00e9 en :values.',\n    'required_with'        => 'El campo :attribute es obligatorio cuando :values est\u00e1 presente.',\n    'required_with_all'    => 'El campo :attribute es obligatorio cuando :values est\u00e1n presentes.',\n    'required_without'     => 'El campo :attribute es obligatorio cuando :values no est\u00e1 presente.',\n    'required_without_all' => 'El campo :attribute es obligatorio cuando ninguno de :values est\u00e1 presente.',\n    'same'                 => ':attribute y :other deben coincidir.',\n    'size'                 => [\n        'numeric' => 'El tama\u00f1o de :attribute debe ser :size.',\n        'file'    => 'El tama\u00f1o de :attribute debe ser :size kilobytes.',\n        'string'  => ':attribute debe contener :size caracteres.',\n        'array'   => ':attribute debe contener :size elementos.',\n    ],\n    'starts_with'          => 'El campo :attribute debe comenzar con uno de los siguientes valores: :values',\n    'string'               => 'El campo :attribute debe ser una cadena de caracteres.',\n    'timezone'             => 'El :attribute debe ser una zona v\u00e1lida.',\n    'unique'               => 'El campo :attribute ya ha sido registrado.',\n    'uploaded'             => 'Subir :attribute ha fallado.',\n    'url'                  => 'El formato :attribute es inv\u00e1lido.',\n    'uuid'                 => 'El campo :attribute debe ser un UUID v\u00e1lido.',\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Custom Validation Language Lines\n    |--------------------------------------------------------------------------\n    |\n    | Here you may specify custom validation messages for attributes using the\n    | convention \"attribute.rule\" to name the lines. This makes it quick to\n    | specify a specific custom language line for a given attribute rule.\n    |\n    *\/\n\n    'custom' => [\n        'password' => [\n            'min' => 'La :attribute debe contener m\u00e1s de :min caracteres',\n        ],\n        'email'    => [\n            'unique' => 'El :attribute ya ha sido registrado.',\n        ],\n    ],\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Custom Validation Attributes\n    |--------------------------------------------------------------------------\n    |\n    | The following language lines are used to swap attribute place-holders\n    | with something more reader friendly such as E-Mail Address instead\n    | of \"email\". This simply helps us make messages a little cleaner.\n    |\n    *\/\n\n    'attributes' => [\n        'address'               => 'direcci\u00f3n',\n        'age'                   => 'edad',\n        'body'                  => 'contenido',\n        'city'                  => 'ciudad',\n        'content'               => 'contenido',\n        'country'               => 'pa\u00eds',\n        'date'                  => 'fecha',\n        'day'                   => 'd\u00eda',\n        'description'           => 'descripci\u00f3n',\n        'email'                 => 'correo electr\u00f3nico',\n        'excerpt'               => 'extracto',\n        'first_name'            => 'nombre',\n        'gender'                => 'g\u00e9nero',\n        'hour'                  => 'hora',\n        'last_name'             => 'apellido',\n        'message'               => 'mensaje',\n        'minute'                => 'minuto',\n        'mobile'                => 'm\u00f3vil',\n        'month'                 => 'mes',\n        'name'                  => 'nombre',\n        'password'              => 'contrase\u00f1a',\n        'password_confirmation' => 'confirmaci\u00f3n de la contrase\u00f1a',\n        'phone'                 => 'tel\u00e9fono',\n        'price'                 => 'precio',\n        'second'                => 'segundo',\n        'sex'                   => 'sexo',\n        'subject'               => 'asunto',\n        'terms'                 => 't\u00e9rminos',\n        'time'                  => 'hora',\n        'title'                 => 't\u00edtulo',\n        'username'              => 'usuario',\n        'year'                  => 'a\u00f1o',\n    ],\n];\n","lang_cluster":"PHP","length":186,"code_uid":"ba78b0a7b3a24ab29da56d1e618a7053"}
{"diff_hunk":"@@ -11,12 +11,16 @@\n \n namespace Sonata\\MediaBundle\\CDN;\n \n+use Symfony\\Bundle\\FrameworkBundle\\Templating\\Helper\\AssetsHelper;\n+\n class Server implements CDNInterface\n {\n+    protected $path;\n+\n     \/**\n-     * @var string\n+     * @var AssetsHelper\n      *\/\n-    protected $path;\n+    private $assetsHelper;\n \n     \/**\n      * @param string $path","old_code":"<?php\n\n\/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\CDN;\n\nclass Server implements CDNInterface\n{\n    \/**\n     * @var string\n     *\/\n    protected $path;\n\n    \/**\n     * @param string $path\n     *\/\n    public function __construct($path)\n    {\n        $this->path = $path;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getPath($relativePath, $isFlushable)\n    {\n        return sprintf('%s\/%s', rtrim($this->path, '\/'), ltrim($relativePath, '\/'));\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function flushByString($string)\n    {\n        \/\/ nothing to do\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function flush($string)\n    {\n        \/\/ nothing to do\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function flushPaths(array $paths)\n    {\n        \/\/ nothing to do\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getFlushStatus($identifier)\n    {\n        \/\/ nothing to do\n    }\n}\n","lang_cluster":"PHP","length":68,"code_uid":"5b2a9a5d700d48fb91ee8617dd420870"}
{"diff_hunk":"@@ -40,8 +40,8 @@ return [\n     | or doing any other checks to ensure the service is functional.\n     |\n     *\/\n-    \n-    'preflight' => false,\n+\n+    'preflight' => true,\n \n     \/*\n     |--------------------------------------------------------------------------","old_code":"<?php\n\nuse function Roots\\env;\n\nreturn [\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Application Environment\n    |--------------------------------------------------------------------------\n    |\n    | This value determines the \"environment\" your application is currently\n    | running in. This may determine how you prefer to configure various\n    | services your application utilizes. Set this in your \".env\" file.\n    |\n    *\/\n\n    'env' => env('WP_ENV', 'production'),\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Application Debug Mode\n    |--------------------------------------------------------------------------\n    |\n    | When your application is in debug mode, detailed error messages with\n    | stack traces will be shown on every error that occurs within your\n    | application. If disabled, a simple generic error page is shown.\n    |\n    *\/\n\n    'debug' => env('WP_DEBUG', false),\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Preflight Checks\n    |--------------------------------------------------------------------------\n    |\n    | This value allows service providers to execute preflight tasks after\n    | booting. These tasks include creating directories, databases, and files,\n    | or doing any other checks to ensure the service is functional.\n    |\n    *\/\n    \n    'preflight' => false,\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Autoloaded Service Providers\n    |--------------------------------------------------------------------------\n    |\n    | The service providers listed here will be automatically loaded on the\n    | request to your application. Feel free to add your own services to\n    | this array to grant expanded functionality to your applications.\n    |\n    *\/\n\n    'providers' => [\n        \/\/ App\\SomeService\\SomeServiceServiceProvider::class,\n    ],\n\n    \/*\n    |--------------------------------------------------------------------------\n    | Class Aliases (formerly \"Facades\")\n    |--------------------------------------------------------------------------\n    |\n    | This array of class aliases will be registered when this application\n    | is started. However, feel free to register as many as you wish as\n    | the aliases are \"lazy\" loaded so they don't hinder performance.\n    |\n    *\/\n\n    'aliases' => [\n        'App' => Illuminate\\Support\\Facades\\App::class,\n        'Artisan' => Illuminate\\Support\\Facades\\Artisan::class,\n        'Auth' => Illuminate\\Support\\Facades\\Auth::class,\n        'Blade' => Illuminate\\Support\\Facades\\Blade::class,\n        'Broadcast' => Illuminate\\Support\\Facades\\Broadcast::class,\n        'Bus' => Illuminate\\Support\\Facades\\Bus::class,\n        'Cache' => Illuminate\\Support\\Facades\\Cache::class,\n        'Config' => Illuminate\\Support\\Facades\\Config::class,\n        'Cookie' => Illuminate\\Support\\Facades\\Cookie::class,\n        'Crypt' => Illuminate\\Support\\Facades\\Crypt::class,\n        'DB' => Illuminate\\Support\\Facades\\DB::class,\n        'Eloquent' => Illuminate\\Database\\Eloquent\\Model::class,\n        'Event' => Illuminate\\Support\\Facades\\Event::class,\n        'File' => Illuminate\\Support\\Facades\\File::class,\n        'Gate' => Illuminate\\Support\\Facades\\Gate::class,\n        'Hash' => Illuminate\\Support\\Facades\\Hash::class,\n        'Lang' => Illuminate\\Support\\Facades\\Lang::class,\n        'Log' => Illuminate\\Support\\Facades\\Log::class,\n        'Mail' => Illuminate\\Support\\Facades\\Mail::class,\n        'Notification' => Illuminate\\Support\\Facades\\Notification::class,\n        'Password' => Illuminate\\Support\\Facades\\Password::class,\n        'Queue' => Illuminate\\Support\\Facades\\Queue::class,\n        'Redirect' => Illuminate\\Support\\Facades\\Redirect::class,\n        'Redis' => Illuminate\\Support\\Facades\\Redis::class,\n        'Request' => Illuminate\\Support\\Facades\\Request::class,\n        'Response' => Illuminate\\Support\\Facades\\Response::class,\n        'Route' => Illuminate\\Support\\Facades\\Route::class,\n        'Schema' => Illuminate\\Support\\Facades\\Schema::class,\n        'Session' => Illuminate\\Support\\Facades\\Session::class,\n        'Storage' => Illuminate\\Support\\Facades\\Storage::class,\n        'URL' => Illuminate\\Support\\Facades\\URL::class,\n        'Validator' => Illuminate\\Support\\Facades\\Validator::class,\n        'View' => Illuminate\\Support\\Facades\\View::class,\n    ],\n];\n","lang_cluster":"PHP","length":107,"code_uid":"0a2a977cb1a542669de83f8dc32d2992"}
{"diff_hunk":"@@ -39,8 +39,6 @@ class LanguageTreeReadAction\n \n \n     \/**\n-     * @IsGranted(\"SETTINGS_READ\")\n-     *\n      * @SWG\\Tag(name=\"Language\")\n      * @SWG\\Parameter(\n      *     name=\"language\",","old_code":"<?php\n\/**\n * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types = 1);\n\nnamespace Ergonode\\Core\\Application\\Controller\\Api\\LanguageTree;\n\nuse Ergonode\\Api\\Application\\Response\\SuccessResponse;\nuse Ergonode\\Core\\Domain\\Repository\\LanguageTreeRepositoryInterface;\nuse Ergonode\\Core\\Domain\\ValueObject\\Language;\nuse Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\IsGranted;\nuse Swagger\\Annotations as SWG;\nuse Symfony\\Component\\HttpFoundation\\Response;\nuse Symfony\\Component\\Routing\\Annotation\\Route;\n\n\/**\n * @Route(\n *     path=\"\/tree\",\n *     methods={\"GET\"}\n * )\n *\/\nclass LanguageTreeReadAction\n{\n    \/**\n     * @var LanguageTreeRepositoryInterface\n     *\/\n    private LanguageTreeRepositoryInterface $repository;\n\n    \/**\n     * @param LanguageTreeRepositoryInterface $repository\n     *\/\n    public function __construct(LanguageTreeRepositoryInterface $repository)\n    {\n        $this->repository = $repository;\n    }\n\n\n    \/**\n     * @IsGranted(\"SETTINGS_READ\")\n     *\n     * @SWG\\Tag(name=\"Language\")\n     * @SWG\\Parameter(\n     *     name=\"language\",\n     *     in=\"path\",\n     *     type=\"string\",\n     *     required=true,\n     *     default=\"en_GB\",\n     *     description=\"Language Code\",\n     * )\n     * @SWG\\Response(\n     *     response=200,\n     *     description=\"Returns Language tree\"\n     * )\n     * @SWG\\Response(\n     *     response=404,\n     *     description=\"Not found\"\n     * )\n     *\n     * @param Language $language\n     *\n     * @return Response\n     *\/\n    public function __invoke(Language $language): Response\n    {\n        $tree = $this->repository->load();\n\n        return new SuccessResponse($tree);\n    }\n}\n","lang_cluster":"PHP","length":72,"code_uid":"b111b80aa0e847639c56de834716ff90"}
{"diff_hunk":"@@ -129,6 +129,8 @@ class MediaHelper extends Helper\n         $provider = $this->getMediaService()\n            ->getProvider($media->getProviderName());\n \n+        $format = $provider->getFormatName($media, $format);\n+\n         return $provider->generatePublicUrl($media, $format);\n     }\n ","old_code":"<?php\n\n\/*\n * This file is part of the Sonata project.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\nnamespace Sonata\\MediaBundle\\Templating\\Helper;\n\nuse Symfony\\Component\\Templating\\Helper\\Helper;\n\n\n\/**\n * MediaHelper manages action inclusions.\n *\n * @author Thomas Rabaix <thomas.rabaix@sonata-project.com>\n *\/\nclass MediaHelper extends Helper\n{\n\n    protected $mediaService = null;\n\n    protected $templating = null;\n\n    \/**\n     * Constructor.\n     *\n     * @param Constructor $media_provider A MediaProvider instance\n     *\/\n    public function __construct($mediaService, $templating)\n    {\n        $this->mediaService    = $mediaService;\n        $this->templating      = $templating;\n    }\n\n    \/**\n     * Returns the canonical name of this helper.\n     *\n     * @return string The canonical name\n     *\/\n    public function getName()\n    {\n        return 'sonata_media';\n    }\n\n    \/**\n     * Returns the provider view for the provided media\n     *\n     * @param Media $media\n     * @param string $format\n     * @param array $options\n     * @return string\n     *\/\n    public function media($media, $format, $options = array())\n    {\n        if (!$media) {\n            return '';\n        }\n\n        $provider = $this\n            ->getMediaService()\n            ->getProvider($media->getProviderName());\n\n        $format = $provider->getFormatName($media, $format);\n\n        $options = $provider->getHelperProperties($media, $format, $options);\n\n        return $this->getTemplating()->render(\n            $provider->getTemplate('helper_view'),\n            array(\n                 'media'    => $media,\n                 'format'   => $format,\n                 'options'  => $options,\n            )\n        );\n    }\n\n    \/**\n     * Returns the thumbnail for the provided media\n     *\n     * @param Media $media\n     * @param string $format\n     * @param array $options\n     * @return string\n     *\/\n    public function thumbnail($media, $format, $options = array())\n    {\n         if (!$media) {\n             return '';\n         }\n\n         $provider = $this->getMediaService()\n            ->getProvider($media->getProviderName());\n\n         $format = $provider->getFormatName($media, $format);\n         $format_definition = $provider->getFormat($format);\n\n         \/\/ build option\n         $options = array_merge(array(\n             'title' => $media->getName(),\n             'width' => $format_definition['width'],\n         ), $options);\n\n         $options['src'] = $provider->generatePublicUrl($media, $format);\n\n         return $this->getTemplating()->render(\n            $provider->getTemplate('helper_thumbnail'),\n            array(\n                 'media'    => $media,\n                 'options'  => $options,\n            )\n         );\n    }\n\n    \/**\n     * @param Media $media\n     * @param string $format\n     * @return string\n     *\/\n    public function path($media, $format)\n    {\n        if (!$media) {\n             return '';\n        }\n\n        $provider = $this->getMediaService()\n           ->getProvider($media->getProviderName());\n\n        return $provider->generatePublicUrl($media, $format);\n    }\n\n    public function getMediaService()\n    {\n        return $this->mediaService;\n    }\n\n    public function getTemplating()\n    {\n        return $this->templating;\n    }\n}\n","lang_cluster":"PHP","length":144,"code_uid":"88171af13f1045d0bd056b1203878ac6"}
{"diff_hunk":"@@ -16,6 +16,11 @@ use Ergonode\\BatchAction\\Domain\\Entity\\BatchActionId;\n use Ergonode\\SharedKernel\\Domain\\AggregateId;\n use Symfony\\Component\\Messenger\\Stamp\\HandledStamp;\n use Ergonode\\BatchAction\\Domain\\Repository\\BatchActionRepositoryInterface;\n+use Ergonode\\Core\\Application\\Security\\User\\CachedUser;\n+use Ergonode\\Account\\Domain\\Repository\\UserRepositoryInterface;\n+use Ergonode\\SharedKernel\\Domain\\Aggregate\\UserId;\n+use Ergonode\\BatchAction\\Domain\\Event\\BatchActionEndedEvent;\n+use Ergonode\\Core\\Application\\Messenger\\Stamp\\UserStamp;\n \n class BatchActionTransport implements TransportInterface\n {","old_code":"<?php\n\/**\n * Copyright \u00a9 Ergonode Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types=1);\n\nnamespace Ergonode\\BatchAction\\Application\\Transport;\n\nuse Symfony\\Component\\Messenger\\Transport\\TransportInterface;\nuse Doctrine\\DBAL\\Connection;\nuse Symfony\\Component\\Messenger\\Envelope;\nuse Ergonode\\BatchAction\\Domain\\Command\\ProcessBatchActionEntryCommand;\nuse Ergonode\\BatchAction\\Domain\\Entity\\BatchActionId;\nuse Ergonode\\SharedKernel\\Domain\\AggregateId;\nuse Symfony\\Component\\Messenger\\Stamp\\HandledStamp;\nuse Ergonode\\BatchAction\\Domain\\Repository\\BatchActionRepositoryInterface;\n\nclass BatchActionTransport implements TransportInterface\n{\n    private Connection $connection;\n\n    private BatchActionRepositoryInterface $repository;\n\n    public function __construct(Connection $connection, BatchActionRepositoryInterface $repository)\n    {\n        $this->connection = $connection;\n        $this->repository = $repository;\n    }\n\n    public function get(): iterable\n    {\n        $this->connection->beginTransaction();\n        $result = [];\n        $record = $this->connection->executeQuery(\n            'SELECT batch_action_id, resource_id \n                 FROM batch_action_entry \n                 WHERE processed_at is NULL LIMIT 1 FOR UPDATE SKIP LOCKED'\n        )->fetchAssociative();\n\n        if (!empty($record)) {\n            echo ($record['resource_id']).PHP_EOL;\n            $result[] = new Envelope(\n                new ProcessBatchActionEntryCommand(\n                    new BatchActionId($record['batch_action_id']),\n                    new AggregateId($record['resource_id'])\n                )\n            );\n        } else {\n            $this->connection->commit();\n        }\n\n        return $result;\n    }\n\n    public function ack(Envelope $envelope): void\n    {\n        \/** @var ProcessBatchActionEntryCommand $message *\/\n        $message = $envelope->getMessage();\n        \/** @var HandledStamp $stamp *\/\n        $stamp = $envelope->last(HandledStamp::class);\n\n        $this->repository->markEntry($message->getId(), $message->getResourceId(), $stamp->getResult());\n        $this->connection->commit();\n    }\n\n    public function reject(Envelope $envelope): void\n    {\n        $this->connection->commit();\n    }\n\n    public function send(Envelope $envelope): Envelope\n    {\n        return $envelope;\n    }\n}\n","lang_cluster":"PHP","length":77,"code_uid":"314528081ec24200877c7e4be30c5569"}
{"diff_hunk":"@@ -3,6 +3,7 @@\n namespace Shopsys\\FrameworkBundle\\Twig;\n \n use Shopsys\\FrameworkBundle\\Component\\Domain\\Domain;\n+use Shopsys\\FrameworkBundle\\Component\\UploadedFile\\UploadedFile;\n use Shopsys\\FrameworkBundle\\Component\\UploadedFile\\UploadedFileFacade;\n use Shopsys\\FrameworkBundle\\Twig\\FileThumbnail\\FileThumbnailExtension;\n use Twig_Extension;","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\Twig;\n\nuse Shopsys\\FrameworkBundle\\Component\\Domain\\Domain;\nuse Shopsys\\FrameworkBundle\\Component\\UploadedFile\\UploadedFileFacade;\nuse Shopsys\\FrameworkBundle\\Twig\\FileThumbnail\\FileThumbnailExtension;\nuse Twig_Extension;\nuse Twig_SimpleFunction;\n\nclass UploadedFileExtension extends Twig_Extension\n{\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Component\\Domain\\Domain\n     *\/\n    protected $domain;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Component\\UploadedFile\\UploadedFileFacade\n     *\/\n    protected $uploadedFileFacade;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Twig\\FileThumbnail\\FileThumbnailExtension\n     *\/\n    protected $fileThumbnailExtension;\n\n    \/**\n     * @param \\Shopsys\\FrameworkBundle\\Component\\Domain\\Domain $domain\n     * @param \\Shopsys\\FrameworkBundle\\Component\\UploadedFile\\UploadedFileFacade $uploadedFileFacade\n     * @param \\Shopsys\\FrameworkBundle\\Twig\\FileThumbnail\\FileThumbnailExtension $fileThumbnailExtension\n     *\/\n    public function __construct(\n        Domain $domain,\n        UploadedFileFacade $uploadedFileFacade,\n        FileThumbnailExtension $fileThumbnailExtension\n    ) {\n        $this->domain = $domain;\n        $this->uploadedFileFacade = $uploadedFileFacade;\n        $this->fileThumbnailExtension = $fileThumbnailExtension;\n    }\n\n    \/**\n     * @return array\n     *\/\n    public function getFunctions()\n    {\n        return [\n            new Twig_SimpleFunction('hasUploadedFile', [$this, 'hasUploadedFile']),\n            new Twig_SimpleFunction('uploadedFileUrl', [$this, 'getUploadedFileUrl']),\n            new Twig_SimpleFunction('uploadedFilePreview', [$this, 'getUploadedFilePreviewHtml'], ['is_safe' => ['html']]),\n            new Twig_SimpleFunction('getUploadedFile', [$this, 'getUploadedFileByEntity']),\n        ];\n    }\n\n    \/**\n     * @param Object $entity\n     * @return bool\n     *\/\n    public function hasUploadedFile($entity)\n    {\n        return $this->uploadedFileFacade->hasUploadedFile($entity);\n    }\n\n    \/**\n     * @param Object $entity\n     * @return string\n     *\/\n    public function getUploadedFileUrl($entity)\n    {\n        $uploadedFile = $this->getUploadedFileByEntity($entity);\n\n        return $this->uploadedFileFacade->getUploadedFileUrl($this->domain->getCurrentDomainConfig(), $uploadedFile);\n    }\n\n    \/**\n     * @param Object $entity\n     * @return string\n     *\/\n    public function getUploadedFilePreviewHtml($entity)\n    {\n        $uploadedFile = $this->getUploadedFileByEntity($entity);\n        $filepath = $this->uploadedFileFacade->getAbsoluteUploadedFileFilepath($uploadedFile);\n        $fileThumbnailInfo = $this->fileThumbnailExtension->getFileThumbnailInfo($filepath);\n\n        if ($fileThumbnailInfo->getIconType() !== null) {\n            $classes = [\n                'svg',\n                'svg-file-' . $fileThumbnailInfo->getIconType(),\n                'list-images__item__image__type',\n                'list-images__item__image__type--' . $fileThumbnailInfo->getIconType(),\n                'text-no-decoration',\n                'cursor-pointer',\n            ];\n\n            return '<i class=\"' . implode(' ', $classes) . '\"><\/i>';\n        } else {\n            return '<img src=\"' . $fileThumbnailInfo->getImageUri() . '\"\/>';\n        }\n    }\n\n    \/**\n     * @param Object $entity\n     * @return \\Shopsys\\FrameworkBundle\\Component\\UploadedFile\\UploadedFile\n     *\/\n    public function getUploadedFileByEntity($entity)\n    {\n        return $this->uploadedFileFacade->getUploadedFileByEntity($entity);\n    }\n\n    \/**\n     * @return string\n     *\/\n    public function getName()\n    {\n        return 'file_extension';\n    }\n}\n","lang_cluster":"PHP","length":118,"code_uid":"f1a5a9d5ec884d22a5ee602902de1a01"}
{"diff_hunk":"@@ -12,9 +12,21 @@\n \n namespace HookAnalytics;\n \n+use Propel\\Runtime\\Connection\\ConnectionInterface;\n+use Thelia\\Model\\ConfigQuery;\n+use Thelia\\Model\\LangQuery;\n use Thelia\\Module\\BaseModule;\n \n class HookAnalytics extends BaseModule\n {\n-\n+    public function update($currentVersion, $newVersion, ConnectionInterface $con = null)\n+    {\n+        if (version_compare($newVersion, $currentVersion, \">\")){\n+            $langs = LangQuery::create()->filterByActive()->find();\n+            $config = ConfigQuery::read(\"hookanalytics_trackingcode\", \"\");\n+            foreach ($langs as $lang){\n+                self::setConfigValue('hookanalytics_trackingcode', $config, $lang->getLocale());\n+            }\n+        }\n+    }\n }","old_code":"<?php\n\/*************************************************************************************\/\n\/*      This file is part of the Thelia package.                                     *\/\n\/*                                                                                   *\/\n\/*      Copyright (c) OpenStudio                                                     *\/\n\/*      email : dev@thelia.net                                                       *\/\n\/*      web : http:\/\/www.thelia.net                                                  *\/\n\/*                                                                                   *\/\n\/*      For the full copyright and license information, please view the LICENSE.txt  *\/\n\/*      file that was distributed with this source code.                             *\/\n\/*************************************************************************************\/\n\nnamespace HookAnalytics;\n\nuse Thelia\\Module\\BaseModule;\n\nclass HookAnalytics extends BaseModule\n{\n\n}\n","lang_cluster":"PHP","length":20,"code_uid":"1db09fad40154babbd9a3ac7e4975952"}
{"diff_hunk":"@@ -33,7 +33,7 @@ class ProfileController extends BaseController\n         $user = $this->getUser();\n \n         return $this->renderTemplate('users\/edit.html.twig', [\n-            'usertitle' => $user->getFullName(),\n+            'usertitle' => $user->getDisplayName(),\n             'user' => $user,\n         ]);\n     }","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Bolt\\Controller\\Backend;\n\nuse Bolt\\Controller\\BaseController;\nuse Doctrine\\Common\\Persistence\\ObjectManager;\nuse Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Security;\nuse Symfony\\Component\\HttpFoundation\\RedirectResponse;\nuse Symfony\\Component\\HttpFoundation\\Request;\nuse Symfony\\Component\\HttpFoundation\\Response;\nuse Symfony\\Component\\Routing\\Annotation\\Route;\nuse Symfony\\Component\\Routing\\Generator\\UrlGeneratorInterface;\nuse Symfony\\Component\\Security\\Core\\Encoder\\UserPasswordEncoderInterface;\n\n\/**\n * Class ProfileController.\n *\n * @Security(\"has_role('ROLE_ADMIN')\")\n *\/\nclass ProfileController extends BaseController\n{\n    \/**\n     * @Route(\"\/profile-edit\", methods={\"GET\"}, name=\"bolt_profile_edit\")\n     *\n     * @throws \\Twig_Error_Loader\n     * @throws \\Twig_Error_Runtime\n     * @throws \\Twig_Error_Syntax\n     *\/\n    public function profileEdit(Request $request): Response\n    {\n        $user = $this->getUser();\n\n        return $this->renderTemplate('users\/edit.html.twig', [\n            'usertitle' => $user->getFullName(),\n            'user' => $user,\n        ]);\n    }\n\n    \/**\n     * @Route(\"\/profile-edit\", methods={\"POST\"}, name=\"bolt_profile_edit_post\")\n     *\n     * @throws \\Twig_Error_Loader\n     * @throws \\Twig_Error_Runtime\n     * @throws \\Twig_Error_Syntax\n     *\/\n    public function profileEditPost(Request $request, UrlGeneratorInterface $urlGenerator, ObjectManager $manager, UserPasswordEncoderInterface $encoder): Response\n    {\n        $user = $this->getUser();\n        $userTitle = $user->getFullName();\n        $url = $urlGenerator->generate('bolt_profile_edit');\n        $locale = current($request->get('locale'));\n        $newPassword = $request->get('password');\n\n        $user->setFullName($request->get('fullName'));\n        $user->setEmail($request->get('email'));\n        $user->setLocale($locale);\n        $user->setbackendTheme($request->get('user')['backendTheme']);\n\n        $hasError = false;\n\n        $usernameValidateOptions = [\n            'options' => [\n                'min_range' => 1,\n            ],\n        ];\n\n        \/\/ Validate username\n        if (! filter_var(mb_strlen($user->getFullName()), FILTER_VALIDATE_INT, $usernameValidateOptions)) {\n            $this->addFlash('danger', 'user.not_valid_username');\n            $hasError = true;\n        }\n\n        \/\/ Validate password\n        if (! empty($newPassword) && mb_strlen($newPassword) < 6) {\n            $this->addFlash('danger', 'user.not_valid_password');\n            $hasError = true;\n        } elseif (! empty($newPassword) && mb_strlen($newPassword) > 6) {\n            $user->setPassword($encoder->encodePassword($user, $newPassword));\n        }\n\n        \/\/ Validate email\n        if (! filter_var($user->getEmail(), FILTER_VALIDATE_EMAIL)) {\n            $this->addFlash('danger', 'user.not_valid_email');\n            $hasError = true;\n        }\n\n        if ($hasError) {\n            return $this->renderTemplate('users\/edit.html.twig', [\n                'usertitle' => $userTitle,\n                'user' => $user,\n            ]);\n        }\n\n        $manager->flush();\n\n        $request->getSession()->set('_locale', $locale);\n\n        return new RedirectResponse($url);\n    }\n}\n","lang_cluster":"PHP","length":102,"code_uid":"8a0d1d0fbe034a6380f69f50a4ac2bf8"}
{"diff_hunk":"@@ -34,7 +34,9 @@ class TCallableObject extends TObject\n         return false;\n     }\n \n-    public function getAssertionString(bool $exact = false): string\n+    public function getAssertionString()\n+\n+    \/** @psalm-mutation-free *\/: string\n     {\n         return 'object';\n     }","old_code":"<?php\n\nnamespace Psalm\\Type\\Atomic;\n\n\/**\n * Denotes an object that is also `callable` (i.e. it has `__invoke` defined).\n *\/\nclass TCallableObject extends TObject\n{\n    public function __toString(): string\n    {\n        return 'callable-object';\n    }\n\n    public function getKey(bool $include_extra = true): string\n    {\n        return 'callable-object';\n    }\n\n    \/**\n     * @param  array<lowercase-string, string> $aliased_classes\n     *\/\n    public function toPhpString(\n        ?string $namespace,\n        array $aliased_classes,\n        ?string $this_class,\n        int $analysis_php_version_id\n    ): ?string {\n        return $analysis_php_version_id >= 7_02_00 ? 'object' : null;\n    }\n\n    public function canBeFullyExpressedInPhp(int $analysis_php_version_id): bool\n    {\n        return false;\n    }\n\n    public function getAssertionString(bool $exact = false): string\n    {\n        return 'object';\n    }\n}\n","lang_cluster":"PHP","length":41,"code_uid":"a653ef7460f14fac99c1111ee65284b3"}
{"diff_hunk":"@@ -50,7 +50,7 @@ class MediaType extends AbstractType\n             'new_on_update' => $options['new_on_update'],\n         )));\n \n-        $builder->addEventListener(FormEvents::BIND, function (FormEvent $event) {\n+        $builder->addEventListener(FormEvents::SUBMIT, function(FormEvent $event) {\n             if ($event->getForm()->has('unlink') && $event->getForm()->get('unlink')->getData()) {\n                 $event->setData(null);\n             }","old_code":"<?php\n\n\/*\n * This file is part of the Sonata project.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Form\\Type;\n\nuse Sonata\\MediaBundle\\Form\\DataTransformer\\ProviderDataTransformer;\nuse Sonata\\MediaBundle\\Provider\\Pool;\nuse Symfony\\Component\\Form\\AbstractType;\nuse Symfony\\Component\\Form\\FormBuilderInterface;\nuse Symfony\\Component\\Form\\FormEvent;\nuse Symfony\\Component\\Form\\FormEvents;\nuse Symfony\\Component\\Form\\FormInterface;\nuse Symfony\\Component\\Form\\FormView;\nuse Symfony\\Component\\OptionsResolver\\OptionsResolver;\nuse Symfony\\Component\\OptionsResolver\\OptionsResolverInterface;\n\nclass MediaType extends AbstractType\n{\n    protected $pool;\n\n    protected $class;\n\n    \/**\n     * @param Pool   $pool\n     * @param string $class\n     *\/\n    public function __construct(Pool $pool, $class)\n    {\n        $this->pool  = $pool;\n        $this->class = $class;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function buildForm(FormBuilderInterface $builder, array $options)\n    {\n        $builder->addModelTransformer(new ProviderDataTransformer($this->pool, $this->class, array(\n            'provider'      => $options['provider'],\n            'context'       => $options['context'],\n            'empty_on_new'  => $options['empty_on_new'],\n            'new_on_update' => $options['new_on_update'],\n        )));\n\n        $builder->addEventListener(FormEvents::BIND, function (FormEvent $event) {\n            if ($event->getForm()->has('unlink') && $event->getForm()->get('unlink')->getData()) {\n                $event->setData(null);\n            }\n        });\n\n        $this->pool->getProvider($options['provider'])->buildMediaType($builder);\n\n        $builder->add('unlink', 'checkbox', array(\n            'mapped'   => false,\n            'data'     => false,\n            'required' => false,\n        ));\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function buildView(FormView $view, FormInterface $form, array $options)\n    {\n        $view->vars['provider'] = $options['provider'];\n        $view->vars['context'] = $options['context'];\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\n     * @deprecated Remove it when bumping requirements to Symfony >=2.7\n     *\/\n    public function setDefaultOptions(OptionsResolverInterface $resolver)\n    {\n        $this->configureOptions($resolver);\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function configureOptions(OptionsResolver $resolver)\n    {\n        $resolver->setDefaults(array(\n            'data_class'    => $this->class,\n            'provider'      => null,\n            'context'       => null,\n            'empty_on_new'  => true,\n            'new_on_update' => true,\n        ));\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getParent()\n    {\n        return 'form';\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getName()\n    {\n        return 'sonata_media_type';\n    }\n}\n","lang_cluster":"PHP","length":116,"code_uid":"0d73d7091141436b85ba1cf72bebb363"}
{"diff_hunk":"@@ -41,6 +41,6 @@ class EntityFilterConfigurator implements TypeConfiguratorInterface\n      *\/\n     public function supports($type, array $options, array $metadata): bool\n     {\n-        return EntityFilter::class === $metadata['type'] && 'association' === $metadata['dataType'] ?? null;\n+        return (EntityFilter::class === $metadata['type']) && ('association' === $metadata['dataType'] ?? null);\n     }\n }","old_code":"<?php\n\nnamespace EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Type\\Configurator;\n\nuse Doctrine\\ORM\\Mapping\\ClassMetadata;\nuse EasyCorp\\Bundle\\EasyAdminBundle\\Form\\Filter\\EntityFilter;\nuse Symfony\\Component\\Form\\FormConfigInterface;\n\n\/**\n * @author Yonel Ceruto <yonelceruto@gmail.com>\n *\/\nclass EntityFilterConfigurator implements TypeConfiguratorInterface\n{\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function configure($name, array $options, array $metadata, FormConfigInterface $parentConfig): array\n    {\n        $entityTypeOptions = $options['value_type_options'] ?? [];\n\n        if (!isset($entityTypeOptions['class'])) {\n            $entityTypeOptions['class'] = $metadata['targetEntity'];\n        }\n\n        if (!isset($entityTypeOptions['multiple']) && $metadata['associationType'] & ClassMetadata::TO_MANY) {\n            $entityTypeOptions['multiple'] = true;\n        } elseif (($metadata['associationType'] & ClassMetadata::TO_ONE) && !isset($options['placeholder']) && (!isset($options['required']) || false === $options['required'])) {\n            $entityTypeOptions['placeholder'] = 'label.form.empty_value';\n        }\n\n        \/\/ Supported associations are displayed using advanced JavaScript widgets\n        $entityTypeOptions['attr']['data-widget'] = 'select2';\n\n        $options['value_type_options'] = $entityTypeOptions;\n\n        return $options;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function supports($type, array $options, array $metadata): bool\n    {\n        return EntityFilter::class === $metadata['type'] && 'association' === $metadata['dataType'] ?? null;\n    }\n}\n","lang_cluster":"PHP","length":46,"code_uid":"3396eeb5aa1742a3b320f291d86f548f"}
{"diff_hunk":"@@ -34,7 +34,7 @@ class MediaAdmin extends Admin\n      *\/\n     public function createQuery($context = 'list')\n     {\n-        $query = $this->getModelManager()->createQuery($this->getClass(), '', $this->root);\n+        $query = $this->getModelManager()->createQuery($this->getClass(), 'a', $this->root);\n \n         foreach ($this->extensions as $extension) {\n             $extension->configureQuery($this, $query, $context);","old_code":"<?php\n\n\/*\n * This file is part of the Sonata package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Admin\\PHPCR;\n\nuse Sonata\\MediaBundle\\Admin\\BaseMediaAdmin as Admin;\nuse Sonata\\AdminBundle\\Datagrid\\DatagridMapper;\nuse Sonata\\AdminBundle\\Route\\RouteCollection;\n\nclass MediaAdmin extends Admin\n{\n    \/**\n     * Path to the root node of media documents.\n     *\n     * @var string\n     *\/\n    protected $root;\n\n    public function setRoot($root)\n    {\n        $this->root = $root;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function createQuery($context = 'list')\n    {\n        $query = $this->getModelManager()->createQuery($this->getClass(), '', $this->root);\n\n        foreach ($this->extensions as $extension) {\n            $extension->configureQuery($this, $query, $context);\n        }\n\n        return $query;\n    }\n\n    public function id($object)\n    {\n        return $this->getUrlsafeIdentifier($object);\n    }\n\n    \/**\n     * @param  \\Sonata\\AdminBundle\\Datagrid\\DatagridMapper $datagridMapper\n     * @return void\n     *\/\n    protected function configureDatagridFilters(DatagridMapper $datagridMapper)\n    {\n        \/\/ TODO disabled filter due to no attached service for filter types: string, checkbox\n\/\/        $datagridMapper\n\/\/            ->add('name')\n\/\/            ->add('providerReference')\n\/\/            ->add('enabled')\n\/\/            ->add('context')\n\/\/        ;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    protected function configureRoutes(RouteCollection $collection)\n    {\n        \/\/ Allow path in id parameter\n        $collection->add('view', $this->getRouterIdParameter() . '\/view', array(), array('id' => '.+', '_method' => 'GET'));\n        $collection->add('show', $this->getRouterIdParameter() . '\/show', array(\n                '_controller' => sprintf('%s:%s', $this->baseControllerName, 'view')\n            ),\n            array('id' => '.+', '_method' => 'GET')\n        );\n    }\n}\n","lang_cluster":"PHP","length":79,"code_uid":"5e935ff5e8334da599c943f57145047b"}
{"diff_hunk":"@@ -4,35 +4,46 @@ declare(strict_types=1);\n \n namespace Tests\\ShopBundle\\Functional\\Model\\Payment;\n \n-use Shopsys\\FrameworkBundle\\Model\\Payment\\PaymentDataFactoryInterface;\n use Shopsys\\FrameworkBundle\\Model\\Pricing\\Vat\\Vat;\n use Shopsys\\FrameworkBundle\\Model\\Pricing\\Vat\\VatData;\n-use Shopsys\\FrameworkBundle\\Model\\Transport\\TransportDataFactoryInterface;\n-use Shopsys\\FrameworkBundle\\Model\\Transport\\TransportFacade;\n use Shopsys\\ShopBundle\\Model\\Payment\\Payment;\n use Shopsys\\ShopBundle\\Model\\Transport\\Transport;\n use Tests\\ShopBundle\\Test\\TransactionFunctionalTestCase;\n \n class PaymentTest extends TransactionFunctionalTestCase\n {\n+    \/**\n+     * @var \\Shopsys\\ShopBundle\\Model\\Payment\\PaymentDataFactory\n+     * @inject\n+     *\/\n+    private $paymentDataFactory;\n+\n+    \/**\n+     * @var \\Shopsys\\ShopBundle\\Model\\Transport\\TransportDataFactory\n+     * @inject\n+     *\/\n+    private $transportDataFactory;\n+\n+    \/**\n+     * @var \\Shopsys\\FrameworkBundle\\Model\\Transport\\TransportFacade\n+     * @inject\n+     *\/\n+    private $transportFacade;\n+\n     public function testRemoveTransportFromPaymentAfterDelete()\n     {\n-        \/** @var \\Shopsys\\ShopBundle\\Model\\Payment\\PaymentDataFactory $paymentDataFactory *\/\n-        $paymentDataFactory = $this->getContainer()->get(PaymentDataFactoryInterface::class);\n-        \/** @var \\Shopsys\\ShopBundle\\Model\\Transport\\TransportDataFactory $transportDataFactory *\/\n-        $transportDataFactory = $this->getContainer()->get(TransportDataFactoryInterface::class);\n         $em = $this->getEntityManager();\n \n         $vatData = new VatData();\n         $vatData->name = 'vat';\n         $vatData->percent = '21';\n         $vat = new Vat($vatData);\n-        $transportData = $transportDataFactory->create();\n+        $transportData = $this->transportDataFactory->create();\n         $transportData->name['cs'] = 'name';\n         $transportData->vat = $vat;\n         $transport = new Transport($transportData);\n \n-        $paymentData = $paymentDataFactory->create();\n+        $paymentData = $this->paymentDataFactory->create();\n         $paymentData->name['cs'] = 'name';\n         $paymentData->vat = $vat;\n ","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Tests\\ShopBundle\\Functional\\Model\\Payment;\n\nuse Shopsys\\FrameworkBundle\\Model\\Payment\\PaymentDataFactoryInterface;\nuse Shopsys\\FrameworkBundle\\Model\\Pricing\\Vat\\Vat;\nuse Shopsys\\FrameworkBundle\\Model\\Pricing\\Vat\\VatData;\nuse Shopsys\\FrameworkBundle\\Model\\Transport\\TransportDataFactoryInterface;\nuse Shopsys\\FrameworkBundle\\Model\\Transport\\TransportFacade;\nuse Shopsys\\ShopBundle\\Model\\Payment\\Payment;\nuse Shopsys\\ShopBundle\\Model\\Transport\\Transport;\nuse Tests\\ShopBundle\\Test\\TransactionFunctionalTestCase;\n\nclass PaymentTest extends TransactionFunctionalTestCase\n{\n    public function testRemoveTransportFromPaymentAfterDelete()\n    {\n        \/** @var \\Shopsys\\ShopBundle\\Model\\Payment\\PaymentDataFactory $paymentDataFactory *\/\n        $paymentDataFactory = $this->getContainer()->get(PaymentDataFactoryInterface::class);\n        \/** @var \\Shopsys\\ShopBundle\\Model\\Transport\\TransportDataFactory $transportDataFactory *\/\n        $transportDataFactory = $this->getContainer()->get(TransportDataFactoryInterface::class);\n        $em = $this->getEntityManager();\n\n        $vatData = new VatData();\n        $vatData->name = 'vat';\n        $vatData->percent = '21';\n        $vat = new Vat($vatData);\n        $transportData = $transportDataFactory->create();\n        $transportData->name['cs'] = 'name';\n        $transportData->vat = $vat;\n        $transport = new Transport($transportData);\n\n        $paymentData = $paymentDataFactory->create();\n        $paymentData->name['cs'] = 'name';\n        $paymentData->vat = $vat;\n\n        $payment = new Payment($paymentData);\n        $payment->addTransport($transport);\n\n        $em->persist($vat);\n        $em->persist($transport);\n        $em->persist($payment);\n        $em->flush();\n\n        \/** @var \\Shopsys\\FrameworkBundle\\Model\\Transport\\TransportFacade $transportFacade *\/\n        $transportFacade = $this->getContainer()->get(TransportFacade::class);\n        $transportFacade->deleteById($transport->getId());\n\n        $this->assertNotContains($transport, $payment->getTransports());\n    }\n}\n","lang_cluster":"PHP","length":53,"code_uid":"5ca22b0846b74899bd42e799ab3e323c"}
{"diff_hunk":"@@ -54,7 +54,7 @@ class DbalBatchActionQuery implements BatchActionQueryInterface\n             $record['all'],\n             $record['processed'],\n             new \\DateTime($record['created_at']),\n-            $record['all'] === $record['processed'] ? new \\DateTime($record['last_processed_at']) : null\n+            $record['all'] === $record['processed'] ? new \\DateTime($record['last_processed_at'] ?? $record['created_at']) : null\n         );\n \n         foreach ($this->getEntries($id, $language) as $entry) {","old_code":"<?php\n\/**\n * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types=1);\n\nnamespace Ergonode\\BatchAction\\Infrastructure\\Persistence\\Query;\n\nuse Ergonode\\BatchAction\\Domain\\Entity\\BatchActionId;\nuse Ergonode\\BatchAction\\Domain\\Model\\BatchActionInformationModel;\nuse Ergonode\\BatchAction\\Domain\\Query\\BatchActionQueryInterface;\nuse Doctrine\\DBAL\\Connection;\nuse Ergonode\\BatchAction\\Domain\\ValueObject\\BatchActionType;\nuse Symfony\\Contracts\\Translation\\TranslatorInterface;\nuse Ergonode\\Core\\Domain\\ValueObject\\Language;\nuse Ergonode\\SharedKernel\\Domain\\AggregateId;\nuse Ergonode\\BatchAction\\Domain\\Model\\BatchActionEntryModel;\n\nclass DbalBatchActionQuery implements BatchActionQueryInterface\n{\n    private Connection $connection;\n\n    private TranslatorInterface $translator;\n\n    public function __construct(Connection $connection, TranslatorInterface $translator)\n    {\n        $this->connection = $connection;\n        $this->translator = $translator;\n    }\n\n    public function getInformation(BatchActionId $id, Language $language): BatchActionInformationModel\n    {\n        $qb = $this->connection->createQueryBuilder();\n\n        $record = $qb\n            ->select('id, created_at, type')\n            ->addSelect('(SELECT count(*) FROM batch_action_entry WHERE batch_action_id = id) AS all')\n            ->addSelect('(SELECT count(*) FROM batch_action_entry WHERE batch_action_id = id \n            AND success IS NOT NULL) AS processed')\n            ->addSelect('(SELECT max(processed_at) FROM batch_action_entry WHERE batch_action_id = id) \n            AS last_processed_at')\n            ->from('batch_action')\n            ->where($qb->expr()->eq('id', ':id'))\n            ->setParameter(':id', $id->getValue())\n            ->execute()\n            ->fetch();\n\n\n        $model = new BatchActionInformationModel(\n            new BatchActionId($record['id']),\n            new BatchActionType($record['type']),\n            $record['all'],\n            $record['processed'],\n            new \\DateTime($record['created_at']),\n            $record['all'] === $record['processed'] ? new \\DateTime($record['last_processed_at']) : null\n        );\n\n        foreach ($this->getEntries($id, $language) as $entry) {\n            $model->addEntry($entry);\n        }\n\n        return $model;\n    }\n\n    private function getEntries(BatchActionId $id, Language $language): array\n    {\n        $result = [];\n        $qb = $this->connection->createQueryBuilder();\n        $records = $qb\n            ->select('fail_reason, resource_id')\n            ->from('batch_action_entry')\n            ->where($qb->expr()->eq('batch_action_id', ':id'))\n            ->setParameter(':id', $id->getValue())\n            ->andWhere($qb->expr()->eq('success', ':success'))\n            ->setParameter(':success', false, \\PDO::PARAM_BOOL)\n            ->execute()\n            ->fetchAll();\n\n        foreach ($records as $record) {\n            $entry = new BatchActionEntryModel(new AggregateId($record['resource_id']));\n            $messages = json_decode($record['fail_reason'], true, 512, JSON_THROW_ON_ERROR);\n            foreach ($messages as $message) {\n                $entry->addMessage($this->translate($message, $language));\n            }\n            $result[] = $entry;\n        }\n\n        return $result;\n    }\n\n    private function translate(array $record, Language $language): string\n    {\n        return $this->translator->trans(\n            $record['message'],\n            $record['properties'],\n            'batch-action',\n            $language->getCode()\n        );\n    }\n}\n","lang_cluster":"PHP","length":102,"code_uid":"a3a9545c8028458f95690f5720a53277"}
{"diff_hunk":"@@ -13,12 +13,14 @@ use RootedData\\Exception\\ValidationException;\n  * Json Response Trait.\n  *\/\n trait JsonResponseTrait {\n+  use CacheableResponseTrait;\n \n   \/**\n    * Private.\n    *\/\n   private function getResponse($message, int $code = 200): JsonResponse {\n-    return new JsonResponse($message, $code, []);\n+    $response = new JsonResponse($message, $code, []);\n+    return $this->addCacheHeaders($response);\n   }\n \n   \/**","old_code":"<?php\n\nnamespace Drupal\\common;\n\nuse Symfony\\Component\\HttpFoundation\\JsonResponse;\nuse OpisErrorPresenter\\Implementation\\MessageFormatterFactory;\nuse OpisErrorPresenter\\Implementation\\PresentedValidationErrorFactory;\nuse OpisErrorPresenter\\Implementation\\Strategies\\BestMatchError;\nuse OpisErrorPresenter\\Implementation\\ValidationErrorPresenter;\nuse RootedData\\Exception\\ValidationException;\n\n\/**\n * Json Response Trait.\n *\/\ntrait JsonResponseTrait {\n\n  \/**\n   * Private.\n   *\/\n  private function getResponse($message, int $code = 200): JsonResponse {\n    return new JsonResponse($message, $code, []);\n  }\n\n  \/**\n   * Create JSON response from a caught exception.\n   *\n   * @param \\Exception $e\n   *   Exception object.\n   * @param int $code\n   *   HTTP response code.\n   *\n   * @return \\Symfony\\Component\\HttpFoundation\\JsonResponse\n   *   A Symfony JSON response.\n   *\/\n  private function getResponseFromException(\\Exception $e, int $code = 400):JsonResponse {\n    $body = [\n      'message' => $e->getMessage(),\n      'status' => $code,\n      \"timestamp\" => date(\"c\"),\n    ];\n    if ($data = $this->getExceptionData($e)) {\n      $body['data'] = $data;\n    }\n    return $this->getResponse((object) $body, $code);\n  }\n\n  \/**\n   * See if we can present more detail about the exception.\n   *\n   * Currently, only RootedJsonData validation errors supported.\n   *\n   * @param \\Exception $e\n   *   Exception object.\n   *\n   * @return array|false\n   *   An array of data to explain the errors.\n   *\/\n  private function getExceptionData(\\Exception $e) {\n    if ($e instanceof ValidationException) {\n      $errors = $e->getResult()->getErrors();\n      $presenter = new ValidationErrorPresenter(\n        new PresentedValidationErrorFactory(\n          new MessageFormatterFactory()\n        ),\n        new BestMatchError()\n      );\n      $presented = $presenter->present(...$errors);\n      return $presented[0];\n    }\n\n    return FALSE;\n  }\n\n}\n","lang_cluster":"PHP","length":74,"code_uid":"044461487dea43a681d8a28eb3bd5b42"}
{"diff_hunk":"@@ -29,6 +29,14 @@ class SkuTest extends TestCase\n         $this->assertEquals($sku, $sku->getValue());\n     }\n \n+    \/**\n+     * @expectedException \\InvalidArgumentException\n+     *\/\n+    public function testInvalidVAlue(): void\n+    {\n+        $sku = new Sku('yKL3rWXluEM7NapnMSVpYHpicQJkpJ0Obfx91mma0xwnQxUfsrwy5Nfki1LUZR4qYolBTlUFDO4RkeINIkjPzMfTSit0bQZJevXA6GMsj0LnSZaiT1bBfr00vtKWqLAPollonRzb6GBVlT5U9I6nC49r3Vnj2jUgpA73CvfnVnFBNnHqCaI2Cu48SKaVSRGgROhoD1dGPvvq98okavZ3ktVHk0TcyyiyfoH52U3gP3J5bNTVZngivjPJAqtOW8TO');\n+    }\n+\n     \/**\n      * @return array\n      *\/","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types = 1);\n\nnamespace Ergonode\\Product\\Tests\\Domain\\ValueObject;\n\nuse Ergonode\\Product\\Domain\\ValueObject\\Sku;\nuse PHPUnit\\Framework\\TestCase;\n\n\/**\n * Class SkuTest\n *\/\nclass SkuTest extends TestCase\n{\n    \/**\n     * @dataProvider data\n     *\n     * @param string $sku\n     *\/\n    public function testGetValue(string $sku): void\n    {\n        $sku = new Sku($sku);\n\n        $this->assertEquals($sku, $sku->getValue());\n    }\n\n    \/**\n     * @return array\n     *\/\n    public function data(): array\n    {\n        return [\n            ['B.AD46SB\/BN926'],\n        ];\n    }\n}\n","lang_cluster":"PHP","length":41,"code_uid":"c2f6eddccdc847f4a488b734ce984d8d"}
{"diff_hunk":"@@ -18,7 +18,11 @@ use Symfony\\Component\\Validator\\Constraints\\NotBlank;\n use Symfony\\Component\\Validator\\Constraints\\NotEqualTo;\n use Symfony\\Component\\Validator\\ExecutionContextInterface;\n use Thelia\\Core\\Translation\\Translator;\n+use Thelia\\Model\\Base\\CountryQuery;\n use Thelia\\Model\\Base\\LangQuery;\n+use Thelia\\Model\\Base\\ModuleQuery;\n+use Thelia\\Model\\Module;\n+use Thelia\\Module\\BaseModule;\n \n \/**\n  * Allow to build a form Coupon","old_code":"<?php\n\/*************************************************************************************\/\n\/*      This file is part of the Thelia package.                                     *\/\n\/*                                                                                   *\/\n\/*      Copyright (c) OpenStudio                                                     *\/\n\/*      email : dev@thelia.net                                                       *\/\n\/*      web : http:\/\/www.thelia.net                                                  *\/\n\/*                                                                                   *\/\n\/*      For the full copyright and license information, please view the LICENSE.txt  *\/\n\/*      file that was distributed with this source code.                             *\/\n\/*************************************************************************************\/\n\nnamespace Thelia\\Form;\n\nuse Symfony\\Component\\Validator\\Constraints\\Callback;\nuse Symfony\\Component\\Validator\\Constraints\\GreaterThanOrEqual;\nuse Symfony\\Component\\Validator\\Constraints\\NotBlank;\nuse Symfony\\Component\\Validator\\Constraints\\NotEqualTo;\nuse Symfony\\Component\\Validator\\ExecutionContextInterface;\nuse Thelia\\Core\\Translation\\Translator;\nuse Thelia\\Model\\Base\\LangQuery;\n\n\/**\n * Allow to build a form Coupon\n *\n * @package Coupon\n * @author  Guillaume MOREL <gmorel@openstudio.fr>\n *\n *\/\nclass CouponCreationForm extends BaseForm\n{\n    \/**\n     * Build Coupon form\n     *\n     * @return void\n     *\/\n    protected function buildForm()\n    {\n        $this->formBuilder\n            ->add(\n                'code',\n                'text',\n                array(\n                    'constraints' => array(\n                        new NotBlank()\n                    )\n                )\n            )\n            ->add(\n                'title',\n                'text',\n                array(\n                    'constraints' => array(\n                        new NotBlank()\n                    )\n                )\n            )\n            ->add(\n                'shortDescription',\n                'text'\n\n            )\n            ->add(\n                'description',\n                'textarea'\n\n            )\n            ->add(\n                'type',\n                'text',\n                array(\n                    'constraints' => array(\n                        new NotBlank(),\n                        new NotEqualTo(\n                            array(\n                                'value' => -1\n                            )\n                        )\n                    )\n                )\n            )\n            ->add(\n                'amount',\n                'money',\n                array(\n                    'constraints' => array(\n                    new NotBlank()\n                ))\n            )\n            ->add(\n                'isEnabled',\n                'text',\n                array()\n            )\n            ->add(\n                'expirationDate',\n                'text',\n                array(\n                    'constraints' => array(\n                        new NotBlank(),\n                        new Callback(array(\n                            \"methods\" => array(\n                                array($this, \"checkLocalizedDate\"),\n                            ),\n                        ))\n                    )\n                )\n            )\n            ->add(\n                'isCumulative',\n                'text',\n                array()\n            )\n            ->add(\n                'isRemovingPostage',\n                'text',\n                array()\n            )\n            ->add(\n                'isAvailableOnSpecialOffers',\n                'text',\n                array()\n            )\n            ->add(\n                'maxUsage',\n                'text',\n                array(\n                    'constraints' => array(\n                        new NotBlank(),\n                        new GreaterThanOrEqual(['value' => -1])\n                    )\n                )\n            )\n            ->add(\n                'locale',\n                'hidden',\n                array(\n                    'constraints' => array(\n                        new NotBlank()\n                    )\n                )\n            );\n    }\n\n    \/**\n     * Validate a date entered with the default Language date format.\n     *\n     * @param string                    $value\n     * @param ExecutionContextInterface $context\n     *\/\n    public function checkLocalizedDate($value, ExecutionContextInterface $context)\n    {\n        $format = LangQuery::create()->findOneByByDefault(true)->getDateFormat();\n\n        if (false === \\DateTime::createFromFormat($format, $value)) {\n            $context->addViolation(Translator::getInstance()->trans(\"Date '%date' is invalid, please enter a valid date using %fmt format\", [\n                '%fmt' => $format,\n                '%date' => $value\n            ]));\n        }\n    }\n\n    \/**\n     * Get form name\n     *\n     * @return string\n     *\/\n    public function getName()\n    {\n        return 'thelia_coupon_creation';\n    }\n}\n","lang_cluster":"PHP","length":172,"code_uid":"e9a425a2da9843e9853f3c7922aca28f"}
{"diff_hunk":"@@ -68,7 +68,9 @@ class DbalProductValueChangedEventProjector\n             $this->insert($productId, $attributeId, $phrase);\n         } elseif ($value instanceof StringCollectionValue) {\n             foreach ($value->getValue() as $language => $phrase) {\n-                $this->insert($productId, $attributeId, $phrase, $language);\n+                if ($phrase) {\n+                    $this->insert($productId, $attributeId, $phrase, $language);\n+                }\n             }\n         } elseif ($value instanceof TranslatableStringValue) {\n             $translation = $value->getValue();","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types = 1);\n\nnamespace Ergonode\\Product\\Infrastructure\\Persistence\\Projector;\n\nuse Doctrine\\DBAL\\Connection;\nuse Doctrine\\DBAL\\DBALException;\nuse Ergonode\\Product\\Domain\\Event\\ProductValueChangedEvent;\nuse Ergonode\\SharedKernel\\Domain\\Aggregate\\AttributeId;\nuse Ergonode\\Value\\Domain\\ValueObject\\StringCollectionValue;\nuse Ergonode\\Value\\Domain\\ValueObject\\StringValue;\nuse Ergonode\\Value\\Domain\\ValueObject\\TranslatableStringValue;\nuse Ergonode\\Value\\Domain\\ValueObject\\ValueInterface;\nuse Ramsey\\Uuid\\Uuid;\n\n\/**\n *\/\nclass DbalProductValueChangedEventProjector\n{\n    private const TABLE_PRODUCT_VALUE = 'product_value';\n    private const TABLE_VALUE_TRANSLATION = 'value_translation';\n\n    \/**\n     * @var Connection\n     *\/\n    private Connection $connection;\n\n    \/**\n     * @param Connection $connection\n     *\/\n    public function __construct(Connection $connection)\n    {\n        $this->connection = $connection;\n    }\n\n    \/**\n     * @param ProductValueChangedEvent $event\n     *\n     * @throws DBALException\n     *\/\n    public function __invoke(ProductValueChangedEvent $event): void\n    {\n        $productId = $event->getAggregateId()->getValue();\n        $attributeId = AttributeId::fromKey($event->getAttributeCode()->getValue())->getValue();\n\n        $this->delete($productId, $attributeId);\n        $this->insertValue($productId, $attributeId, $event->getTo());\n    }\n\n    \/**\n     * @param string         $productId\n     * @param string         $attributeId\n     * @param ValueInterface $value\n     *\n     * @throws DBALException\n     *\/\n    private function insertValue(string $productId, string $attributeId, ValueInterface $value): void\n    {\n        if ($value instanceof StringValue) {\n            $array = $value->getValue();\n            $phrase = reset($array);\n            $this->insert($productId, $attributeId, $phrase);\n        } elseif ($value instanceof StringCollectionValue) {\n            foreach ($value->getValue() as $language => $phrase) {\n                $this->insert($productId, $attributeId, $phrase, $language);\n            }\n        } elseif ($value instanceof TranslatableStringValue) {\n            $translation = $value->getValue();\n            foreach ($translation as $language => $phrase) {\n                $this->insert($productId, $attributeId, $phrase, $language);\n            }\n        } else {\n            throw new \\RuntimeException(\n                sprintf(sprintf('Unknown Value class \"%s\"', \\get_class($value->getValue())))\n            );\n        }\n    }\n\n    \/**\n     * @param string      $productId\n     * @param string      $attributeId\n     * @param string|null $value\n     * @param string|null $language\n     *\n     * @throws DBALException\n     *\/\n    private function insert(string $productId, string $attributeId, ?string $value, ?string $language = null): void\n    {\n        $valueId = Uuid::uuid5(ValueInterface::NAMESPACE, implode('|', [$value, $language]));\n\n        $qb = $this->connection->createQueryBuilder();\n        $result = $qb->select('*')\n            ->from(self::TABLE_VALUE_TRANSLATION)\n            ->where($qb->expr()->eq('id', ':id'))\n            ->setParameter(':id', $valueId->toString())\n            ->execute()\n            ->fetch();\n\n        if (false === $result) {\n            $this->connection->executeQuery(\n                'INSERT INTO value_translation (id, value_id, value, language) '.\n                ' VALUES (?, ?, ?, ?) ON CONFLICT DO NOTHING',\n                [$valueId->toString(), $valueId->toString(), $value, $language ?: null]\n            );\n        }\n\n        $this->connection->insert(\n            self::TABLE_PRODUCT_VALUE,\n            [\n                'product_id' => $productId,\n                'attribute_id' => $attributeId,\n                'value_id' => $valueId,\n            ]\n        );\n    }\n\n    \/**\n     * @param string $productId\n     * @param string $attributeId\n     *\n     * @throws DBALException\n     *\/\n    private function delete(string $productId, string $attributeId): void\n    {\n        $this->connection->delete(\n            self::TABLE_PRODUCT_VALUE,\n            [\n                'product_id' => $productId,\n                'attribute_id' => $attributeId,\n            ]\n        );\n    }\n}\n","lang_cluster":"PHP","length":139,"code_uid":"9152f446151e4ce3b1230c2f51117ad8"}
{"diff_hunk":"@@ -13,7 +13,7 @@\n \n return [\n     'accepted'             => 'Pole musi zosta\u0107 zaakceptowane.',\n-    'accepted_if'          => 'This field must be accepted when :other is :value.',\n+    'accepted_if'          => 'Pole :attribute musi zosta\u0107 zaakceptowane gdy :other ma warto\u015b\u0107 :value.',\n     'active_url'           => 'Pole jest nieprawid\u0142owym adresem URL.',\n     'after'                => 'Pole musi by\u0107 dat\u0105 p\u00f3\u017aniejsz\u0105 od :date.',\n     'after_or_equal'       => 'Pole musi by\u0107 dat\u0105 nie wcze\u015bniejsz\u0105 ni\u017c :date.',","old_code":"<?php\n\n\/*\n|--------------------------------------------------------------------------\n| Validation Language Lines\n|--------------------------------------------------------------------------\n|\n| The following language lines contain the default error messages used by\n| the validator class. Some of these rules have multiple versions such\n| as the size rules. Feel free to tweak each of these messages here.\n|\n*\/\n\nreturn [\n    'accepted'             => 'Pole musi zosta\u0107 zaakceptowane.',\n    'accepted_if'          => 'This field must be accepted when :other is :value.',\n    'active_url'           => 'Pole jest nieprawid\u0142owym adresem URL.',\n    'after'                => 'Pole musi by\u0107 dat\u0105 p\u00f3\u017aniejsz\u0105 od :date.',\n    'after_or_equal'       => 'Pole musi by\u0107 dat\u0105 nie wcze\u015bniejsz\u0105 ni\u017c :date.',\n    'alpha'                => 'Pole mo\u017ce zawiera\u0107 jedynie litery.',\n    'alpha_dash'           => 'Pole mo\u017ce zawiera\u0107 jedynie litery, cyfry i my\u015blniki.',\n    'alpha_num'            => 'Pole mo\u017ce zawiera\u0107 jedynie litery i cyfry.',\n    'array'                => 'Pole musi by\u0107 tablic\u0105.',\n    'attached'             => 'To pole jest ju\u017c do\u0142\u0105czone.',\n    'before'               => 'Pole musi by\u0107 dat\u0105 wcze\u015bniejsz\u0105 od :date.',\n    'before_or_equal'      => 'Pole musi by\u0107 dat\u0105 nie p\u00f3\u017aniejsz\u0105 ni\u017c :date.',\n    'between'              => [\n        'array'   => 'Pole musi sk\u0142ada\u0107 si\u0119 z :min - :max element\u00f3w.',\n        'file'    => 'Pole musi zawiera\u0107 si\u0119 w granicach :min - :max kilobajt\u00f3w.',\n        'numeric' => 'Pole musi zawiera\u0107 si\u0119 w granicach :min - :max.',\n        'string'  => 'Pole musi zawiera\u0107 si\u0119 w granicach :min - :max znak\u00f3w.',\n    ],\n    'boolean'              => 'Pole musi mie\u0107 warto\u015b\u0107 logiczn\u0105 prawda albo fa\u0142sz.',\n    'confirmed'            => 'Potwierdzenie pola nie zgadza si\u0119.',\n    'current_password'     => 'Has\u0142o jest nieprawid\u0142owe.',\n    'date'                 => 'Pole nie jest prawid\u0142ow\u0105 dat\u0105.',\n    'date_equals'          => 'Pole musi by\u0107 dat\u0105 r\u00f3wn\u0105 :date.',\n    'date_format'          => 'Pole nie jest w formacie :format.',\n    'different'            => 'Pole oraz :other musz\u0105 si\u0119 r\u00f3\u017cni\u0107.',\n    'digits'               => 'Pole musi sk\u0142ada\u0107 si\u0119 z :digits cyfr.',\n    'digits_between'       => 'Pole musi mie\u0107 od :min do :max cyfr.',\n    'dimensions'           => 'Pole ma niepoprawne wymiary.',\n    'distinct'             => 'Pole ma zduplikowane warto\u015bci.',\n    'email'                => 'Pole nie jest poprawnym adresem e-mail.',\n    'ends_with'            => 'Pole musi ko\u0144czy\u0107 si\u0119 jedn\u0105 z nast\u0119puj\u0105cych warto\u015bci: :values.',\n    'exists'               => 'Zaznaczona warto\u015b\u0107 jest nieprawid\u0142owa.',\n    'file'                 => 'Pole musi by\u0107 plikiem.',\n    'filled'               => 'Pole nie mo\u017ce by\u0107 puste.',\n    'gt'                   => [\n        'array'   => 'Pole musi mie\u0107 wi\u0119cej ni\u017c :value element\u00f3w.',\n        'file'    => 'Pole musi by\u0107 wi\u0119ksze ni\u017c :value kilobajt\u00f3w.',\n        'numeric' => 'Pole musi by\u0107 wi\u0119ksze ni\u017c :value.',\n        'string'  => 'Pole musi by\u0107 d\u0142u\u017csze ni\u017c :value znak\u00f3w.',\n    ],\n    'gte'                  => [\n        'array'   => 'Pole musi mie\u0107 :value lub wi\u0119cej element\u00f3w.',\n        'file'    => 'Pole musi by\u0107 wi\u0119ksze lub r\u00f3wne :value kilobajt\u00f3w.',\n        'numeric' => 'Pole musi by\u0107 wi\u0119ksze lub r\u00f3wne :value.',\n        'string'  => 'Pole musi by\u0107 d\u0142u\u017csze lub r\u00f3wne :value znak\u00f3w.',\n    ],\n    'image'                => 'Pole musi by\u0107 obrazkiem.',\n    'in'                   => 'Zaznaczony element jest nieprawid\u0142owy.',\n    'in_array'             => 'Pole nie znajduje si\u0119 w :other.',\n    'integer'              => 'Pole musi by\u0107 liczb\u0105 ca\u0142kowit\u0105.',\n    'ip'                   => 'Pole musi by\u0107 prawid\u0142owym adresem IP.',\n    'ipv4'                 => 'Pole musi by\u0107 prawid\u0142owym adresem IPv4.',\n    'ipv6'                 => 'Pole musi by\u0107 prawid\u0142owym adresem IPv6.',\n    'json'                 => 'Pole musi by\u0107 poprawnym ci\u0105giem znak\u00f3w JSON.',\n    'lt'                   => [\n        'array'   => 'Pole musi mie\u0107 mniej ni\u017c :value element\u00f3w.',\n        'file'    => 'Pole musi by\u0107 mniejsze ni\u017c :value kilobajt\u00f3w.',\n        'numeric' => 'Pole musi by\u0107 mniejsze ni\u017c :value.',\n        'string'  => 'Pole musi by\u0107 kr\u00f3tsze ni\u017c :value znak\u00f3w.',\n    ],\n    'lte'                  => [\n        'array'   => 'Pole musi mie\u0107 :value lub mniej element\u00f3w.',\n        'file'    => 'Pole musi by\u0107 mniejsze lub r\u00f3wne :value kilobajt\u00f3w.',\n        'numeric' => 'Pole musi by\u0107 mniejsze lub r\u00f3wne :value.',\n        'string'  => 'Pole musi by\u0107 kr\u00f3tsze lub r\u00f3wne :value znak\u00f3w.',\n    ],\n    'max'                  => [\n        'array'   => 'Pole nie mo\u017ce mie\u0107 wi\u0119cej ni\u017c :max element\u00f3w.',\n        'file'    => 'Pole nie mo\u017ce by\u0107 wi\u0119ksze ni\u017c :max kilobajt\u00f3w.',\n        'numeric' => 'Pole nie mo\u017ce by\u0107 wi\u0119ksze ni\u017c :max.',\n        'string'  => 'Pole nie mo\u017ce by\u0107 d\u0142u\u017csze ni\u017c :max znak\u00f3w.',\n    ],\n    'mimes'                => 'Pole musi by\u0107 plikiem typu :values.',\n    'mimetypes'            => 'Pole musi by\u0107 plikiem typu :values.',\n    'min'                  => [\n        'array'   => 'Pole musi mie\u0107 przynajmniej :min element\u00f3w.',\n        'file'    => 'Pole musi mie\u0107 przynajmniej :min kilobajt\u00f3w.',\n        'numeric' => 'Pole musi by\u0107 nie mniejsze od :min.',\n        'string'  => 'Pole musi mie\u0107 przynajmniej :min znak\u00f3w.',\n    ],\n    'multiple_of'          => 'Pole musi by\u0107 wielokrotno\u015bci\u0105 warto\u015bci :value',\n    'not_in'               => 'Zaznaczona warto\u015b\u0107 jest nieprawid\u0142owa.',\n    'not_regex'            => 'Format pola jest nieprawid\u0142owy.',\n    'numeric'              => 'Pole musi by\u0107 liczb\u0105.',\n    'password'             => 'Has\u0142o jest nieprawid\u0142owe.',\n    'present'              => 'Pole musi by\u0107 obecne.',\n    'prohibited'           => 'To pole jest zabronione.',\n    'prohibited_if'        => 'To pole jest zabronione, gdy :other to :value.',\n    'prohibited_unless'    => 'To pole jest zabronione, chyba \u017ce :other jest w :values.',\n    'prohibits'            => 'This field prohibits :other from being present.',\n    'regex'                => 'Format pola jest nieprawid\u0142owy.',\n    'relatable'            => 'To pole mo\u017ce nie by\u0107 powi\u0105zane z tym zasobem.',\n    'required'             => 'Pole jest wymagane.',\n    'required_if'          => 'Pole jest wymagane gdy :other ma warto\u015b\u0107 :value.',\n    'required_unless'      => 'Pole jest wymagane je\u017celi :other nie znajduje si\u0119 w :values.',\n    'required_with'        => 'Pole jest wymagane gdy :values jest obecny.',\n    'required_with_all'    => 'Pole jest wymagane gdy wszystkie :values s\u0105 obecne.',\n    'required_without'     => 'Pole jest wymagane gdy :values nie jest obecny.',\n    'required_without_all' => 'Pole jest wymagane gdy \u017cadne z :values nie s\u0105 obecne.',\n    'same'                 => 'Pole i :other musz\u0105 by\u0107 takie same.',\n    'size'                 => [\n        'array'   => 'Pole musi zawiera\u0107 :size element\u00f3w.',\n        'file'    => 'Pole musi mie\u0107 :size kilobajt\u00f3w.',\n        'numeric' => 'Pole musi mie\u0107 :size.',\n        'string'  => 'Pole musi mie\u0107 :size znak\u00f3w.',\n    ],\n    'starts_with'          => 'Pole musi zaczyna\u0107 si\u0119 jedn\u0105 z nast\u0119puj\u0105cych warto\u015bci: :values.',\n    'string'               => 'Pole musi by\u0107 ci\u0105giem znak\u00f3w.',\n    'timezone'             => 'Pole musi by\u0107 prawid\u0142ow\u0105 stref\u0105 czasow\u0105.',\n    'unique'               => 'Taka warto\u015b\u0107 ju\u017c wyst\u0119puje.',\n    'uploaded'             => 'Nie uda\u0142o si\u0119 wgra\u0107 pliku.',\n    'url'                  => 'Format pola jest nieprawid\u0142owy.',\n    'uuid'                 => 'Pole musi by\u0107 poprawnym identyfikatorem UUID.',\n    'custom'               => [\n        'attribute-name' => [\n            'rule-name' => 'custom-message',\n        ],\n    ],\n];\n","lang_cluster":"PHP","length":133,"code_uid":"352720699aaa45acaea6a7fbec90ae39"}
{"diff_hunk":"@@ -14,12 +14,21 @@ namespace Sonata\\MediaBundle\\Security;\n use Sonata\\MediaBundle\\Model\\MediaInterface;\n use Symfony\\Component\\DependencyInjection\\ContainerInterface;\n use Symfony\\Component\\HttpFoundation\\Request;\n+use Symfony\\Component\\HttpFoundation\\Session\\SessionInterface;\n use Symfony\\Component\\Translation\\TranslatorInterface;\n \n+\/**\n+ * Class SessionDownloadStrategy.\n+ *\n+ * @author Ahmet Akbana <ahmetakbana@gmail.com>\n+ *\/\n class SessionDownloadStrategy implements DownloadStrategyInterface\n {\n     \/**\n      * @var ContainerInterface\n+     *\n+     * @deprecated Since version 3.x, will be removed in 4.0.\n+     * NEXT_MAJOR : remove this property\n      *\/\n     protected $container;\n ","old_code":"<?php\n\n\/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Security;\n\nuse Sonata\\MediaBundle\\Model\\MediaInterface;\nuse Symfony\\Component\\DependencyInjection\\ContainerInterface;\nuse Symfony\\Component\\HttpFoundation\\Request;\nuse Symfony\\Component\\Translation\\TranslatorInterface;\n\nclass SessionDownloadStrategy implements DownloadStrategyInterface\n{\n    \/**\n     * @var ContainerInterface\n     *\/\n    protected $container;\n\n    \/**\n     * @var TranslatorInterface\n     *\/\n    protected $translator;\n\n    \/**\n     * @var int\n     *\/\n    protected $times;\n\n    \/**\n     * @var string\n     *\/\n    protected $sessionKey = 'sonata\/media\/session\/times';\n\n    \/**\n     * @param TranslatorInterface $translator\n     * @param ContainerInterface  $container\n     * @param int                 $times\n     *\/\n    public function __construct(TranslatorInterface $translator, ContainerInterface $container, $times)\n    {\n        $this->times = $times;\n        $this->container = $container;\n        $this->translator = $translator;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function isGranted(MediaInterface $media, Request $request)\n    {\n        if (!$this->container->has('session')) {\n            return false;\n        }\n\n        $times = $this->getSession()->get($this->sessionKey, 0);\n\n        if ($times >= $this->times) {\n            return false;\n        }\n\n        ++$times;\n\n        $this->getSession()->set($this->sessionKey, $times);\n\n        return true;\n    }\n\n    \/**\n     * {@inheritdoc}\n     *\/\n    public function getDescription()\n    {\n        return $this->translator->trans('description.session_download_strategy', array('%times%' => $this->times), 'SonataMediaBundle');\n    }\n\n    \/**\n     * @return Session\n     *\/\n    private function getSession()\n    {\n        return $this->container->get('session');\n    }\n}\n","lang_cluster":"PHP","length":90,"code_uid":"806d5b6659904fe09dfa58dcf78900d6"}
{"diff_hunk":"@@ -28,6 +28,7 @@ use Thelia\\Type\\TypeCollection;\n  * {@inheritdoc}\n  * @method int getId()\n  * @method int[] getExclude()\n+ * @method int[] getExcludeCode()\n  * @method string getCode()\n  * @method string[] getOrder()\n  *\/","old_code":"<?php\n\/*************************************************************************************\/\n\/*      This file is part of the Thelia package.                                     *\/\n\/*                                                                                   *\/\n\/*      Copyright (c) OpenStudio                                                     *\/\n\/*      email : dev@thelia.net                                                       *\/\n\/*      web : http:\/\/www.thelia.net                                                  *\/\n\/*                                                                                   *\/\n\/*      For the full copyright and license information, please view the LICENSE.txt  *\/\n\/*      file that was distributed with this source code.                             *\/\n\/*************************************************************************************\/\n\nnamespace Thelia\\Core\\Template\\Loop;\n\nuse Propel\\Runtime\\ActiveQuery\\Criteria;\nuse Thelia\\Core\\Template\\Element\\BaseI18nLoop;\nuse Thelia\\Core\\Template\\Element\\PropelSearchLoopInterface;\nuse Thelia\\Core\\Template\\Loop\\Argument\\Argument;\nuse Thelia\\Core\\Template\\Loop\\Argument\\ArgumentCollection;\nuse Thelia\\Model\\ModuleQuery;\nuse Thelia\\Type\\EnumType;\nuse Thelia\\Type\\TypeCollection;\n\n\/**\n * @package Thelia\\Core\\Template\\Loop\n * @author Manuel Raynaud <manu@raynaud.io>\n *\n * {@inheritdoc}\n * @method int getId()\n * @method int[] getExclude()\n * @method string getCode()\n * @method string[] getOrder()\n *\/\nabstract class BaseSpecificModule extends BaseI18nLoop implements PropelSearchLoopInterface\n{\n    protected $timestampable = true;\n\n    \/**\n     * @return \\Thelia\\Core\\Template\\Loop\\Argument\\ArgumentCollection\n     *\/\n    protected function getArgDefinitions()\n    {\n        return new ArgumentCollection(\n            Argument::createIntTypeArgument('id'),\n            Argument::createIntListTypeArgument('exclude'),\n            Argument::createAnyTypeArgument('code'),\n            new Argument(\n                'order',\n                new TypeCollection(\n                    new EnumType(\n                        [\n                        'id',\n                        'id_reverse',\n                        'alpha',\n                        'alpha_reverse',\n                        'manual',\n                        'manual_reverse',\n                        ]\n                    )\n                ),\n                'manual'\n            )\n        );\n    }\n\n    public function buildModelCriteria()\n    {\n        $search = ModuleQuery::create();\n\n        $search->filterByActivate(1);\n\n        if (null !== $id = $this->getId()) {\n            $search->filterById($id);\n        }\n\n        if (null !== $exclude = $this->getExclude()) {\n            $search->filterById($exclude, Criteria::NOT_IN);\n        }\n\n        if (null !== $code = $this->getCode()) {\n            $search->filterByCode($code);\n        }\n\n        $this->configureI18nProcessing($search);\n\n        $search->filterByType($this->getModuleType(), Criteria::EQUAL);\n\n        $order  = $this->getOrder();\n\n        switch ($order) {\n            case \"id\":\n                $search->orderById(Criteria::ASC);\n                break;\n            case \"id_reverse\":\n                $search->orderById(Criteria::DESC);\n                break;\n            case \"alpha\":\n                $search->addAscendingOrderByColumn('i18n_TITLE');\n                break;\n            case \"alpha_reverse\":\n                $search->addDescendingOrderByColumn('i18n_TITLE');\n                break;\n            case \"manual_reverse\":\n                $search->orderByPosition(Criteria::DESC);\n                break;\n            case \"manual\":\n            default:\n                $search->orderByPosition(Criteria::ASC);\n                break;\n        }\n\n        return $search;\n    }\n\n    abstract protected function getModuleType();\n}\n","lang_cluster":"PHP","length":116,"code_uid":"779a61c9e952498b9d512307ae445760"}
{"diff_hunk":"@@ -154,3 +154,9 @@ function array_change_key_case(array $arr, int $case = CASE_LOWER) {}\n  * @return array<int, array<array-key, T>>\n  *\/\n function array_chunk(array $arr, int $size, bool $preserve_keys = false) {}\n+\n+\n+\/**\n+* @param resource|HashContext\n+*\/\n+function hash_update($hash, string $data) : bool {}","old_code":"<?php\n\/**\n * @psalm-template T as array-key\n *\n * @param array<T, mixed> $arr\n * @param mixed           $search_value\n * @param bool            $strict\n * @return array<int, T>\n *\/\nfunction array_keys(array $arr, $search_value = null, bool $strict = false) {}\n\n\/**\n * @psalm-template T\n *\n * @param array<mixed, T> $arr\n * @return array<int, T>\n *\/\nfunction array_values(array $arr) {}\n\n\/**\n * @psalm-template T\n *\n * @param array<mixed, T> $arr\n * @param int $sort_flags\n * @return array<int, T>\n *\/\nfunction array_unique(array $arr, int $sort_flags = 0) {}\n\n\/**\n * @psalm-template TKey as array-key\n * @psalm-template TValue\n *\n * @param array<TKey, TValue> $arr\n * @param array $arr2\n * @param array|null $arr3\n * @param array|null $arr4\n * @return array<TKey, TValue>\n *\/\nfunction array_intersect(array $arr, array $arr2, array $arr3 = null, array $arr4 = null) {}\n\n\/**\n * @psalm-template TKey as array-key\n * @psalm-template TValue\n *\n * @param array<TKey, TValue> $arr\n * @param array $arr2\n * @param array|null $arr3\n * @param array|null $arr4\n * @return array<TKey, TValue>\n *\/\nfunction array_intersect_key(array $arr, array $arr2, array $arr3 = null, array $arr4 = null) {}\n\n\/**\n * @psalm-template TKey as array-key\n * @psalm-template TValue\n *\n * @param array<mixed, TKey> $arr\n * @param array<mixed, TValue> $arr2\n * @return array<TKey, TValue>\n *\/\nfunction array_combine(array $arr, array $arr2) {}\n\n\/**\n * @psalm-template TKey as array-key\n * @psalm-template TValue\n *\n * @param array<TKey, TValue> $arr\n * @param array $arr2\n * @param array|null $arr3\n * @param array|null $arr4\n * @return array<TKey, TValue>\n *\/\nfunction array_diff(array $arr, array $arr2, array $arr3 = null, array $arr4 = null) {}\n\n\/**\n * @psalm-template TKey as array-key\n * @psalm-template TValue\n *\n * @param array<TKey, TValue> $arr\n * @param array $arr2\n * @param array|null $arr3\n * @param array|null $arr4\n * @return array<TKey, TValue>\n *\/\nfunction array_diff_key(array $arr, array $arr2, array $arr3 = null, array $arr4 = null) {}\n\n\/**\n * @psalm-template TKey as array-key\n * @psalm-template TValue\n *\n * @param array<TKey, TValue> $arr\n * @param bool            $preserve_keys\n * @return array<TKey, TValue>\n *\/\nfunction array_reverse(array $arr, bool $preserve_keys = false) {}\n\n\/**\n * @psalm-template TKey as array-key\n * @psalm-template TValue\n *\n * @param array<TKey, TValue> $arr\n * @return array<TValue, TKey>\n *\/\nfunction array_flip(array $arr) {}\n\n\/**\n * @psalm-template TKey as array-key\n *\n * @param array<TKey, mixed> $arr\n * @return TKey|null\n * @psalm-ignore-nullable-return\n *\/\nfunction key($arr) {}\n\n\/**\n * @psalm-template TValue\n *\n * @param TValue $value\n * @return array<int, TValue>\n *\/\nfunction array_fill( int $start_index, int $num, $value) : array {}\n\n\/**\n * @psalm-template T\n *\n * @param mixed           $needle\n * @param array<T, mixed> $haystack\n * @param bool            $strict\n * @return T|false\n *\/\nfunction array_search($needle, array $haystack, bool $strict = false) {}\n\n\/**\n * @template T\n * @param array<mixed,T> $arr\n * @param callable(T,T):int $callback\n * @param-out array<int,T> $arr\n *\/\nfunction usort(array &$arr, callable $callback): bool {}\n\n\/**\n * @psalm-template T\n *\n * @param array<string, T> $arr\n * @return array<string, T>\n *\/\nfunction array_change_key_case(array $arr, int $case = CASE_LOWER) {}\n\n\/**\n * @psalm-template T\n *\n * @param array<array-key, T> $arr\n *\n * @return array<int, array<array-key, T>>\n *\/\nfunction array_chunk(array $arr, int $size, bool $preserve_keys = false) {}\n","lang_cluster":"PHP","length":156,"code_uid":"517eebf94fc149f2825c69fdde4eead5"}
{"diff_hunk":"@@ -37,7 +37,14 @@ class PricingGroupDataFixture extends AbstractReferenceFixture\n \n         $pricingGroupData->name = 'Oby\u010dejn\u00fd z\u00e1kazn\u00edk';\n         $domainId = 2;\n-        $this->createPricingGroup($pricingGroupData, $domainId, self::PRICING_GROUP_ORDINARY_DOMAIN_2);\n+\n+        $alreadyCreatedDemoPricingGroupsByDomain = $this->pricingGroupFacade->getByDomainId($domainId);\n+        if (count($alreadyCreatedDemoPricingGroupsByDomain) > 0) {\n+            $pricingGroup = reset($alreadyCreatedDemoPricingGroupsByDomain);\n+\n+            $this->pricingGroupFacade->edit($pricingGroup->getId(), $pricingGroupData);\n+            $this->addReference(self::PRICING_GROUP_ORDINARY_DOMAIN_2, $pricingGroup);\n+        }\n \n         $pricingGroupData->name = 'VIP z\u00e1kazn\u00edk';\n         $domainId1 = 2;","old_code":"<?php\n\nnamespace Shopsys\\FrameworkBundle\\DataFixtures\\DemoMultidomain;\n\nuse Doctrine\\Common\\Persistence\\ObjectManager;\nuse Shopsys\\FrameworkBundle\\Component\\DataFixture\\AbstractReferenceFixture;\nuse Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroupData;\nuse Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroupDataFactoryInterface;\nuse Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroupFacade;\n\nclass PricingGroupDataFixture extends AbstractReferenceFixture\n{\n    const PRICING_GROUP_ORDINARY_DOMAIN_2 = 'pricing_group_ordinary_domain_2';\n    const PRICING_GROUP_VIP_DOMAIN_2 = 'pricing_group_vip_domain_2';\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroupFacade\n     *\/\n    private $pricingGroupFacade;\n\n    \/**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroupDataFactoryInterface\n     *\/\n    private $pricingGroupDataFactory;\n\n    public function __construct(\n        PricingGroupFacade $pricingGroupFacade,\n        PricingGroupDataFactoryInterface $pricingGroupDataFactory\n    ) {\n        $this->pricingGroupFacade = $pricingGroupFacade;\n        $this->pricingGroupDataFactory = $pricingGroupDataFactory;\n    }\n\n    public function load(ObjectManager $manager)\n    {\n        $pricingGroupData = $this->pricingGroupDataFactory->create();\n\n        $pricingGroupData->name = 'Oby\u010dejn\u00fd z\u00e1kazn\u00edk';\n        $domainId = 2;\n        $this->createPricingGroup($pricingGroupData, $domainId, self::PRICING_GROUP_ORDINARY_DOMAIN_2);\n\n        $pricingGroupData->name = 'VIP z\u00e1kazn\u00edk';\n        $domainId1 = 2;\n        $this->createPricingGroup($pricingGroupData, $domainId1, self::PRICING_GROUP_VIP_DOMAIN_2);\n    }\n\n    \/**\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroupData $pricingGroupData\n     * @param int $domainId\n     * @param string $referenceName\n     *\/\n    private function createPricingGroup(\n        PricingGroupData $pricingGroupData,\n        $domainId,\n        $referenceName\n    ) {\n        $pricingGroup = $this->pricingGroupFacade->create($pricingGroupData, $domainId);\n        $this->addReference($referenceName, $pricingGroup);\n    }\n}\n","lang_cluster":"PHP","length":60,"code_uid":"abeb4ed9a75844c6bf11cea03a8ca32d"}
{"diff_hunk":"@@ -11,19 +11,45 @@ class RedisFacade\n     \/**\n      * @var \\Redis[]\n      *\/\n+    protected $allClients;\n+\n+    \/**\n+     * @var \\Redis[]\n+     *\/\n+    protected $persistentClients;\n+\n+    \/**\n+     * @deprecated This property is deprecated since SSFW 7.3\n+     * @var \\Redis[]\n+     *\/\n     protected $cacheClients;\n \n     \/**\n-     * @param \\Redis[] $cacheClients\n+     * @param \\Redis[] $allClients\n+     * @param \\Redis[] $persistentClients\n      *\/\n-    public function __construct(array $cacheClients)\n+    public function __construct(iterable $allClients, iterable $persistentClients = [])\n     {\n-        $this->cacheClients = $cacheClients;\n+        $this->allClients = $allClients;\n+        $this->persistentClients = $persistentClients;\n+        $this->cacheClients = $this->getCacheClients();\n+    }\n+\n+    \/**\n+     * @return \\Redis[]\n+     *\/\n+    protected function getCacheClients(): iterable\n+    {\n+        foreach ($this->allClients as $redis) {\n+            if (!in_array($redis, $this->persistentClients, true)) {\n+                yield $redis;\n+            }\n+        }\n     }\n \n     public function cleanCache(): void\n     {\n-        foreach ($this->cacheClients as $redis) {\n+        foreach ($this->getCacheClients() as $redis) {\n             $prefix = (string)$redis->getOption(Redis::OPT_PREFIX);\n             $pattern = $prefix . '*';\n             if (!$this->hasAnyKey($redis, $pattern)) {","old_code":"<?php\n\ndeclare(strict_types=1);\n\nnamespace Shopsys\\FrameworkBundle\\Component\\Redis;\n\nuse Redis;\n\nclass RedisFacade\n{\n    \/**\n     * @var \\Redis[]\n     *\/\n    protected $cacheClients;\n\n    \/**\n     * @param \\Redis[] $cacheClients\n     *\/\n    public function __construct(array $cacheClients)\n    {\n        $this->cacheClients = $cacheClients;\n    }\n\n    public function cleanCache(): void\n    {\n        foreach ($this->cacheClients as $redis) {\n            $prefix = (string)$redis->getOption(Redis::OPT_PREFIX);\n            $pattern = $prefix . '*';\n            if (!$this->hasAnyKey($redis, $pattern)) {\n                continue;\n            }\n            $redis->eval(\"return redis.call('del', unpack(redis.call('keys', ARGV[1])))\", [$pattern]);\n        }\n    }\n\n    \/**\n     * @param \\Redis $redis\n     * @param string $pattern\n     * @return bool\n     *\/\n    protected function hasAnyKey(Redis $redis, string $pattern): bool\n    {\n        $keyCount = $redis->eval(\"return table.getn(redis.call('keys', ARGV[1]))\", [$pattern]);\n        return (bool)$keyCount;\n    }\n}\n","lang_cluster":"PHP","length":46,"code_uid":"040b1a2f67854ad8864cd46386d209fe"}
{"diff_hunk":"@@ -30,7 +30,7 @@ class DbJobsFailedJobsUpdate extends Migration\n     public function down()\n     {\n         Schema::table($this->getTableName(), function (Blueprint $table) {\n-            $table->tinyInteger('reserved')->unsigned();\n+            $table->tinyInteger('reserved')->unsigned()->nullable();\n             $table->dropIndex('jobs_queue_reserved_at_index');\n         });\n ","old_code":"<?php\n\nuse October\\Rain\\Database\\Schema\\Blueprint;\nuse October\\Rain\\Database\\Updates\\Migration;\n\nclass DbJobsFailedJobsUpdate extends Migration\n{\n    \/**\n     * Run the migrations.\n     *\n     * @return void\n     *\/\n    public function up()\n    {\n        Schema::table($this->getTableName(), function (Blueprint $table) {\n            $table->dropColumn('reserved');\n            $table->index(['queue', 'reserved_at']);\n        });\n\n        Schema::table($this->getFailedTableName(), function (Blueprint $table) {\n            $table->longText('exception')->nullable()->after('payload');\n        });\n    }\n\n    \/**\n     * Reverse the migrations.\n     *\n     * @return void\n     *\/\n    public function down()\n    {\n        Schema::table($this->getTableName(), function (Blueprint $table) {\n            $table->tinyInteger('reserved')->unsigned();\n            $table->dropIndex('jobs_queue_reserved_at_index');\n        });\n\n        Schema::table($this->getFailedTableName(), function (Blueprint $table) {\n            $table->dropColumn('exception');\n        });\n    }\n\n    protected function getTableName()\n    {\n        return Config::get('queue.connections.database.table', 'jobs');\n    }\n\n    protected function getFailedTableName()\n    {\n        return Config::get('queue.failed.table', 'failed_jobs');\n    }\n}\n","lang_cluster":"PHP","length":51,"code_uid":"6961cfb8995e45df90cd310985b22a4c"}
{"diff_hunk":"@@ -55,6 +55,7 @@ final class ThumbnailCompilerPassTest extends TestCase\n \n final class TestUncallableAddResizerMethod\n {\n+    \/\/ @phpstan-ignore-next-line\n     private function addResizer(): void\n     {\n     }","old_code":"<?php\n\ndeclare(strict_types=1);\n\n\/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n *\/\n\nnamespace Sonata\\MediaBundle\\Tests\\DependencyInjection\\Compiler;\n\nuse PHPUnit\\Framework\\TestCase;\nuse Sonata\\MediaBundle\\DependencyInjection\\Compiler\\ThumbnailCompilerPass;\nuse Sonata\\MediaBundle\\Thumbnail\\FormatThumbnail;\nuse Sonata\\MediaBundle\\Thumbnail\\MessengerThumbnail;\nuse Symfony\\Component\\DependencyInjection\\ContainerBuilder;\nuse Symfony\\Component\\DependencyInjection\\ParameterBag\\ParameterBag;\nuse Symfony\\Component\\DependencyInjection\\ParameterBag\\ParameterBagInterface;\n\nfinal class ThumbnailCompilerPassTest extends TestCase\n{\n    \/**\n     * @dataProvider processProvider\n     *\n     * @phpstan-param class-string $class\n     *\/\n    public function testProcess(bool $expected, string $class, ?ParameterBagInterface $parameterBag = null): void\n    {\n        $container = new ContainerBuilder($parameterBag);\n        $container\n            ->register('foobar')\n            ->addTag('sonata.media.resizer');\n        $thumbnailDefinition = $container->register('sonata.media.thumbnail.format', $class);\n\n        (new ThumbnailCompilerPass())->process($container);\n\n        static::assertSame($expected, $thumbnailDefinition->hasMethodCall('addResizer'));\n    }\n\n    \/**\n     * @phpstan-return iterable<array{0: bool, 1: class-string|string, 2?: ParameterBagInterface}>\n     *\/\n    public function processProvider(): iterable\n    {\n        yield [true, FormatThumbnail::class];\n        yield [false, MessengerThumbnail::class];\n        yield [true, '%foo%', new ParameterBag(['foo' => FormatThumbnail::class])];\n        yield [false, '%bar%', new ParameterBag(['bar' => TestUncallableAddResizerMethod::class])];\n    }\n}\n\nfinal class TestUncallableAddResizerMethod\n{\n    private function addResizer(): void\n    {\n    }\n}\n","lang_cluster":"PHP","length":61,"code_uid":"13644d69c7564c048b381af6158c5579"}
{"diff_hunk":"@@ -31,6 +31,7 @@ class CreateWorkflowCommandHandler\n     public function __invoke(CreateWorkflowCommand $command): void\n     {\n         $workflow = $this->factory->create($command->getId(), $command->getCode(), $command->getStatuses());\n+        $workflow->setDefaultStatus($command->getDefaultStatus());\n \n         $this->repository->save($workflow);\n     }","old_code":"<?php\n\n\/**\n * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n * See LICENSE.txt for license details.\n *\/\n\ndeclare(strict_types=1);\n\nnamespace Ergonode\\Workflow\\Infrastructure\\Handler\\Workflow;\n\nuse Ergonode\\Workflow\\Domain\\Command\\Workflow\\CreateWorkflowCommand;\nuse Ergonode\\Workflow\\Domain\\Factory\\WorkflowFactory;\nuse Ergonode\\Workflow\\Domain\\Repository\\WorkflowRepositoryInterface;\n\nclass CreateWorkflowCommandHandler\n{\n    private WorkflowRepositoryInterface $repository;\n\n    private WorkflowFactory $factory;\n\n    public function __construct(WorkflowRepositoryInterface $repository, WorkflowFactory $factory)\n    {\n        $this->repository = $repository;\n        $this->factory = $factory;\n    }\n\n    \/**\n     * @throws \\Exception\n     *\/\n    public function __invoke(CreateWorkflowCommand $command): void\n    {\n        $workflow = $this->factory->create($command->getId(), $command->getCode(), $command->getStatuses());\n\n        $this->repository->save($workflow);\n    }\n}\n","lang_cluster":"PHP","length":37,"code_uid":"eb880da4cf1a4d1d966b3d32389b8f47"}
{"diff_hunk":"@@ -128,4 +128,30 @@ class CartPage extends AbstractPage\n     {\n         return $this->webDriver->findElement(WebDriverBy::cssSelector('.js-cart-total-price'));\n     }\n+\n+    \/**\n+     * @param string $couponName\n+     *\/\n+    public function applyCoupon($couponName)\n+    {\n+        $couponField = $this->webDriver->findElement(WebDriverBy::cssSelector('#js-promo-code-input'));\n+        $this->tester->fillFieldByElement($couponField, $couponName);\n+        $this->tester->pressKeysByElement($couponField, WebDriverKeys::ENTER);\n+        $this->tester->waitForAjax();\n+    }\n+\n+    public function removeCoupon()\n+    {\n+        $removeCouponButton = $this->webDriver->findElement(WebDriverBy::cssSelector('.box-promo-code__added__remove'));\n+        $this->tester->clickByElement($removeCouponButton);\n+        $this->tester->waitForAjax();\n+    }\n+\n+    \/**\n+     * @param string $string\n+     *\/\n+    public function assertCouponBoxContainsText($string)\n+    {\n+        $this->tester->see($string, WebDriverBy::cssSelector('.box-promo-code'));\n+    }\n }","old_code":"<?php\n\nnamespace Tests\\ShopBundle\\Acceptance\\acceptance\\PageObject\\Front;\n\nuse Facebook\\WebDriver\\WebDriverBy;\nuse Facebook\\WebDriver\\WebDriverKeys;\nuse Tests\\ShopBundle\\Acceptance\\acceptance\\PageObject\\AbstractPage;\n\nclass CartPage extends AbstractPage\n{\n    \/**\n     * @param string $productName\n     * @param int $quantity\n     *\/\n    public function assertProductQuantity($productName, $quantity)\n    {\n        $quantityField = $this->getQuantityFieldByProductName($productName);\n        $this->tester->seeInFieldByElement($quantity, $quantityField);\n    }\n\n    \/**\n     * @param string $productName\n     * @param string $formattedPriceWithCurrency\n     *\/\n    public function assertProductPrice($productName, $formattedPriceWithCurrency)\n    {\n        $productPriceCell = $this->getProductPriceCellByName($productName);\n        $this->tester->seeInElement($formattedPriceWithCurrency, $productPriceCell);\n    }\n\n    \/**\n     * @param string $formattedPriceWithCurrency\n     *\/\n    public function assertTotalPriceWithVat($formattedPriceWithCurrency)\n    {\n        $orderPriceCell = $this->getTotalProductsPriceCell();\n        $this->tester->seeInElement('Total price including VAT: ' . $formattedPriceWithCurrency, $orderPriceCell);\n    }\n\n    \/**\n     * @param string $productName\n     * @param int $quantity\n     *\/\n    public function changeProductQuantity($productName, $quantity)\n    {\n        $quantityField = $this->getQuantityFieldByProductName($productName);\n        $this->tester->fillFieldByElement($quantityField, $quantity);\n        $this->tester->pressKeysByElement($quantityField, WebDriverKeys::ENTER);\n        $this->tester->waitForAjax();\n    }\n\n    \/**\n     * @param string $productName\n     *\/\n    public function removeProductFromCart($productName)\n    {\n        $row = $this->findProductRowInCartByName($productName);\n        $removingButton = $row->findElement(WebDriverBy::cssSelector('.js-cart-item-remove-button'));\n        $this->tester->clickByElement($removingButton);\n    }\n\n    \/**\n     * @param string $productName\n     *\/\n    public function assertProductIsInCartByName($productName)\n    {\n        $this->tester->see($productName, WebDriverBy::cssSelector('.js-cart-item-name'));\n    }\n\n    \/**\n     * @param string $productName\n     *\/\n    public function assertProductIsNotInCartByName($productName)\n    {\n        $this->tester->dontSee($productName, WebDriverBy::cssSelector('.js-cart-item-name'));\n    }\n\n    \/**\n     * @param string $productName\n     * @return \\Facebook\\WebDriver\\WebDriverElement\n     *\/\n    private function getQuantityFieldByProductName($productName)\n    {\n        $row = $this->findProductRowInCartByName($productName);\n\n        return $row->findElement(WebDriverBy::cssSelector('input[name^=\"cart_form[quantities]\"]'));\n    }\n\n    \/**\n     * @param string $productName\n     * @return \\Facebook\\WebDriver\\WebDriverElement\n     *\/\n    private function findProductRowInCartByName($productName)\n    {\n        $rows = $this->webDriver->findElements(WebDriverBy::cssSelector('.js-cart-item'));\n\n        foreach ($rows as $row) {\n            try {\n                $nameCell = $row->findElement(WebDriverBy::cssSelector('.js-cart-item-name'));\n\n                if ($nameCell->getText() === $productName) {\n                    return $row;\n                }\n            } catch (\\Facebook\\WebDriver\\Exception\\NoSuchElementException $ex) {\n                continue;\n            }\n        }\n\n        $message = 'Unable to find row containing product \"' . $productName . '\" in cart.';\n        throw new \\Facebook\\WebDriver\\Exception\\NoSuchElementException($message);\n    }\n\n    \/**\n     * @param string $productName\n     * @return \\Facebook\\WebDriver\\WebDriverElement\n     *\/\n    private function getProductPriceCellByName($productName)\n    {\n        $row = $this->findProductRowInCartByName($productName);\n\n        return $row->findElement(WebDriverBy::cssSelector('.js-cart-item-total-price'));\n    }\n\n    \/**\n     * @return \\Facebook\\WebDriver\\WebDriverElement\n     *\/\n    private function getTotalProductsPriceCell()\n    {\n        return $this->webDriver->findElement(WebDriverBy::cssSelector('.js-cart-total-price'));\n    }\n}\n","lang_cluster":"PHP","length":131,"code_uid":"b0c0c500f8124973853e398a5f2317a3"}
{"diff_hunk":"@@ -31,20 +31,24 @@ if (is_valid_email_addr($user->email_addr)) {\n     $email_text = $user->email_addr;\n }\n \n-form_start(secure_url_base().\"\/edit_email_action.php\", \"post\");\n-form_input_text(\n-    tra(\"New email address\").\n-    \"<br><p class=\\\"text-muted\\\">\".tra(\"Must be a valid address of the form 'name@domain'\").\"<\/p>\",\n-    \"email_addr\", $email_text\n-);\n-\n-\/\/ we need the password here not for verification,\n-\/\/ but because we store it salted with email address,\n-\/\/ which is about to change.\n-\n-form_input_text(tra(\"Password\"), \"passwd\", \"\", \"password\");\n-form_submit(tra(\"Change email address\"));\n-form_end();\n+if ($user->email_addr_change_time + 604800 > time()) {\n+    echo tra(\"Email address was changed within the past 7 days. Please look for an email to $user->previous_email_addr to verify this change.\");\n+} else {\n+    form_start(secure_url_base().\"edit_email_action.php\", \"post\");\n+    form_input_text(\n+        tra(\"New email address\").\n+        \"<br><p class=\\\"text-muted\\\">\".tra(\"Must be a valid address of the form 'name@domain'\").\"<\/p>\",\n+        \"email_addr\", $email_text\n+    );\n+\n+    \/\/ we need the password here not for verification,\n+    \/\/ but because we store it salted with email address,\n+    \/\/ which is about to change.\n+    \n+    form_input_text(tra(\"Password\"), \"passwd\", \"\", \"password\");\n+    form_submit(tra(\"Change email address\"));\n+    form_end();\n+}\n page_tail();\n \n ?>","old_code":"<?php\n\/\/ This file is part of BOINC.\n\/\/ http:\/\/boinc.berkeley.edu\n\/\/ Copyright (C) 2008 University of California\n\/\/\n\/\/ BOINC is free software; you can redistribute it and\/or modify it\n\/\/ under the terms of the GNU Lesser General Public License\n\/\/ as published by the Free Software Foundation,\n\/\/ either version 3 of the License, or (at your option) any later version.\n\/\/\n\/\/ BOINC is distributed in the hope that it will be useful,\n\/\/ but WITHOUT ANY WARRANTY; without even the implied warranty of\n\/\/ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\/\/ See the GNU Lesser General Public License for more details.\n\/\/\n\/\/ You should have received a copy of the GNU Lesser General Public License\n\/\/ along with BOINC.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nrequire_once(\"..\/inc\/util.inc\");\nrequire_once(\"..\/inc\/email.inc\");\n\ncheck_get_args(array());\n\nredirect_to_secure_url(\"edit_email_form.php\");\n$user = get_logged_in_user();\n\npage_head(tra(\"Change email address\"));\n\n$email_text = \"\";\nif (is_valid_email_addr($user->email_addr)) {\n    $email_text = $user->email_addr;\n}\n\nform_start(secure_url_base().\"\/edit_email_action.php\", \"post\");\nform_input_text(\n    tra(\"New email address\").\n    \"<br><p class=\\\"text-muted\\\">\".tra(\"Must be a valid address of the form 'name@domain'\").\"<\/p>\",\n    \"email_addr\", $email_text\n);\n\n\/\/ we need the password here not for verification,\n\/\/ but because we store it salted with email address,\n\/\/ which is about to change.\n\nform_input_text(tra(\"Password\"), \"passwd\", \"\", \"password\");\nform_submit(tra(\"Change email address\"));\nform_end();\npage_tail();\n\n?>\n","lang_cluster":"PHP","length":50,"code_uid":"884439fa061e479a90e7552b6adccb87"}
{"diff_hunk":"@@ -131,7 +131,7 @@ public class DriverCommandExecutor extends HttpCommandExecutor implements Closea\n         Thread.currentThread().interrupt();\n         throw new WebDriverException(\"Timed out waiting for driver server to stop.\", e);\n       } finally {\n-        executorService.shutdownNow();\n+        executorService.shutdown();\n       }\n \n     } else {","old_code":"\/\/ Licensed to the Software Freedom Conservancy (SFC) under one\n\/\/ or more contributor license agreements.  See the NOTICE file\n\/\/ distributed with this work for additional information\n\/\/ regarding copyright ownership.  The SFC licenses this file\n\/\/ to you under the Apache License, Version 2.0 (the\n\/\/ \"License\"); you may not use this file except in compliance\n\/\/ with the License.  You may obtain a copy of the License at\n\/\/\n\/\/   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\/\/\n\/\/ Unless required by applicable law or agreed to in writing,\n\/\/ software distributed under the License is distributed on an\n\/\/ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\/\/ KIND, either express or implied.  See the License for the\n\/\/ specific language governing permissions and limitations\n\/\/ under the License.\n\npackage org.openqa.selenium.remote.service;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.base.Throwables;\n\nimport org.openqa.selenium.WebDriverException;\nimport org.openqa.selenium.internal.Require;\nimport org.openqa.selenium.remote.Command;\nimport org.openqa.selenium.remote.CommandInfo;\nimport org.openqa.selenium.remote.DriverCommand;\nimport org.openqa.selenium.remote.HttpCommandExecutor;\nimport org.openqa.selenium.remote.Response;\n\nimport java.io.Closeable;\nimport java.io.IOException;\nimport java.net.ConnectException;\nimport java.util.Map;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\n\n\/**\n * A specialized {@link HttpCommandExecutor} that will use a {@link DriverService} that lives\n * and dies with a single WebDriver session. The service will be restarted upon each new session\n * request and shutdown after each quit command.\n *\/\npublic class DriverCommandExecutor extends HttpCommandExecutor implements Closeable {\n\n  private final DriverService service;\n  private final ExecutorService executorService = Executors.newFixedThreadPool(2, r -> {\n    Thread thread = new Thread(r);\n    thread.setName(\"Driver Command Executor\");\n    thread.setDaemon(true);\n    return thread;\n  });\n\n  \/**\n   * Creates a new DriverCommandExecutor which will communicate with the driver as configured\n   * by the given {@code service}.\n   *\n   * @param service The DriverService to send commands to.\n   *\/\n  public DriverCommandExecutor(DriverService service) {\n    super(Require.nonNull(\"DriverService\", service.getUrl()));\n    this.service = service;\n  }\n\n  \/**\n   * Creates an {@link DriverCommandExecutor} that supports non-standard\n   * {@code additionalCommands} in addition to the standard.\n   *\n   * @param service driver server\n   * @param additionalCommands additional commands the remote end can process\n   *\/\n  protected DriverCommandExecutor(\n      DriverService service, Map<String, CommandInfo> additionalCommands) {\n    super(additionalCommands, service.getUrl());\n    this.service = service;\n  }\n\n  \/**\n   * Sends the {@code command} to the driver server for execution. The server will be started\n   * if requesting a new session. Likewise, if terminating a session, the server will be shutdown\n   * once a response is received.\n   *\n   * @param command The command to execute.\n   * @return The command response.\n   * @throws IOException If an I\/O error occurs while sending the command.\n   *\/\n  @Override\n  public Response execute(Command command) throws IOException {\n    boolean newlyStarted = false;\n    if (DriverCommand.NEW_SESSION.equals(command.getName())) {\n      boolean wasRunningBefore = service.isRunning();\n      service.start();\n      newlyStarted = !wasRunningBefore && service.isRunning();\n    }\n\n    if (DriverCommand.QUIT.equals(command.getName())) {\n      CompletableFuture<Response> commandComplete = CompletableFuture.supplyAsync(() -> {\n        try {\n          return invokeExecute(command);\n        } catch (Throwable t) {\n          Throwable rootCause = Throwables.getRootCause(t);\n          if (rootCause instanceof IllegalStateException\n              && \"Closed\".equals(rootCause.getMessage())) {\n            return null;\n          }\n          if (rootCause instanceof ConnectException\n              && \"Connection refused\".equals(rootCause.getMessage())) {\n            throw new WebDriverException(\"The driver server has unexpectedly died!\", t);\n          }\n          Throwables.throwIfUnchecked(t);\n          throw new WebDriverException(t);\n        }\n      }, executorService);\n\n      CompletableFuture<Response> processFinished = CompletableFuture.supplyAsync(() -> {\n        service.process.waitFor(service.getTimeout().toMillis());\n        return null;\n      }, executorService);\n\n      try {\n        Response response = (Response) CompletableFuture.anyOf(commandComplete, processFinished)\n          .get(service.getTimeout().toMillis() * 2, TimeUnit.MILLISECONDS);\n        service.stop();\n        return response;\n      } catch (ExecutionException | TimeoutException e) {\n        throw new WebDriverException(\"Timed out waiting for driver server to stop.\", e);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        throw new WebDriverException(\"Timed out waiting for driver server to stop.\", e);\n      } finally {\n        executorService.shutdownNow();\n      }\n\n    } else {\n      try {\n        return invokeExecute(command);\n      } catch (Throwable t) {\n        Throwable rootCause = Throwables.getRootCause(t);\n        if (rootCause instanceof ConnectException &&\n            \"Connection refused\".equals(rootCause.getMessage()) &&\n            !service.isRunning()) {\n          throw new WebDriverException(\"The driver server has unexpectedly died!\", t);\n        }\n        \/\/ an attempt to execute a command in the newly started driver server has failed\n        \/\/ hence need to stop it\n        if (newlyStarted && service.isRunning()) {\n          try {\n            service.stop();\n          } catch (Exception ignored) {\n            \/\/ fall through\n          }\n        }\n        Throwables.throwIfUnchecked(t);\n        throw new WebDriverException(t);\n      }\n    }\n  }\n\n  @VisibleForTesting\n  Response invokeExecute(Command command) throws IOException {\n    return super.execute(command);\n  }\n\n  @Override\n  public void close() {\n    executorService.shutdownNow();\n  }\n}\n","lang_cluster":"Python","length":171,"code_uid":"bc5da2096416414b91671e940ce50747"}
{"diff_hunk":"@@ -81,6 +81,20 @@ class MovingAverage(object):\n     return newAverage\n \n \n+  def reset(self):\n+    \"\"\"reset internal state to empty\"\"\"\n+    self.slidingWindow = []\n+    self.total = 0.0\n+\n+\n+  def isReady(self):\n+    \"\"\"\n+    Use after init and sequence reset() calls.\n+    @return boolean - moving average is accurate if buffer is full.\n+    \"\"\"\n+    return len(self.slidingWindow)==self.windowSize\n+\n+\n   def getSlidingWindow(self):\n     return self.slidingWindow\n ","old_code":"# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2014, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see http:\/\/www.gnu.org\/licenses.\n#\n# http:\/\/numenta.org\/licenses\/\n# ----------------------------------------------------------------------\n\n\"\"\"\nutils.py are a collection of methods that can be reused by different classes\nin our codebase.\n\"\"\"\n\nimport numbers\n\n\nclass MovingAverage(object):\n  \"\"\"Helper class for computing moving average and sliding window\"\"\"\n\n\n  def __init__(self, windowSize, existingHistoricalValues=None):\n    \"\"\"\n    new instance of MovingAverage, so method .next() can be used\n    @param windowSize - length of sliding window\n    @param existingHistoricalValues - construct the object with already\n        some values in it.\n    \"\"\"\n    if not isinstance(windowSize, numbers.Integral):\n      raise TypeError(\"MovingAverage - windowSize must be integer type\")\n    if  windowSize <= 0:\n      raise ValueError(\"MovingAverage - windowSize must be >0\")\n\n    self.windowSize = windowSize\n    if existingHistoricalValues is not None:\n      self.slidingWindow = existingHistoricalValues[\n                              len(existingHistoricalValues)-windowSize:]\n    else:\n      self.slidingWindow = []\n    self.total = float(sum(self.slidingWindow))\n\n\n  @staticmethod\n  def compute(slidingWindow, total, newVal, windowSize):\n    \"\"\"Routine for computing a moving average.\n\n    @param slidingWindow a list of previous values to use in computation that\n        will be modified and returned\n    @param total the sum of the values in slidingWindow to be used in the\n        calculation of the moving average\n    @param newVal a new number compute the new windowed average\n    @param windowSize how many values to use in the moving window\n\n    @returns an updated windowed average, the modified input slidingWindow list,\n        and the new total sum of the sliding window\n    \"\"\"\n    if len(slidingWindow) == windowSize:\n      total -= slidingWindow.pop(0)\n\n    slidingWindow.append(newVal)\n    total += newVal\n    return float(total) \/ len(slidingWindow), slidingWindow, total\n\n\n  def next(self, newValue):\n    \"\"\"Instance method wrapper around compute.\"\"\"\n    newAverage, self.slidingWindow, self.total = self.compute(\n        self.slidingWindow, self.total, newValue, self.windowSize)\n    return newAverage\n\n\n  def getSlidingWindow(self):\n    return self.slidingWindow\n\n\n  def __setstate__(self, state):\n    \"\"\" for loading this object\"\"\"\n    self.__dict__.update(state)\n\n    if not hasattr(self, \"slidingWindow\"):\n      self.slidingWindow = []\n\n    if not hasattr(self, \"total\"):\n      self.total = 0\n      self.slidingWindow = sum(self.slidingWindow)\n\n\n  def __call__(self, value):\n    return self.next(value)\n\n\n  @classmethod\n  def read(cls, proto):\n    movingAverage = object.__new__(cls)\n    movingAverage.windowSize = proto.windowSize\n    movingAverage.slidingWindow = list(proto.slidingWindow)\n    movingAverage.total = proto.total\n    return movingAverage\n\n\n  def write(self, proto):\n    proto.windowSize = self.windowSize\n    proto.slidingWindow = self.slidingWindow\n    proto.total = self.total\n","lang_cluster":"Python","length":116,"code_uid":"9bbaeaeab9ee4a9dbce0e411fcd948bc"}
{"diff_hunk":"@@ -8,11 +8,6 @@ from astroid.__pkginfo__ import version as astroid_version\n \n from pylint.__pkginfo__ import version as pylint_version\n \n-# Allow stopping after the first semicolon\/hash encountered,\n-# so that an option can be continued with the reasons\n-# why it is active or disabled.\n-OPTION_RGX = re.compile(r\"\\s*#.*\\bpylint:\\s*([^;#]+)[;#]{0,1}\")\n-\n PY_EXTS = (\".py\", \".pyc\", \".pyo\", \".pyw\", \".so\", \".dll\")\n \n MSG_STATE_CONFIDENCE = 2","old_code":"# Licensed under the GPL: https:\/\/www.gnu.org\/licenses\/old-licenses\/gpl-2.0.html\n# For details: https:\/\/github.com\/PyCQA\/pylint\/blob\/master\/COPYING\n\nimport re\nimport sys\n\nfrom astroid.__pkginfo__ import version as astroid_version\n\nfrom pylint.__pkginfo__ import version as pylint_version\n\n# Allow stopping after the first semicolon\/hash encountered,\n# so that an option can be continued with the reasons\n# why it is active or disabled.\nOPTION_RGX = re.compile(r\"\\s*#.*\\bpylint:\\s*([^;#]+)[;#]{0,1}\")\n\nPY_EXTS = (\".py\", \".pyc\", \".pyo\", \".pyw\", \".so\", \".dll\")\n\nMSG_STATE_CONFIDENCE = 2\n_MSG_ORDER = \"EWRCIF\"\nMSG_STATE_SCOPE_CONFIG = 0\nMSG_STATE_SCOPE_MODULE = 1\n\n# The line\/node distinction does not apply to fatal errors and reports.\n_SCOPE_EXEMPT = \"FR\"\n\nMSG_TYPES = {\n    \"I\": \"info\",\n    \"C\": \"convention\",\n    \"R\": \"refactor\",\n    \"W\": \"warning\",\n    \"E\": \"error\",\n    \"F\": \"fatal\",\n}\nMSG_TYPES_LONG = {v: k for k, v in MSG_TYPES.items()}\n\nMSG_TYPES_STATUS = {\"I\": 0, \"C\": 16, \"R\": 8, \"W\": 4, \"E\": 2, \"F\": 1}\n\n# You probably don't want to change the MAIN_CHECKER_NAME\n# This would affect rcfile generation and retro-compatibility\n# on all project using [MASTER] in their rcfile.\nMAIN_CHECKER_NAME = \"master\"\n\n\nclass WarningScope:\n    LINE = \"line-based-msg\"\n    NODE = \"node-based-msg\"\n\n\nfull_version = \"pylint %s\\nastroid %s\\nPython %s\" % (\n    pylint_version,\n    astroid_version,\n    sys.version,\n)\n","lang_cluster":"Python","length":53,"code_uid":"1f1975593b074742b469bc5caad2436b"}
{"diff_hunk":"@@ -1,15 +1,17 @@\n # A part of NonVisual Desktop Access (NVDA)\n-# Copyright (C) 2009-2018 NV Access Limited, Aleksey Sadovoy, James Teh, Joseph Lee, Tuukka Ojala\n+# Copyright (C) 2009-2020 NV Access Limited, Aleksey Sadovoy, James Teh, Joseph Lee, Tuukka Ojala\n # This file may be used under the terms of the GNU General Public License, version 2 or later.\n # For more details see: https:\/\/www.gnu.org\/licenses\/gpl-2.0.htmlimport appModuleHandler\n \n import calendar\n import collections\n import time\n-\n import api\n import appModuleHandler\n+from NVDAObjects.IAccessible import getNVDAObjectFromEvent\n import ui\n+import windowUtils\n+import winUser\n \n # A named tuple for holding the elapsed and total playing times from Foobar2000's status bar\n statusBarTimes = collections.namedtuple('StatusBarTimes', ['elapsed', 'total'])","old_code":"# A part of NonVisual Desktop Access (NVDA)\r\n# Copyright (C) 2009-2018 NV Access Limited, Aleksey Sadovoy, James Teh, Joseph Lee, Tuukka Ojala\r\n# This file may be used under the terms of the GNU General Public License, version 2 or later.\r\n# For more details see: https:\/\/www.gnu.org\/licenses\/gpl-2.0.htmlimport appModuleHandler\r\n\r\nimport calendar\r\nimport collections\r\nimport time\r\n\r\nimport api\r\nimport appModuleHandler\r\nimport ui\r\n\r\n# A named tuple for holding the elapsed and total playing times from Foobar2000's status bar\r\nstatusBarTimes = collections.namedtuple('StatusBarTimes', ['elapsed', 'total'])\r\n\r\ndef getParsingFormat(interval):\r\n\t\"\"\"Attempts to find a suitable parsing format string for a HH:MM:SS, MM:SS or SS -style time interval.\"\"\"\r\n\ttimeParts = len(interval.split(\":\"))\r\n\tif timeParts == 1:\r\n\t\treturn \"%S\"\r\n\telif timeParts == 2:\r\n\t\treturn \"%M:%S\"\r\n\telif timeParts == 3:\r\n\t\treturn \"%H:%M:%S\"\r\n\telse:\r\n\t\treturn None\r\n\r\ndef getOutputFormat(seconds):\r\n\t\"\"\"Returns a format string for the given number of seconds with the least leading zeros.\"\"\"\r\n\tif seconds < 60:\r\n\t\treturn \"%S\"\r\n\telif seconds < 3600:\r\n\t\treturn \"%M:%S\"\r\n\telse:\r\n\t\treturn \"%H:%M:%S\"\r\n\r\ndef parseIntervalToTimestamp(interval):\r\n\t\"\"\"Parses a HH:MM:SS, MM:SS or SS -style interval to a timestamp.\"\"\"\r\n\tformat = getParsingFormat(interval)\r\n\treturn calendar.timegm(time.strptime(interval.strip(), format))\r\n\r\nclass AppModule(appModuleHandler.AppModule):\r\n\tstatusBar=None\r\n\r\n\tdef event_gainFocus(self, obj, nextHandler):\r\n\t\tif not self.statusBar: self.statusBar=api.getStatusBar()\r\n\t\tnextHandler()\r\n\r\n\tdef getElapsedAndTotal(self):\r\n\t\tempty = statusBarTimes(None, None)\r\n\t\tif not self.statusBar: return empty\r\n\t\tstatusBarContents = self.statusBar.firstChild.name\r\n\t\ttry:\r\n\t\t\tplayingTimes = statusBarContents.split(\"|\")[4].split(\"\/\")\r\n\t\t\treturn statusBarTimes(playingTimes[0], playingTimes[1])\r\n\t\texcept IndexError:\r\n\t\t\treturn empty\r\n\r\n\tdef getElapsedAndTotalIfPlaying(self):\r\n\t\telapsedAndTotalTime = self.getElapsedAndTotal()\r\n\t\tif elapsedAndTotalTime.elapsed is None and elapsedAndTotalTime.total is None:\r\n\t\t\t# Translators: Reported when no track is playing in Foobar 2000.\r\n\t\t\tui.message(_(\"No track playing\"))\r\n\t\treturn elapsedAndTotalTime\r\n\r\n\tdef script_reportRemainingTime(self,gesture):\r\n\t\telapsedTime, totalTime = self.getElapsedAndTotalIfPlaying()\r\n\t\tif elapsedTime is not None and totalTime is not None:\r\n\t\t\tparsedElapsedTime = parseIntervalToTimestamp(elapsedTime)\r\n\t\t\tparsedTotalTime = parseIntervalToTimestamp(totalTime)\r\n\t\t\tremainingTime = parsedTotalTime - parsedElapsedTime\r\n\t\t\tmsg = time.strftime(getOutputFormat(remainingTime), time.gmtime(remainingTime))\r\n\t\t\tui.message(msg)\r\n\t# Translators: The description of an NVDA command for reading the remaining time of the currently playing track in Foobar 2000.\r\n\tscript_reportRemainingTime.__doc__ = _(\"Reports the remaining time of the currently playing track, if any\")\r\n\r\n\tdef script_reportElapsedTime(self,gesture):\r\n\t\telapsedTime = self.getElapsedAndTotalIfPlaying()[0]\r\n\t\tif elapsedTime is not None:\r\n\t\t\tui.message(elapsedTime)\r\n\t# Translators: The description of an NVDA command for reading the elapsed time of the currently playing track in Foobar 2000.\r\n\tscript_reportElapsedTime.__doc__ = _(\"Reports the elapsed time of the currently playing track, if any\")\r\n\r\n\tdef script_reportTotalTime(self,gesture):\r\n\t\ttotalTime = self.getElapsedAndTotalIfPlaying()[1]\r\n\t\tif totalTime is not None:\r\n\t\t\tui.message(totalTime)\r\n\t# Translators: The description of an NVDA command for reading the length of the currently playing track in Foobar 2000.\r\n\tscript_reportTotalTime.__doc__ = _(\"Reports the length of the currently playing track, if any\")\r\n\r\n\t__gestures = {\r\n\t\t\"kb:control+shift+r\": \"reportRemainingTime\",\r\n\t\t\"kb:control+shift+e\": \"reportElapsedTime\",\r\n\t\t\"kb:control+shift+t\": \"reportTotalTime\",\r\n\t}\r\n","lang_cluster":"Python","length":96,"code_uid":"217c86811a7649978328064b8764e6cb"}
{"diff_hunk":"@@ -134,6 +134,28 @@ class DefaultFormatBundle(object):\n \n @PIPELINES.register_module\n class Collect(object):\n+    \"\"\"\n+    Populates img_meta, which by default includes:\n+\n+        - \"img_shape\": shape of the image input to the network as a tuple\n+            (h, w, c).  Note that images may be zero padded on the bottom\/right\n+            if the batch tensor is larger than this shape.\n+\n+        - \"scale_factor\": a float indicating the preprocessing scale\n+\n+        - \"flip\": a boolean indicating if image flip transform was used\n+\n+        - \"filename\": path to the image file\n+\n+        - \"ori_shape\": original shape of the image as a tuple (h, w, c)\n+\n+        - \"pad_shape\": image shape after padding\n+\n+        - \"img_norm_cfg\": a dict of normalization information:\n+            - mean - per channel mean subtraction\n+            - std - per channel std divisor\n+            - to_rgb - bool indicating if bgr was converted to rgb\n+    \"\"\"\n \n     def __init__(self,\n                  keys,","old_code":"from collections.abc import Sequence\n\nimport mmcv\nimport numpy as np\nimport torch\nfrom mmcv.parallel import DataContainer as DC\n\nfrom ..registry import PIPELINES\n\n\ndef to_tensor(data):\n    \"\"\"Convert objects of various python types to :obj:`torch.Tensor`.\n\n    Supported types are: :class:`numpy.ndarray`, :class:`torch.Tensor`,\n    :class:`Sequence`, :class:`int` and :class:`float`.\n    \"\"\"\n    if isinstance(data, torch.Tensor):\n        return data\n    elif isinstance(data, np.ndarray):\n        return torch.from_numpy(data)\n    elif isinstance(data, Sequence) and not mmcv.is_str(data):\n        return torch.tensor(data)\n    elif isinstance(data, int):\n        return torch.LongTensor([data])\n    elif isinstance(data, float):\n        return torch.FloatTensor([data])\n    else:\n        raise TypeError('type {} cannot be converted to tensor.'.format(\n            type(data)))\n\n\n@PIPELINES.register_module\nclass ToTensor(object):\n\n    def __init__(self, keys):\n        self.keys = keys\n\n    def __call__(self, results):\n        for key in self.keys:\n            results[key] = to_tensor(results[key])\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(keys={})'.format(self.keys)\n\n\n@PIPELINES.register_module\nclass ImageToTensor(object):\n\n    def __init__(self, keys):\n        self.keys = keys\n\n    def __call__(self, results):\n        for key in self.keys:\n            results[key] = to_tensor(results[key].transpose(2, 0, 1))\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(keys={})'.format(self.keys)\n\n\n@PIPELINES.register_module\nclass Transpose(object):\n\n    def __init__(self, keys, order):\n        self.keys = keys\n        self.order = order\n\n    def __call__(self, results):\n        for key in self.keys:\n            results[key] = results[key].transpose(self.order)\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(keys={}, order={})'.format(\n            self.keys, self.order)\n\n\n@PIPELINES.register_module\nclass ToDataContainer(object):\n\n    def __init__(self,\n                 fields=(dict(key='img', stack=True), dict(key='gt_bboxes'),\n                         dict(key='gt_labels'))):\n        self.fields = fields\n\n    def __call__(self, results):\n        for field in self.fields:\n            field = field.copy()\n            key = field.pop('key')\n            results[key] = DC(results[key], **field)\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(fields={})'.format(self.fields)\n\n\n@PIPELINES.register_module\nclass DefaultFormatBundle(object):\n    \"\"\"Default formatting bundle.\n\n    It simplifies the pipeline of formatting common fields, including \"img\",\n    \"proposals\", \"gt_bboxes\", \"gt_labels\", \"gt_masks\" and \"gt_semantic_seg\".\n    These fields are formatted as follows.\n\n    - img: (1)transpose, (2)to tensor, (3)to DataContainer (stack=True)\n    - proposals: (1)to tensor, (2)to DataContainer\n    - gt_bboxes: (1)to tensor, (2)to DataContainer\n    - gt_bboxes_ignore: (1)to tensor, (2)to DataContainer\n    - gt_labels: (1)to tensor, (2)to DataContainer\n    - gt_masks: (1)to tensor, (2)to DataContainer (cpu_only=True)\n    - gt_semantic_seg: (1)unsqueeze dim-0 (2)to tensor,\n                       (3)to DataContainer (stack=True)\n    \"\"\"\n\n    def __call__(self, results):\n        if 'img' in results:\n            img = np.ascontiguousarray(results['img'].transpose(2, 0, 1))\n            results['img'] = DC(to_tensor(img), stack=True)\n        for key in ['proposals', 'gt_bboxes', 'gt_bboxes_ignore', 'gt_labels']:\n            if key not in results:\n                continue\n            results[key] = DC(to_tensor(results[key]))\n        if 'gt_masks' in results:\n            results['gt_masks'] = DC(results['gt_masks'], cpu_only=True)\n        if 'gt_semantic_seg' in results:\n            results['gt_semantic_seg'] = DC(\n                to_tensor(results['gt_semantic_seg'][None, ...]), stack=True)\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n\n@PIPELINES.register_module\nclass Collect(object):\n\n    def __init__(self,\n                 keys,\n                 meta_keys=('filename', 'ori_shape', 'img_shape', 'pad_shape',\n                            'scale_factor', 'flip', 'img_norm_cfg')):\n        self.keys = keys\n        self.meta_keys = meta_keys\n\n    def __call__(self, results):\n        data = {}\n        img_meta = {}\n        for key in self.meta_keys:\n            img_meta[key] = results[key]\n        data['img_meta'] = DC(img_meta, cpu_only=True)\n        for key in self.keys:\n            data[key] = results[key]\n        return data\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(keys={}, meta_keys={})'.format(\n            self.keys, self.meta_keys)\n","lang_cluster":"Python","length":157,"code_uid":"ff0802375805400c80d26eb47220b6a9"}
{"diff_hunk":"@@ -38,6 +38,12 @@ class SlackWebhook(base_notification.BaseNotification):\n             output: a string formatted violation\n         \"\"\"\n         output = ''\n+\n+        if not isinstance(data, dict):\n+            LOGGER.debug('Violation data is not a dictionary type. '\n+                         f'Violation data: {data}')\n+            return '\\t' * (indent + 1) + '`' + str(data) + '`\\n'\n+\n         for key, value in sorted(data.items()):\n             output += '\\t' * indent + '*' + str(key) + '*:'\n             if isinstance(value, dict):","old_code":"# Copyright 2017 The Forseti Security Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Slack webhook notifier to perform notifications.\"\"\"\n\nfrom builtins import str\nimport requests\n\nfrom google.cloud.forseti.common.util import logger\nfrom google.cloud.forseti.notifier.notifiers import base_notification\n\nLOGGER = logger.get_logger(__name__)\n\nTEMP_DIR = '\/tmp'\n\n\nclass SlackWebhook(base_notification.BaseNotification):\n    \"\"\"Slack webhook notifier to perform notifications\"\"\"\n\n    def _dump_slack_output(self, data, indent=0):\n        \"\"\"Iterate over a dictionary and output a custom formatted string\n\n        Args:\n            data (dict): a dictionary of violation data\n            indent (int): number of spaces for indentation\n\n        Returns:\n            output: a string formatted violation\n        \"\"\"\n        output = ''\n        for key, value in sorted(data.items()):\n            output += '\\t' * indent + '*' + str(key) + '*:'\n            if isinstance(value, dict):\n                output += '\\n' + self._dump_slack_output(value,\n                                                         indent + 1) + '\\n'\n            else:\n                if not value:\n                    value = 'n\/a'\n                output += '\\t' * (indent + 1) + '`' + str(value) + '`\\n'\n\n        return output\n\n    def _compose(self, violation):\n        \"\"\"Composes the slack webhook content\n\n        Args:\n            violation (object): Violation to transform to ascii output.\n\n        Returns:\n            webhook_payload: a string formatted violation\n        \"\"\"\n\n        return ('*type*:\\t`{}`\\n*details*:\\n'.format(self.resource) +\n                self._dump_slack_output(violation.get('violation_data'), 1))\n\n    def _send(self, payload):\n        \"\"\"Sends a post to a Slack webhook url\n\n        Args:\n            payload (str): Payload data to send to slack.\n        \"\"\"\n        url = self.notification_config.get('webhook_url')\n        request = requests.post(url, json={'text': payload})\n\n        LOGGER.info(request)\n\n    def run(self):\n        \"\"\"Run the slack webhook notifier\"\"\"\n        if not self.notification_config.get('webhook_url'):\n            LOGGER.warning('No url found, not running Slack notifier.')\n            return\n\n        for violation in self.violations:\n            webhook_payload = self._compose(violation=violation)\n            self._send(payload=webhook_payload)\n","lang_cluster":"Python","length":85,"code_uid":"4b23846d711b42cebf9446a243f54673"}
{"diff_hunk":"@@ -42,3 +42,27 @@ class ImportTest(unittest.TestCase):\n             for f in files:\n                 if f.endswith('.py') and not f.startswith('_'):\n                     __import__(package + '.' + f[:-3])\n+\n+    def import_luigi_test(self):\n+        \"\"\"\n+        Test that the top luigi package can be imported and contains the usual suspects.\n+        \"\"\"\n+        import luigi\n+\n+        # These should exist (if not, this will cause AttributeErrors)\n+        expected = [\n+            luigi.Event,\n+            luigi.Config,\n+            luigi.Task, luigi.ExternalTask, luigi.WrapperTask,\n+            luigi.Target, luigi.LocalTarget, luigi.File,\n+            luigi.namespace,\n+            luigi.RemoteScheduler,\n+            luigi.RPCError,\n+            luigi.run, luigi.build,\n+            luigi.Parameter,\n+            luigi.DateHourParameter, luigi.DateMinuteParameter, luigi.DateParameter,\n+            luigi.DateIntervalParameter, luigi.TimeDeltaParameter,\n+            luigi.IntParameter, luigi.FloatParameter,\n+            luigi.BooleanParameter, luigi.BoolParameter,\n+        ]\n+        self.assertGreater(len(expected), 0)","old_code":"# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\n\nfrom helpers import unittest\n\n\nclass ImportTest(unittest.TestCase):\n\n    def import_test(self):\n        \"\"\"Test that all module can be imported\n        \"\"\"\n\n        luigidir = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)),\n            '..'\n        )\n\n        packagedir = os.path.join(luigidir, 'luigi')\n\n        for root, subdirs, files in os.walk(packagedir):\n            package = os.path.relpath(root, luigidir).replace('\/', '.')\n\n            if '__init__.py' in files:\n                __import__(package)\n\n            for f in files:\n                if f.endswith('.py') and not f.startswith('_'):\n                    __import__(package + '.' + f[:-3])\n","lang_cluster":"Python","length":44,"code_uid":"f10b481ff1724c97b5f4d48a39454472"}
{"diff_hunk":"@@ -88,8 +88,10 @@ class ClientConnection(tcp.BaseHandler, stateobject.StateObject):\n \n \n class ServerConnection(tcp.TCPClient, stateobject.StateObject):\n-    def __init__(self, address):\n-        tcp.TCPClient.__init__(self, address)\n+    def __init__(self, address, source_address=None):\n+        if source_address:\n+            source_address = (source_address, 0)\n+        tcp.TCPClient.__init__(self, address, source_address)\n \n         self.via = None\n         self.timestamp_start = None","old_code":"from __future__ import (absolute_import, print_function, division)\n\nimport copy\nimport os\n\nfrom netlib import tcp, certutils\nfrom .. import stateobject, utils\n\n\nclass ClientConnection(tcp.BaseHandler, stateobject.StateObject):\n    def __init__(self, client_connection, address, server):\n        # Eventually, this object is restored from state. We don't have a\n        # connection then.\n        if client_connection:\n            super(ClientConnection, self).__init__(client_connection, address, server)\n        else:\n            self.connection = None\n            self.server = None\n            self.wfile = None\n            self.rfile = None\n            self.address = None\n            self.clientcert = None\n            self.ssl_established = None\n\n        self.timestamp_start = utils.timestamp()\n        self.timestamp_end = None\n        self.timestamp_ssl_setup = None\n        self.protocol = None\n\n    def __nonzero__(self):\n        return bool(self.connection) and not self.finished\n\n    def __repr__(self):\n        return \"<ClientConnection: {ssl}{address}>\".format(\n            ssl=\"[ssl] \" if self.ssl_established else \"\",\n            address=repr(self.address)\n        )\n\n    @property\n    def tls_established(self):\n        return self.ssl_established\n\n    _stateobject_attributes = dict(\n        ssl_established=bool,\n        timestamp_start=float,\n        timestamp_end=float,\n        timestamp_ssl_setup=float\n    )\n\n    def get_state(self, short=False):\n        d = super(ClientConnection, self).get_state(short)\n        d.update(\n            address=({\n                \"address\": self.address(),\n                \"use_ipv6\": self.address.use_ipv6} if self.address else {}),\n            clientcert=self.cert.to_pem() if self.clientcert else None)\n        return d\n\n    def load_state(self, state):\n        super(ClientConnection, self).load_state(state)\n        self.address = tcp.Address(\n            **state[\"address\"]) if state[\"address\"] else None\n        self.clientcert = certutils.SSLCert.from_pem(\n            state[\"clientcert\"]) if state[\"clientcert\"] else None\n\n    def copy(self):\n        return copy.copy(self)\n\n    def send(self, message):\n        if isinstance(message, list):\n            message = b''.join(message)\n        self.wfile.write(message)\n        self.wfile.flush()\n\n    @classmethod\n    def from_state(cls, state):\n        f = cls(None, tuple(), None)\n        f.load_state(state)\n        return f\n\n    def convert_to_ssl(self, *args, **kwargs):\n        super(ClientConnection, self).convert_to_ssl(*args, **kwargs)\n        self.timestamp_ssl_setup = utils.timestamp()\n\n    def finish(self):\n        super(ClientConnection, self).finish()\n        self.timestamp_end = utils.timestamp()\n\n\nclass ServerConnection(tcp.TCPClient, stateobject.StateObject):\n    def __init__(self, address):\n        tcp.TCPClient.__init__(self, address)\n\n        self.via = None\n        self.timestamp_start = None\n        self.timestamp_end = None\n        self.timestamp_tcp_setup = None\n        self.timestamp_ssl_setup = None\n        self.protocol = None\n\n    def __nonzero__(self):\n        return bool(self.connection) and not self.finished\n\n    def __repr__(self):\n        if self.ssl_established and self.sni:\n            ssl = \"[ssl: {0}] \".format(self.sni)\n        elif self.ssl_established:\n            ssl = \"[ssl] \"\n        else:\n            ssl = \"\"\n        return \"<ServerConnection: {ssl}{address}>\".format(\n            ssl=ssl,\n            address=repr(self.address)\n        )\n\n    @property\n    def tls_established(self):\n        return self.ssl_established\n\n    _stateobject_attributes = dict(\n        timestamp_start=float,\n        timestamp_end=float,\n        timestamp_tcp_setup=float,\n        timestamp_ssl_setup=float,\n        address=tcp.Address,\n        source_address=tcp.Address,\n        cert=certutils.SSLCert,\n        ssl_established=bool,\n        sni=str\n    )\n    _stateobject_long_attributes = {\"cert\"}\n\n    def get_state(self, short=False):\n        d = super(ServerConnection, self).get_state(short)\n        d.update(\n            address=({\"address\": self.address(),\n                     \"use_ipv6\": self.address.use_ipv6} if self.address else {}),\n            source_address=({\"address\": self.source_address(),\n                             \"use_ipv6\": self.source_address.use_ipv6} if self.source_address else None),\n            cert=self.cert.to_pem() if self.cert else None\n        )\n        return d\n\n    def load_state(self, state):\n        super(ServerConnection, self).load_state(state)\n\n        self.address = tcp.Address(\n            **state[\"address\"]) if state[\"address\"] else None\n        self.source_address = tcp.Address(\n            **state[\"source_address\"]) if state[\"source_address\"] else None\n        self.cert = certutils.SSLCert.from_pem(\n            state[\"cert\"]) if state[\"cert\"] else None\n\n    @classmethod\n    def from_state(cls, state):\n        f = cls(tuple())\n        f.load_state(state)\n        return f\n\n    def copy(self):\n        return copy.copy(self)\n\n    def connect(self):\n        self.timestamp_start = utils.timestamp()\n        tcp.TCPClient.connect(self)\n        self.timestamp_tcp_setup = utils.timestamp()\n\n    def send(self, message):\n        if isinstance(message, list):\n            message = b''.join(message)\n        self.wfile.write(message)\n        self.wfile.flush()\n\n    def establish_ssl(self, clientcerts, sni, **kwargs):\n        clientcert = None\n        if clientcerts:\n            if os.path.isfile(clientcerts):\n                clientcert = clientcerts\n            else:\n                path = os.path.join(\n                    clientcerts,\n                    self.address.host.encode(\"idna\")) + \".pem\"\n                if os.path.exists(path):\n                    clientcert = path\n\n        self.convert_to_ssl(cert=clientcert, sni=sni, **kwargs)\n        self.sni = sni\n        self.timestamp_ssl_setup = utils.timestamp()\n\n    def finish(self):\n        tcp.TCPClient.finish(self)\n        self.timestamp_end = utils.timestamp()\n\n\nServerConnection._stateobject_attributes[\"via\"] = ServerConnection\n","lang_cluster":"Python","length":195,"code_uid":"3d736a5394ff4f4290887371bebb96b7"}
{"diff_hunk":"@@ -165,12 +165,15 @@ def request_candidate_sets(days, top, similar):\n @cli.command(name='request_recommendations')\n @click.option(\"--top\", type=int, default=200, help=\"Generate given number of top artist recommendations\")\n @click.option(\"--similar\", type=int, default=200, help=\"Generate given number of similar artist recommendations\")\n-def request_recommendations(top, similar):\n+@click.option(\"--mb_id\", callback=parse_list, default=[], multiple=True, help=\"Generate recommendations for given users\" \\\n+              \" Generate recommendation for all users by default.\")\n+def request_recommendations(top, similar, mb_id):\n     \"\"\" Send the cluster a request to generate recommendations.\n     \"\"\"\n     params = {\n         'recommendation_top_artist_limit': top,\n         'recommendation_similar_artist_limit': similar,\n+        'musicbrainz_id': mb_id\n     }\n     send_request_to_spark_cluster(_prepare_query_message('cf_recording.recommendations.recommend', params=params))\n ","old_code":"import sys\nimport click\nimport listenbrainz.utils as utils\nimport os\nimport pika\nimport ujson\n\nfrom flask import current_app\nfrom listenbrainz.webserver import create_app\n\n\nQUERIES_JSON_PATH = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'request_queries.json')\n\ncli = click.Group()\n\n\nclass InvalidSparkRequestError(Exception):\n    pass\n\n\ndef _get_possible_queries():\n    \"\"\" Return the dict describing all possible queries that can\n    be sent to Spark. Listed in listenbrainz\/spark\/request_queries.json\n    \"\"\"\n    with open(QUERIES_JSON_PATH) as f:\n        return ujson.load(f)\n\n\ndef _prepare_query_message(query, params=None):\n    \"\"\" Prepare the JSON message that needs to be sent to the\n    spark cluster based on the query and the parameters the\n    query needs\n\n    Args:\n        query (str): the name of the query, should be in request_queries.json\n        params (dict): the parameters the query needs, should contain all the params\n            in the correspoding request_queries.json to be valid\n\n    Raises:\n        InvalidSparkRequestError if the query isn't in the list or if the parameters\n        don't match up\n    \"\"\"\n    if params is None:\n        params = {}\n\n    possible_queries = _get_possible_queries()\n    if query not in possible_queries:\n        raise InvalidSparkRequestError(query)\n\n    message = {'query': possible_queries[query]['name']}\n    required_params = set(possible_queries[query]['params'])\n    given_params = set(params.keys())\n    if required_params != given_params:\n        raise InvalidSparkRequestError\n\n    if params:\n        message['params'] = {}\n        for key, value in params.items():\n            message['params'][key] = value\n\n    return ujson.dumps(message)\n\n\ndef send_request_to_spark_cluster(message):\n    with create_app().app_context():\n        rabbitmq_connection = utils.connect_to_rabbitmq(\n            username=current_app.config['RABBITMQ_USERNAME'],\n            password=current_app.config['RABBITMQ_PASSWORD'],\n            host=current_app.config['RABBITMQ_HOST'],\n            port=current_app.config['RABBITMQ_PORT'],\n            virtual_host=current_app.config['RABBITMQ_VHOST'],\n            error_logger=current_app.logger,\n        )\n        try:\n            channel = rabbitmq_connection.channel()\n            channel.exchange_declare(exchange=current_app.config['SPARK_REQUEST_EXCHANGE'], exchange_type='fanout')\n            channel.basic_publish(\n                exchange=current_app.config['SPARK_REQUEST_EXCHANGE'],\n                routing_key='',\n                body=message,\n                properties=pika.BasicProperties(delivery_mode=2,),\n            )\n        except Exception:\n            # this is a relatively non critical part of LB for now, so just log the error and\n            # move ahead\n            current_app.logger.error('Could not send message to spark cluster: %s', ujson.dumps(message), exc_info=True)\n\n\n@cli.command(name=\"request_user_stats\")\n@click.option(\"--type\", 'type_', type=click.Choice(['entity', 'listening_activity']),\n              help=\"Type of statistics to calculate\", required=True)\n@click.option(\"--range\", 'range_', type=click.Choice(['week', 'month', 'year', 'all_time']),\n              help=\"Time range of statistics to calculate\", required=True)\n@click.option(\"--entity\", type=click.Choice(['artists', 'releases', 'recordings']),\n              help=\"Entity for which statistics should be calculated\")\ndef request_user_stats(type_, range_, entity):\n    \"\"\" Send a user stats request to the spark cluster\n    \"\"\"\n    params = {}\n    if type_ == 'entity' and entity:\n        params['entity'] = entity\n    try:\n        send_request_to_spark_cluster(_prepare_query_message(\n            'stats.user.{type}.{range}'.format(range=range_, type=type_), params=params))\n    except InvalidSparkRequestError:\n        click.echo(\"Incorrect arguments provided\")\n\n\n@cli.command(name=\"request_import_full\")\ndef request_import_new_full_dump():\n    \"\"\" Send the cluster a request to import a new full data dump\n    \"\"\"\n    send_request_to_spark_cluster(_prepare_query_message('import.dump.full'))\n\n\n@cli.command(name=\"request_dataframes\")\n@click.option(\"--days\", type=int, default=180, help=\"Request model to be trained on data of given number of days\")\ndef request_dataframes(days):\n    \"\"\" Send the cluster a request to create dataframes.\n    \"\"\"\n    params = {\n        'train_model_window': days,\n    }\n    send_request_to_spark_cluster(_prepare_query_message('cf_recording.recommendations.create_dataframes', params=params))\n\n\ndef parse_list(ctx, args):\n    return list(args)\n\n\n@cli.command(name='request_model')\n@click.option(\"--rank\", callback=parse_list, default=[5, 10], type=int, multiple=True, help=\"Number of hidden features\")\n@click.option(\"--itr\", callback=parse_list, default=[5, 10], type=int, multiple=True, help=\"Number of iterations to run.\")\n@click.option(\"--lmbda\", callback=parse_list, default=[0.1, 10.0], type=float, multiple=True, help=\"Controls over fitting.\")\n@click.option(\"--alpha\", default=3.0, type=float, help=\"Baseline level of confidence weighting applied.\")\ndef request_model(rank, itr, lmbda, alpha):\n    \"\"\" Send the cluster a request to train the model.\n        For more details refer to 'https:\/\/spark.apache.org\/docs\/2.1.0\/mllib-collaborative-filtering.html'\n    \"\"\"\n    params = {\n        'ranks': rank,\n        'lambdas': lmbda,\n        'iterations': itr,\n        'alpha': alpha,\n    }\n\n    send_request_to_spark_cluster(_prepare_query_message('cf_recording.recommendations.train_model', params=params))\n\n\n@cli.command(name='request_candidate_sets')\n@click.option(\"--days\", type=int, default=7, help=\"Request recommendations to be generated on history of given number of days\")\n@click.option(\"--top\", type=int, default=20, help=\"Calculate given number of top artist.\")\n@click.option(\"--similar\", type=int, default=20, help=\"Calculate given number of similar artist.\")\ndef request_candidate_sets(days, top, similar):\n    \"\"\" Send the cluster a request to generate candidate sets.\n    \"\"\"\n    params = {\n        'recommendation_generation_window': days,\n        \"top_artist_limit\": top,\n        \"similar_artist_limit\": similar,\n    }\n    send_request_to_spark_cluster(_prepare_query_message('cf_recording.recommendations.candidate_sets', params=params))\n\n\n@cli.command(name='request_recommendations')\n@click.option(\"--top\", type=int, default=200, help=\"Generate given number of top artist recommendations\")\n@click.option(\"--similar\", type=int, default=200, help=\"Generate given number of similar artist recommendations\")\ndef request_recommendations(top, similar):\n    \"\"\" Send the cluster a request to generate recommendations.\n    \"\"\"\n    params = {\n        'recommendation_top_artist_limit': top,\n        'recommendation_similar_artist_limit': similar,\n    }\n    send_request_to_spark_cluster(_prepare_query_message('cf_recording.recommendations.recommend', params=params))\n\n\n@cli.command(name='request_import_mapping')\ndef request_import_mapping():\n    \"\"\" Send the spark cluster a request to import msid mbid mapping.\n    \"\"\"\n\n    send_request_to_spark_cluster(_prepare_query_message('import.mapping'))\n\n\n@cli.command(name='request_import_artist_relation')\ndef request_import_artist_relation():\n    \"\"\" Send the spark cluster a request to import artist relation.\n    \"\"\"\n\n    send_request_to_spark_cluster(_prepare_query_message('import.artist_relation'))\n","lang_cluster":"Python","length":191,"code_uid":"c64f0490a2b04185be2b85f233c43e4e"}
{"diff_hunk":"@@ -23,13 +23,19 @@ import click\n \n from molecule import logger\n from molecule.command import base\n+from molecule import util\n+\n \n LOG = logger.get_logger(__name__)\n \n \n class Lint(base.Base):\n     \"\"\"\n-    Lint Command Class.\n+    Lint command executes external linters.\n+\n+    You need to remember to install those linters. For convenience, there is a\n+    package extra that installs the most common ones, use it like\n+    ``pip install \"molecule[lint]\"``.\n \n     .. program:: molecule lint\n ","old_code":"#  Copyright (c) 2015-2018 Cisco Systems, Inc.\n#\n#  Permission is hereby granted, free of charge, to any person obtaining a copy\n#  of this software and associated documentation files (the \"Software\"), to\n#  deal in the Software without restriction, including without limitation the\n#  rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or\n#  sell copies of the Software, and to permit persons to whom the Software is\n#  furnished to do so, subject to the following conditions:\n#\n#  The above copyright notice and this permission notice shall be included in\n#  all copies or substantial portions of the Software.\n#\n#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n#  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n#  DEALINGS IN THE SOFTWARE.\n\"\"\"Lint Command Module.\"\"\"\n\nimport click\n\nfrom molecule import logger\nfrom molecule.command import base\n\nLOG = logger.get_logger(__name__)\n\n\nclass Lint(base.Base):\n    \"\"\"\n    Lint Command Class.\n\n    .. program:: molecule lint\n\n    .. option:: molecule lint\n\n        Target the default scenario.\n\n    .. program:: molecule lint --scenario-name foo\n\n    .. option:: molecule lint --scenario-name foo\n\n        Targeting a specific scenario.\n\n    .. program:: molecule --debug lint\n\n    .. option:: molecule --debug lint\n\n        Executing with `debug`.\n\n    .. program:: molecule --base-config base.yml lint\n\n    .. option:: molecule --base-config base.yml lint\n\n        Executing with a `base-config`.\n\n    .. program:: molecule --env-file foo.yml lint\n\n    .. option:: molecule --env-file foo.yml lint\n\n        Load an env file to read variables from when rendering\n        molecule.yml.\n    \"\"\"\n\n    def execute(self):\n        \"\"\"\n        Execute the actions necessary to perform a `molecule lint` and \\\n        returns None.\n\n        :return: None\n        \"\"\"\n        self.print_info()\n        linters = [\n            l\n            for l in [\n                self._config.lint,\n                self._config.verifier.lint,\n                self._config.provisioner.lint,\n            ]\n            if l\n        ]\n\n        for l in linters:\n            l.execute()\n\n\n@base.click_command_ex()\n@click.pass_context\n@click.option(\n    '--scenario-name',\n    '-s',\n    default=base.MOLECULE_DEFAULT_SCENARIO_NAME,\n    help='Name of the scenario to target. ({})'.format(\n        base.MOLECULE_DEFAULT_SCENARIO_NAME\n    ),\n)\ndef lint(ctx, scenario_name):  # pragma: no cover\n    \"\"\"Lint the role (dependency, lint).\"\"\"\n    args = ctx.obj.get('args')\n    subcommand = base._get_subcommand(__name__)\n    command_args = {'subcommand': subcommand}\n\n    base.execute_cmdline_scenarios(scenario_name, args, command_args)\n","lang_cluster":"Python","length":104,"code_uid":"82b6cad838c14d2fb63a90126a18c40f"}
{"diff_hunk":"@@ -12,11 +12,16 @@ class OHEMSampler(BaseSampler):\n                  context,\n                  neg_pos_ub=-1,\n                  add_gt_as_proposals=True,\n+                 stages=0,\n                  **kwargs):\n         super(OHEMSampler, self).__init__(num, pos_fraction, neg_pos_ub,\n                                           add_gt_as_proposals)\n-        self.bbox_roi_extractor = context.bbox_roi_extractor\n-        self.bbox_head = context.bbox_head\n+        if not hasattr(context, \"num_stages\"):\n+            self.bbox_roi_extractor = context.bbox_roi_extractor\n+            self.bbox_head = context.bbox_head\n+        else:\n+            self.bbox_roi_extractor = context.bbox_roi_extractor[stages]\n+            self.bbox_head = context.bbox_head[stages]\n \n     def hard_mining(self, inds, num_expected, bboxes, labels, feats):\n         with torch.no_grad():","old_code":"import torch\n\nfrom .base_sampler import BaseSampler\nfrom ..transforms import bbox2roi\n\n\nclass OHEMSampler(BaseSampler):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 context,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        super(OHEMSampler, self).__init__(num, pos_fraction, neg_pos_ub,\n                                          add_gt_as_proposals)\n        self.bbox_roi_extractor = context.bbox_roi_extractor\n        self.bbox_head = context.bbox_head\n\n    def hard_mining(self, inds, num_expected, bboxes, labels, feats):\n        with torch.no_grad():\n            rois = bbox2roi([bboxes])\n            bbox_feats = self.bbox_roi_extractor(\n                feats[:self.bbox_roi_extractor.num_inputs], rois)\n            cls_score, _ = self.bbox_head(bbox_feats)\n            loss = self.bbox_head.loss(\n                cls_score=cls_score,\n                bbox_pred=None,\n                labels=labels,\n                label_weights=cls_score.new_ones(cls_score.size(0)),\n                bbox_targets=None,\n                bbox_weights=None,\n                reduce=False)['loss_cls']\n            _, topk_loss_inds = loss.topk(num_expected)\n        return inds[topk_loss_inds]\n\n    def _sample_pos(self,\n                    assign_result,\n                    num_expected,\n                    bboxes=None,\n                    feats=None,\n                    **kwargs):\n        # Sample some hard positive samples\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            return self.hard_mining(pos_inds, num_expected, bboxes[pos_inds],\n                                    assign_result.labels[pos_inds], feats)\n\n    def _sample_neg(self,\n                    assign_result,\n                    num_expected,\n                    bboxes=None,\n                    feats=None,\n                    **kwargs):\n        # Sample some hard negative samples\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            return self.hard_mining(neg_inds, num_expected, bboxes[neg_inds],\n                                    assign_result.labels[neg_inds], feats)\n","lang_cluster":"Python","length":68,"code_uid":"0ff3f8278cc24b77bfcebeda9a779a81"}
{"diff_hunk":"@@ -42,20 +42,36 @@ setup(\n         'packaging>=16.8',\n         'pandas>=0.19.2',\n         'pathlib2; python_version<\"3.6\"',   # stdlib backport\n-        'pyarrow>=0.4.0,<0.8.0',            # TODO(dima): Make unit tests work with 0.8.*.\n+        # 'pyarrow',                          # TODO(dima): Make unit tests work with 0.8.*.\n         'pyyaml>=3.12',\n         'requests>=2.12.4',\n         'six>=1.10.0',\n         'tqdm>=4.11.2',\n         'xlrd>=1.0.0',\n     ],\n+    # Install with: pip install -e .\/[img,tests,...]\n     extras_require={\n-        # Use: pip install --editable .\/[tests]\n+        # See quilt.asa.img module\n+        'img': [\n+            'matplotlib>=2.2.2',\n+            'Pillow>=5.1.0'\n+        ],\n+        # See quilt.asa.pytorch module\n+        'pytorch': [\n+            # May not work on Linux, Windows; See https:\/\/pytorch.org\/\n+            'torch>=0.4.0'\n+        ],\n+        # For dev testing\n         'tests': [\n             'funcsigs; python_version<\"3.4\"',   # stdlib backport\n             'mock; python_version<\"3.3\"',\n             'pytest',\n+            'pytest-cov',\n             'responses>=0.7.0',\n+        ],\n+        'torchvision': [\n+            # May not work on Linux, Windows; See https:\/\/pytorch.org\/\n+            'torchvision>=0.2.1'\n         ]\n     },\n     include_package_data=True,","old_code":"from setuptools import setup, find_packages\n\n\ndef readme():\n    readme_short = \"\"\"\n    ``quilt`` is a command-line utility that builds, pushes, and installs\n    data packages. A `data package <https:\/\/blog.quiltdata.com\/data-packages-for-fast-reproducible-python-analysis-c74b78015c7f>`_\n    is a versioned bundle of serialized data wrapped in a Python module.\n\n    ``quilt`` pushes to and pulls from the package registry at quiltdata.com.\n\n    Visit `quiltdata.com <https:\/\/quiltdata.com>`_ for docs and more.\n    \"\"\"\n    return readme_short\n\n\nsetup(\n    name=\"quilt\",\n    version=\"2.10-dev\",\n    packages=find_packages(),\n    description='Quilt is a data package manager',\n    long_description=readme(),\n    classifiers=[\n        'Development Status :: 5 - Production\/Stable',\n        'Intended Audience :: Developers',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n    ],\n    author='quiltdata',\n    author_email='contact@quiltdata.io',\n    license='LICENSE',\n    url='https:\/\/github.com\/quiltdata\/quilt',\n    download_url='https:\/\/github.com\/quiltdata\/quilt\/releases\/tag\/2.8.0',\n    keywords='quilt quiltdata shareable data dataframe package platform pandas',\n    install_requires=[\n        'appdirs>=1.4.0',\n        'enum34; python_version<\"3.0\"',     # stdlib backport\n        'future>=0.16.0',                   # stdlib backport: 'from builtins import xxx', plus others.\n        'packaging>=16.8',\n        'pandas>=0.19.2',\n        'pathlib2; python_version<\"3.6\"',   # stdlib backport\n        'pyarrow>=0.4.0,<0.8.0',            # TODO(dima): Make unit tests work with 0.8.*.\n        'pyyaml>=3.12',\n        'requests>=2.12.4',\n        'six>=1.10.0',\n        'tqdm>=4.11.2',\n        'xlrd>=1.0.0',\n    ],\n    extras_require={\n        # Use: pip install --editable .\/[tests]\n        'tests': [\n            'funcsigs; python_version<\"3.4\"',   # stdlib backport\n            'mock; python_version<\"3.3\"',\n            'pytest',\n            'responses>=0.7.0',\n        ]\n    },\n    include_package_data=True,\n    entry_points={\n        'console_scripts': ['quilt=quilt.tools.main:main'],\n    }\n)\n","lang_cluster":"Python","length":65,"code_uid":"6ad7031b907a49c6919997129360e335"}
{"diff_hunk":"@@ -4,4 +4,5 @@ setup(\n     name='t4_lambda_thumbnail',\n     version='0.0.1',\n     py_modules=['index'],\n+    packages=['tifffile', 'aicsimageio', 'aicsimageio.readers', 'aicsimageio.writers', 'aicsimageio.vendor']\n )","old_code":"from setuptools import setup\n\nsetup(\n    name='t4_lambda_thumbnail',\n    version='0.0.1',\n    py_modules=['index'],\n)\n","lang_cluster":"Python","length":7,"code_uid":"c0fdd410e8bf4886a9f7d179633f8ae6"}
{"diff_hunk":"@@ -5,6 +5,14 @@ from os import path\n from bzt.modules.siege import SiegeExecutor, DataLogReader\n from tests import BZTestCase\n from tests.mocks import EngineEmul\n+from bzt.utils import is_windows\n+\n+\n+def tool_name():\n+    if is_windows():\n+        return 'siege.bat'\n+    else:\n+        return 'siege.sh'\n \n \n def get_res_path(resource):","old_code":"import logging\nimport time\nfrom os import path\n\nfrom bzt.modules.siege import SiegeExecutor, DataLogReader\nfrom tests import BZTestCase\nfrom tests.mocks import EngineEmul\n\n\ndef get_res_path(resource):\n    return path.join(path.dirname(__file__), '..', 'siege', resource)\n\n\nclass TestSiegeExecutor(BZTestCase):\n    def test_iter(self):\n        obj = SiegeExecutor()\n        obj.engine = EngineEmul()\n        obj.settings.merge({\n            \"path\": get_res_path('siege.sh'),})\n        obj.execution.merge({\n            \"concurrency\": 2,\n            \"iterations\": 3,\n            \"scenario\": {\n                \"think-time\": \"1s\",\n                \"requests\": [\"http:\/\/blazedemo.com\",\n                             \"http:\/\/ya.ru\"]}\n        })\n        obj.prepare()\n        obj.startup()\n\n    def test_hold(self):\n        obj = SiegeExecutor()\n        obj.engine = EngineEmul()\n        obj.settings.merge({\n            \"path\": get_res_path('siege.sh'),})\n        obj.execution.merge({\n            \"concurrency\": 2,\n            \"hold-for\": '2s',\n            \"scenario\": {\n                \"headers\": {\n                    'h1': 'value1',\n                    'h2': 'value2'},\n                \"variables\": {\n                    'v1': 1,\n                    'v2': 'TWO'},\n                \"script\": get_res_path('url-file')}})\n        obj.prepare()\n        obj.startup()\n\n    def test_url_exceptions(self):\n        obj = SiegeExecutor()\n        obj.engine = EngineEmul()\n        obj.settings.merge({\n            \"path\": get_res_path('siege.sh'),})\n        obj.execution.merge({\n            \"concurrency\": 2,\n            \"hold-for\": '2s',\n            \"scenario\": {}})\n        try:\n            obj.prepare()\n        except ValueError:\n            return\n        self.fail()\n\n    def test_check_install_exceptions(self):\n        obj = SiegeExecutor()\n        obj.engine = EngineEmul()\n        obj.settings.merge({\n            \"path\": '*',})\n        obj.execution.merge({\n            \"concurrency\": 2,\n            \"hold-for\": '2s',\n            \"scenario\": {}})\n        try:\n            obj.prepare()\n        except RuntimeError:\n            return\n        self.fail()\n\n    def test_repetition_exceptions(self):\n        obj = SiegeExecutor()\n        obj.engine = EngineEmul()\n        obj.settings.merge({\n            \"path\": get_res_path('siege.sh'),})\n        obj.execution.merge({\n            \"concurrency\": 2,\n            \"scenario\": {\n                \"requests\": [\"http:\/\/blazedemo.com\",\n                             \"http:\/\/ya.ru\"]}})\n        obj.prepare()\n        try:\n            obj.startup()\n        except ValueError:\n            return\n        self.fail()\n\n    def test_full_execution(self):\n        obj = SiegeExecutor()\n        obj.engine = EngineEmul()\n        obj.settings.merge({\n            \"path\": get_res_path('siege.sh'),})\n        obj.execution.merge({\n            \"concurrency\": 2,\n            \"iterations\": 3,\n            \"scenario\": {\n                \"requests\": [\"http:\/\/blazedemo.com\",\n                             \"http:\/\/ya.ru\"]}\n        })\n        obj.prepare()\n        obj.startup()\n        try:\n            while not obj.check():\n                time.sleep(obj.engine.check_interval)\n        finally:\n            obj.shutdown()\n\n        obj.post_process()\n        self.assertNotEquals(obj.process, None)\n\n\nclass TestDataLogReader(BZTestCase):\n    def test_read(self):\n        log_path = path.join(get_res_path('siege.out'))\n        obj = DataLogReader(log_path, logging.getLogger(''))\n        list_of_values = list(obj.datapoints(True))\n\n        self.assertEqual(len(list_of_values), 8)\n\n        for values in list_of_values:\n            self.assertTrue(1400000000 < values['ts'] < 1500000000)\n            self.assertEqual(len(values), 5)\n","lang_cluster":"Python","length":131,"code_uid":"196040f8139844749182ce50f92415df"}
{"diff_hunk":"@@ -55,7 +55,7 @@ def translate(tableList, inbuf, typeform=None, cursorPos=None, mode=0):\n \t* returns a list of integers instead of an string with cells, and\n \t* distinguishes between cursor position 0 (cursor at first character) and None (no cursor at all)\n \t\"\"\"\n-\ttext = unicode(inbuf).replace('\\0','')\n+\ttext = inbuf.replace('\\0','')\n \tbraille, brailleToRawPos, rawToBraillePos, brailleCursorPos = louis.translate(\n \t\ttableList,\n \t\ttext,","old_code":"#louisHelper.py\n#A part of NonVisual Desktop Access (NVDA)\n#This file is covered by the GNU General Public License.\n#See the file COPYING for more details.\n#Copyright (C) 2018 NV Access Limited, Babbage B.V.\n\n\"\"\"Helper module to ease communication to and from liblouis.\"\"\"\n\nimport louis\nfrom logHandler import log\nimport config\n\nLOUIS_TO_NVDA_LOG_LEVELS = {\n\tlouis.LOG_ALL: log.DEBUG,\n\tlouis.LOG_DEBUG: log.DEBUG,\n\tlouis.LOG_INFO: log.INFO,\n\tlouis.LOG_WARN: log.WARNING,\n\tlouis.LOG_ERROR: log.ERROR,\n\tlouis.LOG_FATAL: log.ERROR,\n}\n\n@louis.LogCallback\ndef louis_log(level, message):\n\tif not _isDebug():\n\t\treturn\n\tNVDALevel = LOUIS_TO_NVDA_LOG_LEVELS.get(level, log.DEBUG)\n\tif not log.isEnabledFor(NVDALevel):\n\t\treturn\n\tmessage = message.decode(\"ASCII\")\n\tcodepath = \"liblouis at internal log level %d\" % level\n\tlog._log(NVDALevel, message, [], codepath=codepath)\n\ndef _isDebug():\n\treturn config.conf[\"debugLog\"][\"louis\"]\n\ndef initialize():\n\t# Register the liblouis logging callback.\n\tlouis.registerLogCallback(louis_log)\n\t# Set the log level to debug.\n\t# The NVDA logging callback will filter messages appropriately,\n\t# i.e. error messages will be logged at the error level.\n\tlouis.setLogLevel(louis.LOG_DEBUG)\n\ndef terminate():\n\t# Set the log level to off.\n\tlouis.setLogLevel(louis.LOG_OFF)\n\t# Unregister the liblouis logging callback.\n\tlouis.registerLogCallback(None)\n\t# Free liblouis resources\n\tlouis.liblouis.lou_free()\n\ndef translate(tableList, inbuf, typeform=None, cursorPos=None, mode=0):\n\t\"\"\"\n\tConvenience wrapper for louis.translate that:\n\t* returns a list of integers instead of an string with cells, and\n\t* distinguishes between cursor position 0 (cursor at first character) and None (no cursor at all)\n\t\"\"\"\n\ttext = unicode(inbuf).replace('\\0','')\n\tbraille, brailleToRawPos, rawToBraillePos, brailleCursorPos = louis.translate(\n\t\ttableList,\n\t\ttext,\n\t\t# liblouis mutates typeform if it is a list.\n\t\ttypeform=tuple(typeform) if isinstance(typeform, list) else typeform,\n\t\tcursorPos=cursorPos or 0,\n\t\tmode=mode\n\t)\n\t# liblouis gives us back a character string of cells, so convert it to a list of ints.\n\t# For some reason, the highest bit is set, so only grab the lower 8 bits.\n\tbraille = [ord(cell) & 255 for cell in braille]\n\tif cursorPos is None:\n\t\tbrailleCursorPos = None\n\treturn braille, brailleToRawPos, rawToBraillePos, brailleCursorPos\n","lang_cluster":"Python","length":72,"code_uid":"d4750ab42fec4b908bff62d25efc2f54"}
{"diff_hunk":"@@ -19,6 +19,14 @@ Exceptions\/Errors used in Koalas.\n \"\"\"\n \n \n+class GroupByError(Exception):\n+    pass\n+\n+\n+class DataError(GroupByError):\n+    pass\n+\n+\n class SparkPandasIndexingError(Exception):\n     pass\n ","old_code":"#\n# Copyright (C) 2019 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\nExceptions\/Errors used in Koalas.\n\"\"\"\n\n\nclass SparkPandasIndexingError(Exception):\n    pass\n\n\ndef code_change_hint(pandas_function, spark_target_function):\n    if pandas_function is not None and spark_target_function is not None:\n        return \"You are trying to use pandas function {}, use spark function {}\".format(\n            pandas_function, spark_target_function\n        )\n    elif pandas_function is not None and spark_target_function is None:\n        return (\n            \"You are trying to use pandas function {}, checkout the spark \"\n            \"user guide to find a relevant function\"\n        ).format(pandas_function)\n    elif pandas_function is None and spark_target_function is not None:\n        return \"Use spark function {}\".format(spark_target_function)\n    else:  # both none\n        return \"Checkout the spark user guide to find a relevant function\"\n\n\nclass SparkPandasNotImplementedError(NotImplementedError):\n    def __init__(self, pandas_function=None, spark_target_function=None, description=\"\"):\n        self.pandas_source = pandas_function\n        self.spark_target = spark_target_function\n        hint = code_change_hint(pandas_function, spark_target_function)\n        if len(description) > 0:\n            description += \" \" + hint\n        else:\n            description = hint\n        super(SparkPandasNotImplementedError, self).__init__(description)\n\n\nclass PandasNotImplementedError(NotImplementedError):\n    def __init__(\n        self,\n        class_name,\n        method_name=None,\n        arg_name=None,\n        property_name=None,\n        deprecated=False,\n        reason=\"\",\n    ):\n        assert (method_name is None) != (property_name is None)\n        self.class_name = class_name\n        self.method_name = method_name\n        self.arg_name = arg_name\n        if method_name is not None:\n            if arg_name is not None:\n                msg = \"The method `{0}.{1}()` does not support `{2}` parameter. {3}\".format(\n                    class_name, method_name, arg_name, reason\n                )\n            else:\n                if deprecated:\n                    msg = (\n                        \"The method `{0}.{1}()` is deprecated in pandas and will therefore \"\n                        + \"not be supported in Koalas. {2}\"\n                    ).format(class_name, method_name, reason)\n                else:\n                    if reason == \"\":\n                        reason = \" yet.\"\n                    else:\n                        reason = \". \" + reason\n                    msg = \"The method `{0}.{1}()` is not implemented{2}\".format(\n                        class_name, method_name, reason\n                    )\n        else:\n            if deprecated:\n                msg = (\n                    \"The property `{0}.{1}()` is deprecated in pandas and will therefore \"\n                    + \"not be supported in Koalas. {2}\"\n                ).format(class_name, property_name, reason)\n            else:\n                if reason == \"\":\n                    reason = \" yet.\"\n                else:\n                    reason = \". \" + reason\n                msg = \"The property `{0}.{1}()` is not implemented{2}\".format(\n                    class_name, property_name, reason\n                )\n        super(NotImplementedError, self).__init__(msg)\n","lang_cluster":"Python","length":101,"code_uid":"9fdd607226ea4b669e57464b12d01960"}
{"diff_hunk":"@@ -14,10 +14,43 @@\n \n \"\"\" Forseti Installer.\n \n-A stub to call gcp\/install_setup.py which installs into GCP.\n+A stub to call gcp\/run_forseti_installer.py which installs into GCP.\n \"\"\"\n \n-from gcp import install_setup\n+import site\n+\n+import pip\n+\n+\n+INSTALLER_REQUIRED_PACKAGES = [\n+    'ruamel.yaml'\n+]\n+\n+def install(package_name):\n+    \"\"\"Install package.\n+\n+    Args:\n+        package_name (str): Name of the package to install.\n+    \"\"\"\n+    pip.main(['install', package_name, '--user'])\n+\n+\n+def install_required_packages():\n+    \"\"\"Install required packages.\"\"\"\n+    installed_pkgs = [pkg.key for pkg in pip.get_installed_distributions()]\n+    for package in INSTALLER_REQUIRED_PACKAGES:\n+        if package not in installed_pkgs:\n+            install(package)\n+\n+\n \n if __name__ == '__main__':\n-    install_setup.run()\n+    # We need to install all the required packages before importing our modules\n+\n+    # Installing required packages\n+    install_required_packages()\n+    site.main() # Load up the package\n+\n+    # Importing our own modules\n+    from gcp import run_forseti_installer\n+    run_forseti_installer.run()","old_code":"# Copyright 2017 The Forseti Security Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\" Forseti Installer.\n\nA stub to call gcp\/install_setup.py which installs into GCP.\n\"\"\"\n\nfrom gcp import install_setup\n\nif __name__ == '__main__':\n    install_setup.run()\n","lang_cluster":"Python","length":23,"code_uid":"5bc095b2ff8749709c775d661db698ea"}
{"diff_hunk":"@@ -111,6 +111,27 @@ class QuickmarkCompletionModel(base.BaseCompletionModel):\n                 match_field))\n \n \n+class BookmarkCompletionModel(base.BaseCompletionModel):\n+\n+    \"\"\"A CompletionModel filled with all bookmarks.\"\"\"\n+\n+    # pylint: disable=abstract-method\n+\n+    def __init__(self, match_field='url', parent=None):\n+        super().__init__(parent)\n+        cat = self.new_category(\"Bookmarks\")\n+        bookmarks = objreg.get('bookmark-manager').bookmarks.items()\n+        if match_field == 'url':\n+            for bm_url, bm_title in bookmarks:\n+                self.new_item(cat, bm_url, bm_title)\n+        elif match_field == 'title':\n+            for bm_url, bm_title in bookmarks:\n+                self.new_item(cat, bm_title, bm_url)\n+        else:\n+            raise ValueError(\"Invalid value '{}' for match_field!\".format(\n+                match_field))\n+\n+\n class SessionCompletionModel(base.BaseCompletionModel):\n \n     \"\"\"A CompletionModel filled with session names.\"\"\"","old_code":"# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2014-2015 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n\"\"\"Misc. CompletionModels.\"\"\"\n\nfrom qutebrowser.config import config, configdata\nfrom qutebrowser.utils import objreg, log\nfrom qutebrowser.commands import cmdutils\nfrom qutebrowser.completion.models import base\n\n\nclass CommandCompletionModel(base.BaseCompletionModel):\n\n    \"\"\"A CompletionModel filled with all commands and descriptions.\"\"\"\n\n    # pylint: disable=abstract-method\n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        assert cmdutils.cmd_dict\n        cmdlist = []\n        for obj in set(cmdutils.cmd_dict.values()):\n            if (obj.hide or (obj.debug and not objreg.get('args').debug) or\n                    obj.deprecated):\n                pass\n            else:\n                cmdlist.append((obj.name, obj.desc))\n        for name, cmd in config.section('aliases').items():\n            cmdlist.append((name, \"Alias for '{}'\".format(cmd)))\n        cat = self.new_category(\"Commands\")\n        for (name, desc) in sorted(cmdlist):\n            self.new_item(cat, name, desc)\n\n\nclass HelpCompletionModel(base.BaseCompletionModel):\n\n    \"\"\"A CompletionModel filled with help topics.\"\"\"\n\n    # pylint: disable=abstract-method\n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self._init_commands()\n        self._init_settings()\n\n    def _init_commands(self):\n        \"\"\"Fill completion with :command entries.\"\"\"\n        assert cmdutils.cmd_dict\n        cmdlist = []\n        for obj in set(cmdutils.cmd_dict.values()):\n            if (obj.hide or (obj.debug and not objreg.get('args').debug) or\n                    obj.deprecated):\n                pass\n            else:\n                cmdlist.append((':' + obj.name, obj.desc))\n        cat = self.new_category(\"Commands\")\n        for (name, desc) in sorted(cmdlist):\n            self.new_item(cat, name, desc)\n\n    def _init_settings(self):\n        \"\"\"Fill completion with section->option entries.\"\"\"\n        cat = self.new_category(\"Settings\")\n        for sectname, sectdata in configdata.DATA.items():\n            for optname in sectdata.keys():\n                try:\n                    desc = sectdata.descriptions[optname]\n                except (KeyError, AttributeError):\n                    # Some stuff (especially ValueList items) don't have a\n                    # description.\n                    desc = \"\"\n                else:\n                    desc = desc.splitlines()[0]\n                name = '{}->{}'.format(sectname, optname)\n                self.new_item(cat, name, desc)\n\n\nclass QuickmarkCompletionModel(base.BaseCompletionModel):\n\n    \"\"\"A CompletionModel filled with all quickmarks.\"\"\"\n\n    # pylint: disable=abstract-method\n\n    def __init__(self, match_field='url', parent=None):\n        super().__init__(parent)\n        cat = self.new_category(\"Quickmarks\")\n        quickmarks = objreg.get('quickmark-manager').marks.items()\n        if match_field == 'url':\n            for qm_name, qm_url in quickmarks:\n                self.new_item(cat, qm_url, qm_name)\n        elif match_field == 'name':\n            for qm_name, qm_url in quickmarks:\n                self.new_item(cat, qm_name, qm_url)\n        else:\n            raise ValueError(\"Invalid value '{}' for match_field!\".format(\n                match_field))\n\n\nclass SessionCompletionModel(base.BaseCompletionModel):\n\n    \"\"\"A CompletionModel filled with session names.\"\"\"\n\n    # pylint: disable=abstract-method\n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        cat = self.new_category(\"Sessions\")\n        try:\n            for name in objreg.get('session-manager').list_sessions():\n                if not name.startswith('_'):\n                    self.new_item(cat, name)\n        except OSError:\n            log.completion.exception(\"Failed to list sessions!\")\n","lang_cluster":"Python","length":128,"code_uid":"e01ab2b11d4d458e830541191ea95dc7"}
{"diff_hunk":"@@ -54,6 +54,7 @@ DEFAULT_SETTINGS = {\n         'kinto.core.events.setup_transaction_hook',\n     ),\n     'event_listeners': '',\n+    'heartbeat_timeout_seconds': 5,\n     'logging_renderer': 'kinto.core.logs.ClassicLogRenderer',\n     'newrelic_config': None,\n     'newrelic_env': 'dev',","old_code":"\"\"\"Main entry point\n\"\"\"\nimport pkg_resources\n\nfrom cornice import Service as CorniceService\nfrom pyramid.settings import aslist\n\nfrom kinto.core import authentication\nfrom kinto.core import errors\nfrom kinto.core import events\nfrom kinto.core.initialization import (  # NOQA\n    initialize, install_middlewares,\n    load_default_settings)\nfrom kinto.core.utils import (\n    follow_subrequest, current_service, current_resource_name)\nfrom kinto.core.logs import logger\n\n\n# Module version, as defined in PEP-0396.\n__version__ = pkg_resources.get_distribution('kinto').version  # FIXME?\n\n\nDEFAULT_SETTINGS = {\n    'backoff': None,\n    'batch_max_requests': 25,\n    'cache_backend': '',\n    'cache_url': '',\n    'cache_pool_size': 25,\n    'cache_prefix': '',\n    'cors_origins': '*',\n    'cors_max_age_seconds': 3600,\n    'eos': None,\n    'eos_message': None,\n    'eos_url': None,\n    'error_info_link': 'https:\/\/github.com\/Kinto\/kinto\/issues\/',\n    'http_host': None,\n    'http_scheme': None,\n    'id_generator': 'kinto.core.storage.generators.UUID4',\n    'includes': '',\n    'initialization_sequence': (\n        'kinto.core.initialization.setup_request_bound_data',\n        'kinto.core.initialization.setup_json_serializer',\n        'kinto.core.initialization.setup_logging',\n        'kinto.core.initialization.setup_storage',\n        'kinto.core.initialization.setup_permission',\n        'kinto.core.initialization.setup_cache',\n        'kinto.core.initialization.setup_requests_scheme',\n        'kinto.core.initialization.setup_version_redirection',\n        'kinto.core.initialization.setup_deprecation',\n        'kinto.core.initialization.setup_authentication',\n        'kinto.core.initialization.setup_backoff',\n        'kinto.core.initialization.setup_statsd',\n        'kinto.core.initialization.setup_listeners',\n        'kinto.core.events.setup_transaction_hook',\n    ),\n    'event_listeners': '',\n    'logging_renderer': 'kinto.core.logs.ClassicLogRenderer',\n    'newrelic_config': None,\n    'newrelic_env': 'dev',\n    'paginate_by': None,\n    'permission_backend': '',\n    'permission_url': '',\n    'permission_pool_size': 25,\n    'profiler_dir': '\/tmp',\n    'profiler_enabled': False,\n    'project_docs': '',\n    'project_name': '',\n    'project_version': '',\n    'readonly': False,\n    'retry_after_seconds': 30,\n    'statsd_prefix': 'kinto.core',\n    'statsd_url': None,\n    'storage_backend': '',\n    'storage_url': '',\n    'storage_max_fetch_size': 10000,\n    'storage_pool_size': 25,\n    'tm.annotate_user': False,  # Do annotate transactions with the user-id.\n    'transaction_per_request': True,\n    'userid_hmac_secret': '',\n    'version_prefix_redirect_enabled': True,\n    'trailing_slash_redirect_enabled': True,\n    'multiauth.groupfinder': 'kinto.core.authorization.groupfinder',\n    'multiauth.policies': 'basicauth',\n    'multiauth.policy.basicauth.use': ('kinto.core.authentication.'\n                                       'BasicAuthAuthenticationPolicy'),\n    'multiauth.authorization_policy': ('kinto.core.authorization.'\n                                       'AuthorizationPolicy')\n}\n\n\nclass Service(CorniceService):\n    \"\"\"Subclass of the default cornice service.\n\n    This is useful in order to attach specific behaviours without monkey\n    patching the default cornice service (which would impact other uses of it)\n    \"\"\"\n    default_cors_headers = ('Backoff', 'Retry-After', 'Alert',\n                            'Content-Length')\n\n    def error_handler(self, error):\n        return errors.json_error_handler(error)\n\n    @classmethod\n    def init_from_settings(cls, settings):\n        cls.cors_origins = tuple(aslist(settings['cors_origins']))\n        cors_max_age = settings['cors_max_age_seconds']\n        cls.cors_max_age = int(cors_max_age) if cors_max_age else None\n\n\ndef includeme(config):\n    settings = config.get_settings()\n\n    # Heartbeat registry.\n    config.registry.heartbeats = {}\n\n    # Public settings registry.\n    config.registry.public_settings = {'batch_max_requests', 'readonly'}\n\n    # Directive to declare arbitrary API capabilities.\n    def add_api_capability(config, identifier, description=\"\", url=\"\", **kw):\n        existing = config.registry.api_capabilities.get(identifier)\n        if existing:\n            error_msg = \"The '%s' API capability was already registered (%s).\"\n            raise ValueError(error_msg % (identifier, existing))\n\n        capability = dict(description=description, url=url, **kw)\n        config.registry.api_capabilities[identifier] = capability\n\n    config.add_directive('add_api_capability', add_api_capability)\n    config.registry.api_capabilities = {}\n\n    # Resource events helpers.\n    config.add_request_method(events.get_resource_events,\n                              name='get_resource_events')\n    config.add_request_method(events.notify_resource_event,\n                              name='notify_resource_event')\n\n    # Setup cornice.\n    config.include(\"cornice\")\n\n    # Per-request transaction.\n    config.include(\"pyramid_tm\")\n\n    # Add CORS settings to the base kinto.core Service class.\n    Service.init_from_settings(settings)\n\n    # Setup components.\n    for step in aslist(settings['initialization_sequence']):\n        step_func = config.maybe_dotted(step)\n        step_func(config)\n\n    # Custom helpers.\n    config.add_request_method(follow_subrequest)\n    config.add_request_method(authentication.prefixed_userid, property=True)\n    config.add_request_method(lambda r: {'id': r.prefixed_userid},\n                              name='get_user_info')\n    config.add_request_method(current_resource_name, reify=True)\n    config.add_request_method(current_service, reify=True)\n    config.commit()\n\n    # Include plugins after init, unlike pyramid includes.\n    includes = aslist(settings['includes'])\n    for app in includes:\n        config.include(app)\n\n    # # Show settings to output.\n    # for key, value in settings.items():\n    #     logger.info('Using %s = %s' % (key, value))\n\n    # Scan views.\n    config.scan(\"kinto.core.views\")\n\n    # Give sign of life.\n    msg = \"%(project_name)s %(project_version)s starting.\"\n    logger.info(msg % settings)\n","lang_cluster":"Python","length":175,"code_uid":"e556ec3ca52d4b07b36883ed38abdb76"}
{"diff_hunk":"@@ -158,6 +158,10 @@ class CollectionDeletionTest(BaseWebTest, unittest.TestCase):\n         self.app.put_json(self.collection_url, MINIMALIST_COLLECTION,\n                           headers=headers, status=201)\n \n+    def test_records_permissions_are_removed_after_collection_deleted(self):\n+        self.assertDictEqual(self.permission.get_object_permissions(\n+                             self.record_url), {})\n+\n \n class CollectionCreationTest(BaseWebTest, unittest.TestCase):\n ","old_code":"import unittest\n\nfrom kinto.core.testing import get_user_headers\n\nfrom .support import (BaseWebTest, MINIMALIST_BUCKET,\n                      MINIMALIST_COLLECTION, MINIMALIST_RECORD)\n\n\nclass CollectionViewTest(BaseWebTest, unittest.TestCase):\n\n    collections_url = '\/buckets\/beers\/collections'\n    collection_url = '\/buckets\/beers\/collections\/barley'\n\n    def setUp(self):\n        super().setUp()\n        self.app.put_json('\/buckets\/beers', MINIMALIST_BUCKET,\n                          headers=self.headers)\n        resp = self.app.put_json(self.collection_url,\n                                 MINIMALIST_COLLECTION,\n                                 headers=self.headers)\n        self.record = resp.json['data']\n\n    def test_collection_endpoint_lists_them_all(self):\n        resp = self.app.get(self.collections_url, headers=self.headers)\n        records = resp.json['data']\n        self.assertEqual(len(records), 1)\n        self.assertEqual(records[0]['id'], 'barley')\n\n    def test_collections_can_be_put_with_simple_name(self):\n        self.assertEqual(self.record['id'], 'barley')\n\n    def test_collections_name_should_be_simple(self):\n        self.app.put_json('\/buckets\/beers\/collections\/__barley__',\n                          MINIMALIST_COLLECTION,\n                          headers=self.headers,\n                          status=400)\n\n    def test_collections_should_reject_unaccepted_request_content_type(self):\n        headers = {**self.headers, 'Content-Type': 'text\/plain'}\n        self.app.put('\/buckets\/beers\/collections\/barley',\n                     MINIMALIST_COLLECTION,\n                     headers=headers,\n                     status=415)\n\n    def test_unknown_bucket_raises_403(self):\n        other_bucket = self.collections_url.replace('beers', 'sodas')\n        self.app.get(other_bucket, headers=self.headers, status=403)\n\n    def test_unknown_collection_raises_404(self):\n        other_collection = self.collection_url.replace('barley', 'pills')\n        resp = self.app.get(other_collection, headers=self.headers, status=404)\n        self.assertEqual(resp.json['details']['id'], 'pills')\n        self.assertEqual(resp.json['details']['resource_name'], 'collection')\n\n    def test_collections_are_isolated_by_bucket(self):\n        other_bucket = self.collection_url.replace('beers', 'sodas')\n        self.app.put_json('\/buckets\/sodas',\n                          MINIMALIST_BUCKET,\n                          headers=self.headers)\n        self.app.get(other_bucket, headers=self.headers, status=404)\n\n    def test_create_permissions_can_be_added_on_collections(self):\n        collection = {**MINIMALIST_COLLECTION, 'permissions': {'record:create': ['fxa:user']}}\n        resp = self.app.put_json('\/buckets\/beers\/collections\/barley',\n                                 collection,\n                                 headers=self.headers,\n                                 status=200)\n        permissions = resp.json['permissions']\n        self.assertIn('fxa:user', permissions['record:create'])\n\n    def test_wrong_create_permissions_cannot_be_added_on_collections(self):\n        collection = {**MINIMALIST_COLLECTION, 'permissions': {'collection:create': ['fxa:user']}}\n        self.app.put_json('\/buckets\/beers\/collections\/barley',\n                          collection,\n                          headers=self.headers,\n                          status=400)\n\n    def test_collections_can_handle_arbitrary_attributes(self):\n        fingerprint = \"5866f245a00bb3a39100d31b2f14d453\"\n        collection = {**MINIMALIST_COLLECTION, 'data': {'fingerprint': fingerprint}}\n        resp = self.app.put_json('\/buckets\/beers\/collections\/barley',\n                                 collection,\n                                 headers=self.headers,\n                                 status=200)\n        data = resp.json['data']\n        self.assertIn('fingerprint', data)\n        self.assertEqual(data['fingerprint'], fingerprint)\n\n    def test_collections_can_be_filtered_by_arbitrary_attribute(self):\n        collection = {**MINIMALIST_COLLECTION, 'data': {'size': 3}}\n        self.app.put_json('\/buckets\/beers\/collections\/moderator',\n                          collection,\n                          headers=self.headers)\n        resp = self.app.get('\/buckets\/beers\/collections?has_size=true&min_size=2',\n                            headers=self.headers)\n        data = resp.json['data']\n        self.assertEqual(len(data), 1)\n\n\nclass CollectionDeletionTest(BaseWebTest, unittest.TestCase):\n\n    collection_url = '\/buckets\/beers\/collections\/barley'\n\n    def setUp(self):\n        super().setUp()\n        bucket = {**MINIMALIST_BUCKET,\n                  'permissions': {'collection:create': ['system.Everyone'],\n                                  'read': ['system.Everyone']}}\n        self.app.put_json('\/buckets\/beers', bucket,\n                          headers=self.headers)\n        self.app.put_json(self.collection_url, MINIMALIST_COLLECTION,\n                          headers=self.headers)\n        resp = self.app.post_json(self.collection_url + '\/records',\n                                  MINIMALIST_RECORD,\n                                  headers=self.headers)\n        self.previous_ts = resp.headers['ETag']\n        record_id = resp.json['data']['id']\n        self.record_url = self.collection_url + '\/records\/{}'.format(record_id)\n        self.app.delete(self.collection_url, headers=self.headers)\n\n    def test_collections_can_be_deleted(self):\n        self.app.get(self.collection_url, headers=self.headers, status=404)\n\n    def test_collections_can_be_deleted_in_bulk(self):\n        alice_headers = get_user_headers('alice')\n        self.app.put_json('\/buckets\/beers\/collections\/1',\n                          MINIMALIST_COLLECTION, headers=self.headers)\n        self.app.put_json('\/buckets\/beers\/collections\/2',\n                          MINIMALIST_COLLECTION, headers=alice_headers)\n        self.app.put_json('\/buckets\/beers\/collections\/3',\n                          MINIMALIST_COLLECTION, headers=alice_headers)\n        self.app.delete('\/buckets\/beers\/collections',\n                        headers=alice_headers)\n        resp = self.app.get('\/buckets\/beers\/collections', headers=self.headers)\n        self.assertEqual(len(resp.json['data']), 1)\n\n    def test_records_of_collection_are_deleted_too(self):\n        self.app.put_json(self.collection_url, MINIMALIST_COLLECTION,\n                          headers=self.headers)\n        self.app.get(self.record_url, headers=self.headers, status=404)\n\n        # Verify tombstones\n        resp = self.app.get('{}\/records?_since=0'.format(self.collection_url),\n                            headers=self.headers)\n        self.assertEqual(len(resp.json['data']), 0)\n\n    def test_timestamps_are_refreshed_when_collection_is_recreated(self):\n        # Kinto\/kinto#1223\n        # Recreate with same name.\n        self.app.put_json(self.collection_url, MINIMALIST_COLLECTION,\n                          headers=self.headers)\n        resp = self.app.get(self.collection_url + '\/records', headers=self.headers)\n        records_ts = resp.headers['ETag']\n        self.assertNotEqual(self.previous_ts, records_ts)\n\n    def test_can_be_created_after_deletion_with_if_none_match_star(self):\n        headers = {**self.headers, 'If-None-Match': '*'}\n        self.app.put_json(self.collection_url, MINIMALIST_COLLECTION,\n                          headers=headers, status=201)\n\n\nclass CollectionCreationTest(BaseWebTest, unittest.TestCase):\n\n    collections_url = '\/buckets\/beers\/collections'\n\n    def setUp(self):\n        super().setUp()\n        self.app.put_json('\/buckets\/beers', MINIMALIST_BUCKET,\n                          headers=self.headers)\n\n    def test_collection_can_be_created_with_post(self):\n        r = self.app.post_json(self.collections_url,\n                               MINIMALIST_COLLECTION,\n                               headers=self.headers)\n        self.assertEqual(r.status_code, 201)\n        self.assertTrue(len(r.json['data']['id']) == 8)\n\n    def test_collection_can_be_specified_in_post(self):\n        collection = 'barley'\n        r = self.app.post_json(self.collections_url,\n                               {'data': {'id': collection}},\n                               headers=self.headers)\n        self.assertEqual(r.status_code, 201)\n        self.assertEqual(r.json['data']['id'], collection)\n\n    def test_collection_already_exists_post(self):\n        collection = \"barley\"\n        self.app.post_json(self.collections_url,\n                           {'data': {'id': collection}},\n                           headers=self.headers)\n        r = self.app.post_json(self.collections_url,\n                               {'data': {'id': collection}},\n                               headers=self.headers)\n        self.assertEqual(r.json['data']['id'], collection)\n        self.assertEqual(r.status_code, 200)\n","lang_cluster":"Python","length":195,"code_uid":"9a54e7d999874285a9c1dacebb839cdc"}
{"diff_hunk":"@@ -71,6 +71,7 @@ setup(\n         'tqdm>=4.26.0',\n         'urllib3<1.25,>=1.21.1',            # required by requests\n         'requests_futures==1.0.0',\n+        'git-pylint-commit-hook',\n     ],\n     extras_require={\n         'pyarrow': [","old_code":"import os\nimport sys\n\nfrom pathlib import Path\n\nfrom setuptools import setup, find_packages\nfrom setuptools.command.install import install\n\nVERSION = Path(Path(__file__).parent, \"quilt3\", \"VERSION\").read_text().strip()\n\ndef readme():\n    readme_short = \"\"\"\n    Quilt manages data like code (with packages, repositories, browsing and\n    revision history) so that teams can experiment faster in machine learning,\n    biotech, and other data-driven domains.\n\n    The `quilt3` PyPi package allows you to build, push, and install data packages.\n    Visit the `documentation quickstart <https:\/\/docs.quiltdata.com\/quickstart>`_\n    to learn more.\n    \"\"\"\n    return readme_short\n\n\nclass VerifyVersionCommand(install):\n    \"\"\"Custom command to verify that the git tag matches our version\"\"\"\n    description = 'verify that the git tag matches our version'\n\n    def run(self):\n        tag = os.getenv('CIRCLE_TAG')\n\n        if tag != VERSION:\n            info = \"Git tag: {0} does not match the version of this app: {1}\".format(\n                tag, VERSION\n            )\n            sys.exit(info)\n\nsetup(\n    name=\"quilt3\",\n    version=VERSION,\n    packages=find_packages(),\n    description='Quilt: where data comes together',\n    long_description=readme(),\n    python_requires='>=3.6',\n    classifiers=[\n        'Development Status :: 5 - Production\/Stable',\n        'Intended Audience :: Developers',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n    ],\n    author='quiltdata',\n    author_email='contact@quiltdata.io',\n    license='LICENSE',\n    url='https:\/\/github.com\/quiltdata\/quilt',\n    keywords='',\n    install_requires=[\n        'appdirs>=1.4.0',\n        'aws-requests-auth>=0.4.2',\n        'boto3>=1.10.0',\n        'dnspython>=1.16.0',\n        'flask',\n        'flask_cors',\n        'flask_json',\n        'jsonlines==1.2.0',\n        'packaging>=16.8',\n        'python-dateutil<=2.8.0',           # 2.8.1 conflicts with botocore\n        'PyYAML>=5.3',\n        'requests>=2.12.4',\n        'tenacity>=5.1.1',\n        'tqdm>=4.26.0',\n        'urllib3<1.25,>=1.21.1',            # required by requests\n        'requests_futures==1.0.0',\n    ],\n    extras_require={\n        'pyarrow': [\n            'numpy>=1.14.0',                # required by pandas, but missing from its dependencies.\n            'pandas>=0.19.2',\n            'pyarrow>=0.14.1',              # as of 7\/5\/19: linux\/circleci bugs on 0.14.0\n        ],\n        'tests': [\n            'codecov',\n            'numpy>=1.14.0',                # required by pandas, but missing from its dependencies.\n            'pandas>=0.19.2',\n            'pyarrow>=0.14.1',              # as of 7\/5\/19: linux\/circleci bugs on 0.14.0\n            'pytest<5.1.0',                 # TODO: Fix pytest.ensuretemp in conftest.py\n            'pytest-cov',\n            'pytest-env',\n            'responses',\n            'tox',\n            'detox',\n            'tox-pytest-summary',\n        ],\n    },\n    include_package_data=True,\n    entry_points={\n        'console_scripts': ['quilt3=quilt3.main:main'],\n    },\n    cmdclass={\n        'verify': VerifyVersionCommand,\n    }\n)\n","lang_cluster":"Python","length":102,"code_uid":"2206003065af48299005a22348701260"}
{"diff_hunk":"@@ -18,13 +18,47 @@ def sum(iterable, start=0):\n     else:\n         return _builtin_sum(iterable, start)\n \n-def count(iterable, start=0):\n+\n+def count(iterable=None):\n     if isinstance(iterable, BaseExpr):\n         return ReduceExpr(\"count\", iterable)\n+    elif iterable is None:\n+        return CountExpr()\n+    else:\n+        return _builtin_sum(1 for x in iterable)\n \n-def first(iterable, start=0):\n+\n+def first(iterable):\n     if isinstance(iterable, BaseExpr):\n         return ReduceExpr(\"first\", iterable)\n+    else:\n+        for x in iterable:\n+            if x:\n+                return x\n+              \n+              \n+class CountExpr(BaseExpr):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def is_reduce_expr(self, ee):\n+        return True\n+\n+    def resolve(self):\n+       self._stype = int\n+#        self._expr.resolve()\n+#         if self._stype is None:\n+#             raise ValueError(\n+#                 \"Cannot compute %s of a variable of type %s\"\n+#                 % (self._op, expr_stype))\n+\n+    def evaluate_eager(self, ee):\n+        return core.expr_count(ee.groupby)\n+\n+    def __str__(self):\n+        return \"\"\n+\n+\n \n class ReduceExpr(BaseExpr):\n     __slots__ = [\"_op\", \"_expr\"]","old_code":"#!\/usr\/bin\/env python3\n# \u00a9 H2O.ai 2018; -*- encoding: utf-8 -*-\n#   This Source Code Form is subject to the terms of the Mozilla Public\n#   License, v. 2.0. If a copy of the MPL was not distributed with this\n#   file, You can obtain one at http:\/\/mozilla.org\/MPL\/2.0\/.\n#-------------------------------------------------------------------------------\n# mean_expr, sd_expr, minmax_expr can eventually be merged into here too\n\nfrom .base_expr import BaseExpr\nfrom .consts import reduce_opcodes, ops_rules\nfrom datatable.lib import core\n\n_builtin_sum = sum\n\ndef sum(iterable, start=0):\n    if isinstance(iterable, BaseExpr):\n        return ReduceExpr(\"sum\", iterable)\n    else:\n        return _builtin_sum(iterable, start)\n\ndef count(iterable, start=0):\n    if isinstance(iterable, BaseExpr):\n        return ReduceExpr(\"count\", iterable)\n\ndef first(iterable, start=0):\n    if isinstance(iterable, BaseExpr):\n        return ReduceExpr(\"first\", iterable)\n\nclass ReduceExpr(BaseExpr):\n    __slots__ = [\"_op\", \"_expr\"]\n\n    def __init__(self, op, expr):\n        super().__init__()\n        self._op = op\n        self._expr = expr\n\n    def is_reduce_expr(self, ee):\n        return True\n\n    def resolve(self):\n        self._expr.resolve()\n        expr_stype = self._expr.stype\n        self._stype = ops_rules.get((self._op, expr_stype))\n        if self._stype is None:\n            raise ValueError(\n                \"Cannot compute %s of a variable of type %s\"\n                % (self._op, expr_stype))\n\n    def evaluate_eager(self, ee):\n        col = self._expr.evaluate_eager(ee)\n        opcode = reduce_opcodes[self._op]\n        return core.expr_reduceop(opcode, col, ee.groupby)\n\n\n    def __str__(self):\n        return \"%s(%s)\" % (self._op, self._expr)\n\n","lang_cluster":"Python","length":57,"code_uid":"008a6e6bf35a48dc97e7558c6aa9efcd"}
{"diff_hunk":"@@ -66,6 +66,10 @@ def get_argparser():\n                         \"session even if one would be restored.\",\n                         action='store_true')\n     parser.add_argument('--json-args', help=argparse.SUPPRESS)\n+    parser.add_argument('--target', choices=['auto', 'tab', 'tab-bg',\n+                        'tab-silent', 'tab-bg-silent', 'window'],\n+                        help=\"How the urls should be opened if there is \"\n+                        \"already a qutebrowser instance running.\")\n \n     debug = parser.add_argument_group('debug arguments')\n     debug.add_argument('-l', '--loglevel', dest='loglevel',","old_code":"# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2014-2015 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\n\"\"\"Early initialization and main entry point.\"\"\"\n\nimport sys\nimport json\n\nimport qutebrowser\ntry:\n    from qutebrowser.misc.checkpyver import check_python_version\nexcept ImportError:\n    try:\n        # python2\n        from .misc.checkpyver import check_python_version\n    except (SystemError, ValueError):\n        # Import without module - SystemError on Python3, ValueError (?!?) on\n        # Python2\n        sys.stderr.write(\"Please don't run this script directly, do something \"\n                         \"like   python3 -m qutebrowser   instead.\\n\")\n        sys.stderr.flush()\n        sys.exit(100)\ncheck_python_version()\n\nimport argparse\nfrom qutebrowser.misc import earlyinit\n\n\ndef get_argparser():\n    \"\"\"Get the argparse parser.\"\"\"\n    parser = argparse.ArgumentParser(\"usage: qutebrowser\",\n                                     description=qutebrowser.__description__)\n    parser.add_argument('-c', '--confdir', help=\"Set config directory (empty \"\n                        \"for no config storage).\")\n    parser.add_argument('--datadir', help=\"Set data directory (empty for \"\n                        \"no data storage).\")\n    parser.add_argument('--cachedir', help=\"Set cache directory (empty for \"\n                        \"no cache storage).\")\n    parser.add_argument('--basedir', help=\"Base directory for all storage. \"\n                        \"Other --*dir arguments are ignored if this is given.\")\n    parser.add_argument('-V', '--version', help=\"Show version and quit.\",\n                        action='store_true')\n    parser.add_argument('-s', '--set', help=\"Set a temporary setting for \"\n                        \"this session.\", nargs=3, action='append',\n                        dest='temp_settings', default=[],\n                        metavar=('SECTION', 'OPTION', 'VALUE'))\n    parser.add_argument('-r', '--restore', help=\"Restore a named session.\",\n                        dest='session')\n    parser.add_argument('-R', '--override-restore', help=\"Don't restore a \"\n                        \"session even if one would be restored.\",\n                        action='store_true')\n    parser.add_argument('--json-args', help=argparse.SUPPRESS)\n\n    debug = parser.add_argument_group('debug arguments')\n    debug.add_argument('-l', '--loglevel', dest='loglevel',\n                       help=\"Set loglevel\", default='info')\n    debug.add_argument('--logfilter',\n                       help=\"Comma-separated list of things to be logged \"\n                       \"to the debug log on stdout.\")\n    debug.add_argument('--loglines',\n                       help=\"How many lines of the debug log to keep in RAM \"\n                       \"(-1: unlimited).\",\n                       default=2000, type=int)\n    debug.add_argument('--debug', help=\"Turn on debugging options.\",\n                       action='store_true')\n    debug.add_argument('--nocolor', help=\"Turn off colored logging.\",\n                       action='store_false', dest='color')\n    debug.add_argument('--harfbuzz', choices=['old', 'new', 'system', 'auto'],\n                       default='auto', help=\"HarfBuzz engine version to use. \"\n                       \"Default: auto.\")\n    debug.add_argument('--relaxed-config', action='store_true',\n                       help=\"Silently remove unknown config options.\")\n    debug.add_argument('--nowindow', action='store_true', help=\"Don't show \"\n                       \"the main window.\")\n    debug.add_argument('--debug-exit', help=\"Turn on debugging of late exit.\",\n                       action='store_true')\n    debug.add_argument('--pdb-postmortem', action='store_true',\n                       help=\"Drop into pdb on exceptions.\")\n    debug.add_argument('--temp-basedir', action='store_true', help=\"Use a \"\n                       \"temporary basedir.\")\n    debug.add_argument('--no-err-windows', action='store_true', help=\"Don't \"\n                       \"show any error windows (used for tests\/smoke.py).\")\n    # For the Qt args, we use store_const with const=True rather than\n    # store_true because we want the default to be None, to make\n    # utils.qt:get_args easier.\n    debug.add_argument('--qt-name', help=\"Set the window name.\",\n                       metavar='NAME')\n    debug.add_argument('--qt-style', help=\"Set the Qt GUI style to use.\",\n                       metavar='STYLE')\n    debug.add_argument('--qt-stylesheet', help=\"Override the Qt application \"\n                       \"stylesheet.\", metavar='STYLESHEET')\n    debug.add_argument('--qt-widgetcount', help=\"Print debug message at the \"\n                       \"end about number of widgets left undestroyed and \"\n                       \"maximum number of widgets existed at the same time.\",\n                       action='store_const', const=True)\n    debug.add_argument('--qt-reverse', help=\"Set the application's layout \"\n                       \"direction to right-to-left.\", action='store_const',\n                       const=True)\n    debug.add_argument('--qt-qmljsdebugger', help=\"Activate the QML\/JS \"\n                       \"debugger with a specified port. 'block' is optional \"\n                       \"and will make the application wait until a debugger \"\n                       \"connects to it.\", metavar='port:PORT[,block]')\n    parser.add_argument('command', nargs='*', help=\"Commands to execute on \"\n                        \"startup.\", metavar=':command')\n    # URLs will actually be in command\n    parser.add_argument('url', nargs='*', help=\"URLs to open on startup \"\n                        \"(empty as a window separator).\")\n    return parser\n\n\ndef main():\n    \"\"\"Main entry point for qutebrowser.\"\"\"\n    parser = get_argparser()\n    if sys.platform == 'darwin' and getattr(sys, 'frozen', False):\n        # Ignore Mac OS X' idiotic -psn_* argument...\n        # http:\/\/stackoverflow.com\/questions\/19661298\/\n        # http:\/\/sourceforge.net\/p\/cx-freeze\/mailman\/message\/31041783\/\n        argv = [arg for arg in sys.argv[1:] if not arg.startswith('-psn_0_')]\n    else:\n        argv = sys.argv[1:]\n    args = parser.parse_args(argv)\n    if args.json_args is not None:\n        # Restoring after a restart.\n        # When restarting, we serialize the argparse namespace into json, and\n        # construct a \"fake\" argparse.Namespace here based on the data loaded\n        # from json.\n        data = json.loads(args.json_args)\n        args = argparse.Namespace(**data)\n    earlyinit.earlyinit(args)\n    # We do this imports late as earlyinit needs to be run first (because of\n    # the harfbuzz fix and version checking).\n    from qutebrowser import app\n    return app.run(args)\n","lang_cluster":"Python","length":149,"code_uid":"d36098952bba4b3fb94184db05c8cd26"}
{"diff_hunk":"@@ -1,8 +1,8 @@\n-#vkCodes.py\n-#A part of NonVisual Desktop Access (NVDA)\n-#This file is covered by the GNU General Public License.\n-#See the file COPYING for more details.\n-#Copyright (C) 2007-2010 Michael Curran <mick@kulgan.net>, James Teh <jamie@jantrid.net>\n+# vkCodes.py\n+# A part of NonVisual Desktop Access (NVDA)\n+# This file is covered by the GNU General Public License.\n+# See the file COPYING for more details.\n+# Copyright (C) 2007-2021 Michael Curran <mick@kulgan.net>, James Teh <jamie@jantrid.net>, Quin Marilyn.\n \n \"\"\"Maps between Windows virtual key (vk) codes and NVDA key names.\n These names are used when binding keyboard gestures to scripts.","old_code":"#vkCodes.py\r\n#A part of NonVisual Desktop Access (NVDA)\r\n#This file is covered by the GNU General Public License.\r\n#See the file COPYING for more details.\r\n#Copyright (C) 2007-2010 Michael Curran <mick@kulgan.net>, James Teh <jamie@jantrid.net>\r\n\r\n\"\"\"Maps between Windows virtual key (vk) codes and NVDA key names.\r\nThese names are used when binding keyboard gestures to scripts.\r\n\"\"\"\r\n\r\n#: Maps vk codes to key names.\r\n#: The dict key is a tuple of (vkCode, extended),\r\n#: where vkCode is the vk code and extended is a bool specifying whether the key is an extended key.\r\n#: If extended is C{None}, the extended state of the key is irrelevant to the mapping;\r\n#: i.e. the name is the same in either case.\r\n#: The dict value is the key name.\r\n#: @type: dict with keys of tuple(int, bool) and values of str\r\nbyCode = {\r\n\t(0x01, None): \"leftMouse\",\r\n\t(0x02, None): \"rightMouse\",\r\n\t(0x03, None): \"break\",\r\n\t(0x04, None): \"middleMouse\",\r\n\t(0x08, None): \"backspace\",\r\n\t(0x09, None): \"tab\",\r\n\t(0x0C, None): \"numpad5\",\r\n\t(0x0D, False): \"enter\",\r\n\t(0x0D, True): \"numpadEnter\",\r\n\t(0x10, None): \"shift\",\r\n\t(0x11, None): \"control\",\r\n\t(0x12, None): \"alt\",\r\n\t(0x13, None): \"pause\",\r\n\t(0x14, None): \"capsLock\",\r\n\t(0x18, None): \"IMEFinalMode\",\r\n\t(0x1B, None): \"escape\",\r\n\t(0x1C, None): \"IMEConvert\",\r\n\t(0x1D, None): \"IMENonconvert\",\r\n\t(0x1E, None): \"IMEAccept\",\r\n\t(0x1F, None): \"IMEModeChange\",\r\n\t(0x20, None): \"space\",\r\n\t(0x21, True): \"pageUp\",\r\n\t(0x21, False): \"numpad9\",\r\n\t(0x22, True): \"pageDown\",\r\n\t(0x22, False): \"numpad3\",\r\n\t(0x23, True): \"end\",\r\n\t(0x23, False): \"numpad1\",\r\n\t(0x24, True): \"home\",\r\n\t(0x24, False): \"numpad7\",\r\n\t(0x25, True): \"leftArrow\",\r\n\t(0x25, False): \"numpad4\",\r\n\t(0x26, True): \"upArrow\",\r\n\t(0x26, False): \"numpad8\",\r\n\t(0x27, True): \"rightArrow\",\r\n\t(0x27, False): \"numpad6\",\r\n\t(0x28, True): \"downArrow\",\r\n\t(0x28, False): \"numpad2\",\r\n\t(0x29, None): \"select\",\r\n\t(0x2A, None): \"print\",\r\n\t(0x2B, None): \"execute\",\r\n\t(0x2C, None): \"printScreen\",\r\n\t(0x2D, True): \"insert\",\r\n\t(0x2D, False): \"numpadInsert\",\r\n\t(0x2E, True): \"delete\",\r\n\t(0x2E, False): \"numpadDelete\",\r\n\t(0x2F, None): \"help\",\r\n\t(0x5B, None): \"leftWindows\",\r\n\t(0x5C, None): \"rightWindows\",\r\n\t(0x5D, None): \"applications\",\r\n\t(0x5F, None): \"sleep\",\r\n\t(0x60, None): \"numLockNumpad0\",\r\n\t(0x61, None): \"numLockNumpad1\",\r\n\t(0x62, None): \"numLockNumpad2\",\r\n\t(0x63, None): \"numLockNumpad3\",\r\n\t(0x64, None): \"numLockNumpad4\",\r\n\t(0x65, None): \"numLockNumpad5\",\r\n\t(0x66, None): \"numLockNumpad6\",\r\n\t(0x67, None): \"numLockNumpad7\",\r\n\t(0x68, None): \"numLockNumpad8\",\r\n\t(0x69, None): \"numLockNumpad9\",\r\n\t(0x6A, None): \"numpadMultiply\",\r\n\t(0x6B, None): \"numpadPlus\",\r\n\t(0x6C, None): \"numpadSeparator\",\r\n\t(0x6D, None): \"numpadMinus\",\r\n\t(0x6E, None): \"numpadDecimal\",\r\n\t(0x6F, None): \"numpadDivide\",\r\n\t(0x70, None): \"f1\",\r\n\t(0x71, None): \"f2\",\r\n\t(0x72, None): \"f3\",\r\n\t(0x73, None): \"f4\",\r\n\t(0x74, None): \"f5\",\r\n\t(0x75, None): \"f6\",\r\n\t(0x76, None): \"f7\",\r\n\t(0x77, None): \"f8\",\r\n\t(0x78, None): \"f9\",\r\n\t(0x79, None): \"f10\",\r\n\t(0x7A, None): \"f11\",\r\n\t(0x7B, None): \"f12\",\r\n\t(0x7C, None): \"f13\",\r\n\t(0x7D, None): \"f14\",\r\n\t(0x7E, None): \"f15\",\r\n\t(0x7F, None): \"f16\",\r\n\t(0x80, None): \"f17\",\r\n\t(0x81, None): \"f18\",\r\n\t(0x82, None): \"f19\",\r\n\t(0x83, None): \"f20\",\r\n\t(0x84, None): \"f21\",\r\n\t(0x85, None): \"f22\",\r\n\t(0x86, None): \"f23\",\r\n\t(0x87, None): \"f24\",\r\n\t(0x90, None): \"numLock\",\r\n\t(0x91, None): \"scrollLock\",\r\n\t(0xA0, None): \"leftShift\",\r\n\t(0xA1, None): \"rightShift\",\r\n\t(0xA2, None): \"leftControl\",\r\n\t(0xA3, None): \"rightControl\",\r\n\t(0xA4, None): \"leftAlt\",\r\n\t(0xA5, None): \"rightAlt\",\r\n\t(0xA6, None): \"browserBack\",\r\n\t(0xA7, None): \"browserForward\",\r\n\t(0xA8, None): \"browserRefresh\",\r\n\t(0xA9, None): \"browserStop\",\r\n\t(0xAA, None): \"browserSearch\",\r\n\t(0xAB, None): \"browserFavorites\",\r\n\t(0xAC, None): \"browserHome\",\r\n\t(0xAD, None): \"volumeMute\",\r\n\t(0xAE, None): \"volumeDown\",\r\n\t(0xAF, None): \"volumeUp\",\r\n\t(0xB0, None): \"mediaNextTrack\",\r\n\t(0xB1, None): \"mediaPrevTrack\",\r\n\t(0xB2, None): \"mediaStop\",\r\n\t(0xB3, None): \"mediaPlayPause\",\r\n\t(0xB4, None): \"launchMail\",\r\n\t(0xB5, None): \"launchMediaPlayer\",\r\n\t(0xB6, None): \"launchApp1\",\r\n\t(0xB7, None): \"launchApp2\",\r\n}\r\n\r\n#: Maps key names to vk codes.\r\n#: This is the inverse of the L{byCode} map\r\n#: except that names are all lower case to make case insensitive lookup easier.\r\n#: @type: dict with keys of str and values of tuple(int, bool)\r\nbyName = dict((name.lower(), code) for code, name in byCode.items())\r\n\r\n# Used by SendInput for non-keyboard input to pass Unicode characters as if they were keystrokes.\r\n# The scan code is the Unicode character.\r\n# See https:\/\/msdn.microsoft.com\/en-us\/library\/windows\/desktop\/ms646271(v=vs.85).aspx\r\nVK_PACKET = 0xE7\r\n","lang_cluster":"Python","length":146,"code_uid":"fd1885d5b6ea460ba5789b824b745416"}
{"diff_hunk":"@@ -127,3 +127,34 @@ def unescape(s):\n \n   return s\n \n+\n+#############################################################################\n+def parseSdr(s):\n+  \"\"\"Parses a string containing only 0's and 1's and return a Python list object.\n+  \"\"\"\n+  assert isinstance(s, basestring)\n+  try:\n+    sdr = []\n+    for c in s:\n+      if c == \"0\":\n+        sdr.append(0)\n+      elif c == \"1\":\n+        sdr.append(1)\n+    return sdr\n+  except ValueError:\n+    pass\n+  raise ValueError(\"The provided string %s is malformed. The string should \"\n+                   \"have only 0's and 1's.\")\n+\n+\n+#############################################################################\n+def serializeSdr(sdr):\n+  \"\"\"Serialize Python list object containing only 0's and 1's to string.\n+  \"\"\"\n+  s = \"\"\n+  for elem in sdr:\n+    if elem == 0:\n+      s += \"0\"\n+    elif elem == 1:\n+      s += \"1\"\n+  return s","old_code":"# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see http:\/\/www.gnu.org\/licenses.\n#\n# http:\/\/numenta.org\/licenses\/\n# ----------------------------------------------------------------------\n\n\"\"\"\nCollection of utilities to process input data\n\"\"\"\n\nimport datetime\n# Workaround for this error: \n#  \"ImportError: Failed to import _strptime because the import lockis held by \n#     another thread\"\n\n# These are the supported timestamp formats to parse. The first is used for\n# serializing datetimes. Functions in this file rely on specific formats from\n# this tuple so be careful when changing the indices for existing formats.\nDATETIME_FORMATS = ('%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M:%S:%f',\n                    '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%Y-%m-%d',\n                    '%m\/%d\/%Y %H:%M', '%m\/%d\/%y %H:%M',\n                    '%Y-%m-%dT%H:%M:%S.%fZ', '%Y-%m-%dT%H:%M:%SZ',\n                    '%Y-%m-%dT%H:%M:%S')\n\n\n#############################################################################\ndef parseTimestamp(s):\n  \"\"\"Parses a textual datetime format and return a Python datetime object.\n\n  The supported format is: yyyy-mm-dd h:m:s.ms\n\n  The time component is optional\n  hours are 00..23 (no AM\/PM)\n  minutes are 00..59\n  seconds are 00..59\n  micro-seconds are 000000..999999\n  \"\"\"\n  s = s.strip()\n  for pattern in DATETIME_FORMATS:\n    try:\n      return datetime.datetime.strptime(s, pattern)\n    except ValueError:\n      pass\n  raise ValueError('The provided timestamp %s is malformed. The supported '\n                   'formats are: [%s]' % (s, ', '.join(DATETIME_FORMATS)))\n\n\n#############################################################################\ndef serializeTimestamp(t):\n  return t.strftime(DATETIME_FORMATS[0])\n\n\n#############################################################################\ndef serializeTimestampNoMS(t):\n  return t.strftime(DATETIME_FORMATS[2])\n\n\n#############################################################################\ndef parseBool(s):\n  l = s.lower()\n  if l in (\"true\", \"t\", \"1\"):\n    return True\n  if l in (\"false\", \"f\", \"0\"):\n    return False\n  raise Exception(\"Unable to convert string '%s' to a boolean value\" % s)\n\n\n#############################################################################\ndef floatOrNone(f):\n  if f == 'None':\n    return None\n  return float(f)\n\n\n#############################################################################\ndef intOrNone(i):\n  if i.strip() == 'None' or i.strip() == 'NULL':\n    return None\n  return int(i)\n\n\n#############################################################################\ndef escape(s):\n  \"\"\"Escape commas, tabs, newlines and dashes in a string\n\n  Commas are encoded as tabs\n  \"\"\"\n  if s is None:\n    return ''\n  \n  assert isinstance(s, basestring), \\\n        \"expected %s but got %s; value=%s\" % (basestring, type(s), s)\n  s = s.replace('\\\\', '\\\\\\\\')\n  s = s.replace('\\n', '\\\\n')\n  s = s.replace('\\t', '\\\\t')\n  s = s.replace(',', '\\t')\n  return s\n\n\n#############################################################################\ndef unescape(s):\n  \"\"\"Unescapes a string that may contain commas, tabs, newlines and dashes\n\n  Commas are decoded from tabs\n  \"\"\"\n  #assert isinstance(s, str)\n  assert isinstance(s, basestring)\n  s = s.replace('\\t', ',')\n  s = s.replace('\\\\,', ',')\n  s = s.replace('\\\\n', '\\n')\n  s = s.replace('\\\\\\\\', '\\\\')\n\n  return s\n\n","lang_cluster":"Python","length":129,"code_uid":"97ab80f5fe654853bc043f976f1f47d1"}
{"diff_hunk":"@@ -35,9 +35,11 @@ class ScalarSpaceEncoderTest(unittest.TestCase):\n   def testScalarSpaceEncoder(self):\n     \"\"\"scalar space encoder\"\"\"\n     # use of forced=True is not recommended, but used in the example for readibility, see scalar.py\n-    sse = ScalarSpaceEncoder(1,1,2,False,2,1,1,None,0,False,\"delta\", forced=True)\n+    sse = ScalarSpaceEncoder(w=21,minval=1,maxval=2,n=100,radius=1,\n+            resolution=1,name=\"SP1\",verbosity=0,clipInput=False,space=\"delta\")\n     self.assertTrue(sse.isDelta())\n-    sse = ScalarSpaceEncoder(1,1,2,False,2,1,1,None,0,False,\"absolute\", forced=True)\n+    sse = ScalarSpaceEncoder(w=21,minval=1,maxval=2,n=100,radius=1,\n+            resolution=1,name=\"sp2\",verbosity=0,clipInput=False,space=\"absolute\")\n     self.assertFalse(sse.isDelta())\n \n      ","old_code":"#!\/usr\/bin\/env python\n# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see http:\/\/www.gnu.org\/licenses.\n#\n# http:\/\/numenta.org\/licenses\/\n# ----------------------------------------------------------------------\n\n\"\"\"Unit tests for scalar space encoder\"\"\"\n\nimport unittest2 as unittest\n\nfrom nupic.encoders.scalarspace import ScalarSpaceEncoder\n\n\n#########################################################################\nclass ScalarSpaceEncoderTest(unittest.TestCase):\n  '''Unit tests for ScalarSpaceEncoder class'''\n\n\n  def testScalarSpaceEncoder(self):\n    \"\"\"scalar space encoder\"\"\"\n    # use of forced=True is not recommended, but used in the example for readibility, see scalar.py\n    sse = ScalarSpaceEncoder(1,1,2,False,2,1,1,None,0,False,\"delta\", forced=True)\n    self.assertTrue(sse.isDelta())\n    sse = ScalarSpaceEncoder(1,1,2,False,2,1,1,None,0,False,\"absolute\", forced=True)\n    self.assertFalse(sse.isDelta())\n\n     \n###########################################\nif __name__ == '__main__':\n  unittest.main()\n","lang_cluster":"Python","length":46,"code_uid":"99450607a58d4507939600ed18fc74f2"}
{"diff_hunk":"@@ -13,7 +13,8 @@ def anchor_target(anchor_list,\n                   cfg,\n                   gt_labels_list=None,\n                   cls_out_channels=1,\n-                  sampling=True):\n+                  sampling=True,\n+                  need_unmap=True):\n     \"\"\"Compute regression and classification targets for anchors.\n \n     Args:","old_code":"import torch\n\nfrom ..bbox import assign_and_sample, BBoxAssigner, SamplingResult, bbox2delta\nfrom ..utils import multi_apply\n\n\ndef anchor_target(anchor_list,\n                  valid_flag_list,\n                  gt_bboxes_list,\n                  img_metas,\n                  target_means,\n                  target_stds,\n                  cfg,\n                  gt_labels_list=None,\n                  cls_out_channels=1,\n                  sampling=True):\n    \"\"\"Compute regression and classification targets for anchors.\n\n    Args:\n        anchor_list (list[list]): Multi level anchors of each image.\n        valid_flag_list (list[list]): Multi level valid flags of each image.\n        gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image.\n        img_metas (list[dict]): Meta info of each image.\n        target_means (Iterable): Mean value of regression targets.\n        target_stds (Iterable): Std value of regression targets.\n        cfg (dict): RPN train configs.\n\n    Returns:\n        tuple\n    \"\"\"\n    num_imgs = len(img_metas)\n    assert len(anchor_list) == len(valid_flag_list) == num_imgs\n\n    # anchor number of multi levels\n    num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n    # concat all level anchors and flags to a single tensor\n    for i in range(num_imgs):\n        assert len(anchor_list[i]) == len(valid_flag_list[i])\n        anchor_list[i] = torch.cat(anchor_list[i])\n        valid_flag_list[i] = torch.cat(valid_flag_list[i])\n\n    # compute targets for each image\n    if gt_labels_list is None:\n        gt_labels_list = [None for _ in range(num_imgs)]\n    (all_labels, all_label_weights, all_bbox_targets, all_bbox_weights,\n     pos_inds_list, neg_inds_list) = multi_apply(\n         anchor_target_single,\n         anchor_list,\n         valid_flag_list,\n         gt_bboxes_list,\n         gt_labels_list,\n         img_metas,\n         target_means=target_means,\n         target_stds=target_stds,\n         cfg=cfg,\n         cls_out_channels=cls_out_channels,\n         sampling=sampling)\n    # no valid anchors\n    if any([labels is None for labels in all_labels]):\n        return None\n    # sampled anchors of all images\n    num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n    num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n    # split targets to a list w.r.t. multiple levels\n    labels_list = images_to_levels(all_labels, num_level_anchors)\n    label_weights_list = images_to_levels(all_label_weights, num_level_anchors)\n    bbox_targets_list = images_to_levels(all_bbox_targets, num_level_anchors)\n    bbox_weights_list = images_to_levels(all_bbox_weights, num_level_anchors)\n    return (labels_list, label_weights_list, bbox_targets_list,\n            bbox_weights_list, num_total_pos, num_total_neg)\n\n\ndef images_to_levels(target, num_level_anchors):\n    \"\"\"Convert targets by image to targets by feature level.\n\n    [target_img0, target_img1] -> [target_level0, target_level1, ...]\n    \"\"\"\n    target = torch.stack(target, 0)\n    level_targets = []\n    start = 0\n    for n in num_level_anchors:\n        end = start + n\n        level_targets.append(target[:, start:end].squeeze(0))\n        start = end\n    return level_targets\n\n\ndef anchor_target_single(flat_anchors,\n                         valid_flags,\n                         gt_bboxes,\n                         gt_labels,\n                         img_meta,\n                         target_means,\n                         target_stds,\n                         cfg,\n                         cls_out_channels=1,\n                         sampling=True):\n    inside_flags = anchor_inside_flags(flat_anchors, valid_flags,\n                                       img_meta['img_shape'][:2],\n                                       cfg.allowed_border)\n    if not inside_flags.any():\n        return (None, ) * 6\n    # assign gt and sample anchors\n    anchors = flat_anchors[inside_flags, :]\n\n    if sampling:\n        assign_result, sampling_result = assign_and_sample(\n            anchors, gt_bboxes, None, None, cfg)\n    else:\n        bbox_assigner = BBoxAssigner(**cfg.assigner)\n        assign_result = bbox_assigner.assign(anchors, gt_bboxes, None,\n                                             gt_labels)\n        pos_inds = torch.nonzero(\n            assign_result.gt_inds > 0).squeeze(-1).unique()\n        neg_inds = torch.nonzero(\n            assign_result.gt_inds == 0).squeeze(-1).unique()\n        gt_flags = anchors.new_zeros(anchors.shape[0], dtype=torch.uint8)\n        sampling_result = SamplingResult(pos_inds, neg_inds, anchors,\n                                         gt_bboxes, assign_result, gt_flags)\n\n    num_valid_anchors = anchors.shape[0]\n    bbox_targets = torch.zeros_like(anchors)\n    bbox_weights = torch.zeros_like(anchors)\n    labels = anchors.new_zeros(num_valid_anchors, dtype=torch.long)\n    label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)\n\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    if len(pos_inds) > 0:\n        pos_bbox_targets = bbox2delta(sampling_result.pos_bboxes,\n                                      sampling_result.pos_gt_bboxes,\n                                      target_means, target_stds)\n        bbox_targets[pos_inds, :] = pos_bbox_targets\n        bbox_weights[pos_inds, :] = 1.0\n        if gt_labels is None:\n            labels[pos_inds] = 1\n        else:\n            labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n        if cfg.pos_weight <= 0:\n            label_weights[pos_inds] = 1.0\n        else:\n            label_weights[pos_inds] = cfg.pos_weight\n    if len(neg_inds) > 0:\n        label_weights[neg_inds] = 1.0\n\n    # map up to original set of anchors\n    num_total_anchors = flat_anchors.size(0)\n    labels = unmap(labels, num_total_anchors, inside_flags)\n    label_weights = unmap(label_weights, num_total_anchors, inside_flags)\n    if cls_out_channels > 1:\n        labels, label_weights = expand_binary_labels(labels, label_weights,\n                                                     cls_out_channels)\n    bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)\n    bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n\n    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds,\n            neg_inds)\n\n\ndef expand_binary_labels(labels, label_weights, cls_out_channels):\n    bin_labels = labels.new_full(\n        (labels.size(0), cls_out_channels), 0, dtype=torch.float32)\n    inds = torch.nonzero(labels >= 1).squeeze()\n    if inds.numel() > 0:\n        bin_labels[inds, labels[inds] - 1] = 1\n    bin_label_weights = label_weights.view(-1, 1).expand(\n        label_weights.size(0), cls_out_channels)\n    return bin_labels, bin_label_weights\n\n\ndef anchor_inside_flags(flat_anchors, valid_flags, img_shape,\n                        allowed_border=0):\n    img_h, img_w = img_shape[:2]\n    if allowed_border >= 0:\n        inside_flags = valid_flags & \\\n            (flat_anchors[:, 0] >= -allowed_border) & \\\n            (flat_anchors[:, 1] >= -allowed_border) & \\\n            (flat_anchors[:, 2] < img_w + allowed_border) & \\\n            (flat_anchors[:, 3] < img_h + allowed_border)\n    else:\n        inside_flags = valid_flags\n    return inside_flags\n\n\ndef unmap(data, count, inds, fill=0):\n    \"\"\" Unmap a subset of item (data) back to the original set of items (of\n    size count) \"\"\"\n    if data.dim() == 1:\n        ret = data.new_full((count, ), fill)\n        ret[inds] = data\n    else:\n        new_size = (count, ) + data.size()[1:]\n        ret = data.new_full(new_size, fill)\n        ret[inds, :] = data\n    return ret\n","lang_cluster":"Python","length":195,"code_uid":"ad2fcfdf679b4545a862090a8e953393"}
{"diff_hunk":"@@ -17,15 +17,10 @@\n # You should have received a copy of the GNU General Public License\n # along with qutebrowser.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n \n+# pylint: disable=unused-import\n import pytest\n import pytest_bdd as bdd\n \n-# pylint: disable=unused-import\n from end2end.features.test_yankpaste_bdd import init_fake_clipboard\n \n-\n-pytestmark = pytest.mark.qtwebengine_todo(\"Caret mode is not implemented\",\n-                                          run=False)\n-\n-\n bdd.scenarios('caret.feature')","old_code":"# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2015-2017 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n\nimport pytest\nimport pytest_bdd as bdd\n\n# pylint: disable=unused-import\nfrom end2end.features.test_yankpaste_bdd import init_fake_clipboard\n\n\npytestmark = pytest.mark.qtwebengine_todo(\"Caret mode is not implemented\",\n                                          run=False)\n\n\nbdd.scenarios('caret.feature')\n","lang_cluster":"Python","length":31,"code_uid":"5d22e96ac4a44c758fabaa57cfe9ed38"}
{"diff_hunk":"@@ -1,25 +1,35 @@\n import os\n-from datetime import datetime\n+from datetime import datetime, timedelta\n \n from dagster import Partition\n from dagster.core.definitions import PipelineDefinition\n-from dagster.core.definitions.partition import PartitionScheduleDefinition\n from dagster.core.execution.api import create_execution_plan\n-from hacker_news.pipelines.download_pipeline import download_pipeline\n-from hacker_news.schedules.hourly_hn_download_schedule import hourly_hn_download_schedule\n+from hacker_news.jobs.download_job import download_prod_job, download_staging_job\n \n \n def assert_partitioned_schedule_builds(\n-    schedule_def: PartitionScheduleDefinition,\n-    pipeline_def: PipelineDefinition,\n-    partition: datetime,\n+    job_def: PipelineDefinition,\n+    start: datetime,\n+    end: datetime,\n ):\n-    run_config = schedule_def.get_partition_set().run_config_for_partition(Partition(partition))\n-    create_execution_plan(pipeline_def, run_config=run_config, mode=schedule_def.mode)\n+    mode = job_def.mode_definitions[0]\n+    partition_set = mode.get_partition_set_def(job_def.name)\n+    run_config = partition_set.run_config_for_partition(Partition((start, end)))\n+    create_execution_plan(job_def, run_config=run_config)\n \n \n def test_daily_download_schedule():\n     os.environ[\"SLACK_DAGSTER_ETL_BOT_TOKEN\"] = \"something\"\n+    start = datetime.strptime(\"2020-10-01\", \"%Y-%m-%d\")\n+    end = start + timedelta(hours=1)\n+\n+    assert_partitioned_schedule_builds(\n+        download_prod_job,\n+        start,\n+        end,\n+    )\n     assert_partitioned_schedule_builds(\n-        hourly_hn_download_schedule, download_pipeline, datetime.strptime(\"2020-10-01\", \"%Y-%m-%d\")\n+        download_staging_job,\n+        start,\n+        end,\n     )","old_code":"import os\nfrom datetime import datetime\n\nfrom dagster import Partition\nfrom dagster.core.definitions import PipelineDefinition\nfrom dagster.core.definitions.partition import PartitionScheduleDefinition\nfrom dagster.core.execution.api import create_execution_plan\nfrom hacker_news.pipelines.download_pipeline import download_pipeline\nfrom hacker_news.schedules.hourly_hn_download_schedule import hourly_hn_download_schedule\n\n\ndef assert_partitioned_schedule_builds(\n    schedule_def: PartitionScheduleDefinition,\n    pipeline_def: PipelineDefinition,\n    partition: datetime,\n):\n    run_config = schedule_def.get_partition_set().run_config_for_partition(Partition(partition))\n    create_execution_plan(pipeline_def, run_config=run_config, mode=schedule_def.mode)\n\n\ndef test_daily_download_schedule():\n    os.environ[\"SLACK_DAGSTER_ETL_BOT_TOKEN\"] = \"something\"\n    assert_partitioned_schedule_builds(\n        hourly_hn_download_schedule, download_pipeline, datetime.strptime(\"2020-10-01\", \"%Y-%m-%d\")\n    )\n","lang_cluster":"Python","length":25,"code_uid":"5f2cf11bd0d74c1cabba9f4d7080e35d"}
{"diff_hunk":"@@ -18,11 +18,13 @@ from .retina_head import RetinaHead\n from .retina_sepbn_head import RetinaSepBNHead\n from .rpn_head import RPNHead\n from .ssd_head import SSDHead\n+from .yolact_head import YolactHead, YolactProtonet, YolactSegmHead\n \n __all__ = [\n     'AnchorFreeHead', 'AnchorHead', 'GuidedAnchorHead', 'FeatureAdaption',\n     'RPNHead', 'GARPNHead', 'RetinaHead', 'RetinaSepBNHead', 'GARetinaHead',\n     'SSDHead', 'FCOSHead', 'RepPointsHead', 'FoveaHead',\n     'FreeAnchorRetinaHead', 'ATSSHead', 'FSAFHead', 'NASFCOSHead',\n-    'PISARetinaHead', 'PISASSDHead', 'GFLHead', 'CornerHead'\n+    'PISARetinaHead', 'PISASSDHead', 'GFLHead', 'CornerHead', 'YolactHead',\n+    'YolactSegmHead', 'YolactProtonet'\n ]","old_code":"from .anchor_free_head import AnchorFreeHead\nfrom .anchor_head import AnchorHead\nfrom .atss_head import ATSSHead\nfrom .corner_head import CornerHead\nfrom .fcos_head import FCOSHead\nfrom .fovea_head import FoveaHead\nfrom .free_anchor_retina_head import FreeAnchorRetinaHead\nfrom .fsaf_head import FSAFHead\nfrom .ga_retina_head import GARetinaHead\nfrom .ga_rpn_head import GARPNHead\nfrom .gfl_head import GFLHead\nfrom .guided_anchor_head import FeatureAdaption, GuidedAnchorHead\nfrom .nasfcos_head import NASFCOSHead\nfrom .pisa_retinanet_head import PISARetinaHead\nfrom .pisa_ssd_head import PISASSDHead\nfrom .reppoints_head import RepPointsHead\nfrom .retina_head import RetinaHead\nfrom .retina_sepbn_head import RetinaSepBNHead\nfrom .rpn_head import RPNHead\nfrom .ssd_head import SSDHead\n\n__all__ = [\n    'AnchorFreeHead', 'AnchorHead', 'GuidedAnchorHead', 'FeatureAdaption',\n    'RPNHead', 'GARPNHead', 'RetinaHead', 'RetinaSepBNHead', 'GARetinaHead',\n    'SSDHead', 'FCOSHead', 'RepPointsHead', 'FoveaHead',\n    'FreeAnchorRetinaHead', 'ATSSHead', 'FSAFHead', 'NASFCOSHead',\n    'PISARetinaHead', 'PISASSDHead', 'GFLHead', 'CornerHead'\n]\n","lang_cluster":"Python","length":28,"code_uid":"aa73328766b546f8a06419181260abfd"}
{"diff_hunk":"@@ -37,16 +37,18 @@ setup(\n     keywords='quilt quiltdata shareable data dataframe package platform pandas',\n     install_requires=[\n         'appdirs>=1.4.0',\n-        'future>=0.16.0',\n+        'funcsigs; python_version<\"3.4\"',  # stdlib backport, test only\n+        'future>=0.16.0',   # unused\n         'packaging>=16.8',\n         'pandas>=0.19.2',\n+        'pathlib2; python_version<\"3.6\"',    # stdlib backport\n         'pyarrow>=0.4.0,<0.8.0', # TODO(dima): Make unit tests work with 0.8.*.\n         'pyOpenSSL>=16.2.0',  # Note: not actually used at the moment.\n         'pyyaml>=3.12',\n         'requests>=2.12.4',\n-        'responses>=0.7.0',\n+        'responses>=0.7.0',  # test only\n         'six>=1.10.0',\n-        'tables>=3.3.0',\n+        'tables>=3.3.0',  # unused\n         'tqdm>=4.11.2',\n         'xlrd>=1.0.0',\n     ],","old_code":"from setuptools import setup, find_packages\n\n\ndef readme():\n    readme_short = \"\"\"\n    ``quilt`` is a command-line utility that builds, pushes, and installs\n    data packages. A `data package <https:\/\/blog.quiltdata.com\/data-packages-for-fast-reproducible-python-analysis-c74b78015c7f>`_\n    is a versioned bundle of serialized data wrapped in a Python module.\n\n    ``quilt`` pushes to and pulls from the package registry at quiltdata.com.\n\n    Visit `quiltdata.com <https:\/\/quiltdata.com>`_ for docs and more.\n    \"\"\"\n    return readme_short\n\n\nsetup(\n    name=\"quilt\",\n    version=\"2.8.4-dev\",\n    packages=find_packages(),\n    description='Quilt is a data package manager',\n    long_description=readme(),\n    classifiers=[\n        'Development Status :: 5 - Production\/Stable',\n        'Intended Audience :: Developers',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n    ],\n    author='quiltdata',\n    author_email='contact@quiltdata.io',\n    license='LICENSE',\n    url='https:\/\/github.com\/quiltdata\/quilt',\n    download_url='https:\/\/github.com\/quiltdata\/quilt\/releases\/tag\/2.8.0',\n    keywords='quilt quiltdata shareable data dataframe package platform pandas',\n    install_requires=[\n        'appdirs>=1.4.0',\n        'future>=0.16.0',\n        'packaging>=16.8',\n        'pandas>=0.19.2',\n        'pyarrow>=0.4.0,<0.8.0', # TODO(dima): Make unit tests work with 0.8.*.\n        'pyOpenSSL>=16.2.0',  # Note: not actually used at the moment.\n        'pyyaml>=3.12',\n        'requests>=2.12.4',\n        'responses>=0.7.0',\n        'six>=1.10.0',\n        'tables>=3.3.0',\n        'tqdm>=4.11.2',\n        'xlrd>=1.0.0',\n    ],\n    include_package_data=True,\n    entry_points={\n        'console_scripts': ['quilt=quilt.tools.main:main'],\n    }\n)\n","lang_cluster":"Python","length":57,"code_uid":"94c4b0b463094ce3be5a4d821d99b74a"}
{"diff_hunk":"@@ -46,7 +46,7 @@ def open_browser(url: str) -> bool:\n     browsers = (\n         \"windows-default\", \"macosx\",\n         \"wslview %s\",\n-        \"x-www-browser %s\", \"gnome-open %s\",\n+        \"x-www-browser %s\", \"gnome-open %s\", \"xdg-open\",\n         \"google-chrome\", \"chrome\", \"chromium\", \"chromium-browser\",\n         \"firefox\", \"opera\", \"safari\",\n     )","old_code":"import webbrowser\n\nfrom mitmproxy import ctx\n\n\nclass WebAddon:\n    def load(self, loader):\n        loader.add_option(\n            \"web_open_browser\", bool, True,\n            \"Start a browser.\"\n        )\n        loader.add_option(\n            \"web_debug\", bool, False,\n            \"Enable mitmweb debugging.\"\n        )\n        loader.add_option(\n            \"web_port\", int, 8081,\n            \"Web UI port.\"\n        )\n        loader.add_option(\n            \"web_host\", str, \"127.0.0.1\",\n            \"Web UI host.\"\n        )\n\n    def running(self):\n        if hasattr(ctx.options, \"web_open_browser\") and ctx.options.web_open_browser:\n            web_url = f\"http:\/\/{ctx.options.web_host}:{ctx.options.web_port}\/\"\n            success = open_browser(web_url)\n            if not success:\n                ctx.log.info(\n                    f\"No web browser found. Please open a browser and point it to {web_url}\",\n                )\n\n\ndef open_browser(url: str) -> bool:\n    \"\"\"\n    Open a URL in a browser window.\n    In contrast to webbrowser.open, we limit the list of suitable browsers.\n    This gracefully degrades to a no-op on headless servers, where webbrowser.open\n    would otherwise open lynx.\n\n    Returns:\n        True, if a browser has been opened\n        False, if no suitable browser has been found.\n    \"\"\"\n    browsers = (\n        \"windows-default\", \"macosx\",\n        \"wslview %s\",\n        \"x-www-browser %s\", \"gnome-open %s\",\n        \"google-chrome\", \"chrome\", \"chromium\", \"chromium-browser\",\n        \"firefox\", \"opera\", \"safari\",\n    )\n    for browser in browsers:\n        try:\n            b = webbrowser.get(browser)\n        except webbrowser.Error:\n            pass\n        else:\n            if b.open(url):\n                return True\n    return False\n","lang_cluster":"Python","length":61,"code_uid":"98dbbdbac41a4d5b86b4c9f75c553db1"}
{"diff_hunk":"@@ -1,12 +1,40 @@\n # Licensed under the GPL: https:\/\/www.gnu.org\/licenses\/old-licenses\/gpl-2.0.html\n # For details: https:\/\/github.com\/PyCQA\/pylint\/blob\/master\/LICENSE\n \n+from typing import Optional, Union, cast\n+\n import astroid\n \n from pylint import checkers, interfaces\n from pylint.checkers import utils\n \n \n+def _check_if_dict_keys_used(\n+    node: Union[astroid.For, astroid.Comprehension]\n+) -> Optional[str]:\n+    if not isinstance(node.iter, astroid.Call):\n+        # Is it a dictionary?\n+        if not isinstance(node.iter, (astroid.Name, astroid.Attribute)):\n+            return None\n+        inferred = utils.safe_infer(node.iter)\n+        if not isinstance(inferred, (astroid.Dict, astroid.DictComp)):\n+            return None\n+        iterating_object_name = node.iter.as_string()\n+\n+    else:\n+        # Is it a proper keys call?\n+        if (\n+            isinstance(node.iter.func, astroid.Name)\n+            or node.iter.func.attrname != \"keys\"\n+        ):\n+            return None\n+        inferred = utils.safe_infer(node.iter.func)\n+        if not isinstance(inferred, (astroid.BoundMethod, astroid.Dict)):\n+            return None\n+        iterating_object_name = node.iter.as_string().partition(\".\")[0]\n+    return iterating_object_name\n+\n+\n class RecommendationChecker(checkers.BaseChecker):\n \n     __implements__ = (interfaces.IAstroidChecker,)","old_code":"# Licensed under the GPL: https:\/\/www.gnu.org\/licenses\/old-licenses\/gpl-2.0.html\n# For details: https:\/\/github.com\/PyCQA\/pylint\/blob\/master\/LICENSE\n\nimport astroid\n\nfrom pylint import checkers, interfaces\nfrom pylint.checkers import utils\n\n\nclass RecommendationChecker(checkers.BaseChecker):\n\n    __implements__ = (interfaces.IAstroidChecker,)\n    name = \"refactoring\"\n    msgs = {\n        \"C0200\": (\n            \"Consider using enumerate instead of iterating with range and len\",\n            \"consider-using-enumerate\",\n            \"Emitted when code that iterates with range and len is \"\n            \"encountered. Such code can be simplified by using the \"\n            \"enumerate builtin.\",\n        ),\n        \"C0201\": (\n            \"Consider iterating the dictionary directly instead of calling .keys()\",\n            \"consider-iterating-dictionary\",\n            \"Emitted when the keys of a dictionary are iterated through the .keys() \"\n            \"method. It is enough to just iterate through the dictionary itself, as \"\n            'in \"for key in dictionary\".',\n        ),\n    }\n\n    @staticmethod\n    def _is_builtin(node, function):\n        inferred = utils.safe_infer(node)\n        if not inferred:\n            return False\n        return utils.is_builtin_object(inferred) and inferred.name == function\n\n    @utils.check_messages(\"consider-iterating-dictionary\")\n    def visit_call(self, node):\n        if not isinstance(node.func, astroid.Attribute):\n            return\n        if node.func.attrname != \"keys\":\n            return\n        if not isinstance(node.parent, (astroid.For, astroid.Comprehension)):\n            return\n\n        inferred = utils.safe_infer(node.func)\n        if not isinstance(inferred, astroid.BoundMethod) or not isinstance(\n            inferred.bound, astroid.Dict\n        ):\n            return\n\n        if isinstance(node.parent, (astroid.For, astroid.Comprehension)):\n            self.add_message(\"consider-iterating-dictionary\", node=node)\n\n    @utils.check_messages(\"consider-using-enumerate\")\n    def visit_for(self, node):\n        \"\"\"Emit a convention whenever range and len are used for indexing.\"\"\"\n        # Verify that we have a `range([start], len(...), [stop])` call and\n        # that the object which is iterated is used as a subscript in the\n        # body of the for.\n\n        # Is it a proper range call?\n        if not isinstance(node.iter, astroid.Call):\n            return\n        if not self._is_builtin(node.iter.func, \"range\"):\n            return\n        if not node.iter.args:\n            return\n        is_constant_zero = (\n            isinstance(node.iter.args[0], astroid.Const)\n            and node.iter.args[0].value == 0\n        )\n        if len(node.iter.args) == 2 and not is_constant_zero:\n            return\n        if len(node.iter.args) > 2:\n            return\n\n        # Is it a proper len call?\n        if not isinstance(node.iter.args[-1], astroid.Call):\n            return\n        second_func = node.iter.args[-1].func\n        if not self._is_builtin(second_func, \"len\"):\n            return\n        len_args = node.iter.args[-1].args\n        if not len_args or len(len_args) != 1:\n            return\n        iterating_object = len_args[0]\n        if not isinstance(iterating_object, astroid.Name):\n            return\n        # If we're defining __iter__ on self, enumerate won't work\n        scope = node.scope()\n        if iterating_object.name == \"self\" and scope.name == \"__iter__\":\n            return\n\n        # Verify that the body of the for loop uses a subscript\n        # with the object that was iterated. This uses some heuristics\n        # in order to make sure that the same object is used in the\n        # for body.\n        for child in node.body:\n            for subscript in child.nodes_of_class(astroid.Subscript):\n                if not isinstance(subscript.value, astroid.Name):\n                    continue\n\n                value = subscript.slice\n                if isinstance(value, astroid.Index):\n                    value = value.value\n                if not isinstance(value, astroid.Name):\n                    continue\n                if value.name != node.target.name:\n                    continue\n                if iterating_object.name != subscript.value.name:\n                    continue\n                if subscript.value.scope() != node.scope():\n                    # Ignore this subscript if it's not in the same\n                    # scope. This means that in the body of the for\n                    # loop, another scope was created, where the same\n                    # name for the iterating object was used.\n                    continue\n                self.add_message(\"consider-using-enumerate\", node=node)\n                return\n","lang_cluster":"Python","length":121,"code_uid":"0947b125b7f143a0bb2871fc46fe8026"}
{"diff_hunk":"@@ -153,7 +153,7 @@ def qute_help(win_id, request):\n             url=request.url().toDisplayString(),\n             error=\"This most likely means the documentation was not generated \"\n                   \"properly. If you are running qutebrowser from the git \"\n-                  \"repository, please run scripts\/asciidoc2html.py.\"\n+                  \"repository, please run scripts\/asciidoc2html.py. \"\n                   \"If you're running a released version this is a bug, please \"\n                   \"use :report to report it.\",\n             icon='')","old_code":"# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2014-2015 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http:\/\/www.gnu.org\/licenses\/>.\n#\n# pylint complains when using .render() on jinja templates, so we make it shut\n# up for this whole module.\n\n# pylint: disable=no-member\n# https:\/\/bitbucket.org\/logilab\/pylint\/issue\/490\/\n\n\"\"\"Handler functions for different qute:... pages.\n\nModule attributes:\n    pyeval_output: The output of the last :pyeval command.\n\"\"\"\n\nimport functools\nimport configparser\n\nfrom PyQt5.QtCore import pyqtSlot, QObject\nfrom PyQt5.QtNetwork import QNetworkReply\n\nimport qutebrowser\nfrom qutebrowser.browser.network import schemehandler, networkreply\nfrom qutebrowser.utils import (version, utils, jinja, log, message, docutils,\n                               objreg)\nfrom qutebrowser.config import configexc, configdata\n\n\npyeval_output = \":pyeval was never called\"\n\n\nclass QuteSchemeHandler(schemehandler.SchemeHandler):\n\n    \"\"\"Scheme handler for qute: URLs.\"\"\"\n\n    def createRequest(self, _op, request, _outgoing_data):\n        \"\"\"Create a new request.\n\n        Args:\n             request: const QNetworkRequest & req\n             _op: Operation op\n             _outgoing_data: QIODevice * outgoingData\n\n        Return:\n            A QNetworkReply.\n        \"\"\"\n        path = request.url().path()\n        host = request.url().host()\n        # An url like \"qute:foo\" is split as \"scheme:path\", not \"scheme:host\".\n        log.misc.debug(\"url: {}, path: {}, host {}\".format(\n            request.url().toDisplayString(), path, host))\n        try:\n            handler = HANDLERS[path]\n        except KeyError:\n            try:\n                handler = HANDLERS[host]\n            except KeyError:\n                errorstr = \"No handler found for {}!\".format(\n                    request.url().toDisplayString())\n                return networkreply.ErrorNetworkReply(\n                    request, errorstr, QNetworkReply.ContentNotFoundError,\n                    self.parent())\n        try:\n            data = handler(self._win_id, request)\n        except OSError as e:\n            return networkreply.ErrorNetworkReply(\n                request, str(e), QNetworkReply.ContentNotFoundError,\n                self.parent())\n        return networkreply.FixedDataNetworkReply(\n            request, data, 'text\/html', self.parent())\n\n\nclass JSBridge(QObject):\n\n    \"\"\"Javascript-bridge for special qute:... pages.\"\"\"\n\n    def __init__(self, parent=None):\n        super().__init__(parent)\n\n    @pyqtSlot(int, str, str, str)\n    def set(self, win_id, sectname, optname, value):\n        \"\"\"Slot to set a setting from qute:settings.\"\"\"\n        try:\n            objreg.get('config').set('conf', sectname, optname, value)\n        except (configexc.Error, configparser.Error) as e:\n            message.error(win_id, e)\n\n\ndef qute_pyeval(_win_id, _request):\n    \"\"\"Handler for qute:pyeval. Return HTML content as bytes.\"\"\"\n    html = jinja.env.get_template('pre.html').render(\n        title='pyeval', content=pyeval_output)\n    return html.encode('UTF-8', errors='xmlcharrefreplace')\n\n\ndef qute_version(_win_id, _request):\n    \"\"\"Handler for qute:version. Return HTML content as bytes.\"\"\"\n    html = jinja.env.get_template('version.html').render(\n        title='Version info', version=version.version(),\n        copyright=qutebrowser.__copyright__)\n    return html.encode('UTF-8', errors='xmlcharrefreplace')\n\n\ndef qute_plainlog(_win_id, _request):\n    \"\"\"Handler for qute:plainlog. Return HTML content as bytes.\"\"\"\n    if log.ram_handler is None:\n        text = \"Log output was disabled.\"\n    else:\n        text = log.ram_handler.dump_log()\n    html = jinja.env.get_template('pre.html').render(title='log', content=text)\n    return html.encode('UTF-8', errors='xmlcharrefreplace')\n\n\ndef qute_log(_win_id, _request):\n    \"\"\"Handler for qute:log. Return HTML content as bytes.\"\"\"\n    if log.ram_handler is None:\n        html_log = None\n    else:\n        html_log = log.ram_handler.dump_log(html=True)\n    html = jinja.env.get_template('log.html').render(\n        title='log', content=html_log)\n    return html.encode('UTF-8', errors='xmlcharrefreplace')\n\n\ndef qute_gpl(_win_id, _request):\n    \"\"\"Handler for qute:gpl. Return HTML content as bytes.\"\"\"\n    return utils.read_file('html\/COPYING.html').encode('ASCII')\n\n\ndef qute_help(win_id, request):\n    \"\"\"Handler for qute:help. Return HTML content as bytes.\"\"\"\n    try:\n        utils.read_file('html\/doc\/index.html')\n    except FileNotFoundError:\n        html = jinja.env.get_template('error.html').render(\n            title=\"Error while loading documentation\",\n            url=request.url().toDisplayString(),\n            error=\"This most likely means the documentation was not generated \"\n                  \"properly. If you are running qutebrowser from the git \"\n                  \"repository, please run scripts\/asciidoc2html.py.\"\n                  \"If you're running a released version this is a bug, please \"\n                  \"use :report to report it.\",\n            icon='')\n        return html.encode('UTF-8', errors='xmlcharrefreplace')\n    urlpath = request.url().path()\n    if not urlpath or urlpath == '\/':\n        urlpath = 'index.html'\n    else:\n        urlpath = urlpath.lstrip('\/')\n    if not docutils.docs_up_to_date(urlpath):\n        message.error(win_id, \"Your documentation is outdated! Please re-run \"\n                      \"scripts\/asciidoc2html.py.\")\n    path = 'html\/doc\/{}'.format(urlpath)\n    return utils.read_file(path).encode('UTF-8', errors='xmlcharrefreplace')\n\n\ndef qute_settings(win_id, _request):\n    \"\"\"Handler for qute:settings. View\/change qute configuration.\"\"\"\n    config_getter = functools.partial(objreg.get('config').get, raw=True)\n    html = jinja.env.get_template('settings.html').render(\n        win_id=win_id, title='settings', config=configdata,\n        confget=config_getter)\n    return html.encode('UTF-8', errors='xmlcharrefreplace')\n\n\nHANDLERS = {\n    'pyeval': qute_pyeval,\n    'version': qute_version,\n    'plainlog': qute_plainlog,\n    'log': qute_log,\n    'gpl': qute_gpl,\n    'help': qute_help,\n    'settings': qute_settings,\n}\n","lang_cluster":"Python","length":190,"code_uid":"e9363b730b2941b5a497594540ff5b52"}
{"diff_hunk":"@@ -55,6 +55,9 @@ def writeModelParamsToFile(modelParams, name):\n   outDir = os.path.join(os.getcwd(), 'model_params')\n   if not os.path.isdir(outDir):\n     os.mkdir(outDir)\n+    # Create an __init__.py so the params are recognized.\n+    initPath = os.path.join(outDir, '__init__.py')\n+    open(initPath, 'a').close()\n   outPath = os.path.join(os.getcwd(), 'model_params', paramsName)\n   with open(outPath, \"wb\") as outFile:\n     modelParamsString = modelParamsToString(modelParams)","old_code":"# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU Affero Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU Affero Public License for more details.\n#\n# You should have received a copy of the GNU Affero Public License\n# along with this program.  If not, see http:\/\/www.gnu.org\/licenses.\n#\n# http:\/\/numenta.org\/licenses\/\n# ----------------------------------------------------------------------\n\"\"\"\nGroups together the code dealing with swarming.\n(This is a component of the One Hot Gym Prediction Tutorial.)\n\"\"\"\nimport os\nimport pprint\n\n# add logging to output errors to stdout\nimport logging\nlogging.basicConfig()\n\nfrom nupic.swarming import permutations_runner\nfrom swarm_description import SWARM_DESCRIPTION\n\nINPUT_FILE = \"rec-center-hourly.csv\"\nDESCRIPTION = (\n  \"This script runs a swarm on the input data (rec-center-hourly.csv) and\\n\"\n  \"creates a model parameters file in the `model_params` directory containing\\n\"\n  \"the best model found by the swarm. Dumps a bunch of crud to stdout because\\n\"\n  \"that is just what swarming does at this point. You really don't need to\\n\"\n  \"pay any attention to it.\\n\"\n  )\n\n\n\ndef modelParamsToString(modelParams):\n  pp = pprint.PrettyPrinter(indent=2)\n  return pp.pformat(modelParams)\n\n\n\ndef writeModelParamsToFile(modelParams, name):\n  cleanName = name.replace(\" \", \"_\").replace(\"-\", \"_\")\n  paramsName = \"%s_model_params.py\" % cleanName\n  outDir = os.path.join(os.getcwd(), 'model_params')\n  if not os.path.isdir(outDir):\n    os.mkdir(outDir)\n  outPath = os.path.join(os.getcwd(), 'model_params', paramsName)\n  with open(outPath, \"wb\") as outFile:\n    modelParamsString = modelParamsToString(modelParams)\n    outFile.write(\"MODEL_PARAMS = \\\\\\n%s\" % modelParamsString)\n  return outPath\n\n\n\ndef swarmForBestModelParams(swarmConfig, name, maxWorkers=4):\n  outputLabel = name\n  permWorkDir = os.path.abspath('swarm')\n  if not os.path.exists(permWorkDir):\n    os.mkdir(permWorkDir)\n  modelParams = permutations_runner.runWithConfig(\n    swarmConfig,\n    {\"maxWorkers\": maxWorkers, \"overwrite\": True},\n    outputLabel=outputLabel,\n    outDir=permWorkDir,\n    permWorkDir=permWorkDir,\n    verbosity=0\n  )\n  modelParamsFile = writeModelParamsToFile(modelParams, name)\n  return modelParamsFile\n\n\n\ndef printSwarmSizeWarning(size):\n  if size is \"small\":\n    print \"= THIS IS A DEBUG SWARM. DON'T EXPECT YOUR MODEL RESULTS TO BE GOOD.\"\n  elif size is \"medium\":\n    print \"= Medium swarm. Sit back and relax, this could take awhile.\"\n  else:\n    print \"= LARGE SWARM! Might as well load up the Star Wars Trilogy.\"\n\n\n\ndef swarm(filePath):\n  name = os.path.splitext(os.path.basename(filePath))[0]\n  print \"=================================================\"\n  print \"= Swarming on %s data...\" % name\n  printSwarmSizeWarning(SWARM_DESCRIPTION[\"swarmSize\"])\n  print \"=================================================\"\n  modelParams = swarmForBestModelParams(SWARM_DESCRIPTION, name)\n  print \"\\nWrote the following model param files:\"\n  print \"\\t%s\" % modelParams\n\n\n\nif __name__ == \"__main__\":\n  print DESCRIPTION\n  swarm(INPUT_FILE)\n","lang_cluster":"Python","length":108,"code_uid":"a1f938b03d8448ebb5f870563cc924c8"}
{"diff_hunk":"@@ -49,7 +49,7 @@ def stripControlChars(string):\n \n def compactHash(string):\n   hash = md5()\n-  hash.update(string)\n+  hash.update(string.encode('unicode_escape'))\n   return hash.hexdigest()\n \n ","old_code":"\"\"\"Copyright 2008 Orbitz WorldWide\n   Copyright 2011 Chris Davis\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\"\"\"\n\nfrom graphite.logger import log\nimport time\ntry:\n  from hashlib import md5\nexcept ImportError:\n  from md5 import md5\nimport bisect\n\ndef hashRequest(request):\n  # Normalize the request parameters so ensure we're deterministic\n  queryParams = [\"%s=%s\" % (key, '&'.join(values))\n                 for (key,values) in request.GET.lists()\n                 if not key.startswith('_')]\n\n  normalizedParams = ','.join( sorted(queryParams) ) or 'noParam'\n  myHash = stripControlChars(normalizedParams) #memcached doesn't like unprintable characters in its keys\n\n  return compactHash(myHash)\n\n\ndef hashData(targets, startTime, endTime):\n  targetsString = ','.join(targets)\n  startTimeString = startTime.strftime(\"%Y%m%d_%H%M\")\n  endTimeString = endTime.strftime(\"%Y%m%d_%H%M\")\n  myHash = targetsString + '@' + startTimeString + ':' + endTimeString\n  myHash = stripControlChars(myHash)\n\n  return compactHash(myHash)\n\n\ndef stripControlChars(string):\n  return filter(lambda char: ord(char) >= 33, string)\n\n\ndef compactHash(string):\n  hash = md5()\n  hash.update(string)\n  return hash.hexdigest()\n\n\nclass ConsistentHashRing:\n  def __init__(self, nodes, replica_count=100):\n    self.ring = []\n    self.ring_len = len(self.ring)\n    self.nodes = set()\n    self.nodes_len = len(self.nodes)\n    self.replica_count = replica_count\n    for node in nodes:\n      self.add_node(node)\n\n  def compute_ring_position(self, key):\n    big_hash = md5( str(key) ).hexdigest()\n    small_hash = int(big_hash[:4], 16) \n    return small_hash\n\n  def add_node(self, key):\n    self.nodes.add(key)\n    self.nodes_len = len(self.nodes)\n    for i in range(self.replica_count):\n      replica_key = \"%s:%d\" % (key, i)\n      position = self.compute_ring_position(replica_key)\n      entry = (position, key)\n      bisect.insort(self.ring, entry)\n    self.ring_len = len(self.ring)\n\n  def remove_node(self, key):\n    self.nodes.discard(key)\n    self.nodes_len = len(self.nodes)\n    self.ring = [entry for entry in self.ring if entry[1] != key]\n    self.ring_len = len(self.ring)\n\n  def get_node(self, key):\n    assert self.ring\n    position = self.compute_ring_position(key)\n    search_entry = (position, None)\n    index = bisect.bisect_left(self.ring, search_entry) % self.ring_len\n    entry = self.ring[index]\n    return entry[1]\n\n  def get_nodes(self, key):\n    nodes = []\n    position = self.compute_ring_position(key)\n    search_entry = (position, None)\n    index = bisect.bisect_left(self.ring, search_entry) % self.ring_len\n    last_index = (index - 1) % self.ring_len\n    nodes_len = len(nodes)\n    while nodes_len < self.nodes_len and index != last_index:\n      next_entry = self.ring[index]\n      (position, next_node) = next_entry\n      if next_node not in nodes:\n        nodes.append(next_node)\n        nodes_len += 1\n\n      index = (index + 1) % self.ring_len\n\n    return nodes\n","lang_cluster":"Python","length":111,"code_uid":"6d4044b07fb3474f80f1f065859f0a37"}
{"diff_hunk":"@@ -2,7 +2,6 @@\n \n import sys\n import os\n-sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), \"..\/listenstore\"))\n sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), \"..\"))\n from redis import Redis\n from redis_pubsub import RedisPubSubSubscriber, NoSubscriberNameSetException, WriteFailException","old_code":"#!\/usr\/bin\/env python\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), \"..\/listenstore\"))\nsys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), \"..\"))\nfrom redis import Redis\nfrom redis_pubsub import RedisPubSubSubscriber, NoSubscriberNameSetException, WriteFailException\nimport config\n\nimport ujson\nimport logging\nfrom listen import Listen\nfrom listenstore.listenstore import PostgresListenStore\nfrom time import time, sleep\n\nREPORT_FREQUENCY = 5000\nSUBSCRIBER_NAME = \"pg\"\nKEYSPACE_NAME = \"listen\"\n\nclass RedisConsumer(RedisPubSubSubscriber):\n    def __init__(self, redis, database_uri):\n        RedisPubSubSubscriber.__init__(self, redis, KEYSPACE_NAME, __name__)\n\n        self.total_inserts = 0\n        self.inserts = 0\n        self.time = 0\n        self.ls = PostgresListenStore({\n          'SQLALCHEMY_DATABASE_URI': database_uri,\n        })\n\n    def write(self, listen_dicts):\n        t0 = time()\n        listens = []\n        for listen in listen_dicts:\n            listens.append(Listen().from_json(listen))\n        self.ls.insert(listens)\n        self.time += time() - t0\n\n        return True\n\n    def start(self):\n        self.log.info(\"RedisListenConsumer started\")\n\n        self.register(SUBSCRIBER_NAME)\n        while True:\n            try:\n                count = self.subscriber()\n            except NoSubscriberNameSetException as e:\n                self.log.error(\"RedisListenConsumer has no subscriber name set.\")\n                return\n            except WriteFailException as e:\n                self.log.error(\"RedisListenConsumer failed to write to Postgres.\")\n                return\n\n            if not count:\n                continue\n\n            # collect and occasionally print some stats\n            self.inserts += count\n            if self.inserts >= REPORT_FREQUENCY:\n                self.total_inserts += self.inserts\n                self.log.info(\"Inserted %d rows in %.1fs (%.2f listens\/sec). Total %d rows.\" % \\\n                    (count, self.time, count \/ self.time, self.total_inserts))\n                self.inserts = 0\n                self.time = 0\n\nif __name__ == \"__main__\":\n    r = Redis(host=config.REDIS_HOST, port=config.REDIS_PORT)\n    rc = RedisConsumer(r,config.SQLALCHEMY_DATABASE_URI)\n    rc.start()\n","lang_cluster":"Python","length":71,"code_uid":"09dfe5359db24a07ac7eee4d7fb19007"}
{"diff_hunk":"@@ -12,7 +12,9 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\"\"\"\n \n-import ldap, traceback\n+import traceback\n+\n+import ldap\n from django.conf import settings\n from django.contrib.auth.models import User\n ","old_code":"\"\"\"Copyright 2008 Orbitz WorldWide\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\"\"\"\n\nimport ldap, traceback\nfrom django.conf import settings\nfrom django.contrib.auth.models import User\n\n\nclass LDAPBackend:\n  def authenticate(self, username=None, password=None):\n    if settings.LDAP_USER_DN_TEMPLATE is not None:\n      settings.LDAP_BASE_USER = settings.LDAP_USER_DN_TEMPLATE % {'username': username}\n      settings.LDAP_BASE_PASS = password\n    try:\n      conn = ldap.initialize(settings.LDAP_URI)\n      conn.protocol_version = ldap.VERSION3\n      if settings.LDAP_USE_TLS:\n        conn.start_tls_s()\n      conn.simple_bind_s( settings.LDAP_BASE_USER, settings.LDAP_BASE_PASS )\n    except ldap.LDAPError:\n      traceback.print_exc()\n      return None\n\n    scope = ldap.SCOPE_SUBTREE\n    filter = settings.LDAP_USER_QUERY % username\n    returnFields = ['dn','mail']\n    try:\n      resultID = conn.search( settings.LDAP_SEARCH_BASE, scope, filter, returnFields )\n      resultType, resultData = conn.result( resultID, 0 )\n      if len(resultData) != 1: # User does not exist\n        return None\n\n      userDN = resultData[0][0]\n      try:\n        userMail = resultData[0][1]['mail'][0].decode(\"utf-8\")\n      except Exception:\n        userMail = \"Unknown\"\n\n      conn.simple_bind_s(userDN,password)\n      try:\n        user = User.objects.get(username=username)\n      except Exception:  # First time login, not in django's database\n        # To prevent login from django db user\n        randomPasswd = User.objects.make_random_password(length=16)\n        user = User.objects.create_user(username, userMail, randomPasswd)\n        user.save()\n\n      return user\n\n    except ldap.INVALID_CREDENTIALS:\n      traceback.print_exc()\n      return None\n\n  def get_user(self,user_id):\n    try:\n      return User.objects.get(pk=user_id)\n    except User.DoesNotExist:\n      return None\n","lang_cluster":"Python","length":69,"code_uid":"0694847367c44e1c98a01432a0caba20"}
{"diff_hunk":"@@ -22,17 +22,32 @@ Solr utilities.\n \"\"\"\n \n \n+import os\n+import urllib2\n+import re\n import time\n-from invenio.config import CFG_SOLR_URL\n+\n+from invenio.config import (\n+    CFG_SOLR_URL,\n+    CFG_BIBINDEX_FULLTEXT_INDEX_LOCAL_FILES_ONLY,\n+    CFG_BIBINDEX_SPLASH_PAGES\n+)\n from invenio.bibtask import write_message, task_get_option, task_update_progress, \\\n                             task_sleep_now_if_required\n+from invenio.htmlutils import get_links_in_html_page\n+from invenio.websubmit_file_converter import convert_file\n from invenio.dbquery import run_sql\n-from invenio.search_engine import record_exists\n-from invenio.bibdocfile import BibRecDocs\n+from invenio.search_engine import record_exists, get_field_tags\n+from invenio.search_engine_utils import get_fieldvalues\n+from invenio.bibdocfile import BibRecDocs, bibdocfile_url_p, download_url\n from invenio.solrutils_bibindex_indexer import replace_invalid_solr_characters\n from invenio.bibindex_engine import create_range_list\n from invenio.errorlib import register_exception\n from invenio.bibrank_bridge_utils import get_tags, get_field_content_in_utf8\n+from invenio.bibtask import write_message\n+\n+\n+SOLR_CONNECTION = None\n \n \n if CFG_SOLR_URL:","old_code":"# -*- coding: utf-8 -*-\n#\n# This file is part of Invenio.\n# Copyright (C) 2011 CERN.\n#\n# Invenio is free software; you can redistribute it and\/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2 of the\n# License, or (at your option) any later version.\n#\n# Invenio is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Invenio; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.\n\n\"\"\"\nSolr utilities.\n\"\"\"\n\n\nimport time\nfrom invenio.config import CFG_SOLR_URL\nfrom invenio.bibtask import write_message, task_get_option, task_update_progress, \\\n                            task_sleep_now_if_required\nfrom invenio.dbquery import run_sql\nfrom invenio.search_engine import record_exists\nfrom invenio.bibdocfile import BibRecDocs\nfrom invenio.solrutils_bibindex_indexer import replace_invalid_solr_characters\nfrom invenio.bibindex_engine import create_range_list\nfrom invenio.errorlib import register_exception\nfrom invenio.bibrank_bridge_utils import get_tags, get_field_content_in_utf8\n\n\nif CFG_SOLR_URL:\n    import solr\n    SOLR_CONNECTION = solr.SolrConnection(CFG_SOLR_URL) # pylint: disable=E1101\n\n\ndef solr_add_ranges(id_ranges):\n    sub_range_length = task_get_option(\"flush\")\n    id_ranges_to_index = []\n    for id_range in id_ranges:\n        lower_recid = id_range[0]\n        upper_recid = id_range[1]\n        i_low = lower_recid\n        while i_low <= upper_recid:\n            i_up = min(i_low + sub_range_length - 1, upper_recid)\n            id_ranges_to_index.append((i_low, i_up))\n            i_low += sub_range_length\n\n    tags_to_index = get_tags()\n    # Indexes latest records first by reversing\n    # This allows the ranker to return better results during long indexing\n    # runs as the ranker cuts the hitset using latest records\n    id_ranges_to_index.reverse()\n    next_commit_counter = 0\n    for id_range_to_index in id_ranges_to_index:\n        lower_recid = id_range_to_index[0]\n        upper_recid = id_range_to_index[1]\n        status_msg = \"Solr ranking indexer called for %s-%s\" % (lower_recid, upper_recid)\n        write_message(status_msg)\n        task_update_progress(status_msg)\n        next_commit_counter = solr_add_range(lower_recid, upper_recid, tags_to_index, next_commit_counter)\n\n    solr_commit_if_necessary(next_commit_counter, final_commit=True)\n\n\ndef solr_commit_if_necessary(next_commit_counter, final_commit=False, recid=None):\n    # Counter full or final commit if counter set\n    if next_commit_counter == task_get_option(\"flush\") - 1 or (final_commit and next_commit_counter > 0):\n        recid_info = ''\n        if recid:\n            recid_info = ' for recid=%s' % recid\n        status_msg = 'Solr ranking indexer COMMITTING' + recid_info\n        write_message(status_msg)\n        task_update_progress(status_msg)\n\n        try:\n            # Commits might cause an exception, most likely a\n            # timeout while hitting a background merge\n            # Changes will then be committed later by the\n            # calling (periodical) task\n            # Also, autocommits can be used in the solrconfig\n            SOLR_CONNECTION.commit()\n        except:\n            register_exception(alert_admin=True)\n        next_commit_counter = 0\n\n        task_sleep_now_if_required(can_stop_too=True)\n    else:\n        next_commit_counter = next_commit_counter + 1\n    return next_commit_counter\n\n\ndef solr_add_range(lower_recid, upper_recid, tags_to_index, next_commit_counter):\n    \"\"\"\n    Adds the regarding field values of all records from the lower recid to the upper one to Solr.\n    It preserves the fulltext information.\n    \"\"\"\n    for recid in range(lower_recid, upper_recid + 1):\n        if record_exists(recid):\n            abstract        = get_field_content_in_utf8(recid, 'abstract', tags_to_index)\n            author          = get_field_content_in_utf8(recid, 'author', tags_to_index)\n            keyword         = get_field_content_in_utf8(recid, 'keyword', tags_to_index)\n            title           = get_field_content_in_utf8(recid, 'title', tags_to_index)\n            try:\n                bibrecdocs  = BibRecDocs(recid)\n                fulltext    = unicode(bibrecdocs.get_text(), 'utf-8')\n            except:\n                fulltext    = ''\n\n            solr_add(recid, abstract, author, fulltext, keyword, title)\n            next_commit_counter = solr_commit_if_necessary(next_commit_counter,recid=recid)\n\n    return next_commit_counter\n\n\ndef solr_add(recid, abstract, author, fulltext, keyword, title):\n    \"\"\"\n    Helper function that adds word similarity ranking relevant indexes to Solr.\n    \"\"\"\n    try:\n        SOLR_CONNECTION.add(id=recid,\n                            abstract=replace_invalid_solr_characters(abstract),\n                            author=replace_invalid_solr_characters(author),\n                            fulltext=replace_invalid_solr_characters(fulltext),\n                            keyword=replace_invalid_solr_characters(keyword),\n                            title=replace_invalid_solr_characters(title))\n    except:\n        register_exception(alert_admin=True)\n\n\ndef word_similarity_solr(run):\n    return word_index(run)\n\n\ndef get_recIDs_by_date(dates=\"\"):\n    \"\"\"Returns recIDs modified between DATES[0] and DATES[1].\n       If DATES is not set, then returns records modified since the last run of\n       the ranking method.\n    \"\"\"\n    if not dates:\n        write_message(\"Using the last update time for the rank method\")\n        res = run_sql('SELECT last_updated FROM rnkMETHOD WHERE name=\"wrd\"')\n\n        if not res:\n            return\n        if not res[0][0]:\n            dates = (\"0000-00-00\",'')\n        else:\n            dates = (res[0][0],'')\n\n    if dates[1]:\n        res = run_sql('SELECT id FROM bibrec WHERE modification_date >= %s AND modification_date <= %s ORDER BY id ASC', (dates[0], dates[1]))\n    else:\n        res = run_sql('SELECT id FROM bibrec WHERE modification_date >= %s ORDER BY id ASC', (dates[0],))\n\n    return create_range_list([row[0] for row in res])\n\n\ndef word_index(run): # pylint: disable=W0613\n    \"\"\"\n    Runs the indexing task.\n    \"\"\"\n    # Explicitly set ids\n    id_option = task_get_option(\"id\")\n    if len(id_option):\n        solr_add_ranges([(id_elem[0], id_elem[1]) for id_elem in id_option])\n\n    # Indexes modified ids since last run\n    else:\n        starting_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n        id_ranges = get_recIDs_by_date()\n        if id_ranges:\n            solr_add_ranges([(id_range[0], id_range[1]) for id_range in id_ranges])\n            run_sql('UPDATE rnkMETHOD SET last_updated=%s WHERE name=\"wrd\"', (starting_time, ))\n        else:\n            write_message(\"No new records. Solr index is up to date\")\n\n    write_message(\"Solr ranking indexer completed\")\n","lang_cluster":"Python","length":184,"code_uid":"770f5719161f4e6196af3a3d63667723"}
{"diff_hunk":"@@ -25,17 +25,6 @@ except:\n     xr = None\n \n \n-def toarray(v, index_value=False):\n-    \"\"\"\n-    Interface helper function to turn dask Arrays into numpy arrays as\n-    necessary. If index_value is True, a value is returned instead of\n-    an array holding a single value.\n-    \"\"\"\n-    if dask and isinstance(v, dask.array.Array):\n-        arr =  v.compute()\n-        return arr[()] if index_value else arr\n-    else:\n-        return v\n \n def compute_edges(edges):\n     \"\"\"","old_code":"import itertools\n\nimport param\nimport numpy as np\n\nfrom ..core import Dataset, OrderedDict\nfrom ..core.operation import ElementOperation\nfrom ..core.util import (is_nan, sort_topologically, one_to_one,\n                         cartesian_product, is_cyclic)\n\ntry:\n    import pandas as pd\n    from ..core.data import PandasInterface\nexcept:\n    pd = None\n\ntry:\n    import dask\nexcept:\n    dask = None\n\ntry:\n    import xarray as xr\nexcept:\n    xr = None\n\n\ndef toarray(v, index_value=False):\n    \"\"\"\n    Interface helper function to turn dask Arrays into numpy arrays as\n    necessary. If index_value is True, a value is returned instead of\n    an array holding a single value.\n    \"\"\"\n    if dask and isinstance(v, dask.array.Array):\n        arr =  v.compute()\n        return arr[()] if index_value else arr\n    else:\n        return v\n\ndef compute_edges(edges):\n    \"\"\"\n    Computes edges from a number of bin centers,\n    throwing an exception if the edges are not\n    evenly spaced.\n    \"\"\"\n    widths = np.diff(edges)\n    if np.allclose(widths, widths[0]):\n        width = widths[0]\n    else:\n        raise ValueError('Centered bins have to be of equal width.')\n    edges -= width\/2.\n    return np.concatenate([edges, [edges[-1]+width]])\n\n\ndef reduce_fn(x):\n    \"\"\"\n    Aggregation function to get the first non-zero value.\n    \"\"\"\n    values = x.values if pd and isinstance(x, pd.Series) else x\n    for v in values:\n        if not is_nan(v):\n            return v\n    return np.NaN\n\n\nclass categorical_aggregate2d(ElementOperation):\n    \"\"\"\n    Generates a gridded Dataset of 2D aggregate arrays indexed by the\n    first two dimensions of the passed Element, turning all remaining\n    dimensions into value dimensions. The key dimensions of the\n    gridded array are treated as categorical indices. Useful for data\n    indexed by two independent categorical variables such as a table\n    of population values indexed by country and year. Data that is\n    indexed by continuous dimensions should be binned before\n    aggregation. The aggregation will retain the global sorting order\n    of both dimensions.\n\n    >> table = Table([('USA', 2000, 282.2), ('UK', 2005, 58.89)],\n                     kdims=['Country', 'Year'], vdims=['Population'])\n    >> categorical_aggregate2d(table)\n    Dataset({'Country': ['USA', 'UK'], 'Year': [2000, 2005],\n             'Population': [[ 282.2 , np.NaN], [np.NaN,   58.89]]},\n            kdims=['Country', 'Year'], vdims=['Population'])\n    \"\"\"\n\n    datatype = param.List(['xarray', 'grid'] if xr else ['grid'], doc=\"\"\"\n        The grid interface types to use when constructing the gridded Dataset.\"\"\")\n\n    def _get_coords(self, obj):\n        \"\"\"\n        Get the coordinates of the 2D aggregate, maintaining the correct\n        sorting order.\n        \"\"\"\n        xdim, ydim = obj.dimensions(label=True)[:2]\n        xcoords = obj.dimension_values(xdim, False)\n        ycoords = obj.dimension_values(ydim, False)\n\n        # Determine global orderings of y-values using topological sort\n        grouped = obj.groupby(xdim, container_type=OrderedDict,\n                              group_type=Dataset).values()\n        orderings = OrderedDict()\n        sort = True\n        for group in grouped:\n            vals = group.dimension_values(ydim, False)\n            if len(vals) == 1:\n                orderings[vals[0]] = [vals[0]]\n            else:\n                for i in range(len(vals)-1):\n                    p1, p2 = vals[i:i+2]\n                    orderings[p1] = [p2]\n            if sort:\n                if vals.dtype.kind in ('i', 'f'):\n                    sort = (np.diff(vals)>=0).all()\n                else:\n                    sort = np.array_equal(np.sort(vals), vals)\n        if sort or one_to_one(orderings, ycoords):\n            ycoords = np.sort(ycoords)\n        elif not is_cyclic(orderings):\n            ycoords = list(itertools.chain(*sort_topologically(orderings)))\n        return xcoords, ycoords\n\n\n    def _aggregate_dataset(self, obj, xcoords, ycoords):\n        \"\"\"\n        Generates a gridded Dataset from a column-based dataset and\n        lists of xcoords and ycoords\n        \"\"\"\n        dim_labels = obj.dimensions(label=True)\n        vdims = obj.dimensions()[2:]\n        xdim, ydim = dim_labels[:2]\n        shape = (len(ycoords), len(xcoords))\n        nsamples = np.product(shape)\n\n        ys, xs = cartesian_product([ycoords, xcoords], copy=True)\n        data = {xdim: xs, ydim: ys}\n        for vdim in vdims:\n            values = np.empty(nsamples)\n            values[:] = np.NaN\n            data[vdim.name] = values\n        dtype = 'dataframe' if pd else 'dictionary'\n        dense_data = Dataset(data, kdims=obj.kdims, vdims=obj.vdims, datatype=[dtype])\n        concat_data = obj.interface.concatenate([dense_data, obj], datatype=[dtype])\n        reindexed = concat_data.reindex([xdim, ydim], vdims)\n        if pd:\n            df = PandasInterface.as_dframe(reindexed)\n            df = df.groupby([xdim, ydim], sort=False).first().reset_index()\n            agg = reindexed.clone(df)\n        else:\n            agg = reindexed.aggregate([xdim, ydim], reduce_fn)\n\n        # Convert data to a gridded dataset\n        grid_data = {xdim: xcoords, ydim: ycoords}\n        for vdim in vdims:\n            grid_data[vdim.name] = agg.dimension_values(vdim).reshape(shape)\n        return agg.clone(grid_data, kdims=[xdim, ydim], vdims=vdims,\n                         datatype=self.p.datatype)\n\n\n    def _process(self, obj, key=None):\n        \"\"\"\n        Generates a categorical 2D aggregate by inserting NaNs at all\n        cross-product locations that do not already have a value assigned.\n        Returns a 2D gridded Dataset object.\n        \"\"\"\n        if isinstance(obj, Dataset) and obj.interface.gridded:\n            return obj\n        elif obj.ndims > 2:\n            raise ValueError(\"Cannot aggregate more than two dimensions\")\n        elif len(obj.dimensions()) < 3:\n            raise ValueError(\"Must have at two dimensions to aggregate over\"\n                             \"and one value dimension to aggregate on.\")\n\n        dtype = 'dataframe' if pd else 'dictionary'\n        obj = Dataset(obj, datatype=[dtype])\n        xcoords, ycoords = self._get_coords(obj)\n        return self._aggregate_dataset(obj, xcoords, ycoords)\n","lang_cluster":"Python","length":176,"code_uid":"0b43281431c34c80b4cea88d915faa4c"}
{"diff_hunk":"@@ -53,6 +53,17 @@ def main(global_config, config=None, **settings):\n     # Retro-compatibility with first Kinto clients.\n     config.registry.public_settings.add('cliquet.batch_max_requests')\n \n+    # Expose capability\n+    schema_enabled = asbool(\n+        settings['experimental_collection_schema_validation']\n+    )\n+    if schema_enabled:\n+        config.add_api_capability(\n+            \"schema\",\n+            description=\"Validates collection records with JSON schemas.\",\n+            url=\"http:\/\/kinto.readthedocs.org\/en\/latest\/api\/1.x\/\"\n+                \"collections.html#collection-json-schema\")\n+\n     # Scan Kinto views.\n     kwargs = {}\n     flush_enabled = asbool(settings.get('flush_endpoint_enabled'))","old_code":"import pkg_resources\nimport logging\n\nimport cliquet\nfrom pyramid.config import Configurator\nfrom pyramid.settings import asbool\nfrom pyramid.security import Authenticated\n\nfrom kinto.authorization import RouteFactory\n\n# Module version, as defined in PEP-0396.\n__version__ = pkg_resources.get_distribution(__package__).version\n\n# Implemented HTTP API Version\nHTTP_API_VERSION = '1.4'\n\n# Main kinto logger\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT_SETTINGS = {\n    'retry_after_seconds': 3,\n    'cache_backend': 'cliquet.cache.memory',\n    'permission_backend': 'cliquet.permission.memory',\n    'storage_backend': 'cliquet.storage.memory',\n    'project_docs': 'https:\/\/kinto.readthedocs.org\/',\n    'bucket_create_principals': Authenticated,\n    'multiauth.authorization_policy': (\n        'kinto.authorization.AuthorizationPolicy'),\n    'experimental_collection_schema_validation': 'False',\n    'http_api_version': HTTP_API_VERSION\n}\n\n\ndef main(global_config, config=None, **settings):\n    if not config:\n        config = Configurator(settings=settings, root_factory=RouteFactory)\n\n    # Force project name, since it determines settings prefix.\n    config.add_settings({'cliquet.project_name': 'kinto'})\n\n    cliquet.initialize(config,\n                       version=__version__,\n                       default_settings=DEFAULT_SETTINGS)\n\n    settings = config.get_settings()\n\n    # In Kinto API 1.x, a default bucket is available.\n    # Force its inclusion if not specified in settings.\n    if 'kinto.plugins.default_bucket' not in settings['includes']:\n        config.include('kinto.plugins.default_bucket')\n\n    # Retro-compatibility with first Kinto clients.\n    config.registry.public_settings.add('cliquet.batch_max_requests')\n\n    # Scan Kinto views.\n    kwargs = {}\n    flush_enabled = asbool(settings.get('flush_endpoint_enabled'))\n    if not flush_enabled:\n        kwargs['ignore'] = 'kinto.views.flush'\n    config.scan(\"kinto.views\", **kwargs)\n\n    app = config.make_wsgi_app()\n\n    # Install middleware (idempotent if disabled)\n    return cliquet.install_middlewares(app, settings)\n","lang_cluster":"Python","length":66,"code_uid":"c76eb49803b448e5860702a1281c57b6"}
{"diff_hunk":"@@ -90,7 +90,7 @@ def handler(event, context):\n             forwarding_target = ddb_new_image['data'][MSG_BODY_MESSAGE_TARGET]\n             target_name = forwarding_target.split(':')[-1]\n             if forwarding_target.startswith('kinesis:'):\n-                ddb_new_image['data'][MSG_BODY_MESSAGE_TARGET] = 's3:\/test_chain_result'\n+                ddb_new_image['data'][MSG_BODY_MESSAGE_TARGET] = 's3:test_chain_result'\n                 kinesis_record['Data'] = json.dumps(ddb_new_image['data'])\n                 forward_event_to_target_stream(kinesis_record, target_name)\n             elif forwarding_target.startswith('s3:'):","old_code":"import json\nimport base64\nimport logging\nimport boto3.dynamodb.types\nfrom io import BytesIO\nfrom localstack.utils.aws import aws_stack\nfrom localstack.utils.common import to_str, to_bytes\n\nTEST_BUCKET_NAME = 'test-bucket'\nKINESIS_STREAM_NAME = 'test_stream_1'\nMSG_BODY_RAISE_ERROR_FLAG = 'raise_error'\nMSG_BODY_MESSAGE_TARGET = 'message_target'\n\nlogging.basicConfig(level=logging.INFO)\nLOGGER = logging.getLogger(__name__)\nLOGGER.setLevel(logging.INFO)\n\n\n# Subclass of boto's TypeDeserializer for DynamoDB\n# to adjust for DynamoDB Stream format.\nclass TypeDeserializer(boto3.dynamodb.types.TypeDeserializer):\n    def _deserialize_n(self, value):\n        return float(value)\n\n    def _deserialize_b(self, value):\n        return value        # already in Base64\n\n\ndef handler(event, context):\n    \"\"\" Generic event forwarder Lambda. \"\"\"\n\n    # print test messages (to test CloudWatch Logs integration)\n    LOGGER.info('Lambda log message - logging module')\n    print('Lambda log message - print function')\n\n    if MSG_BODY_RAISE_ERROR_FLAG in event:\n        raise Exception('Test exception (this is intentional)')\n\n    if 'httpMethod' in event:\n        # looks like this is a call from an AWS_PROXY API Gateway\n        try:\n            body = json.loads(event['body'])\n        except Exception:\n            body = {}\n        body['pathParameters'] = event.get('pathParameters')\n        body['requestContext'] = event.get('requestContext')\n        body['queryStringParameters'] = event.get('queryStringParameters')\n        body['httpMethod'] = event.get('httpMethod')\n        body['body'] = event.get('body')\n        if body['httpMethod'] == 'DELETE':\n            return {'statusCode': 204}\n\n        status_code = body.get('return_status_code', 200)\n        headers = body.get('return_headers', {})\n        body = body.get('return_raw_body') or body\n        return {\n            'body': body,\n            'statusCode': status_code,\n            'headers': headers,\n            'multiValueHeaders': {'set-cookie': ['language=en-US', 'theme=blue moon']},\n        }\n\n    if 'Records' not in event:\n        result_map = {'event': event, 'context': {}}\n        result_map['context']['invoked_function_arn'] = context.invoked_function_arn\n        result_map['context']['function_version'] = context.function_version\n        result_map['context']['function_name'] = context.function_name\n\n        if hasattr(context, 'client_context'):\n            result_map['context']['client_context'] = context.client_context\n\n        return result_map\n\n    raw_event_messages = []\n    for record in event['Records']:\n        # Deserialize into Python dictionary and extract the\n        # \"NewImage\" (the new version of the full ddb document)\n        ddb_new_image = deserialize_event(record)\n\n        if MSG_BODY_RAISE_ERROR_FLAG in ddb_new_image.get('data', {}):\n            raise Exception('Test exception (this is intentional)')\n\n        # Place the raw event message document into the Kinesis message format\n        kinesis_record = {\n            'PartitionKey': 'key123',\n            'Data': json.dumps(ddb_new_image)\n        }\n\n        if MSG_BODY_MESSAGE_TARGET in ddb_new_image.get('data', {}):\n            forwarding_target = ddb_new_image['data'][MSG_BODY_MESSAGE_TARGET]\n            target_name = forwarding_target.split(':')[-1]\n            if forwarding_target.startswith('kinesis:'):\n                ddb_new_image['data'][MSG_BODY_MESSAGE_TARGET] = 's3:\/test_chain_result'\n                kinesis_record['Data'] = json.dumps(ddb_new_image['data'])\n                forward_event_to_target_stream(kinesis_record, target_name)\n            elif forwarding_target.startswith('s3:'):\n                s3_client = aws_stack.connect_to_service('s3')\n                test_data = to_bytes(json.dumps({'test_data': ddb_new_image['data']['test_data']}))\n                s3_client.upload_fileobj(BytesIO(test_data), TEST_BUCKET_NAME, target_name)\n        else:\n            raw_event_messages.append(kinesis_record)\n\n    # Forward messages to Kinesis\n    forward_events(raw_event_messages)\n\n\ndef deserialize_event(event):\n    # Deserialize into Python dictionary and extract the \"NewImage\" (the new version of the full ddb document)\n    ddb = event.get('dynamodb')\n    if ddb:\n        result = {\n            '__action_type': event.get('eventName'),\n        }\n\n        ddb_deserializer = TypeDeserializer()\n        if ddb.get('OldImage'):\n            result['old_image'] = ddb_deserializer.deserialize({'M': ddb.get('OldImage')})\n        if ddb.get('NewImage'):\n            result['new_image'] = ddb_deserializer.deserialize({'M': ddb.get('NewImage')})\n\n        return result\n    kinesis = event.get('kinesis')\n    if kinesis:\n        assert kinesis['sequenceNumber']\n        kinesis['data'] = json.loads(to_str(base64.b64decode(kinesis['data'])))\n        return kinesis\n    sqs = event.get('sqs')\n    if sqs:\n        result = {'data': event['body']}\n        return result\n    sns = event.get('Sns')\n    if sns:\n        result = {'data': sns['Message']}\n        return result\n\n\ndef forward_events(records):\n    if not records:\n        return\n    kinesis = aws_stack.connect_to_service('kinesis')\n    kinesis.put_records(StreamName=KINESIS_STREAM_NAME, Records=records)\n\n\ndef forward_event_to_target_stream(record, stream_name):\n    kinesis = aws_stack.connect_to_service('kinesis')\n    kinesis.put_record(StreamName=stream_name, Data=record['Data'], PartitionKey=record['PartitionKey'])\n","lang_cluster":"Python","length":146,"code_uid":"e1792b2161594457a0093347929a9b6d"}
{"diff_hunk":"@@ -3,6 +3,7 @@ import sys\n from cliquet.scripts import cliquet\n from pyramid.scripts import pserve\n from pyramid.paster import bootstrap\n+from config import template\n \n CONFIG_FILE = 'config\/kinto.ini'\n ","old_code":"import argparse\nimport sys\nfrom cliquet.scripts import cliquet\nfrom pyramid.scripts import pserve\nfrom pyramid.paster import bootstrap\n\nCONFIG_FILE = 'config\/kinto.ini'\n\n\ndef main(args=None):\n    \"\"\"The main routine.\"\"\"\n    if args is None:\n        args = sys.argv[1:]\n\n    parser = argparse.ArgumentParser(description=\"Kinto commands\")\n    parser.add_argument('--ini',\n                        help='Application configuration file',\n                        dest='ini_file',\n                        required=False,\n                        default=CONFIG_FILE)\n\n    subparsers = parser.add_subparsers(title='subcommands',\n                                       description='valid subcommands',\n                                       help='init\/start\/migrate')\n\n    parser_init = subparsers.add_parser('init')\n    parser_init.set_defaults(which='init')\n\n    parser_migrate = subparsers.add_parser('migrate')\n    parser_migrate.set_defaults(which='migrate')\n\n    parser_start = subparsers.add_parser('start')\n    parser_start.set_defaults(which='start')\n\n    args = vars(parser.parse_args())\n    config_file = args['ini_file']\n    env = bootstrap(config_file)\n\n    if args['which'] == 'init':\n        # Not implemented yet\n        pass\n    elif args['which'] == 'migrate':\n        cliquet.init_schema(env)\n    elif args['which'] == 'start':\n        pserve_argv = ['pserve', config_file, '--reload']\n        pserve.main(pserve_argv)\n\n\nif __name__ == \"__main__\":\n    main()\n","lang_cluster":"Python","length":50,"code_uid":"a37169b805d9488db2b2055c77cd0776"}
{"diff_hunk":"@@ -38,6 +38,7 @@ VIOLATION_RESOURCES = {\n     'FIREWALL_WHITELIST_VIOLATION': 'firewall_rule_violations',\n     'GROUP_VIOLATION': 'groups_violations',\n     'KE_VERSION_VIOLATION': 'ke_version_violations',\n+    'KE_JMESPATH_VIOLATION': 'ke_jmespath_violations',\n     'IAM_POLICY_VIOLATION': 'iam_policy_violations',\n     'IAP_VIOLATION': 'iap_violations',\n     'INSTANCE_NETWORK_INTERFACE_VIOLATION': (","old_code":"# Copyright 2017 The Forseti Security Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Provides violations map\"\"\"\n\nfrom google.cloud.forseti.common.data_access import violation_format as vf\n\n\nVIOLATION_MAP = {\n    'violations': vf.format_violation,\n    'buckets_acl_violations': vf.format_violation,\n    'cloudsql_acl_violations': vf.format_violation,\n    'groups_violations': vf.format_groups_violation,\n}\n\nVIOLATION_RESOURCES = {\n    'AUDIT_LOGGING_VIOLATION': 'audit_logging_violations',\n    'BIGQUERY_VIOLATION': 'bigquery_acl_violations',\n    'BLACKLIST_VIOLATION': 'blacklist_violations',\n    'BUCKET_VIOLATION': 'buckets_acl_violations',\n    'CLOUD_SQL_VIOLATION': 'cloudsql_acl_violations',\n    'ENABLED_APIS_VIOLATION': 'enabled_apis_violations',\n    'FORWARDING_RULE_VIOLATION': 'forwarding_rule_violations',\n    'FIREWALL_BLACKLIST_VIOLATION': 'firewall_rule_violations',\n    'FIREWALL_MATCHES_VIOLATION': 'firewall_rule_violations',\n    'FIREWALL_REQUIRED_VIOLATION': 'firewall_rule_violations',\n    'FIREWALL_WHITELIST_VIOLATION': 'firewall_rule_violations',\n    'GROUP_VIOLATION': 'groups_violations',\n    'KE_VERSION_VIOLATION': 'ke_version_violations',\n    'IAM_POLICY_VIOLATION': 'iam_policy_violations',\n    'IAP_VIOLATION': 'iap_violations',\n    'INSTANCE_NETWORK_INTERFACE_VIOLATION': (\n        'instance_network_interface_violations'),\n    'LOG_SINK_VIOLATION': 'log_sink_violations',\n    'SERVICE_ACCOUNT_KEY_VIOLATION': (\n        'service_account_key_violations'),\n}\n","lang_cluster":"Python","length":48,"code_uid":"e49dece45d194b778e287c3275bafdf1"}
{"diff_hunk":"@@ -37,6 +37,15 @@ class MonitorMixinBase(object):\n \n \n   def __init__(self, *args, **kwargs):\n+    \"\"\"\n+    Note: If you set the kwarg \"__name__\", then pretty-printing of traces and\n+          metrics will include the name you specify as a tag before every title.\n+    \"\"\"\n+    self.name = None\n+    if \"__name__\" in kwargs:\n+      self.name = kwargs[\"__name__\"]\n+      del kwargs[\"__name__\"]\n+\n     super(MonitorMixinBase, self).__init__(*args, **kwargs)\n \n     # Mapping from key (string) => trace (Trace)","old_code":"# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2014, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and\/or modify\n# it under the terms of the GNU General Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see http:\/\/www.gnu.org\/licenses.\n#\n# http:\/\/numenta.org\/licenses\/\n# ----------------------------------------------------------------------\n\n\"\"\"\nMonitorMixinBase class used in monitor mixin framework.\n\"\"\"\n\nimport abc\n\nfrom prettytable import PrettyTable\n\n\nclass MonitorMixinBase(object):\n  \"\"\"\n  Base class for MonitorMixin. Each subclass will be a mixin for a particular\n  algorithm.\n  \"\"\"\n  __metaclass__ = abc.ABCMeta\n\n\n  def __init__(self, *args, **kwargs):\n    super(MonitorMixinBase, self).__init__(*args, **kwargs)\n\n    # Mapping from key (string) => trace (Trace)\n    self._traces = None\n    self._data = None\n    self.clearHistory()\n\n\n  def clearHistory(self):\n    \"\"\"\n    Clears the stored history.\n    \"\"\"\n    self._traces = {}\n    self._data = {}\n\n\n  @staticmethod\n  def prettyPrintTraces(traces, breakOnResets=None):\n    \"\"\"\n    Returns pretty-printed table of traces.\n\n    @param traces (list) Traces to print in table\n    @param breakOnResets (BoolsTrace) Trace of resets to break table on\n\n    @return (string) Pretty-printed table of traces.\n    \"\"\"\n    assert len(traces) > 0, \"No traces found\"\n    table = PrettyTable([\"Iteration\"] + [trace.title for trace in traces])\n\n    for i in xrange(len(traces[0].data)):\n      if breakOnResets and breakOnResets.data[i]:\n        table.add_row([\"<reset>\"] * (len(traces) + 1))\n      table.add_row([i] +\n        [trace.prettyPrintDatum(trace.data[i]) for trace in traces])\n\n    return table.get_string()\n\n\n  @staticmethod\n  def prettyPrintMetrics(metrics):\n    \"\"\"\n    Returns pretty-printed table of metrics.\n\n    @param metrics (list) Traces to print in table\n\n    @return (string) Pretty-printed table of metrics.\n    \"\"\"\n    assert len(metrics) > 0, \"No metrics found\"\n    table = PrettyTable([\"Metric\",\n                         \"min\", \"max\", \"sum\", \"mean\", \"standard deviation\"])\n\n    for metric in metrics:\n      table.add_row([metric.title,\n                     metric.min,\n                     metric.max,\n                     metric.sum,\n                     metric.mean,\n                     metric.standardDeviation])\n\n    return table.get_string()\n\n\n  def getDefaultTraces(self, verbosity=1):\n    \"\"\"\n    Returns list of default traces. (To be overridden.)\n\n    @param verbosity (int) Verbosity level\n\n    @return (list) Default traces\n    \"\"\"\n    return []\n\n\n  def getDefaultMetrics(self, verbosity=1):\n    \"\"\"\n    Returns list of default metrics. (To be overridden.)\n\n    @param verbosity (int) Verbosity level\n\n    @return (list) Default metrics\n    \"\"\"\n    return []\n","lang_cluster":"Python","length":121,"code_uid":"3ddc7323e7de4cd38cae4432a880a900"}
{"diff_hunk":"@@ -86,7 +86,7 @@ class Cache(CacheBase):\n         if ttl == 0:\n             self.delete(key)\n         else:\n-            # We can't use touch here because we need to update the TTL value in the record.\n+            # We can't use touch here because we need to update the TTL value in the object.\n             value = self.get(key)\n             self.set(key, value, ttl)\n ","old_code":"import logging\nfrom functools import wraps\nfrom math import ceil, floor\nfrom time import time\n\nfrom pyramid.settings import aslist\n\nfrom kinto.core.cache import CacheBase\nfrom kinto.core.storage import exceptions\nfrom kinto.core.utils import json, memcache\n\nlogger = logging.getLogger(__name__)\n\n\ndef wrap_memcached_error(func):\n    @wraps(func)\n    def wrapped(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except TypeError:\n            raise\n        except (\n            memcache.Client.MemcachedKeyError,\n            memcache.Client.MemcachedStringEncodingError,\n        ) as e:\n            logger.exception(e)\n            raise exceptions.BackendError(original=e)\n\n    return wrapped\n\n\ndef create_from_config(config, prefix=\"\"):\n    \"\"\"Redis client instantiation from settings.\n    \"\"\"\n    settings = config.get_settings()\n    hosts = aslist(settings[prefix + \"hosts\"])\n    return memcache.Client(hosts)\n\n\nclass Cache(CacheBase):\n    \"\"\"Cache backend implementation using Redis.\n\n    Enable in configuration::\n\n        kinto.cache_backend = kinto.core.cache.memcached\n\n    *(Optional)* Instance location URI can be customized::\n\n        kinto.cache_hosts = 127.0.0.1:11211 127.0.0.1:11212\n\n    :noindex:\n\n    \"\"\"\n\n    def __init__(self, client, *args, **kwargs):\n        super(Cache, self).__init__(*args, **kwargs)\n        self._client = client\n\n    def initialize_schema(self, dry_run=False):\n        # Nothing to do.\n        pass\n\n    @wrap_memcached_error\n    def flush(self):\n        self._client.flush_all()\n\n    @wrap_memcached_error\n    def _get(self, key):\n        value = self._client.get(self.prefix + key)\n        if not value:\n            return None, 0\n        data = json.loads(value)\n        return data[\"value\"], data[\"ttl\"]\n\n    def ttl(self, key):\n        _, ttl = self._get(key)\n        val = ttl - time()\n        return floor(val)\n\n    def get(self, key):\n        value, _ = self._get(key)\n        return value\n\n    @wrap_memcached_error\n    def expire(self, key, ttl):\n        if ttl == 0:\n            self.delete(key)\n        else:\n            # We can't use touch here because we need to update the TTL value in the record.\n            value = self.get(key)\n            self.set(key, value, ttl)\n\n    @wrap_memcached_error\n    def set(self, key, value, ttl):\n        if isinstance(value, bytes):\n            raise TypeError(\"a string-like object is required, not 'bytes'\")\n        value = json.dumps({\"value\": value, \"ttl\": ceil(time() + ttl)})\n        self._client.set(self.prefix + key, value, int(ttl))\n\n    @wrap_memcached_error\n    def delete(self, key):\n        value = self.get(key)\n        self._client.delete(self.prefix + key)\n        return value\n\n\ndef load_from_config(config):\n    settings = config.get_settings()\n    client = create_from_config(config, prefix=\"cache_\")\n    return Cache(client, cache_prefix=settings[\"cache_prefix\"])\n","lang_cluster":"Python","length":110,"code_uid":"5193fc5fde274c819f139cde4b101148"}
{"diff_hunk":"@@ -45,31 +45,26 @@ def _assemble_request_line(request_data):\n     Args:\n         request_data (mitmproxy.net.http.request.RequestData)\n     \"\"\"\n-    form = request_data.first_line_format\n-    if form == \"relative\":\n+    if request_data.method.upper() == b\"CONNECT\":\n         return b\"%s %s %s\" % (\n             request_data.method,\n-            request_data.path,\n+            request_data.authority,\n             request_data.http_version\n         )\n-    elif form == \"authority\":\n-        return b\"%s %s:%d %s\" % (\n+    elif request_data.authority:\n+        return b\"%s %s:\/\/%s%s %s\" % (\n             request_data.method,\n-            request_data.host,\n-            request_data.port,\n+            request_data.scheme,\n+            request_data.authority,\n+            request_data.path,\n             request_data.http_version\n         )\n-    elif form == \"absolute\":\n-        return b\"%s %s:\/\/%s:%d%s %s\" % (\n+    else:\n+        return b\"%s %s %s\" % (\n             request_data.method,\n-            request_data.scheme,\n-            request_data.host,\n-            request_data.port,\n             request_data.path,\n             request_data.http_version\n         )\n-    else:\n-        raise RuntimeError(\"Invalid request form\")\n \n \n def _assemble_request_headers(request_data):","old_code":"from mitmproxy import exceptions\n\n\ndef assemble_request(request):\n    if request.data.content is None:\n        raise exceptions.HttpException(\"Cannot assemble flow with missing content\")\n    head = assemble_request_head(request)\n    body = b\"\".join(assemble_body(request.data.headers, [request.data.content]))\n    return head + body\n\n\ndef assemble_request_head(request):\n    first_line = _assemble_request_line(request.data)\n    headers = _assemble_request_headers(request.data)\n    return b\"%s\\r\\n%s\\r\\n\" % (first_line, headers)\n\n\ndef assemble_response(response):\n    if response.data.content is None:\n        raise exceptions.HttpException(\"Cannot assemble flow with missing content\")\n    head = assemble_response_head(response)\n    body = b\"\".join(assemble_body(response.data.headers, [response.data.content]))\n    return head + body\n\n\ndef assemble_response_head(response):\n    first_line = _assemble_response_line(response.data)\n    headers = _assemble_response_headers(response.data)\n    return b\"%s\\r\\n%s\\r\\n\" % (first_line, headers)\n\n\ndef assemble_body(headers, body_chunks):\n    if \"chunked\" in headers.get(\"transfer-encoding\", \"\").lower():\n        for chunk in body_chunks:\n            if chunk:\n                yield b\"%x\\r\\n%s\\r\\n\" % (len(chunk), chunk)\n        yield b\"0\\r\\n\\r\\n\"\n    else:\n        for chunk in body_chunks:\n            yield chunk\n\n\ndef _assemble_request_line(request_data):\n    \"\"\"\n    Args:\n        request_data (mitmproxy.net.http.request.RequestData)\n    \"\"\"\n    form = request_data.first_line_format\n    if form == \"relative\":\n        return b\"%s %s %s\" % (\n            request_data.method,\n            request_data.path,\n            request_data.http_version\n        )\n    elif form == \"authority\":\n        return b\"%s %s:%d %s\" % (\n            request_data.method,\n            request_data.host,\n            request_data.port,\n            request_data.http_version\n        )\n    elif form == \"absolute\":\n        return b\"%s %s:\/\/%s:%d%s %s\" % (\n            request_data.method,\n            request_data.scheme,\n            request_data.host,\n            request_data.port,\n            request_data.path,\n            request_data.http_version\n        )\n    else:\n        raise RuntimeError(\"Invalid request form\")\n\n\ndef _assemble_request_headers(request_data):\n    \"\"\"\n    Args:\n        request_data (mitmproxy.net.http.request.RequestData)\n    \"\"\"\n    return bytes(request_data.headers)\n\n\ndef _assemble_response_line(response_data):\n    return b\"%s %d %s\" % (\n        response_data.http_version,\n        response_data.status_code,\n        response_data.reason,\n    )\n\n\ndef _assemble_response_headers(response):\n    return bytes(response.headers)\n","lang_cluster":"Python","length":92,"code_uid":"f274fc33e1ca4465b1c3258ee567fce5"}
{"diff_hunk":"@@ -2,17 +2,15 @@ from setuptools import setup, find_packages\n \n def readme():\n     readme_short = \"\"\"\n-    [Quilt] is a data package manager.\n-    `quilt` is a command-line tool that builds, retrieves, and stores\n-    data packages. A data package is a namespace of binary data frames\n-    (and files).\n+    Quilt is a data package manager.\n \n-    `quilt` works in conjunction with a server-side registry,\n-    not covered in this document. `quilt` currently pushes to and pulls from\n-    the registry at [quiltdata.com](https:\/\/quiltdata.com\/). In the near\n-    future users will be able to browse packages in the registry. You can\n-    use the registry to install data packages from the community, or publish\n-    packages for others to use.\n+    `quilt` is a command-line tool that builds, pushes, and installs\n+    data packages. A [data package](https:\/\/blog.quiltdata.com\/data-packages-for-fast-reproducible-python-analysis-c74b78015c7f)\n+    is a versioned bundle of serialized data wrapped in a Python module.\n+\n+    `quilt` pushes to and pulls from the data registry at [quiltdata.com](https:\/\/quiltdata.com\/).\n+\n+    Visit [quiltdata.com](https:\/\/quiltdata.com) for docs and more.\n     \"\"\"\n     return readme_short\n ","old_code":"from setuptools import setup, find_packages\n\ndef readme():\n    readme_short = \"\"\"\n    [Quilt] is a data package manager.\n    `quilt` is a command-line tool that builds, retrieves, and stores\n    data packages. A data package is a namespace of binary data frames\n    (and files).\n\n    `quilt` works in conjunction with a server-side registry,\n    not covered in this document. `quilt` currently pushes to and pulls from\n    the registry at [quiltdata.com](https:\/\/quiltdata.com\/). In the near\n    future users will be able to browse packages in the registry. You can\n    use the registry to install data packages from the community, or publish\n    packages for others to use.\n    \"\"\"\n    return readme_short\n\nsetup(\n    name=\"quilt\",\n    version=\"2.4.1\",\n    packages=find_packages(),\n    description='Quilt is an open-source data frame registry',\n    long_description=readme(),\n    classifiers=[\n        'Development Status :: 3 - Alpha',\n        'Intended Audience :: Developers',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3.6',\n    ],\n    author='quiltdata',\n    author_email='contact@quiltdata.io',\n    license='LICENSE',\n    url='https:\/\/github.com\/quiltdata\/quilt',\n    download_url='https:\/\/github.com\/quiltdata\/quilt\/releases\/tag\/2.4.1-beta',\n    keywords='quilt quiltdata shareable data dataframe package platform pandas',\n    install_requires=[\n        'appdirs>=1.4.0',\n        'future>=0.16.0',\n        'packaging>=16.8',\n        'pandas>=0.19.2',\n        'pyOpenSSL>=16.2.0',\n        'pyyaml>=3.12',\n        'requests>=2.12.4',\n        'responses>=0.5.1',\n        'six>=1.10.0',\n        'tables>=3.3.0',\n        'tqdm>=4.11.2',\n        'xlrd>=1.0.0',\n    ],\n    include_package_data=True,\n    entry_points={\n        'console_scripts': ['quilt=quilt.tools.main:main'],\n    }\n)\n","lang_cluster":"Python","length":57,"code_uid":"c0c8eb56520543c7b230f865d8c044b0"}
{"diff_hunk":"@@ -5,43 +5,9 @@\n #See the file COPYING for more details.\n \n import os\n+from buildVersion import *\n \n-def _updateVersionFromVCS():\n-\t\"\"\"Update the version from version control system metadata if possible.\n-\t\"\"\"\n-\tglobal version\n-\t# The root of the Git working tree will be the parent of this module's directory.\n-\tgitDir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \".git\")\n-\ttry:\n-\t\thead = file(os.path.join(gitDir, \"HEAD\"), \"r\").read().rstrip()\n-\t\tif not head.startswith(\"ref: \"):\n-\t\t\t# Detached head.\n-\t\t\tversion = \"source-DETACHED-%s\" % head[:7]\n-\t\t\treturn\n-\t\t# Strip the \"ref: \" prefix to get the ref.\n-\t\tref = head[5:]\n-\t\tcommit = file(os.path.join(gitDir, ref), \"r\").read().rstrip()\n-\t\tversion = \"source-%s-%s\" % (\n-\t\t\tos.path.basename(ref),\n-\t\t\tcommit[:7])\n-\texcept:\n-\t\tpass\n-\n-# ticket:3763#comment:19: name must be str, not unicode.\n-# Otherwise, py2exe will break.\n-name=\"NVDA\"\n longName=_(\"NonVisual Desktop Access\")\n-version_year=2017\n-version_major=4\n-version_minor=0\n-version_build=0\n-version=\"%s.%s.%sdev\"%(version_year,version_major,version_minor)\n-publisher=\"unknown\"\n-updateVersionType=None\n-try:\n-\tfrom _buildVersion import version, publisher, updateVersionType, version_build\n-except ImportError:\n-\t_updateVersionFromVCS()\n description=_(\"A free and open source screen reader for Microsoft Windows\")\n url=\"http:\/\/www.nvaccess.org\/\"\n copyrightYears=\"2006-2017\"","old_code":"#versionInfo.py\r\n#A part of NonVisual Desktop Access (NVDA)\r\n#Copyright (C) 2006-2016 NV Access Limited\r\n#This file is covered by the GNU General Public License.\r\n#See the file COPYING for more details.\r\n\r\nimport os\r\n\r\ndef _updateVersionFromVCS():\r\n\t\"\"\"Update the version from version control system metadata if possible.\r\n\t\"\"\"\r\n\tglobal version\r\n\t# The root of the Git working tree will be the parent of this module's directory.\r\n\tgitDir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \".git\")\r\n\ttry:\r\n\t\thead = file(os.path.join(gitDir, \"HEAD\"), \"r\").read().rstrip()\r\n\t\tif not head.startswith(\"ref: \"):\r\n\t\t\t# Detached head.\r\n\t\t\tversion = \"source-DETACHED-%s\" % head[:7]\r\n\t\t\treturn\r\n\t\t# Strip the \"ref: \" prefix to get the ref.\r\n\t\tref = head[5:]\r\n\t\tcommit = file(os.path.join(gitDir, ref), \"r\").read().rstrip()\r\n\t\tversion = \"source-%s-%s\" % (\r\n\t\t\tos.path.basename(ref),\r\n\t\t\tcommit[:7])\r\n\texcept:\r\n\t\tpass\r\n\r\n# ticket:3763#comment:19: name must be str, not unicode.\r\n# Otherwise, py2exe will break.\r\nname=\"NVDA\"\r\nlongName=_(\"NonVisual Desktop Access\")\r\nversion_year=2017\r\nversion_major=4\r\nversion_minor=0\r\nversion_build=0\r\nversion=\"%s.%s.%sdev\"%(version_year,version_major,version_minor)\r\npublisher=\"unknown\"\r\nupdateVersionType=None\r\ntry:\r\n\tfrom _buildVersion import version, publisher, updateVersionType, version_build\r\nexcept ImportError:\r\n\t_updateVersionFromVCS()\r\ndescription=_(\"A free and open source screen reader for Microsoft Windows\")\r\nurl=\"http:\/\/www.nvaccess.org\/\"\r\ncopyrightYears=\"2006-2017\"\r\ncopyright=_(\"Copyright (C) {years} NVDA Contributors\").format(\r\n\tyears=copyrightYears)\r\naboutMessage=_(u\"\"\"{longName} ({name})\r\nVersion: {version}\r\nURL: {url}\r\n{copyright}\r\n\r\n{name} is covered by the GNU General Public License (Version 2). You are free to share or change this software in any way you like as long as it is accompanied by the license and you make all source code available to anyone who wants it. This applies to both original and modified copies of this software, plus any derivative works.\r\nFor further details, you can view the license from the Help menu.\r\nIt can also be viewed online at: http:\/\/www.gnu.org\/licenses\/old-licenses\/gpl-2.0.html\r\n\r\n{name} is developed by NV Access, a non-profit organisation committed to helping and promoting free and open source solutions for blind and vision impaired people.\r\nIf you find NVDA useful and want it to continue to improve, please consider donating to NV Access. You can do this by selecting Donate from the NVDA menu.\"\"\").format(**globals())\r\n\r\n# A test version is anything other than a final or rc release.\r\nisTestVersion = not version[0].isdigit() or \"alpha\" in version or \"beta\" in version or \"dev\" in version\r\n","lang_cluster":"Python","length":63,"code_uid":"0924cbd2112c4c94b43eb55f8e008976"}
{"diff_hunk":"@@ -1,53 +1,56 @@\n from dagit.schema import dauphin\n from dagit.schema import model\n from ..version import __version__\n+from dagster.core.execution import ExecutionSelector\n \n \n class DauphinQuery(dauphin.ObjectType):\n     class Meta:\n-        name = 'Query'\n+        name = \"Query\"\n \n     version = dauphin.NonNull(dauphin.String)\n     pipelineOrError = dauphin.Field(\n-        dauphin.NonNull('PipelineOrError'),\n-        name=dauphin.NonNull(dauphin.String),\n-        solidSubset=dauphin.Argument(dauphin.List(dauphin.NonNull(dauphin.String)), required=False),\n+        dauphin.NonNull(\"PipelineOrError\"), params=dauphin.NonNull(\"ExecutionSelector\")\n     )\n     pipeline = dauphin.Field(\n-        dauphin.NonNull('Pipeline'),\n-        name=dauphin.NonNull(dauphin.String),\n-        solidSubset=dauphin.Argument(dauphin.List(dauphin.NonNull(dauphin.String)), required=False),\n+        dauphin.NonNull(\"Pipeline\"), params=dauphin.NonNull(\"ExecutionSelector\")\n     )\n-    pipelinesOrError = dauphin.NonNull('PipelinesOrError')\n-    pipelines = dauphin.Field(dauphin.NonNull('PipelineConnection'))\n+    pipelinesOrError = dauphin.NonNull(\"PipelinesOrError\")\n+    pipelines = dauphin.Field(dauphin.NonNull(\"PipelineConnection\"))\n \n     type = dauphin.Field(\n-        'Type',\n+        \"Type\",\n         pipelineName=dauphin.NonNull(dauphin.String),\n         typeName=dauphin.Argument(dauphin.NonNull(dauphin.String)),\n     )\n \n-    pipelineRuns = dauphin.non_null_list('PipelineRun')\n-    pipelineRun = dauphin.Field('PipelineRun', runId=dauphin.NonNull(dauphin.ID))\n+    pipelineRuns = dauphin.non_null_list(\"PipelineRun\")\n+    pipelineRun = dauphin.Field(\"PipelineRun\", runId=dauphin.NonNull(dauphin.ID))\n \n     isPipelineConfigValid = dauphin.Field(\n-        dauphin.NonNull('PipelineConfigValidationResult'),\n-        args={'executionParams': dauphin.Argument(dauphin.NonNull('PipelineExecutionParams'))},\n+        dauphin.NonNull(\"PipelineConfigValidationResult\"),\n+        args={\n+            \"pipeline\": dauphin.Argument(dauphin.NonNull(\"ExecutionSelector\")),\n+            \"config\": dauphin.Argument(\"PipelineConfig\"),\n+        },\n     )\n \n     executionPlan = dauphin.Field(\n-        dauphin.NonNull('ExecutionPlanResult'),\n-        args={'executionParams': dauphin.Argument(dauphin.NonNull('PipelineExecutionParams'))},\n+        dauphin.NonNull(\"ExecutionPlanResult\"),\n+        args={\n+            \"pipeline\": dauphin.Argument(dauphin.NonNull(\"ExecutionSelector\")),\n+            \"config\": dauphin.Argument(\"PipelineConfig\"),\n+        },\n     )\n \n     def resolve_version(self, _info):\n         return __version__\n \n     def resolve_pipelineOrError(self, info, **kwargs):\n-        return model.get_pipeline(info, kwargs['name'], kwargs.get('solidSubset'))\n+        return model.get_pipeline(info, kwargs[\"params\"].to_selector())\n \n     def resolve_pipeline(self, info, **kwargs):\n-        return model.get_pipeline_or_raise(info, kwargs['name'], kwargs.get('solidSubset'))\n+        return model.get_pipeline_or_raise(info, kwargs[\"params\"].to_selector())\n \n     def resolve_pipelinesOrError(self, info):\n         return model.get_pipelines(info)","old_code":"from dagit.schema import dauphin\nfrom dagit.schema import model\nfrom ..version import __version__\n\n\nclass DauphinQuery(dauphin.ObjectType):\n    class Meta:\n        name = 'Query'\n\n    version = dauphin.NonNull(dauphin.String)\n    pipelineOrError = dauphin.Field(\n        dauphin.NonNull('PipelineOrError'),\n        name=dauphin.NonNull(dauphin.String),\n        solidSubset=dauphin.Argument(dauphin.List(dauphin.NonNull(dauphin.String)), required=False),\n    )\n    pipeline = dauphin.Field(\n        dauphin.NonNull('Pipeline'),\n        name=dauphin.NonNull(dauphin.String),\n        solidSubset=dauphin.Argument(dauphin.List(dauphin.NonNull(dauphin.String)), required=False),\n    )\n    pipelinesOrError = dauphin.NonNull('PipelinesOrError')\n    pipelines = dauphin.Field(dauphin.NonNull('PipelineConnection'))\n\n    type = dauphin.Field(\n        'Type',\n        pipelineName=dauphin.NonNull(dauphin.String),\n        typeName=dauphin.Argument(dauphin.NonNull(dauphin.String)),\n    )\n\n    pipelineRuns = dauphin.non_null_list('PipelineRun')\n    pipelineRun = dauphin.Field('PipelineRun', runId=dauphin.NonNull(dauphin.ID))\n\n    isPipelineConfigValid = dauphin.Field(\n        dauphin.NonNull('PipelineConfigValidationResult'),\n        args={'executionParams': dauphin.Argument(dauphin.NonNull('PipelineExecutionParams'))},\n    )\n\n    executionPlan = dauphin.Field(\n        dauphin.NonNull('ExecutionPlanResult'),\n        args={'executionParams': dauphin.Argument(dauphin.NonNull('PipelineExecutionParams'))},\n    )\n\n    def resolve_version(self, _info):\n        return __version__\n\n    def resolve_pipelineOrError(self, info, **kwargs):\n        return model.get_pipeline(info, kwargs['name'], kwargs.get('solidSubset'))\n\n    def resolve_pipeline(self, info, **kwargs):\n        return model.get_pipeline_or_raise(info, kwargs['name'], kwargs.get('solidSubset'))\n\n    def resolve_pipelinesOrError(self, info):\n        return model.get_pipelines(info)\n\n    def resolve_pipelines(self, info):\n        return model.get_pipelines_or_raise(info)\n\n    def resolve_type(self, info, pipelineName, typeName):\n        return model.get_pipeline_type(info, pipelineName, typeName)\n\n    def resolve_pipelineRuns(self, info):\n        return model.get_runs(info)\n\n    def resolve_pipelineRun(self, info, runId):\n        return model.get_run(info, runId)\n\n    def resolve_isPipelineConfigValid(self, info, executionParams):\n        return model.validate_pipeline_config(info, **executionParams)\n\n    def resolve_executionPlan(self, info, executionParams):\n        return model.get_execution_plan(info, **executionParams)\n\n\nclass StartPipelineExecutionMutation(dauphin.Mutation):\n    class Meta:\n        name = 'StartPipelineExecutionMutation'\n\n    class Arguments:\n        executionParams = dauphin.NonNull('PipelineExecutionParams')\n\n    Output = dauphin.NonNull('StartPipelineExecutionResult')\n\n    def mutate(self, info, executionParams):\n        return model.start_pipeline_execution(info, **executionParams)\n\n\nclass DauphinMutation(dauphin.ObjectType):\n    class Meta:\n        name = 'Mutation'\n\n    start_pipeline_execution = StartPipelineExecutionMutation.Field()\n\n\nclass DauphinSubscription(dauphin.ObjectType):\n    class Meta:\n        name = 'Subscription'\n\n    pipelineRunLogs = dauphin.Field(\n        dauphin.NonNull('PipelineRunLogsSubscriptionPayload'),\n        runId=dauphin.Argument(dauphin.NonNull(dauphin.ID)),\n        after=dauphin.Argument('Cursor'),\n    )\n\n    def resolve_pipelineRunLogs(self, info, runId, after=None):\n        return model.get_pipeline_run_observable(info, runId, after)\n\n\nclass DauphinPipelineExecutionParams(dauphin.InputObjectType):\n    class Meta:\n        name = 'PipelineExecutionParams'\n\n    pipelineName = dauphin.NonNull(dauphin.String)\n    config = dauphin.Field('PipelineConfig')\n\n\nclass DauphinPipelineConfig(dauphin.GenericScalar, dauphin.Scalar):\n    class Meta:\n        name = 'PipelineConfig'\n        description = '''This type is used when passing in a configuration object\n        for pipeline configuration. This is any-typed in the GraphQL type system,\n        but must conform to the constraints of the dagster config type system'''\n","lang_cluster":"Python","length":121,"code_uid":"032eaf40517f49e580688afe2e843026"}
{"diff_hunk":"@@ -15,10 +15,17 @@ module Bolt\n         @object_open = true\n       end\n \n+      def print_event(node, event)\n+        case event[:type]\n+        when :node_result\n+          print_result(node, event[:result])\n+        end\n+      end\n+\n       def print_result(node, result)\n         item = {\n           name: node.uri,\n-          status: result.is_a?(Bolt::ErrorResult) ? 'failure' : 'success',\n+          status: result.success? ? 'success' : 'failure',\n           result: result.to_result\n         }\n ","old_code":"module Bolt\n  class Outputter\n    class JSON < Bolt::Outputter\n      def initialize(stream = $stdout)\n        @items_open = false\n        @object_open = false\n        @preceding_item = false\n        super(stream)\n      end\n\n      def print_head\n        @stream.puts '{ \"items\": ['\n        @preceding_item = false\n        @items_open = true\n        @object_open = true\n      end\n\n      def print_result(node, result)\n        item = {\n          name: node.uri,\n          status: result.is_a?(Bolt::ErrorResult) ? 'failure' : 'success',\n          result: result.to_result\n        }\n\n        @stream.puts ',' if @preceding_item\n        @stream.puts item.to_json\n        @preceding_item = true\n      end\n\n      def print_summary(results, elapsed_time)\n        @stream.puts \"],\\n\"\n        @preceding_item = false\n        @items_open = false\n        @stream.puts format('\"node_count\": %d, \"elapsed_time\": %d }',\n                            results.size,\n                            elapsed_time)\n      end\n\n      def print_plan(result)\n        @stream.puts result.to_json\n      end\n\n      def fatal_error(e)\n        @stream.puts \"],\\n\" if @items_open\n        @stream.puts '\"_error\": ' if @object_open\n        @stream.puts e.to_json\n        @stream.puts '}' if @object_open\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":51,"code_uid":"48a7407e6aa44bd1ae5017a554aff22f"}
{"diff_hunk":"@@ -1,8 +1,14 @@\n+<% content_for :additional_header_links do %>\n+  <% if current_user.has_subscription_with_mentor? %>\n+    <li class=\"mentor\">\n+      <%= mentor_image(current_user.mentor) %>\n+      <%= mentor_contact_link(current_user.mentor) %>\n+    <\/li>\n+  <% end %>\n+<% end %>\n+\n <section class=\"workshops vertical-slider revealed\">\n-  <figure class=\"meta product-card\">\n-    <%= render 'mentor' %>\n-    <%= render 'trails' %>\n-  <\/figure>\n+  <%= render 'trails' %>\n   <%= render partial: 'products\/workshop', collection: online_workshops %>\n   <%= render partial: 'products\/workshop', collection: in_person_workshops %>\n <\/section>","old_code":"<section class=\"workshops vertical-slider revealed\">\n  <figure class=\"meta product-card\">\n    <%= render 'mentor' %>\n    <%= render 'trails' %>\n  <\/figure>\n  <%= render partial: 'products\/workshop', collection: online_workshops %>\n  <%= render partial: 'products\/workshop', collection: in_person_workshops %>\n<\/section>\n\n<section class=\"screencasts vertical-slider revealed\">\n  <%= render partial: 'products\/video', collection: videos %>\n<\/section>\n\n<section class=\"reading vertical-slider revealed\">\n  <%= render partial: 'products\/book', collection: books %>\n<\/section>\n\n<footer>\n  <%= link_to forum_url, target: '_blank' do %>\n    <span>Get answers from us, collaborate with other members &mdash; View the <%= t('shared.subscription.name') %> forum &rarr;<\/span>\n  <% end %>\n<\/footer>\n","lang_cluster":"Ruby","length":22,"code_uid":"f62d5c2699ef4c78b2fb197a9082ca94"}
{"diff_hunk":"@@ -27,13 +27,16 @@ module Blacklight\n     EOS\n \n     def add_solr_wrapper\n-      if options[:jettywrapper]\n-        generate 'blacklight:solr4'\n-      elsif solr_version == 'latest'\n-        generate 'blacklight:solr5'\n-      else\n-        generate \"blacklight:solr#{solr_version}\"\n-      end\n+      generator_options = '--jettywrapper' if options[:jettywrapper]\n+      solr_generator = case\n+                       when options[:jettywrapper]\n+                         'blacklight:solr4'\n+                       when solr_version == 'latest'\n+                         'blacklight:solr5'\n+                       else\n+                         \"blacklight:solr#{solr_version}\"\n+                       end\n+      generate solr_generator, generator_options\n     end\n \n     def bundle_install","old_code":"# frozen_string_literal: true\nmodule Blacklight\n  class Install < Rails::Generators::Base\n    \n    source_root File.expand_path('..\/templates', __FILE__)\n    \n    argument     :model_name  , type: :string , default: \"user\"\n    argument     :controller_name, type: :string , default: \"catalog\"\n    argument     :document_name, type: :string , default: \"solr_document\"\n    argument     :search_builder_name, type: :string , default: \"search_builder\"\n    argument     :solr_version, type: :string , default: \"latest\"\n\n    class_option :devise      , type: :boolean, default: false, aliases: \"-d\", desc: \"Use Devise as authentication logic.\"\n    class_option :jettywrapper, type: :boolean, default: false, desc: \"Use jettywrapper to download and control Jetty\"\n    class_option :marc        , type: :boolean, default: false, aliases: \"-m\", desc: \"Generate MARC-based demo .\"\n\n    desc <<-EOS\n      This generator makes the following changes to your application:\n       1. Generates blacklight:models\n       2. Generates utilities for working with solr \n       3. Creates a number of public assets, including images, stylesheets, and javascript\n       4. Injects behavior into your user application_controller.rb\n       5. Adds example configurations for dealing with MARC-like data\n       6. Adds Blacklight routes to your .\/config\/routes.rb\n\n      Thank you for Installing Blacklight.\n    EOS\n\n    def add_solr_wrapper\n      if options[:jettywrapper]\n        generate 'blacklight:solr4'\n      elsif solr_version == 'latest'\n        generate 'blacklight:solr5'\n      else\n        generate \"blacklight:solr#{solr_version}\"\n      end\n    end\n\n    def bundle_install\n      Bundler.with_clean_env do\n        run \"bundle install\"\n      end\n    end\n\n    # Copy all files in templates\/public\/ directory to public\/\n    # Call external generator in AssetsGenerator, so we can\n    # leave that callable seperately too. \n    def copy_public_assets \n      generate \"blacklight:assets\"\n    end\n    \n    def generate_blacklight_document\n      generate 'blacklight:document', document_name\n    end\n\n    def generate_search_builder\n      generate 'blacklight:search_builder', search_builder_name\n    end\n\n    def generate_blacklight_models\n      generate 'blacklight:models'\n    end\n    \n    def generate_blacklight_user\n\n      generator_args = [model_name]\n      if options[:devise]\n        generator_args << \"--devise #{options[:devise]}\"\n      end\n      \n      generate 'blacklight:user', generator_args.join(\" \")\n    end\n\n    def generate_controller\n      generate 'blacklight:controller', controller_name\n    end\n    \n    def add_default_catalog_route\n      route(\"root to: \\\"#{controller_name}#index\\\"\")\n    end\n\n    def add_sass_configuration\n\n      insert_into_file \"config\/application.rb\", :after => \"config.assets.enabled = true\" do <<EOF\n\n      # Default SASS Configuration, check out https:\/\/github.com\/rails\/sass-rails for details\n      config.assets.compress = !Rails.env.development?\nEOF\n      end\n    end\n\n    def inject_blacklight_i18n_strings\n      copy_file \"blacklight.en.yml\", \"config\/locales\/blacklight.en.yml\"\n    end\n\n    def generate_blacklight_marc_demo\n      if options[:marc]\n        blacklight_marc = String.new('blacklight-marc')\n        gem blacklight_marc, '~> 6.0'\n\n        Bundler.with_clean_env do\n          run \"bundle install\"\n        end\n\n        generate 'blacklight:marc:install'\n      end\n    end\n\n    def add_routes\n      route \"mount Blacklight::Engine => '\/'\"\n    end\n  end\nend\n","lang_cluster":"Ruby","length":113,"code_uid":"1b67fca820fc48e384067195a791f509"}
{"diff_hunk":"@@ -123,13 +123,15 @@ module Bolt\n       end\n \n       def print_plan(result)\n-        # If a hash or array, pretty-print as JSON\n-        if result.is_a?(Hash) || result.is_a?(Array)\n-          if result.empty?\n-            # Avoids extra lines for an empty result\n+        # If the object has a json representation display it\n+        if result.respond_to?(:to_json)\n+          # Guard against to_json methods that don't accept options\n+          # and don't print empty results on multiple lines\n+          if result.method(:to_json).arity == 0 ||\n+             (result.respond_to?(:empty?) && result.empty?)\n             @stream.puts(result.to_json)\n           else\n-            @stream.puts(::JSON.pretty_generate(result))\n+            @stream.puts(::JSON.pretty_generate(result, quirks_mode: true))\n           end\n         else\n           @stream.puts result.to_s","old_code":"require 'terminal-table'\nmodule Bolt\n  class Outputter\n    class Human < Bolt::Outputter\n      COLORS = { red: \"31\",\n                 green: \"32\",\n                 yellow: \"33\" }.freeze\n\n      def print_head; end\n\n      def colorize(color, string)\n        if @stream.isatty\n          \"\\033[#{COLORS[color]}m#{string}\\033[0m\"\n        else\n          string\n        end\n      end\n\n      def indent(indent, string)\n        indent = ' ' * indent\n        string.gsub(\/^\/, indent.to_s)\n      end\n\n      def remove_trail(string)\n        string.sub(\/\\s\\z\/, '')\n      end\n\n      def print_event(event)\n        case event[:type]\n        when :node_start\n          print_start(event[:target])\n        when :node_result\n          print_result(event[:result])\n        end\n      end\n\n      def print_start(target)\n        @stream.puts(colorize(:green, \"Started on #{target.host}...\"))\n      end\n\n      def print_result(result)\n        if result.success?\n          @stream.puts(colorize(:green, \"Finished on #{result.target.host}:\"))\n        else\n          @stream.puts(colorize(:red, \"Failed on #{result.target.host}:\"))\n        end\n\n        if result.error_hash\n          @stream.puts(colorize(:red, remove_trail(indent(2, result.error_hash['msg']))))\n        end\n\n        if result.message\n          @stream.puts(remove_trail(indent(2, result.message)))\n        end\n\n        # There is more information to output\n        if result.generic_value\n          # Use special handling if the result looks like a command or script result\n          if result.generic_value.keys == %w[stdout stderr exit_code]\n            unless result['stdout'].strip.empty?\n              @stream.puts(indent(2, \"STDOUT:\"))\n              @stream.puts(indent(4, result['stdout']))\n            end\n            unless result['stderr'].strip.empty?\n              @stream.puts(indent(2, \"STDERR:\"))\n              @stream.puts(indent(4, result['stderr']))\n            end\n          else\n            @stream.puts(indent(2, ::JSON.pretty_generate(result.generic_value)))\n          end\n        end\n      end\n\n      def print_summary(results, elapsed_time)\n        @stream.puts format(\"Ran on %d node%s in %.2f seconds\",\n                            results.size,\n                            results.size == 1 ? '' : 's',\n                            elapsed_time)\n      end\n\n      def print_table(results)\n        @stream.puts Terminal::Table.new(\n          rows: results,\n          style: {\n            border_x: '',\n            border_y: '',\n            border_i: '',\n            padding_left: 0,\n            padding_right: 3,\n            border_top: false,\n            border_bottom: false\n          }\n        )\n      end\n\n      # @param [Hash] A hash representing the task\n      def print_task_info(task)\n        # Building lots of strings...\n        pretty_params = \"\"\n        task_info = \"\"\n        usage = \"bolt task run --nodes, -n <node-name> #{task['name']}\"\n\n        if task['parameters']\n          task['parameters'].each do |k, v|\n            pretty_params << \"- #{k}: #{v['type']}\\n\"\n            pretty_params << \"    #{v['description']}\\n\" if v['description']\n            usage << if !v['type'].to_s.include? \"Optional\"\n                       \" #{k}=<value>\"\n                     else\n                       \" [#{k}=<value>]\"\n                     end\n          end\n        end\n\n        usage << \" [--noop]\" if task['supports_noop']\n\n        task_info << \"\\n#{task['name']}\"\n        task_info << \" - #{task['description']}\" if task['description']\n        task_info << \"\\n\\n\"\n        task_info << \"USAGE:\\n#{usage}\\n\\n\"\n        task_info << \"PARAMETERS:\\n#{pretty_params}\\n\" if task['parameters']\n        @stream.puts(task_info)\n      end\n\n      def print_plan(result)\n        # If a hash or array, pretty-print as JSON\n        if result.is_a?(Hash) || result.is_a?(Array)\n          if result.empty?\n            # Avoids extra lines for an empty result\n            @stream.puts(result.to_json)\n          else\n            @stream.puts(::JSON.pretty_generate(result))\n          end\n        else\n          @stream.puts result.to_s\n        end\n      end\n\n      def fatal_error(e)\n        @stream.puts(colorize(:red, e.message))\n        if e.is_a? Bolt::RunFailure\n          @stream.puts ::JSON.pretty_generate(e.resultset)\n        end\n      end\n    end\n\n    def print_message(message)\n      @stream.puts(message)\n    end\n  end\nend\n","lang_cluster":"Ruby","length":151,"code_uid":"ff1edd8957f94c609d4d576f23762efa"}
{"diff_hunk":"@@ -1,4 +1,4 @@\n-\/\/snippet-sourcedescription:[DeleteServerCertificate.java demonstrates how to delete an AWS Identity and Access Management (IAM) server certificate.]\n+\/\/snippet-sourcedescription:[DeleteServerCertificate.java demonstrates how to delete an AWS Identity and Access Management (AWS IAM) server certificate.]\n \/\/snippet-keyword:[AWS SDK for Java v2]\n \/\/snippet-keyword:[Code Sample]\n \/\/snippet-service:[AWS IAM]","old_code":"\/\/snippet-sourcedescription:[DeleteServerCertificate.java demonstrates how to delete an AWS Identity and Access Management (IAM) server certificate.]\r\n\/\/snippet-keyword:[AWS SDK for Java v2]\r\n\/\/snippet-keyword:[Code Sample]\r\n\/\/snippet-service:[AWS IAM]\r\n\/\/snippet-sourcetype:[full-example]\r\n\/\/snippet-sourcedate:[11\/02\/2020]\r\n\/\/snippet-sourceauthor:[scmacdon-aws]\r\n\r\n\/*\r\n   Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\r\n   SPDX-License-Identifier: Apache-2.0\r\n*\/\r\npackage com.example.iam;\r\n\r\n\/\/ snippet-start:[iam.java2.delete_server_certificate.import]\r\nimport software.amazon.awssdk.services.iam.model.DeleteServerCertificateRequest;\r\nimport software.amazon.awssdk.regions.Region;\r\nimport software.amazon.awssdk.services.iam.IamClient;\r\nimport software.amazon.awssdk.services.iam.model.IamException;\r\n\/\/ snippet-end:[iam.java2.delete_server_certificate.import]\r\n\r\npublic class DeleteServerCertificate {\r\n    public static void main(String[] args) {\r\n\r\n        final String USAGE = \"\\n\" +\r\n                \"Usage:\\n\" +\r\n                \"    DeleteServerCertificate <certName> \\n\\n\" +\r\n                \"Where:\\n\" +\r\n                \"    certName - a certificate name to delete. \\n\\n\" ;\r\n\r\n        if (args.length != 1) {\r\n            System.out.println(USAGE);\r\n            System.exit(1);\r\n        }\r\n\r\n        \/\/ Read the command line argument\r\n        String certName = args[0];\r\n\r\n        Region region = Region.AWS_GLOBAL;\r\n        IamClient iam = IamClient.builder()\r\n                .region(region)\r\n                .build();\r\n\r\n        deleteCert(iam, certName) ;\r\n        System.out.println(\"Done\");\r\n        iam.close();\r\n    }\r\n\r\n    \/\/ snippet-start:[iam.java2.delete_server_certificate.main]\r\n    public static void deleteCert(IamClient iam,String certName ) {\r\n\r\n        try {\r\n            DeleteServerCertificateRequest request =\r\n                DeleteServerCertificateRequest.builder()\r\n                        .serverCertificateName(certName)\r\n                        .build();\r\n\r\n            iam.deleteServerCertificate(request);\r\n            System.out.println(\"Successfully deleted server certificate \" +\r\n                    certName);\r\n\r\n        } catch (IamException e) {\r\n            System.err.println(e.awsErrorDetails().errorMessage());\r\n            System.exit(1);\r\n        }\r\n    }\r\n    \/\/ snippet-end:[iam.java2.delete_server_certificate.main]\r\n}\r\n\r\n","lang_cluster":"Ruby","length":69,"code_uid":"208e29dbfb1b4e598290fb6ac78baa2f"}
{"diff_hunk":"@@ -11,11 +11,15 @@ feature 'Account Settings' do\n \n   scenario 'user views paid purchases' do\n     user = create(:user)\n-    create_list(:paid_purchase, 3, user: user)\n+    create_list(:paid_purchase, 4, user: user, created_at: 6.minutes.ago)\n+    purchase_two = create(:paid_purchase, user: user, created_at: 5.minutes.ago)\n+    purchase_one = create(:paid_purchase, user: user, created_at: 1.minutes.ago)\n \n     visit edit_my_account_path(as: user)\n \n     expect_to_see_my_paid_purchases(user)\n+    expect(purchase_one.purchaseable_name).\n+      to appear_before(purchase_two.purchaseable_name)\n   end\n \n   scenario 'user with no purchases' do","old_code":"require 'spec_helper'\n\nfeature 'Account Settings' do\n  scenario 'user views subscription' do\n    user = create(:subscriber)\n\n    visit edit_my_account_path(as: user)\n\n    expect_to_see_my_subscription\n  end\n\n  scenario 'user views paid purchases' do\n    user = create(:user)\n    create_list(:paid_purchase, 3, user: user)\n\n    visit edit_my_account_path(as: user)\n\n    expect_to_see_my_paid_purchases(user)\n  end\n\n  scenario 'user with no purchases' do\n    user = create(:user)\n\n    visit edit_my_account_path(as: user)\n\n    expect(page).not_to have_content 'Your purchases'\n  end\n\n  scenario 'user with only unpaid purchases' do\n    user = create(:user)\n    create_list(:unpaid_purchase, 3, user: user)\n\n    visit edit_my_account_path(as: user)\n\n    expect(page).not_to have_content 'Your purchases'\n  end\n\n  scenario 'user with paid and unpaid purchases' do\n    user = create(:user)\n    book = create(:book, name: 'A Great Book')\n    screencast = create(:screencast, name: 'A Great Video')\n    create(:paid_purchase, purchaseable: book, user: user)\n    create(:unpaid_purchase, purchaseable: screencast, user: user)\n\n    visit edit_my_account_path(as: user)\n\n    expect(page).to have_content book.name\n    expect(page).not_to have_content screencast.name\n  end\n\n  scenario 'user edits account information' do\n    user = create(:user, name: 'Test User')\n\n    visit edit_my_account_path(as: user)\n\n    fill_in 'Name', with: 'Change Name'\n    click_button 'Update account'\n\n    visit edit_my_account_path(as: user)\n\n    expect(field_labeled('Name').value).to eq 'Change Name'\n  end\n\n  private \n\n  def expect_to_see_my_subscription\n    expect(page).to have_css(\"ol.subscription li\")\n  end\n\n  def expect_to_see_my_paid_purchases(user)\n    expect(page).to have_css(\"ol.purchases li\", count: user.paid_purchases.count)\n  end\nend\n","lang_cluster":"Ruby","length":73,"code_uid":"fcc7fadf9dd44e448764d274b440a9b0"}
{"diff_hunk":"@@ -36,6 +36,7 @@ RSpec.configure do |config|\n   end\n \n   config.filter_run_excluding appveyor_agents: true unless ENV['APPVEYOR_AGENTS']\n+  config.filter_run_excluding windows: true unless ENV['BOLT_WINDOWS']\n \n   # rspec-mocks config\n   config.mock_with :rspec do |mocks|","old_code":"# frozen_string_literal: true\n\n# See http:\/\/rubydoc.info\/gems\/rspec-core\/RSpec\/Core\/Configuration\n\nrequire 'bolt'\nrequire 'bolt\/logger'\nrequire 'logging'\nrequire 'rspec\/logging_helper'\n# Make sure puppet is required for the 'reset puppet settings' context\nrequire 'puppet_pal'\n\nENV['RACK_ENV'] = 'test'\n$LOAD_PATH.unshift File.join(__dir__, 'lib')\n\nRSpec.shared_context 'reset puppet settings' do\n  after :each do\n    # reset puppet settings so that they can be initialized again\n    Puppet.settings.instance_exec do\n      clear_everything_for_tests\n    end\n  end\nend\n\nRSpec.configure do |config|\n  Bolt::Logger.initialize_logging\n  include RSpec::LoggingHelper\n  config.capture_log_messages\n\n  # rspec-expectations config\n  config.expect_with :rspec do |expectations|\n    #     be_bigger_than(2).and_smaller_than(4).description\n    #     # => \"be bigger than 2 and smaller than 4\"\n    # ...rather than:\n    #     # => \"be bigger than 2\"\n    expectations.include_chain_clauses_in_custom_matcher_descriptions = true\n  end\n\n  config.filter_run_excluding appveyor_agents: true unless ENV['APPVEYOR_AGENTS']\n\n  # rspec-mocks config\n  config.mock_with :rspec do |mocks|\n    # Prevents you from mocking or stubbing a method that does not exist on\n    # a real object.\n    mocks.verify_partial_doubles = true\n  end\n\n  config.before :each do\n    # Disable analytics while running tests\n    ENV['BOLT_DISABLE_ANALYTICS'] = 'true'\n  end\n\n  # This will be default in future rspec, leave it on\n  config.shared_context_metadata_behavior = :apply_to_host_groups\n\n  # Allows RSpec to persist some state between runs in order to support\n  # the `--only-failures` and `--next-failure` CLI options.\n  config.example_status_persistence_file_path = \"spec\/examples.txt\"\n\n  # config.warnings = true\n\n  # Make it possible to include the 'reset puppet settings' shared context\n  # in a group (or even an individual test) by specifying\n  # `:reset_puppet_settings' metadata on the group\/test\n  config.include_context 'reset puppet settings', :reset_puppet_settings\nend\n","lang_cluster":"Ruby","length":65,"code_uid":"79cd8212828b4423854d24bbbb975c67"}
{"diff_hunk":"@@ -6,18 +6,25 @@ class Approval < ActiveRecord::Base\n     end\n     state :actionable do\n       event :approve, transitions_to: :approved\n-      event :reject, transitions_to: :rejected\n     end\n     state :approved\n-    state :rejected\n+    \n+    # workflow doesn't touch active record\n+    # manually updating 'updated_at'\n+    # https:\/\/github.com\/geekq\/workflow\/issues\/96\n+    on_transition do |from, to, triggering_event, *event_args|\n+      self.touch\n+    end\n   end\n \n   belongs_to :proposal\n-  has_one :cart, through: :proposal\n   belongs_to :user\n+  belongs_to :parent, class_name: 'Approval'\n+  has_one :cart, through: :proposal\n   has_one :api_token, -> { fresh }\n   has_one :approval_group, through: :cart\n   has_one :user_role, -> { where(approval_group_id: cart.approval_group.id, user_id: self.user_id) }\n+  has_many :child_approvals, class_name: 'Approval', foreign_key: 'parent_id'\n \n   delegate :full_name, :email_address, :to => :user, :prefix => true\n   delegate :approvals, :to => :cart, :prefix => true","old_code":"class Approval < ActiveRecord::Base\n  include WorkflowModel\n  workflow do\n    state :pending do\n      event :make_actionable, transitions_to: :actionable\n    end\n    state :actionable do\n      event :approve, transitions_to: :approved\n      event :reject, transitions_to: :rejected\n    end\n    state :approved\n    state :rejected\n  end\n\n  belongs_to :proposal\n  has_one :cart, through: :proposal\n  belongs_to :user\n  has_one :api_token, -> { fresh }\n  has_one :approval_group, through: :cart\n  has_one :user_role, -> { where(approval_group_id: cart.approval_group.id, user_id: self.user_id) }\n\n  delegate :full_name, :email_address, :to => :user, :prefix => true\n  delegate :approvals, :to => :cart, :prefix => true\n\n  acts_as_list scope: :proposal\n\n  # TODO validates_uniqueness_of :user_id, scope: cart_id\n\n  self.statuses.each do |status|\n    scope status, -> { where(status: status) }\n  end\n\n  default_scope { order('position ASC') }\n\n  # TODO remove\n  def cart_id\n    self.proposal.cart.id\n  end\n\n  # TODO we should probably store this value\n  def approved_at\n    if self.approved?\n      self.updated_at\n    else\n      nil\n    end\n  end\n\n  # Used by the state machine\n  def on_rejected_entry(new_state, event)\n    self.proposal.reject!\n  end\n\n  # Used by the state machine\n  def on_approved_entry(new_state, event)\n    self.proposal.partial_approve!\n    Dispatcher.on_approval_approved(self)\n  end\nend\n","lang_cluster":"Ruby","length":59,"code_uid":"0c54cc1bd676457a9a81d58996850d5d"}
{"diff_hunk":"@@ -5,8 +5,15 @@ class Trail < ActiveRecord::Base\n \n   belongs_to :topic\n   has_many :statuses, as: :completeable, dependent: :destroy\n-  has_many :steps, -> { order \"position ASC\" }, dependent: :destroy\n-  has_many :exercises, through: :steps\n+  has_many :steps,\n+    -> { order \"position ASC\" },\n+    dependent: :destroy,\n+    inverse_of: :trail\n+  has_many :exercises,\n+    through: :steps,\n+    source: :completeable,\n+    source_type: \"Exercise\"\n+  has_many :videos, through: :steps, source: :completeable, source_type: \"Video\"\n \n   friendly_id :name, use: [:slugged, :finders]\n ","old_code":"class Trail < ActiveRecord::Base\n  extend FriendlyId\n\n  validates :name, :description, :topic, presence: true\n\n  belongs_to :topic\n  has_many :statuses, as: :completeable, dependent: :destroy\n  has_many :steps, -> { order \"position ASC\" }, dependent: :destroy\n  has_many :exercises, through: :steps\n\n  friendly_id :name, use: [:slugged, :finders]\n\n  def self.published\n    where(published: true)\n  end\n\n  # Override setters so it preserves the order\n  def exercise_ids=(new_exercise_ids)\n    super\n    new_exercise_ids = new_exercise_ids.reject(&:blank?).map(&:to_i)\n\n    new_exercise_ids.each_with_index do |exercise_id, index|\n      steps.where(exercise_id: exercise_id).update_all(position: index + 1)\n    end\n  end\n\n  def steps_remaining_for(user)\n    ExerciseWithProgressQuery.\n      new(user: user, exercises: exercises).\n      count { |exercise| exercise.state != Status::COMPLETE }\n  end\n\n  def update_state_for(user)\n    TrailWithProgress.new(self, user: user).update_status\n  end\n\n  def self.completed_for(user)\n    all.\n      map { |trail| TrailWithProgress.new(trail, user: user) }.\n      select(&:complete?)\n  end\n\n  def self.most_recent_published\n    order(created_at: :desc).published\n  end\nend\n","lang_cluster":"Ruby","length":46,"code_uid":"5d5d73780eca4944aa8e3157642768f2"}
{"diff_hunk":"@@ -1,8 +1,11 @@\n module TrailsHelper\n   def trail_breadcrumbs(trail, separator = \">\")\n-    [trail.topic, trail].map { |obj| link_to(obj, obj) }.\n-      unshift(link_to(\"Trails\", practice_path)).\n-      join(\" #{separator} \").html_safe\n+    topics_links = trail.topics.map { |topic| link_to(topic, topic) }\n+    links = [ link_to(\"Trails\", practice_path) ] +\n+      topics_links +\n+      [ link_to(trail, trail) ]\n+\n+    links.join(\" #{separator} \").html_safe\n   end\n \n   def completeable_link(completeable, &block)","old_code":"module TrailsHelper\n  def trail_breadcrumbs(trail, separator = \">\")\n    [trail.topic, trail].map { |obj| link_to(obj, obj) }.\n      unshift(link_to(\"Trails\", practice_path)).\n      join(\" #{separator} \").html_safe\n  end\n\n  def completeable_link(completeable, &block)\n    if completeable.is_a?(Exercise)\n      link_to completeable.url, title: completeable.name, &block\n    else\n      link_to completeable, title: completeable.name, &block\n    end\n  end\n\n  def trail_call_to_action(trail)\n    if current_user.subscriber?\n      start_trail_link(trail.first_completeable)\n    elsif current_user.sampler?\n      or_visit_trail(trail) { |video| start_trail_link(video) }\n    else\n      or_visit_trail(trail) { |video| auth_to_access_button(video) }\n    end\n  end\n\n  def or_visit_trail(trail)\n    trail.sample_video.map { |video|\n      yield video\n    }.unwrap_or(\n      visit_trail_link(trail)\n    )\n  end\n\n  def start_trail_link(url)\n    link_to(\n      t(\"trails.start_trail\"),\n      url,\n      class: \"start-trail cta-button small-button\",\n    )\n  end\n\n  def visit_trail_link(trail)\n    link_to(\n      t(\"trails.visit_trail\"),\n      trail,\n      class: \"cta-button small-button\",\n    )\n  end\n\n  def auth_to_access_button(video, cta_text: t(\"trails.start_for_free\"))\n    cta_classes = \"cta-button subscribe-cta light-bg\"\n    link_to video_auth_to_access_path(video), class: cta_classes do\n      image_tag(\"github-black.svg\", class: \"logo\", alt: \"\") + cta_text\n    end\n  end\nend\n","lang_cluster":"Ruby","length":56,"code_uid":"0426b809e06747f6a85140b033c89129"}
{"diff_hunk":"@@ -6,8 +6,8 @@ module Travis\n       class Scala < Jvm\n \n         DEFAULTS = {\n-          scala: '2.10.4',\n-          jdk:   'default'\n+          scala: '2.12.1',\n+          jdk:   'oraclejdk8'\n         }\n \n         SBT_PATH = '\/usr\/local\/bin\/sbt'","old_code":"require 'travis\/build\/script\/shared\/jvm'\n\nmodule Travis\n  module Build\n    class Script\n      class Scala < Jvm\n\n        DEFAULTS = {\n          scala: '2.10.4',\n          jdk:   'default'\n        }\n\n        SBT_PATH = '\/usr\/local\/bin\/sbt'\n        SBT_SHA  = '4ad1b8a325f75c1a66f3fd100635da5eb28d9c91'\n        SBT_URL  = \"https:\/\/raw.githubusercontent.com\/paulp\/sbt-extras\/#{SBT_SHA}\/sbt\"\n\n        def configure\n          super\n          if use_sbt?\n            sh.echo \"Updating sbt\", ansi: :green\n\n            update_sbt\n          end\n        end\n\n        def export\n          super\n          sh.export 'TRAVIS_SCALA_VERSION', version, echo: false\n        end\n\n        def setup\n          super\n          sh.if use_sbt? do\n            sh.export 'JVM_OPTS', '@\/etc\/sbt\/jvmopts', echo: true\n            sh.export 'SBT_OPTS', '@\/etc\/sbt\/sbtopts', echo: true\n          end\n        end\n\n        def announce\n          super\n          sh.echo \"Using Scala #{version}\"\n        end\n\n        def install\n          sh.if not_use_sbt? do\n            super\n          end\n        end\n\n        def script\n          sh.if use_sbt? do\n            sh.cmd \"sbt#{sbt_args} ++#{version} test\"\n          end\n          sh.else do\n            super\n          end\n        end\n\n        def cache_slug\n          super << \"--scala-\" << version\n        end\n\n        private\n\n          def version\n            config[:scala].to_s\n          end\n\n          def sbt_args\n            config[:sbt_args] && \" #{config[:sbt_args]}\"\n          end\n\n          def use_sbt?\n            '-d project || -f build.sbt'\n          end\n\n          def not_use_sbt?\n            '! -d project && ! -f build.sbt'\n          end\n\n          def update_sbt\n            return if app_host.empty?\n\n            sh.cmd \"curl -sf -o sbt.tmp https:\/\/#{app_host}\/files\/sbt\", echo: false\n            sh.if \"$? -ne 0\" do\n              sh.cmd \"curl -sf -o sbt.tmp #{SBT_URL}\", assert: true\n            end\n            sh.raw \"sed -e '\/addSbt \\\\(warn\\\\|info\\\\)\/d' sbt.tmp | sudo tee #{SBT_PATH} > \/dev\/null && rm -f sbt.tmp\"\n          end\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":93,"code_uid":"94e67f531d514c32aefcf3a04e21bf73"}
{"diff_hunk":"@@ -25,20 +25,24 @@ module Bolt\n     def to_json(opts = nil)\n       to_h.to_json(opts)\n     end\n+\n+    def to_puppet_error\n+      Puppet::DataTypes::Error.from_asserted_hash(to_h)\n+    end\n   end\n \n   class RunFailure < Error\n-    attr_reader :resultset\n+    attr_reader :result_set\n \n-    def initialize(resultset, action, object)\n+    def initialize(result_set, action, object)\n       details = {\n-        action: action,\n-        object: object,\n-        failed_targets: resultset.error_set.names\n+        'action' => action,\n+        'object' =>  object,\n+        'result_set' => result_set\n       }\n-      message = \"Plan aborted: #{action} '#{object}' failed on #{details[:failed_targets].length} nodes\"\n+      message = \"Plan aborted: #{action} '#{object}' failed on #{result_set.error_set.length} nodes\"\n       super(message, 'bolt\/run-failure', details)\n-      @resultset = resultset\n+      @result_set = result_set\n       @error_code = 2\n     end\n   end","old_code":"module Bolt\n  class Error < RuntimeError\n    attr_reader :kind, :details, :issue_code, :error_code\n\n    def initialize(msg, kind, details = nil, issue_code = nil)\n      super(msg)\n      @kind = kind\n      @issue_code = issue_code\n      @details = details || {}\n      @error_code ||= 1\n    end\n\n    def msg\n      message\n    end\n\n    def to_h\n      h = { 'kind' =>  kind,\n            'msg' => message,\n            'details' => details }\n      h['issue_code'] = issue_code if issue_code\n      h\n    end\n\n    def to_json(opts = nil)\n      to_h.to_json(opts)\n    end\n  end\n\n  class RunFailure < Error\n    attr_reader :resultset\n\n    def initialize(resultset, action, object)\n      details = {\n        action: action,\n        object: object,\n        failed_targets: resultset.error_set.names\n      }\n      message = \"Plan aborted: #{action} '#{object}' failed on #{details[:failed_targets].length} nodes\"\n      super(message, 'bolt\/run-failure', details)\n      @resultset = resultset\n      @error_code = 2\n    end\n  end\n\n  class PlanFailure < Error\n    def initialize(*args)\n      super(*args)\n      @error_code = 2\n    end\n  end\nend\n","lang_cluster":"Ruby","length":52,"code_uid":"ee5198b1b39a4f15aca7e9313bbfc550"}
{"diff_hunk":"@@ -4,11 +4,15 @@\n \n <header>\n   <div class=\"topic-image\">\n-    <%= image_tag \"topics\/#{topic.slug}.svg\" %>\n+    <%= link_to topic do %>\n+      <%= image_tag \"topics\/#{topic.slug}.svg\" %>\n+    <% end %>\n   <\/div>\n \n   <div class=\"topic-title\">\n-    <h2><%= trail.topic_name %><\/h2>\n+    <div class=\"trail-topic-name\">\n+      <%= render topic %>\n+    <\/div>\n     <h1><%= trail.name %><\/h1>\n     <p><%= trail.description %><\/p>\n   <\/div>","old_code":"<div class=\"breadcrumbs\">\n  <%= trail_breadcrumbs(trail) %>\n<\/div>\n\n<header>\n  <div class=\"topic-image\">\n    <%= image_tag \"topics\/#{topic.slug}.svg\" %>\n  <\/div>\n\n  <div class=\"topic-title\">\n    <h2><%= trail.topic_name %><\/h2>\n    <h1><%= trail.name %><\/h1>\n    <p><%= trail.description %><\/p>\n  <\/div>\n<\/header>\n","lang_cluster":"Ruby","length":15,"code_uid":"8138d236e85f4ccc8e0e39302c94fbff"}
{"diff_hunk":"@@ -1,12 +1,13 @@\n <% content_for :subject, @purchase.purchaseable_name %>\n \n-<h3 class=\"video-headline\">Watch or Download Video<\/h3>\n-<h2 class=\"video-headline\">\n-  <%= pluralize @purchase.purchaseable.videos.size, 'video' %> in the series\n-<\/h2>\n+<div class=\"text-box-wrapper\">\n+  <div class=\"text-box\">\n+    <h2>Watch or Download Video<\/h2>\n+    <p class=\"workshop-type\"><%= pluralize @purchaseable.videos.size, 'video' %> in the series<\/p>\n+    <section id=\"videos\">\n+      <%= render @purchaseable.videos.ordered, purchase: @purchase %>\n+    <\/section>\n+  <\/div>\n+<\/div>\n \n-<section id=\"videos\">\n-  <%= render @purchase.purchaseable.videos.ordered, purchase: @purchase %>\n-<\/section>\n-\n-<%= render 'videos\/footer' %>\n+<%= render sidebar_partial_name(@purchaseable), purchaseable: @purchaseable %>","old_code":"<% content_for :subject, @purchase.purchaseable_name %>\n\n<h3 class=\"video-headline\">Watch or Download Video<\/h3>\n<h2 class=\"video-headline\">\n  <%= pluralize @purchase.purchaseable.videos.size, 'video' %> in the series\n<\/h2>\n\n<section id=\"videos\">\n  <%= render @purchase.purchaseable.videos.ordered, purchase: @purchase %>\n<\/section>\n\n<%= render 'videos\/footer' %>\n","lang_cluster":"Ruby","length":12,"code_uid":"5d9655562de34c0bbb03c5776b36ca1b"}
{"diff_hunk":"@@ -1,8 +1,8 @@\n feature \"Inactive users\" do\n   scenario \"not included in approving official dropdown\" do\n-    inactive_approving_official = create(:user, :inactive)\n-    active_approving_official = create(:user, :active)\n-    user = create(:user)\n+    inactive_approving_official = create(:user, :inactive, client_slug: 'ncr')\n+    active_approving_official = create(:user, :active, client_slug: 'ncr')\n+    user = create(:user, client_slug: 'ncr')\n     login_as(user)\n \n     visit new_ncr_work_order_path","old_code":"feature \"Inactive users\" do\n  scenario \"not included in approving official dropdown\" do\n    inactive_approving_official = create(:user, :inactive)\n    active_approving_official = create(:user, :active)\n    user = create(:user)\n    login_as(user)\n\n    visit new_ncr_work_order_path\n\n    expect(page).to have_content(active_approving_official.email_address)\n    expect(page).not_to have_content(inactive_approving_official.email_address)\n  end\nend\n","lang_cluster":"Ruby","length":13,"code_uid":"f27dd342b39d40f290d3641c813a1d59"}
{"diff_hunk":"@@ -3,7 +3,7 @@\n   <%= render 'previous_next_doc' %>\n \n    \n-<% @page_title = t('blacklight.search.show.title', :document_title => document_show_html_title, :application_name => application_name) -%>\n+<% @page_title = t('blacklight.search.show.title', :document_title => document_show_html_title, :application_name => application_name).html_safe -%>\n <% content_for(:head) { render_link_rel_alternates } -%>\n <%# this should be in a partial -%>\n ","old_code":"<div id=\"content\" class=\"col-md-9 show-document\">\n\n  <%= render 'previous_next_doc' %>\n\n   \n<% @page_title = t('blacklight.search.show.title', :document_title => document_show_html_title, :application_name => application_name) -%>\n<% content_for(:head) { render_link_rel_alternates } -%>\n<%# this should be in a partial -%>\n\n<div id=\"document\" class=\"document <%= render_document_class %>\" itemscope  itemtype=\"<%= @document.itemtype %>\">\n  <div id=\"doc_<%= @document.id.to_s.parameterize %>\">\n  \n    <% # bookmark\/folder functions -%>\n    <%= render_document_partials @document, blacklight_config.view_config(:show).partials %>\n \n  <\/div>\n<\/div>\n\n\n\n  <% if @document.respond_to?(:export_as_openurl_ctx_kev) %>\n    <!-- \n         \/\/ COinS, for Zotero among others. \n         \/\/ This document_partial_name(@document) business is not quite right,\n         \/\/ but has been there for a while. \n    -->\n    <span class=\"Z3988\" title=\"<%= @document.export_as_openurl_ctx_kev(document_partial_name(@document)) %>\"><\/span>\n  <% end %>\n\n<\/div>\n\n<div id=\"sidebar\" class=\"col-md-3\">\n   <%= render_document_sidebar_partial %>\n<\/div>\n","lang_cluster":"Ruby","length":34,"code_uid":"46b47ff8a9124d7bafe8fe15d5264f84"}
{"diff_hunk":"@@ -5,12 +5,24 @@ module Travis\n     module Appliances\n       class UpdateAptKeys < Base\n         def apply\n-          command = <<-EOF\n-          if command -v apt-get &>\/dev\/null && [[ -d \/var\/lib\/apt\/lists ]]; then\n-            LANG=C apt-key list | awk -F'[ \/]+' '\/expired:\/{printf \"apt-key adv --recv-keys --keyserver keys.gnupg.net %s\\\\n\", $3}' | sudo sh &>\/dev\/null\n-          fi\n-          EOF\n-          sh.cmd command, echo: false\n+          sh.if '\"$TRAVIS_OS_NAME\" == linux' do\n+            command = <<~KEYUPDATE\n+            apt-key adv --list-public-keys --with-fingerprint --with-colons \\\\\n+              | awk -F: '\n+                  $1==\"pub\" && $2==\"e\" { state=\"expired\"}\n+                  $1==\"fpr\" && state == \"expired\" {\n+                    out = sprintf(\"%s %s\", out, $(NF -1))\n+                    state=\"valid\"\n+                  }\n+                  END {\n+                    if (length(out)>0)\n+                      printf \"sudo apt-key adv --recv-keys --keyserver keys.gnupg.net %s\", out\n+                  }\n+                ' \\\\\n+              | sh &>\/dev\/null\n+            KEYUPDATE\n+            sh.cmd command, echo: false\n+          end\n         end\n       end\n     end","old_code":"require 'travis\/build\/appliances\/base'\n\nmodule Travis\n  module Build\n    module Appliances\n      class UpdateAptKeys < Base\n        def apply\n          command = <<-EOF\n          if command -v apt-get &>\/dev\/null && [[ -d \/var\/lib\/apt\/lists ]]; then\n            LANG=C apt-key list | awk -F'[ \/]+' '\/expired:\/{printf \"apt-key adv --recv-keys --keyserver keys.gnupg.net %s\\\\n\", $3}' | sudo sh &>\/dev\/null\n          fi\n          EOF\n          sh.cmd command, echo: false\n        end\n      end\n    end\n  end\nend\n\n","lang_cluster":"Ruby","length":19,"code_uid":"d56b01980d7c42f2a24dcbd80a32fc16"}
{"diff_hunk":"@@ -1,2 +1 @@\n-GITHUB_USER = ENV['GITHUB_USER']\n-GITHUB_PASSWORD = ENV['GITHUB_PASSWORD']\n+GITHUB_ACCESS_TOKEN = ENV['GITHUB_ACCESS_TOKEN']","old_code":"GITHUB_USER = ENV['GITHUB_USER']\nGITHUB_PASSWORD = ENV['GITHUB_PASSWORD']\n","lang_cluster":"Ruby","length":2,"code_uid":"ced8364576704e798cf01f441a4fb212"}
{"diff_hunk":"@@ -18,17 +18,26 @@ module Bolt\n       format: 'human'\n     }.freeze\n \n-    TRANSPORT_OPTIONS = %i[insecure password run_as sudo_password extensions\n-                           key tty tmpdir user connect_timeout cacert\n+    TRANSPORT_OPTIONS = %i[host_key_check password run_as sudo_password extensions\n+                           ssl key tty tmpdir user connect_timeout cacert\n                            token_file orch_task_environment service_url].freeze\n \n     TRANSPORT_DEFAULTS = {\n       connect_timeout: 10,\n       orch_task_environment: 'production',\n-      insecure: false,\n       tty: false\n     }.freeze\n \n+    TRANSPORT_SPECIFIC_DEFAULTS = {\n+      ssh: {\n+        host_key_check: true\n+      },\n+      winrm: {\n+        ssl: true\n+      },\n+      pcp: {}\n+    }.freeze\n+\n     TRANSPORTS = %i[ssh winrm pcp].freeze\n \n     def initialize(**kwargs)","old_code":"require 'yaml'\nrequire 'bolt\/cli'\nrequire 'logging'\n\nmodule Bolt\n  Config = Struct.new(\n    :concurrency,\n    :format,\n    :log_level,\n    :modulepath,\n    :transport,\n    :transports\n  ) do\n\n    DEFAULTS = {\n      concurrency: 100,\n      transport: 'ssh',\n      format: 'human'\n    }.freeze\n\n    TRANSPORT_OPTIONS = %i[insecure password run_as sudo_password extensions\n                           key tty tmpdir user connect_timeout cacert\n                           token_file orch_task_environment service_url].freeze\n\n    TRANSPORT_DEFAULTS = {\n      connect_timeout: 10,\n      orch_task_environment: 'production',\n      insecure: false,\n      tty: false\n    }.freeze\n\n    TRANSPORTS = %i[ssh winrm pcp].freeze\n\n    def initialize(**kwargs)\n      super()\n      @logger = Logging.logger[self]\n      DEFAULTS.merge(kwargs).each { |k, v| self[k] = v }\n\n      self[:transports] ||= {}\n      TRANSPORTS.each do |transport|\n        unless self[:transports][transport]\n          self[:transports][transport] = {}\n        end\n        TRANSPORT_DEFAULTS.each do |k, v|\n          unless self[:transports][transport][k]\n            self[:transports][transport][k] = v\n          end\n        end\n      end\n    end\n\n    def default_paths\n      root_path = File.expand_path(File.join('~', '.puppetlabs'))\n      [File.join(root_path, 'bolt.yaml'), File.join(root_path, 'bolt.yml')]\n    end\n\n    def read_config_file(path)\n      path_passed = path\n      if path.nil?\n        found_default = default_paths.select { |p| File.exist?(p) }\n        if found_default.size > 1\n          @logger.warn \"Config files found at #{found_default.join(', ')}, using the first\"\n        end\n        # Use first found, fall back to first default and try to load even if it didn't exist\n        path = found_default.first || default_paths.first\n      end\n\n      path = File.expand_path(path)\n      # safe_load doesn't work with psych in ruby 2.0\n      # The user controls the configfile so this isn't a problem\n      # rubocop:disable YAMLLoad\n      File.open(path, \"r:UTF-8\") { |f| YAML.load(f.read) }\n    rescue Errno::ENOENT\n      if path_passed\n        raise Bolt::CLIError, \"Could not read config file: #{path}\"\n      end\n    # In older releases of psych SyntaxError is not a subclass of Exception\n    rescue Psych::SyntaxError\n      raise Bolt::CLIError, \"Could not parse config file: #{path}\"\n    rescue Psych::Exception\n      raise Bolt::CLIError, \"Could not parse config file: #{path}\"\n    rescue IOError, SystemCallError\n      raise Bolt::CLIError, \"Could not read config file: #{path}\"\n    end\n\n    def update_from_file(data)\n      if data['modulepath']\n        self[:modulepath] = data['modulepath'].split(File::PATH_SEPARATOR)\n      end\n\n      if data['concurrency']\n        self[:concurrency] = data['concurrency']\n      end\n\n      if data['format']\n        self[:format] = data['format'] if data['format']\n      end\n\n      if data['ssh']\n        if data['ssh']['private-key']\n          self[:transports][:ssh][:key] = data['ssh']['private-key']\n        end\n        if data['ssh']['insecure']\n          self[:transports][:ssh][:insecure] = data['ssh']['insecure']\n        end\n        if data['ssh']['connect-timeout']\n          self[:transports][:ssh][:connect_timeout] = data['ssh']['connect-timeout']\n        end\n        if data['ssh']['tmpdir']\n          self[:transports][:ssh][:tmpdir] = data['ssh']['tmpdir']\n        end\n        if data['ssh']['run-as']\n          self[:transports][:ssh][:run_as] = data['ssh']['run-as']\n        end\n      end\n\n      if data['winrm']\n        if data['winrm']['connect-timeout']\n          self[:transports][:winrm][:connect_timeout] = data['winrm']['connect-timeout']\n        end\n        if data['winrm']['insecure']\n          self[:transports][:winrm][:insecure] = data['winrm']['insecure']\n        end\n        if data['winrm']['tmpdir']\n          self[:transports][:winrm][:tmpdir] = data['winrm']['tmpdir']\n        end\n        if data['winrm']['cacert']\n          self[:transports][:winrm][:cacert] = data['winrm']['cacert']\n        end\n        if data['winrm']['extensions']\n          # Accept a single entry or a list, ensure each is prefixed with '.'\n          self[:transports][:winrm][:extensions] =\n            [data['winrm']['extensions']].flatten.map { |ext| ext[0] != '.' ? '.' + ext : ext }\n        end\n      end\n\n      if data['pcp']\n        if data['pcp']['service-url']\n          self[:transports][:pcp][:service_url] = data['pcp']['service-url']\n        end\n        if data['pcp']['cacert']\n          self[:transports][:pcp][:cacert] = data['pcp']['cacert']\n        end\n        if data['pcp']['token-file']\n          self[:transports][:pcp][:token_file] = data['pcp']['token-file']\n        end\n        if data['pcp']['task-environment']\n          self[:transports][:pcp][:orch_task_environment] = data['pcp']['task-environment']\n        end\n      end\n    end\n\n    def load_file(path)\n      data = read_config_file(path)\n      update_from_file(data) if data\n    end\n\n    def update_from_cli(options)\n      %i[concurrency transport format modulepath].each do |key|\n        self[key] = options[key] if options[key]\n      end\n\n      if options[:debug]\n        self[:log_level] = :debug\n      elsif options[:verbose]\n        self[:log_level] = :info\n      end\n\n      TRANSPORT_OPTIONS.each do |key|\n        # TODO: We should eventually make these transport specific\n        TRANSPORTS.each do |transport|\n          self[:transports][transport][key] = options[key] if options[key]\n        end\n      end\n    end\n\n    def validate\n      TRANSPORTS.each do |transport|\n        self[:transports][transport]\n      end\n\n      unless %w[human json].include? self[:format]\n        raise Bolt::CLIError, \"Unsupported format: '#{self[:format]}'\"\n      end\n\n      if self[:transports][:ssh][:sudo_password] && self[:transports][:ssh][:run_as].nil?\n        @logger.warn(\"--sudo-password will not be used without specifying a \" \\\n                     \"user to escalate to with --run-as\")\n      end\n\n      self[:transports].each_value do |v|\n        timeout_value = v[:connect_timeout]\n        unless timeout_value.is_a?(Integer) || timeout_value.nil?\n          error_msg = \"connect-timeout value must be an Integer, received #{timeout_value}:#{timeout_value.class}\"\n          raise Bolt::CLIError, error_msg\n        end\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":200,"code_uid":"f8af9da13ba54d3f8056734ad9949b6b"}
{"diff_hunk":"@@ -4,6 +4,32 @@ module Faker\n   class Games\n     class Minecraft < Base\n       class << self\n+        ##\n+        # Produces the name of an achievement from Minecraft.\n+        #\n+        # @return [String]\n+        #\n+        # @example\n+        #   Faker::Games::Minecraft.achievement #=> \"Time to Mine!\"\n+        #\n+        # @faker.version new\n+        def achievement\n+          fetch('games.minecraft.achievement')\n+        end\n+\n+        ##\n+        # Produces the name of a biome from Minecraft.\n+        #\n+        # @return [String]\n+        #\n+        # @example\n+        #   Faker::Games::Minecraft.biome #=> \"Jungle\"\n+        #\n+        # @faker.version new\n+        def biome\n+          fetch('games.minecraft.biome')\n+        end\n+\n         ##\n         # Produces the name of a block from Minecraft.\n         #","old_code":"# frozen_string_literal: true\n\nmodule Faker\n  class Games\n    class Minecraft < Base\n      class << self\n        ##\n        # Produces the name of a block from Minecraft.\n        #\n        # @return [String]\n        #\n        # @example\n        #   Faker::Games::Minecraft.block #=> \"Stone\"\n        #\n        # @faker.version 2.13.0\n        def block\n          fetch('games.minecraft.blocks')\n        end\n\n        ##\n        # Produces the name of an item from Minecraft.\n        #\n        # @return [String]\n        #\n        # @example\n        #   Faker::Games::Minecraft.item #=> \"Iron Shovel\"\n        #\n        # @faker.version 2.13.0\n        def item\n          fetch('games.minecraft.items')\n        end\n\n        ##\n        # Produces the name of a mob from Minecraft.\n        #\n        # @return [String]\n        #\n        # @example\n        #   Faker::Games::Minecraft.item #=> \"Sheep\"\n        #\n        # @faker.version 2.13.0\n        def mob\n          fetch('games.minecraft.mobs')\n        end\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":48,"code_uid":"2d841011b5124071a73fb351545c4826"}
{"diff_hunk":"@@ -18,8 +18,9 @@ class ProposalsController < ApplicationController\n \n   def index\n     @CLOSED_PROPOSAL_LIMIT = 10\n+\n     @pending_data = self.listing.pending\n-    @approved_data = self.listing.approved(@CLOSED_PROPOSAL_LIMIT)\n+    @approved_data = self.listing.approved.alter_query{ |rel| rel.limit(@CLOSED_PROPOSAL_LIMIT) }\n     @cancelled_data = self.listing.cancelled\n   end\n ","old_code":"class ProposalsController < ApplicationController\n  include TokenAuth\n\n  before_filter :authenticate_user!, except: :approve\n  # TODO use Policy for all actions\n  before_filter ->{authorize self.proposal}, only: [:show, :cancel, :cancel_form]\n  before_filter :needs_token_on_get, only: :approve\n  before_filter :validate_access, only: :approve\n  helper_method :display_status\n  add_template_helper ProposalsHelper\n  rescue_from Pundit::NotAuthorizedError, with: :auth_errors\n\n  def show\n    @proposal = self.proposal.decorate\n    @show_comments = true\n    @include_comments_files = true\n  end\n\n  def index\n    @CLOSED_PROPOSAL_LIMIT = 10\n    @pending_data = self.listing.pending\n    @approved_data = self.listing.approved(@CLOSED_PROPOSAL_LIMIT)\n    @cancelled_data = self.listing.cancelled\n  end\n\n  def archive\n    @proposals_data = self.listing.closed\n  end\n\n  def cancel_form\n    @proposal = self.proposal.decorate\n  end\n\n  def cancel\n    if params[:reason_input].present?\n      proposal = Proposal.find params[:id]\n      comments = \"Request cancelled with comments: \" + params[:reason_input]\n      proposal.cancel!\n      proposal.comments.create!(comment_text: comments, user_id: current_user.id)\n\n      flash[:success] = \"Your request has been cancelled\"\n      redirect_to proposal_path, id: proposal.id\n      Dispatcher.new.deliver_cancellation_emails(proposal)\n    else\n      redirect_to cancel_form_proposal_path, id: params[:id],\n                                             alert: \"A reason for cancellation is required.\n                                                     Please indicate why this request needs\n                                                     to be cancelled.\"\n    end\n  end\n\n  def approve\n    approval = self.proposal.existing_approval_for(current_user)\n    if approval.user.delegates_to?(current_user)\n      # assign them to the approval\n      approval.update_attributes!(user: current_user)\n    end\n\n    approval.approve!\n    flash[:success] = \"You have approved #{proposal.public_identifier}.\"\n    redirect_to proposal\n  end\n\n  # @todo - this is acting more like an index; rename existing #index to #mine\n  # or similar, then rename #query to #index\n  def query\n    query_listing = self.listing\n    @proposals_data = query_listing.query\n\n    @text = params[:text]\n    @start_date = query_listing.start_date\n    @end_date = query_listing.end_date\n  end\n\n\n  protected\n\n  def proposal\n    @cached_proposal ||= Proposal.find params[:id]\n  end\n\n  def auth_errors(exception)\n    if ['cancel','cancel_form'].include? params[:action]\n      redirect_to proposal_path, :alert => exception.message\n    else\n      super\n    end\n  end\n\n  def listing\n    Query::Proposal::Listing.new(current_user, params)\n  end\nend\n","lang_cluster":"Ruby","length":93,"code_uid":"50085176ae9a42ad83b07cb307902f74"}
{"diff_hunk":"@@ -29,10 +29,9 @@ module Faker\n         fetch('elder_scrolls.first_name')\n       end\n \n-      def first_name\n+      def last_name\n         fetch('elder_scrolls.last_name')\n       end\n-\n     end\n   end\n end","old_code":"module Faker\n  class ElderScrolls < Base\n    class << self\n      def race\n        fetch('elder_scrolls.race')\n      end\n\n      def city\n        fetch('elder_scrolls.city')\n      end\n\n      def creature\n        fetch('elder_scrolls.creature')\n      end\n\n      def region\n        fetch('elder_scrolls.region')\n      end\n\n      def dragon\n        fetch('elder_scrolls.dragon')\n      end\n\n      def name\n        \"#{fetch('elder_scrolls.first_name')} #{fetch('elder_scrolls.last_name')}\"\n      end\n\n      def first_name\n        fetch('elder_scrolls.first_name')\n      end\n\n      def first_name\n        fetch('elder_scrolls.last_name')\n      end\n\n    end\n  end\nend\n","lang_cluster":"Ruby","length":38,"code_uid":"9957131274424eb1837cfb0d10af3dd3"}
{"diff_hunk":"@@ -1,25 +1,11 @@\n-# snippet-comment:[These are tags for the AWS doc team's sample catalog. Do not remove.]\n-# snippet-sourceauthor:[Doug-AWS]\n-# snippet-sourcedescription:[Lists your Polly lexicons.]\n-# snippet-keyword:[Amazon Polly]\n-# snippet-keyword:[list_lexicons method]\n-# snippet-keyword:[Ruby]\n-# snippet-sourcesyntax:[ruby]\n-# snippet-service:[polly]\n-# snippet-keyword:[Code Sample]\n-# snippet-sourcetype:[full-example]\n-# snippet-sourcedate:[2018-03-16]\n-# Copyright 2010-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n-#\n-# This file is licensed under the Apache License, Version 2.0 (the \"License\").\n-# You may not use this file except in compliance with the License. A copy of the\n-# License is located at\n-#\n-# http:\/\/aws.amazon.com\/apache2.0\/\n-#\n-# This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS\n-# OF ANY KIND, either express or implied. See the License for the specific\n-# language governing permissions and limitations under the License.\n+# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+# SPDX-License-Identifier: Apache-2.0\n+\n+# Purpose:\n+# polly_list_lexicons.rb demonstrates how to\n+# list your Amazon Polly lexicons using Amazon Polly using the AWS SKD for Ruby.\n+\n+# snippet-start:[polly.ruby.listLexicons]\n \n require 'aws-sdk-polly'  # In v2: require 'aws-sdk'\n ","old_code":"# snippet-comment:[These are tags for the AWS doc team's sample catalog. Do not remove.]\r\n# snippet-sourceauthor:[Doug-AWS]\r\n# snippet-sourcedescription:[Lists your Polly lexicons.]\r\n# snippet-keyword:[Amazon Polly]\r\n# snippet-keyword:[list_lexicons method]\r\n# snippet-keyword:[Ruby]\n# snippet-sourcesyntax:[ruby]\r\n# snippet-service:[polly]\r\n# snippet-keyword:[Code Sample]\r\n# snippet-sourcetype:[full-example]\r\n# snippet-sourcedate:[2018-03-16]\r\n# Copyright 2010-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\r\n#\r\n# This file is licensed under the Apache License, Version 2.0 (the \"License\").\r\n# You may not use this file except in compliance with the License. A copy of the\r\n# License is located at\r\n#\r\n# http:\/\/aws.amazon.com\/apache2.0\/\r\n#\r\n# This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS\r\n# OF ANY KIND, either express or implied. See the License for the specific\r\n# language governing permissions and limitations under the License.\r\n\r\nrequire 'aws-sdk-polly'  # In v2: require 'aws-sdk'\r\n\r\nbegin\r\n  # Create an Amazon Polly client using\r\n  # credentials from the shared credentials file ~\/.aws\/credentials\r\n  # and the configuration (region) from the shared configuration file ~\/.aws\/config\r\n  polly = Aws::Polly::Client.new\r\n\r\n  resp = polly.list_lexicons\r\n\r\n  resp.lexicons.each do |l|\r\n    puts l.name\r\n    puts '  Alphabet:' + l.attributes.alphabet\r\n    puts '  Language:' + l.attributes.language\r\n    puts\r\n  end\r\nrescue StandardError => ex\r\n  puts 'Could not get lexicons'\r\n  puts 'Error message:'\r\n  puts ex.message\r\nend\r\n","lang_cluster":"Ruby","length":44,"code_uid":"93080156fa4d438d8dddf55253981b4f"}
{"diff_hunk":"@@ -76,7 +76,7 @@ module Gsa18f\n \n     # @todo - this is pretty ugly\n     def public_identifier\n-      self.cart.id\n+      \"##{self.cart.id}\"\n     end\n \n     def total_price","old_code":"module Gsa18f\n  # Make sure all table names use 'gsa18f_XXX'\n  def self.table_name_prefix\n    'gsa18f_'\n  end\n\n  DATA = YAML.load_file(\"#{Rails.root}\/config\/data\/18f.yaml\")\n  \n\n  class Procurement < ActiveRecord::Base\n    URGENCY = DATA['URGENCY']\n    OFFICES = DATA['OFFICES']\n    RECURRENCE = DATA['RECURRENCE']\n\n    # TODO include ProposalDelegate\n\n    has_one :proposal, as: :client_data\n    # TODO remove the dependence\n    has_one :cart, through: :proposal\n    \n    validates :cost_per_unit, numericality: {\n      greater_than_or_equal_to: 0,\n      less_than_or_equal_to: 3000\n    }\n    validates :quantity, numericality: {\n      greater_than_or_equal_to: 1\n    }\n    validates :product_name_and_description, presence: true\n\n    def init_and_save_cart(requester)\n      cart = Cart.create(\n        proposal_attributes: {flow: 'linear', client_data: self}\n      )\n      cart.set_requester(requester)\n      self.add_approvals\n      Dispatcher.deliver_new_proposal_emails(proposal)\n      cart\n    end\n    \n    def update_cart(cart)\n      cart.proposal.approvals.destroy_all\n      self.add_approvals\n      cart.restart!\n      cart\n    end\n\n    def add_approvals\n      self.cart.add_approver(Gsa18f::Procurement.approver_email)\n    end\n\n    # Ignore values in certain fields if they aren't relevant. May want to\n    # split these into different models\n    def self.relevant_fields(recurring)\n      fields = [:office, :justification, :link_to_product, :quantity,\n        :date_requested, :urgency, :additional_info, :cost_per_unit, \n        :product_name_and_description, :recurring]\n      if recurring\n        fields += [:recurring_interval, :recurring_length]\n      end \n      fields\n    end\n\n    def relevant_fields\n      Gsa18f::Procurement.relevant_fields(self.recurring)\n    end\n\n    def fields_for_display\n      attributes = self.relevant_fields\n      attributes.map! {|key| [Procurement.human_attribute_name(key), self[key]]}\n      attributes.push([\"Total Price\", total_price])\n    end\n\n    def client\n      \"gsa18f\"\n    end\n\n    # @todo - this is pretty ugly\n    def public_identifier\n      self.cart.id\n    end\n\n    def total_price\n      (self.cost_per_unit * self.quantity) || 0.0\n    end\n\n    # may be replaced with paper-trail or similar at some point\n    def version\n      self.updated_at.to_i\n    end\n\n    def name\n      self.product_name_and_description\n    end\n\n    def self.approver_email\n      ENV['GSA18F_APPROVER_EMAIL'] || '18fapprover@gsa.gov'\n    end\n  end\nend\n","lang_cluster":"Ruby","length":99,"code_uid":"818661844fd741d99c3859c07ee7d51b"}
{"diff_hunk":"@@ -56,7 +56,7 @@ class AnswersController < ApplicationController\n         }\n       }).find(p_params[:plan_id])\n       @question = @answer.question\n-      @section = @plan.get_section(@question.section_id)\n+      @section = @plan.sections.find_by(id: @question.section_id)\n       template = @section.phase.template\n \n       render json: {","old_code":"class AnswersController < ApplicationController\n  respond_to :html\n\n\t# POST \/answers\/create_or_update\n  def create_or_update\n    p_params = permitted_params()\n\n    #First it is checked plan exists and question exist for that plan\n    begin\n      p = Plan.find(p_params[:plan_id])\n      if !p.question_exists?(p_params[:question_id])\n        render(status: :not_found, json:\n        { msg: _(\"There is no question with id %{question_id} associated to plan id %{plan_id} for which to create or update an answer\") % { :question_id => p_params[:question_id], :plan_id => p_params[:plan_id] }})\n        return\n      end\n    rescue ActiveRecord::RecordNotFound\n      render(status: :not_found, json:\n        { msg: _('There is no plan with id %{id} for which to create or update an answer') %{ :id => p_params[:plan_id] }})\n      return\n    end\n    q = Question.find(p_params[:question_id])\n\n    Answer.transaction do\n      begin\n        @answer = Answer.find_by!({ plan_id: p_params[:plan_id], question_id: p_params[:question_id] })\n        authorize @answer\n        @answer.update(p_params.merge({ user_id: current_user.id }))\n        if p_params[:question_option_ids].present?\n          @answer.touch() # Saves the record with the updated_at set to the current time. Needed if only answer.question_options is updated\n        end\n        if q.question_format.rda_metadata?\n          @answer.update_answer_hash(JSON.parse(params[:standards]), p_params[:text])\n          @answer.save!\n        end\n      rescue ActiveRecord::RecordNotFound\n        @answer = Answer.new(p_params.merge({ user_id: current_user.id }))\n        @answer.lock_version = 1\n        authorize @answer\n        if q.question_format.rda_metadata?\n          @answer.update_answer_hash(JSON.parse(params[:standards]), p_params[:text])\n        end\n        @answer.save!\n      rescue ActiveRecord::StaleObjectError\n        @stale_answer = @answer\n        @answer = Answer.find_by({plan_id: p_params[:plan_id], question_id: p_params[:question_id]})\n      end\n    end\n\n    if @answer.present?\n      @plan = Plan.includes({\n        sections: {\n          questions: [\n            :answers,\n            :question_format\n          ]\n        }\n      }).find(p_params[:plan_id])\n      @question = @answer.question\n      @section = @plan.get_section(@question.section_id)\n      template = @section.phase.template\n\n      render json: {\n        \"question\" => {\n          \"id\" => @question.id,\n          \"answer_lock_version\" => @answer.lock_version,\n          \"locking\" => @stale_answer ?\n            render_to_string(partial: 'answers\/locking', locals: { question: @question, answer: @stale_answer, user: @answer.user }, formats: [:html]) :\n            nil,\n          \"form\" => render_to_string(partial: 'answers\/new_edit', locals: { template: template, question: @question, answer: @answer, readonly: false, locking: false, base_template_org: template.base_org }, formats: [:html]),\n          \"answer_status\" => render_to_string(partial: 'answers\/status', locals: { answer: @answer}, formats: [:html])\n        },\n        \"section\" => {\n          \"id\" => @section.id,\n          \"progress\" => render_to_string(partial: '\/org_admin\/sections\/progress', locals: { section: @section, plan: @plan }, formats: [:html])\n        },\n        \"plan\" => {\n          \"id\" => @plan.id,\n          \"progress\" => render_to_string(:partial => 'plans\/progress', locals: { plan: @plan, current_phase: @section.phase }, formats: [:html])\n        }\n      }.to_json\n    end\n\n  end # End update\n\n  private\n    def permitted_params\n      permitted = params.require(:answer).permit(:id, :text, :plan_id, :user_id, :question_id, :lock_version, :question_option_ids => [])\n      if !params[:answer][:question_option_ids].nil? && !permitted[:question_option_ids].present? #If question_option_ids has been filtered out because it was a scalar value (e.g. radiobutton answer)\n        permitted[:question_option_ids] = [params[:answer][:question_option_ids]] # then convert to an Array\n      end\n      if !permitted[:id].present?\n        permitted.delete(:id)\n      end\n      return permitted\n    end # End permitted_params\nend\n","lang_cluster":"Ruby","length":96,"code_uid":"272f5590385c4fde90c4da23091aaf37"}
{"diff_hunk":"@@ -165,6 +165,8 @@ module Bolt\n       def print_plan_result(plan_result)\n         if plan_result.value.nil?\n           @stream.puts(\"Plan completed successfully with no result\")\n+        elsif plan_result.value.is_a? Bolt::ApplyFailure\n+          @stream.puts(colorize(:red, plan_result.value.message))\n         else\n           @stream.puts(::JSON.pretty_generate(plan_result, quirks_mode: true))\n         end","old_code":"# frozen_string_literal: true\n\nrequire 'terminal-table'\nmodule Bolt\n  class Outputter\n    class Human < Bolt::Outputter\n      COLORS = { red: \"31\",\n                 green: \"32\",\n                 yellow: \"33\" }.freeze\n\n      def print_head; end\n\n      def colorize(color, string)\n        if @color && @stream.isatty\n          \"\\033[#{COLORS[color]}m#{string}\\033[0m\"\n        else\n          string\n        end\n      end\n\n      def indent(indent, string)\n        indent = ' ' * indent\n        string.gsub(\/^\/, indent.to_s)\n      end\n\n      def remove_trail(string)\n        string.sub(\/\\s\\z\/, '')\n      end\n\n      def print_event(event)\n        case event[:type]\n        when :node_start\n          print_start(event[:target])\n        when :node_result\n          print_result(event[:result])\n        end\n      end\n\n      def print_start(target)\n        @stream.puts(colorize(:green, \"Started on #{target.host}...\"))\n      end\n\n      def print_result(result)\n        if result.success?\n          @stream.puts(colorize(:green, \"Finished on #{result.target.host}:\"))\n        else\n          @stream.puts(colorize(:red, \"Failed on #{result.target.host}:\"))\n        end\n\n        if result.error_hash\n          @stream.puts(colorize(:red, remove_trail(indent(2, result.error_hash['msg']))))\n        end\n\n        if result.message\n          @stream.puts(remove_trail(indent(2, result.message)))\n        end\n\n        # There is more information to output\n        if result.generic_value\n          # Use special handling if the result looks like a command or script result\n          if result.generic_value.keys == %w[stdout stderr exit_code]\n            unless result['stdout'].strip.empty?\n              @stream.puts(indent(2, \"STDOUT:\"))\n              @stream.puts(indent(4, result['stdout']))\n            end\n            unless result['stderr'].strip.empty?\n              @stream.puts(indent(2, \"STDERR:\"))\n              @stream.puts(indent(4, result['stderr']))\n            end\n          else\n            @stream.puts(indent(2, ::JSON.pretty_generate(result.generic_value)))\n          end\n        end\n      end\n\n      def print_summary(results, elapsed_time)\n        ok_set = results.ok_set\n        unless ok_set.empty?\n          @stream.puts format('Successful on %<size>d node%<plural>s: %<names>s',\n                              size: ok_set.size,\n                              plural: ok_set.size == 1 ? '' : 's',\n                              names: ok_set.names.join(','))\n        end\n\n        error_set = results.error_set\n        unless error_set.empty?\n          @stream.puts colorize(:red,\n                                format('Failed on %<size>d node%<plural>s: %<names>s',\n                                       size: error_set.size,\n                                       plural: error_set.size == 1 ? '' : 's',\n                                       names: error_set.names.join(',')))\n        end\n\n        @stream.puts format('Ran on %<size>d node%<plural>s in %<elapsed>.2f seconds',\n                            size: results.size,\n                            plural: results.size == 1 ? '' : 's',\n                            elapsed: elapsed_time)\n      end\n\n      def print_table(results)\n        @stream.puts Terminal::Table.new(\n          rows: results,\n          style: {\n            border_x: '',\n            border_y: '',\n            border_i: '',\n            padding_left: 0,\n            padding_right: 3,\n            border_top: false,\n            border_bottom: false\n          }\n        )\n      end\n\n      # @param [Hash] task A hash representing the task\n      def print_task_info(task)\n        # Building lots of strings...\n        pretty_params = +\"\"\n        task_info = +\"\"\n        usage = +\"bolt task run --nodes <node-name> #{task['name']}\"\n\n        if task['parameters']\n          replace_data_type(task['parameters'])\n          task['parameters'].each do |k, v|\n            pretty_params << \"- #{k}: #{v['type']}\\n\"\n            pretty_params << \"    #{v['description']}\\n\" if v['description']\n            usage << if v['type'].is_a?(Puppet::Pops::Types::POptionalType)\n                       \" [#{k}=<value>]\"\n                     else\n                       \" #{k}=<value>\"\n                     end\n          end\n        end\n\n        usage << \" [--noop]\" if task['supports_noop']\n\n        task_info << \"\\n#{task['name']}\"\n        task_info << \" - #{task['description']}\" if task['description']\n        task_info << \"\\n\\n\"\n        task_info << \"USAGE:\\n#{usage}\\n\\n\"\n        task_info << \"PARAMETERS:\\n#{pretty_params}\\n\" if task['parameters']\n        @stream.puts(task_info)\n      end\n\n      # @param [Hash] plan A hash representing the plan\n      def print_plan_info(plan)\n        # Building lots of strings...\n        pretty_params = +\"\"\n        plan_info = +\"\"\n        usage = +\"bolt plan run #{plan['name']}\"\n\n        plan['parameters']&.each do |name, p|\n          pretty_params << \"- #{name}: #{p['type']}\\n\"\n          usage << (p.include?('default_value') ? \" [#{name}=<value>]\" : \" #{name}=<value>\")\n        end\n\n        plan_info << \"\\n#{plan['name']}\"\n        plan_info << \"\\n\\n\"\n        plan_info << \"USAGE:\\n#{usage}\\n\\n\"\n        plan_info << \"PARAMETERS:\\n#{pretty_params}\\n\" if plan['parameters']\n        @stream.puts(plan_info)\n      end\n\n      # @param [Bolt::PlanResult] plan_result A PlanResult object\n      def print_plan_result(plan_result)\n        if plan_result.value.nil?\n          @stream.puts(\"Plan completed successfully with no result\")\n        else\n          @stream.puts(::JSON.pretty_generate(plan_result, quirks_mode: true))\n        end\n      end\n\n      def fatal_error(err)\n        @stream.puts(colorize(:red, err.message))\n        if err.is_a? Bolt::RunFailure\n          @stream.puts ::JSON.pretty_generate(err.result_set)\n        end\n\n        if @trace && err.backtrace\n          err.backtrace.each do |line|\n            @stream.puts(colorize(:red, \"\\t#{line}\"))\n          end\n        end\n      end\n    end\n\n    def print_message(message)\n      @stream.puts(message)\n    end\n  end\nend\n","lang_cluster":"Ruby","length":191,"code_uid":"091bc938e43f4887b5e03bf30838fed6"}
{"diff_hunk":"@@ -69,8 +69,20 @@ module ClientDataMixin\n       column_names.sort.map { |attribute| human_attribute_name(attribute) }\n     end\n \n+    def self.foreign_key_to_method_map\n+      @_fk_map ||= Hash[reflect_on_all_associations(:belongs_to).map { |a| [a.foreign_key, a.name] }]\n+    end\n+\n     def csv_fields\n-      self.class.column_names.sort.map { |attribute| send(attribute) }\n+      field_values = []\n+      self.class.column_names.sort.each do |attribute|\n+        if self.class.foreign_key_to_method_map[attribute]\n+          field_values << send(self.class.foreign_key_to_method_map[attribute])\n+        else\n+          field_values << send(attribute)\n+        end\n+      end\n+      field_values\n     end\n   end\n end","old_code":"module ClientDataMixin\n  extend ActiveSupport::Concern\n\n  included do\n    include FiscalYearMixin\n\n    Proposal::CLIENT_MODELS << self\n\n    has_paper_trail class_name: \"C2Version\"\n\n    has_one :proposal, as: :client_data\n    has_many :steps, through: :proposal\n    has_many :individual_steps, -> { individual }, class_name: \"Steps::Individual\", through: :proposal\n    has_many :observations, through: :proposal\n    has_many :observers, through: :observations, source: :user\n    has_many :comments, through: :proposal\n    has_one :requester, through: :proposal\n    has_many :completers, through: :proposal\n\n    accepts_nested_attributes_for :proposal\n\n    validates :proposal, presence: true\n\n    delegate(\n      :approvers,\n      :purchasers,\n      :add_observer,\n      :add_requester,\n      :currently_awaiting_step_users,\n      :ineligible_approvers,\n      :set_requester,\n      :status,\n      to: :proposal\n    )\n\n    scope :with_proposal_scope, ->(status) { joins(:proposal).merge(Proposal.send(status)) }\n    scope :closed, -> { with_proposal_scope(:closed) }\n\n    Proposal.statuses.each do |status|\n      scope status, -> { with_proposal_scope(status) }\n      delegate \"#{status}?\".to_sym, to: :proposal\n    end\n\n    Proposal.events.each do |event|\n      delegate \"#{event}!\".to_sym, to: :proposal\n    end\n\n    def self.client_slug\n      to_s.deconstantize.downcase\n    end\n\n    def client_slug\n      self.class.client_slug\n    end\n\n    def slug_matches?(user)\n      user.client_slug == client_slug\n    end\n\n    def self.slug_matches?(user)\n      user.client_slug == self.client_slug\n    end\n\n    def self.expense_type_options\n      []\n    end\n\n    def self.csv_headers\n      column_names.sort.map { |attribute| human_attribute_name(attribute) }\n    end\n\n    def csv_fields\n      self.class.column_names.sort.map { |attribute| send(attribute) }\n    end\n  end\nend\n","lang_cluster":"Ruby","length":76,"code_uid":"6e66b8b62ae44c99bee2baaed301e858"}
{"diff_hunk":"@@ -20,6 +20,8 @@ C2::Application.routes.draw do\n   match \"\/auth\/:provider\/callback\" => \"auth#oauth_callback\", via: [:get]\n   get \"\/auth\/failure\" => \"auth#failure\"\n   post \"\/logout\" => \"auth#logout\"\n+  resources :users, only: [:update]\n+\n   resources :help, only: [:index, :show]\n \n   # mandrill-rails","old_code":"C2::Application.routes.draw do\n  use_doorkeeper do\n    controllers applications: \"oauth\/applications\"\n  end\n\n  ActiveAdmin.routes(self)\n  root to: \"home#index\"\n  get \"\/error\" => \"home#error\"\n  get \"\/profile\" => \"profile#show\"\n  get \"\/beta\" => \"profile#beta\"\n  post \"\/profile\" => \"profile#update\"\n  get \"\/summary\" => \"summary#index\"\n  get \"\/summary\/:fiscal_year\" => \"summary#index\"\n  get \"\/feedback\" => \"feedback#index\"\n  get \"\/feedback\/thanks\" => \"feedback#thanks\"\n  post \"\/feedback\" => \"feedback#create\"\n  get \"\/activity-feed\/:proposal_id\/update_feed\" => \"comments#update_feed\"\n  get \"\/approval-feed\/:id\/update_approvals\" => \"proposals#update_approvals_card\"\n\n  match \"\/auth\/:provider\/callback\" => \"auth#oauth_callback\", via: [:get]\n  get \"\/auth\/failure\" => \"auth#failure\"\n  post \"\/logout\" => \"auth#logout\"\n  resources :help, only: [:index, :show]\n\n  # mandrill-rails\n  resource :inbox, controller: \"inbox\", only: [:show, :create]\n\n  if ENV[\"API_ENABLED\"] == \"true\"\n    namespace :api do\n      namespace :v2 do\n        resources :proposals\n      end\n    end\n  end\n\n  resources :proposals, only: [:index, :show] do\n    member do\n      get \"approve\"   # this route has special protection to prevent the confused deputy problem\n      get \"complete\"  # if you are adding a new controller which performs an action, use post instead\n      post \"complete\"\n      post \"approve\"\n\n      get \"cancel_form\"\n      post \"cancel\"\n      get \"history\"\n      get \"activate_design\" => \"proposals#activate_detail_design\"\n      get \"revert_design\" => \"proposals#revert_detail_design\"\n    end\n\n    collection do\n      get \"archive\"\n      get \"download\", defaults: { format: \"csv\" }\n      get \"query\"\n      get \"query_count\"\n    end\n\n    resources :comments, only: :create\n    resources :attachments, only: [:create, :destroy, :show]\n    resources :observations, only: [:create, :destroy]\n  end\n\n  resources :reports, only: [:index, :show, :create, :destroy] do\n    member do\n      post :preview\n    end\n  end\n  resources :scheduled_reports, only: [:create, :update]\n\n  namespace :ncr do\n    resources :work_orders, except: [:index, :destroy]\n    get \"\/dashboard\" => \"dashboard#index\"\n  end\n\n  namespace :gsa18f do\n    resources :procurements, except: [:index, :destroy]\n    get \"\/dashboard\" => \"dashboard#index\"\n  end\n\n  mount Peek::Railtie => \"\/peek\"\n  if Rails.env.development?\n    mount LetterOpenerWeb::Engine => \"letter_opener\"\n    mount Konacha::Engine, at: \"konacha\" if defined?(Konacha)\n    mount Blazer::Engine, at: \"blazer\"\n  end\nend\n","lang_cluster":"Ruby","length":85,"code_uid":"848f96e27ec04e02b8529ee300030752"}
{"diff_hunk":"@@ -16,6 +16,32 @@ module Faker\n         def character\n           fetch('dragon_ball.characters')\n         end\n+\n+        ##\n+        # Produces the name of a race from Dragon Ball.\n+        #\n+        # @return [String]\n+        #\n+        # @example\n+        #   Faker::Games::DragonBall.race #=> \"Saiyan\"\n+        #\n+        # @faker.version 1.8.0\n+        def race\n+          fetch('dragon_ball.races')\n+        end\n+\n+        ##\n+        # Produces the name of a planet from Dragon Ball.\n+        #\n+        # @return [String]\n+        #\n+        # @example\n+        #   Faker::Games::DragonBall.planet #=> \"Namek\"\n+        #\n+        # @faker.version 1.8.0\n+        def planet\n+          fetch('dragon_ball.planets')\n+        end\n       end\n     end\n   end","old_code":"# frozen_string_literal: true\n\nmodule Faker\n  class JapaneseMedia\n    class DragonBall < Base\n      class << self\n        ##\n        # Produces the name of a character from Dragon Ball.\n        #\n        # @return [String]\n        #\n        # @example\n        #   Faker::Games::DragonBall.character #=> \"Goku\"\n        #\n        # @faker.version 1.8.0\n        def character\n          fetch('dragon_ball.characters')\n        end\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":22,"code_uid":"3101e332e2b8440e847a470870bd6950"}
{"diff_hunk":"@@ -3,18 +3,56 @@\n module Faker\n   class WorldCup < Base\n     class << self\n+      ##\n+      # Produces a national team name.\n+      #\n+      # @return [String]\n+      #\n+      # @example\n+      #   Faker::WorldCup.team #=> \"Iran\"\n+      #\n+      # @faker.version next\n       def team\n         fetch('world_cup.teams')\n       end\n \n+      ##\n+      # Produces a city name hosting the world cup match.\n+      #\n+      # @return [String]\n+      #\n+      # @example\n+      #   Faker::WorldCup.city #=> \"Moscow\"\n+      #\n+      # @faker.version next\n       def city\n         fetch('world_cup.cities')\n       end\n \n+      ##\n+      # Produces a stadium name hosting the world cup match.\n+      #\n+      # @return [String]\n+      #\n+      # @example\n+      #   Faker::WorldCup.stadium #=> \"Rostov Arena\"\n+      #\n+      # @faker.version next\n       def stadium\n         fetch('world_cup.stadiums')\n       end\n \n+      ##\n+      # Produces a random national team name from a group. See below examples\n+      #\n+      # @return [String]\n+      #\n+      # @example\n+      #   Faker::WorldCup.group(group: 'group_B') #=> \"Spain\"\n+      # @example\n+      #   Faker::WorldCup.group #=> \"Russia\"\n+      #\n+      # @faker.version next\n       def group(legacy_group = NOT_GIVEN, group: 'group_A')\n         warn_for_deprecated_arguments do |keywords|\n           keywords << :group if legacy_group != NOT_GIVEN","old_code":"# frozen_string_literal: true\n\nmodule Faker\n  class WorldCup < Base\n    class << self\n      def team\n        fetch('world_cup.teams')\n      end\n\n      def city\n        fetch('world_cup.cities')\n      end\n\n      def stadium\n        fetch('world_cup.stadiums')\n      end\n\n      def group(legacy_group = NOT_GIVEN, group: 'group_A')\n        warn_for_deprecated_arguments do |keywords|\n          keywords << :group if legacy_group != NOT_GIVEN\n        end\n\n        fetch(\"world_cup.groups.#{group}\")\n      end\n\n      def roster(legacy_country = NOT_GIVEN, legacy_type = NOT_GIVEN, country: 'Egypt', type: 'coach')\n        warn_for_deprecated_arguments do |keywords|\n          keywords << :country if legacy_country != NOT_GIVEN\n          keywords << :type if legacy_type != NOT_GIVEN\n        end\n\n        fetch(\"world_cup.rosters.#{country}.#{type}\")\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":36,"code_uid":"518ffc4fdfb54791b6e56060cca87534"}
{"diff_hunk":"@@ -1,15 +1,20 @@\n # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n # SPDX - License - Identifier: Apache - 2.0\n \n+# Purpose\n+# This code example demonstrates how to get the contents of an encrypted object\n+# in an Amazon Simple Storage Solution (Amazon S3) bucket.\n+\n+# snippet-start:[s3.s3_get_cskms_decrypt_item.rb]\n+\n require 'aws-sdk-s3'\n \n-# Gets the contents of an encrypted object in an Amazon S3 bucket.\n-#\n+\n # Prerequisites:\n #\n # - An Amazon S3 bucket.\n # - An encrypted object in the bucket to get.\n-# \n+#\n # @param s3_encryption_client [Aws::S3::EncryptionV2::Client]\n #   An initialized Amazon S3 V2 encryption client.\n # @param bucket_name [String] The name of the bucket.","old_code":"# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\r\n# SPDX - License - Identifier: Apache - 2.0\r\n\r\nrequire 'aws-sdk-s3'\r\n\r\n# Gets the contents of an encrypted object in an Amazon S3 bucket.\r\n#\r\n# Prerequisites:\r\n#\r\n# - An Amazon S3 bucket.\r\n# - An encrypted object in the bucket to get.\r\n# \r\n# @param s3_encryption_client [Aws::S3::EncryptionV2::Client]\r\n#   An initialized Amazon S3 V2 encryption client.\r\n# @param bucket_name [String] The name of the bucket.\r\n# @param object_key [String] The name of the encrypted object to get.\r\n# @return [String] If successful, the object's content; otherwise,\r\n#   diagnostic information about the unsuccessful attempt.\r\n# @example\r\n#   s3_encryption_client = Aws::S3::EncryptionV2::Client.new(\r\n#     region: 'us-east-1',\r\n#     kms_key_id: '9041e78c-7a20-4db3-929e-828abEXAMPLE',\r\n#     key_wrap_schema: :kms_context,\r\n#     content_encryption_schema: :aes_gcm_no_padding,\r\n#     security_profile: :v2\r\n#   )\r\n#   puts get_decrypted_object_content(\r\n#     s3_encryption_client,\r\n#     'doc-example-bucket',\r\n#     'my-file.txt'\r\n#   )\r\ndef get_decrypted_object_content(\r\n  s3_encryption_client,\r\n  bucket_name,\r\n  object_key\r\n)\r\n  response = s3_encryption_client.get_object(\r\n    bucket: bucket_name,\r\n    key: object_key\r\n  )\r\n  if defined?(response.body)\r\n    return response.body.read\r\n  else\r\n    return 'Error: Object content empty or unavailable.'\r\n  end\r\nrescue StandardError => e\r\n  return \"Error getting object content: #{e.message}\"\r\nend\r\n\r\n# Full example call:\r\ndef run_me\r\n  bucket_name = 'doc-example-bucket'\r\n  object_key = 'my-file.txt'\r\n  region = 'us-east-1'\r\n  kms_key_id = '9041e78c-7a20-4db3-929e-828abEXAMPLE'\r\n\r\n  # Note that in the following call:\r\n  # - key_wrap_schema must be kms_context for AWS KMS.\r\n  # - To allow reading and decrypting objects that are encrypted by the\r\n  #   Amazon S3 V1 encryption client instead, use :v2_and_legacy instead of :v2.\r\n  s3_encryption_client = Aws::S3::EncryptionV2::Client.new(\r\n    region: region,\r\n    kms_key_id: kms_key_id,\r\n    key_wrap_schema: :kms_context,\r\n    content_encryption_schema: :aes_gcm_no_padding,\r\n    security_profile: :v2\r\n  )\r\n\r\n  puts get_decrypted_object_content(\r\n    s3_encryption_client,\r\n    bucket_name,\r\n    object_key\r\n  )\r\nend\r\n\r\nrun_me if $PROGRAM_NAME == __FILE__\r\n","lang_cluster":"Ruby","length":76,"code_uid":"ebe3d8f4f53e4e32b0555e8649f161c7"}
{"diff_hunk":"@@ -4,13 +4,14 @@ end\n \n module Aix\n   class Host < Unix::Host\n-    [ 'user', 'group', 'file' ].each do |lib|\n+    [ 'user', 'group', 'file', 'exec' ].each do |lib|\n         require \"beaker\/host\/aix\/#{lib}\"\n     end\n \n     include Aix::User\n     include Aix::Group\n     include Aix::File\n+    include Aix::Exec\n \n   end\n end","old_code":"[ 'host', 'command_factory' ].each do |lib|\n  require \"beaker\/#{lib}\"\nend\n\nmodule Aix\n  class Host < Unix::Host\n    [ 'user', 'group', 'file' ].each do |lib|\n        require \"beaker\/host\/aix\/#{lib}\"\n    end\n\n    include Aix::User\n    include Aix::Group\n    include Aix::File\n\n  end\nend\n","lang_cluster":"Ruby","length":16,"code_uid":"077e936b9b1d46f08d62fb23f4433526"}
{"diff_hunk":"@@ -1,26 +1,21 @@\n class Product < ActiveRecord::Base\n-  # Associations\n   has_many :announcements, as: :announceable, dependent: :destroy\n   has_many :classifications, as: :classifiable\n   has_many :downloads, as: :purchaseable\n   has_many :purchases, as: :purchaseable\n   has_many :topics, through: :classifications\n-  has_many :videos, as: :watchable\n \n-  # Nested Attributes\n   accepts_nested_attributes_for :downloads, allow_destroy: true\n \n-  # Validations\n   validates :name, presence: true\n   validates :fulfillment_method, presence: true\n   validates :sku, presence: true\n-  validates :product_type, presence: true\n+  validates :type, presence: true\n \n-  # Plugins\n   has_attached_file :product_image, {\n     styles: {\n       book: '230x300#',\n-      video: '153x100#'\n+      screencast: '153x100#'\n     }\n   }.merge(PAPERCLIP_STORAGE_OPTIONS)\n ","old_code":"class Product < ActiveRecord::Base\n  # Associations\n  has_many :announcements, as: :announceable, dependent: :destroy\n  has_many :classifications, as: :classifiable\n  has_many :downloads, as: :purchaseable\n  has_many :purchases, as: :purchaseable\n  has_many :topics, through: :classifications\n  has_many :videos, as: :watchable\n\n  # Nested Attributes\n  accepts_nested_attributes_for :downloads, allow_destroy: true\n\n  # Validations\n  validates :name, presence: true\n  validates :fulfillment_method, presence: true\n  validates :sku, presence: true\n  validates :product_type, presence: true\n\n  # Plugins\n  has_attached_file :product_image, {\n    styles: {\n      book: '230x300#',\n      video: '153x100#'\n    }\n  }.merge(PAPERCLIP_STORAGE_OPTIONS)\n\n  def self.active\n    where active: true\n  end\n\n  def self.books\n    where product_type: 'book'\n  end\n\n  def self.videos\n    where product_type: 'video'\n  end\n\n  def self.ordered\n    order 'name ASC'\n  end\n\n  def self.newest_first\n    order 'created_at DESC'\n  end\n\n  def meta_keywords\n    topics.meta_keywords\n  end\n\n  def announcement\n    @announcement ||= announcements.current\n  end\n\n  def subscription?\n    false\n  end\n\n  def image_url\n    raw_url = product_image.url(product_type_symbol)\n    product_image_file_name? ? raw_url : \"\/assets\/#{raw_url}\"\n  end\n\n  def product_type_symbol\n    product_type.split(' ')[0].downcase.to_sym\n  rescue\n    :book\n  end\n\n  def to_param\n    \"#{id}-#{name.parameterize}\"\n  end\n\n  def individual_price\n    apply_discount(original_individual_price)\n  end\n\n  def company_price\n    apply_discount(original_company_price)\n  end\n\n  def original_company_price\n    self[:company_price] || 0\n  end\n\n  def original_individual_price\n    self[:individual_price] || 0\n  end\n\n  def discounted?\n    discount_percentage > 0\n  end\n\n  def starts_on(purchase_date)\n    purchase_date\n  end\n\n  def ends_on(purchase_date)\n    purchase_date\n  end\n\n  def purchase_for(user)\n    purchases.paid.where(user_id: user).first\n  end\n\n  def book_filename\n    name.parameterize\n  end\n\n  def title\n    \"#{name}: a #{product_type} by thoughtbot\"\n  end\n\n  def offering_type\n    product_type\n  end\n\n  def alternates\n    []\n  end\n\n  def fulfilled_with_github?\n    github_team.present?\n  end\n\n  private\n\n  def apply_discount(price)\n    price - (price * discount_percentage * 0.01)\n  end\nend\n","lang_cluster":"Ruby","length":131,"code_uid":"043f64cc5ddb4a73b1d95a901f15994c"}
{"diff_hunk":"@@ -1,3 +1,13 @@\n+<% if @filter.present? %>\n+  <p><%= _(<<-TEXT\n+    The data on the usage dashboard is historical in nature. This means that the number of records below may not\n+    match the count shown on the usage dashboard. For example if one of your users created a plan in October and\n+    then removed that plan in November, it would have been included on the usage dashboard's total for October but\n+    would not appear in the list below.\n+  TEXT\n+  ) %><\/p>\n+<% end %>\n+\n <div class=\"table-responsive\">\n   <table class=\"table table-hover\" id=\"my-plans\">\n     <thead>","old_code":"<div class=\"table-responsive\">\n  <table class=\"table table-hover\" id=\"my-plans\">\n    <thead>\n      <tr>\n        <th scope=\"col\"><%= _('Project Title') %>&nbsp;<%= paginable_sort_link('plans.title') %><\/th>\n        <th scope=\"col\"><%= _('Template') %>&nbsp;<%= paginable_sort_link('templates.title') %><\/th>\n        <th scope=\"col\"><%= _('Organisation') %>&nbsp;<%= paginable_sort_link('orgs.name') %><\/th>\n        <th scope=\"col\"><%= _('Owner') %><\/th>\n        <th scope=\"col\" class=\"date-column\"><%= _('Updated') %>&nbsp;<%= paginable_sort_link('plans.updated_at') %><\/th>\n        <th scope=\"col\"><%= _('Visibility') %><\/th>\n      <\/tr>\n    <\/thead>\n    <tbody>\n      <% scope.each do |plan| %>\n        <tr>\n          <td>\n            <% if plan.readable_by?(current_user.id) %>\n              <%= link_to \"#{plan.title.length > 60 ? \"#{plan.title[0..59]} ...\" : plan.title}\", plan_path(plan) %>\n            <% else %>\n              <%= plan.title.truncate(60) %>\n            <% end %>\n          <\/td>\n          <td><%= plan.template.title %><\/td>\n          <td><%= plan.owner.org.name %><\/td>\n          <td><%= plan.owner.name(false) %><\/td>\n          <td><%= l(plan.updated_at.to_date, formats: :short) %><\/td>\n          <td class=\"plan-visibility\">\n            <%= plan.visibility === 'is_test' ? _('Test') : sanitize(display_visibility(plan.visibility)) %>\n          <\/td>\n        <\/tr>\n      <% end %>\n    <\/tbody>\n  <\/table>\n<\/div>\n","lang_cluster":"Ruby","length":34,"code_uid":"ce13e1aca36748baa5811dbdcea3a098"}
{"diff_hunk":"@@ -82,27 +82,34 @@ module Beaker\n     #Default configuration steps to be run for a given hypervisor.  Any additional configuration to be done\n     #to the provided SUT for test execution to be successful.\n     def configure(opts = {})\n-      return unless @options[:configure]\n-      run_in_parallel = run_in_parallel? opts, @options, 'configure'\n-      block_on @hosts, { :run_in_parallel => run_in_parallel} do |host|\n-        if host[:timesync]\n-          timesync(host, @options)\n+      begin\n+        return unless @options[:configure]\n+        run_in_parallel = run_in_parallel? opts, @options, 'configure'\n+        block_on @hosts, { :run_in_parallel => run_in_parallel} do |host|\n+          if host[:timesync]\n+            timesync(host, @options)\n+          end\n         end\n-      end\n-      if @options[:root_keys]\n-        sync_root_keys(@hosts, @options)\n-      end\n-      if @options[:add_el_extras]\n-        add_el_extras(@hosts, @options)\n-      end\n-      if @options[:disable_iptables]\n-        disable_iptables @hosts, @options\n-      end\n-      if @options[:set_env]\n-        set_env(@hosts, @options)\n-      end\n-      if @options[:disable_updates]\n-        disable_updates(@hosts, @options)\n+        if @options[:root_keys]\n+          sync_root_keys(@hosts, @options)\n+        end\n+        if @options[:add_el_extras]\n+          add_el_extras(@hosts, @options)\n+        end\n+        if @options[:disable_iptables]\n+          disable_iptables @hosts, @options\n+        end\n+        if @options[:set_env]\n+          set_env(@hosts, @options)\n+        end\n+        if @options[:disable_updates]\n+          disable_updates(@hosts, @options)\n+        end\n+      rescue SignalException => ex\n+        if ex.signo == 15 #SIGTERM\n+          report_and_raise(@logger, ex, \"validate\")\n+        end\n+        raise\n       end\n     end\n ","old_code":"[ 'host_prebuilt_steps' ].each do |lib|\n  require \"beaker\/#{lib}\"\nend\n\nmodule Beaker\n  #The Beaker class that interacts to all the supported hypervisors\n  class Hypervisor\n    include HostPrebuiltSteps\n\n    #Generates an array with all letters a thru z and numbers 0 thru 9\n    CHARMAP = ('a'..'z').to_a + ('0'..'9').to_a\n\n    #Hypervisor creator method.  Creates the appropriate hypervisor class object based upon\n    #the provided hypervisor type selected, then provisions hosts with hypervisor.\n    #@param [String] type The type of hypervisor to create - one of aix, solaris, vsphere, fusion,\n    #                     blimpy, vcloud or vagrant\n    #@param [Array<Host>] hosts_to_provision The hosts to be provisioned with the selected hypervisor\n    #@param [Hash] options options Options to alter execution\n    #@option options [String] :host_name_prefix (nil) Prefix host name if set\n    def self.create(type, hosts_to_provision, options)\n      @logger = options[:logger]\n      @logger.notify(\"Beaker::Hypervisor, found some #{type} boxes to create\")\n\n      hyper_class = case type\n        when \/^noop$\/\n          Beaker::Noop\n        when \/^(default)|(none)$\/\n          Beaker::Hypervisor\n        else\n          # Custom hypervisor\n          require \"beaker\/hypervisor\/#{type}\"\n          Beaker.const_get(type.split('_').collect(&:capitalize).join)\n        end\n\n      hypervisor = hyper_class.new(hosts_to_provision, options)\n      self.set_ssh_connection_preference(hosts_to_provision, hypervisor)\n      hypervisor.provision if options[:provision]\n\n      hypervisor\n    end\n\n    def initialize(hosts, options)\n      @hosts = hosts\n      @options = options\n    end\n\n    #Provisioning steps for be run for a given hypervisor.  Default is nil.\n    def provision\n      nil\n    end\n\n    #Cleanup steps to be run for a given hypervisor.  Default is nil.\n    def cleanup\n      nil\n    end\n\n    DEFAULT_CONNECTION_PREFERENCE = [:ip, :vmhostname, :hostname]\n    # SSH connection method preference. Can be overwritten by hypervisor to change the order\n    def connection_preference(host)\n      DEFAULT_CONNECTION_PREFERENCE\n    end\n\n    def self.set_ssh_connection_preference(hosts_to_provision, hypervisor)\n      hosts_to_provision.each do |host|\n        ssh_methods = hypervisor.connection_preference(host) + DEFAULT_CONNECTION_PREFERENCE\n        if host[:ssh_preference]\n          # If user has provided ssh_connection_preference in hosts file then concat the preference provided by hypervisor\n          # Followed by concatenating the default preference and keeping the unique once\n          ssh_methods = host[:ssh_preference] + ssh_methods\n        end\n        host[:ssh_connection_preference] = ssh_methods.uniq\n      end\n    end\n\n    #Proxy package managers on tests hosts created by this hypervisor, runs before validation and configuration.\n    def proxy_package_manager\n      if @options[:package_proxy]\n        package_proxy(@hosts, @options)\n      end\n    end\n\n    #Default configuration steps to be run for a given hypervisor.  Any additional configuration to be done\n    #to the provided SUT for test execution to be successful.\n    def configure(opts = {})\n      return unless @options[:configure]\n      run_in_parallel = run_in_parallel? opts, @options, 'configure'\n      block_on @hosts, { :run_in_parallel => run_in_parallel} do |host|\n        if host[:timesync]\n          timesync(host, @options)\n        end\n      end\n      if @options[:root_keys]\n        sync_root_keys(@hosts, @options)\n      end\n      if @options[:add_el_extras]\n        add_el_extras(@hosts, @options)\n      end\n      if @options[:disable_iptables]\n        disable_iptables @hosts, @options\n      end\n      if @options[:set_env]\n        set_env(@hosts, @options)\n      end\n      if @options[:disable_updates]\n        disable_updates(@hosts, @options)\n      end\n    end\n\n    #Default validation steps to be run for a given hypervisor.  Ensures that SUTs meet requirements to be\n    #beaker test nodes.\n    def validate\n      if @options[:validate]\n        validate_host(@hosts, @options)\n      end\n    end\n\n    #Generate a random string composted of letter and numbers\n    #prefixed with value of {Beaker::Hypervisor::create} option :host_name_prefix\n    def generate_host_name\n      n = CHARMAP[rand(25)] + (0...14).map{CHARMAP[rand(CHARMAP.length)]}.join\n      if @options[:host_name_prefix]\n        return @options[:host_name_prefix] + n\n      end\n      n\n    end\n\n  end\nend\n\nrequire \"beaker\/hypervisor\/noop\"\n","lang_cluster":"Ruby","length":130,"code_uid":"14e3137e9080470795aa7a397d344626"}
{"diff_hunk":"@@ -2,6 +2,9 @@ class Trail < ApplicationRecord\n   extend FriendlyId\n \n   include PgSearch\n+\n+  DEFAULT_IMAGE_URL = \"https:\/\/images.thoughtbot.com\/upcase\/trail-title-cards\/default.jpg\"\n+\n   multisearchable against: [:name, :description], if: :published?\n \n   validates :name, :description, presence: true","old_code":"class Trail < ApplicationRecord\n  extend FriendlyId\n\n  include PgSearch\n  multisearchable against: [:name, :description], if: :published?\n\n  validates :name, :description, presence: true\n\n  has_many :classifications, as: :classifiable\n  has_many :repositories, dependent: :destroy\n  has_many :statuses, as: :completeable, dependent: :destroy\n  has_many :topics, through: :classifications\n  has_many :users, through: :statuses\n  has_many \\\n    :steps,\n    -> { order \"steps.position ASC\" },\n    dependent: :destroy,\n    inverse_of: :trail\n  has_many :exercises,\n    through: :steps,\n    source: :completeable,\n    source_type: \"Exercise\"\n  has_many :videos, through: :steps, source: :completeable, source_type: \"Video\"\n\n  friendly_id :name, use: [:slugged, :finders]\n\n  def self.accessible_without_subscription?\n    false\n  end\n\n  def self.published\n    where(published: true)\n  end\n\n  def self.completed_for(user)\n    TrailWithProgressQuery.new(all, user: user).select(&:complete?)\n  end\n\n  def to_s\n    name\n  end\n\n  # Override setters so it preserves the order\n  def step_ids=(new_step_ids)\n    super\n    new_step_ids = new_step_ids.reject(&:blank?).map(&:to_i)\n\n    new_step_ids.each_with_index do |step_id, index|\n      steps.where(id: step_id).update_all(position: index + 1)\n    end\n  end\n\n  def completeables\n    steps.map(&:completeable)\n  end\n\n  def update_state_for(user)\n    TrailWithProgress.new(\n      self,\n      user: user,\n      status_finder: StatusFinder.new(user: user),\n    ).update_status\n  end\n\n  def self.most_recent_published\n    order(created_at: :desc).published\n  end\n\n  def teachers\n    Teacher.joins(:video).merge(videos).to_a.uniq(&:user_id)\n  end\n\n  def topic_name\n    topic.name\n  end\n\n  def first_completeable\n    first_step.completeable\n  end\n\n  def sample_video\n    videos.where(accessible_without_subscription: true).first.wrapped\n  end\n\n  def time_to_complete\n    videos.sum(:length_in_minutes) + exercise_time\n  end\n\n  private\n\n  def first_step\n    if steps.loaded?\n      steps.sort_by(&:position).first\n    else\n      steps.first\n    end\n  end\n\n  def exercise_time\n    exercises.count * Exercise::AVERAGE_COMPLETION_TIME_IN_MINUTES\n  end\nend\n","lang_cluster":"Ruby","length":102,"code_uid":"acf8911567254a09a399d39e75f5fe29"}
{"diff_hunk":"@@ -1,32 +1,34 @@\n describe CommunicartMailerHelper do\n   describe '#approval_action_url' do\n     it \"returns a URL\" do\n-      approval = FactoryGirl.create(:approval, :with_cart, :with_user)\n+      approval = FactoryGirl.create(:approval, :with_proposal, :with_user)\n       token = approval.create_api_token!\n+      proposal = approval.proposal\n+\n+      expect(proposal).to receive(:version).and_return(123)\n \n       url = helper.approval_action_url(approval)\n       uri = Addressable::URI.parse(url)\n+      expect(uri.path).to eq(\"\/proposals\/#{proposal.id}\/approve\")\n       expect(uri.query_values).to eq(\n-        'approver_action' => 'approve',\n-        'cart_id' => approval.cart_id.to_s,\n         'cch' => token.access_token,\n-        'version' => approval.proposal.version.to_s\n+        'version' => '123'\n       )\n     end\n \n-    it \"links to the cart if the approver has delegates\" do\n+    it \"leaves out the token if the approver has delegates\" do\n       approver = FactoryGirl.create(:user, :with_delegate)\n-      approval = FactoryGirl.create(:approval, :with_cart, user: approver)\n+      approval = FactoryGirl.create(:approval, :with_proposal, user: approver)\n       approval.create_api_token!\n+      proposal = approval.proposal\n+\n+      expect(proposal).to receive(:version).and_return(123)\n \n       url = helper.approval_action_url(approval)\n       uri = Addressable::URI.parse(url)\n-      expect(uri.path).to eq('\/approval_response')\n-      cart = approval.cart\n+      expect(uri.path).to eq(\"\/proposals\/#{proposal.id}\/approve\")\n       expect(uri.query_values).to eq(\n-        'approver_action' => 'approve',\n-        'cart_id' => cart.id.to_s,\n-        'version' => cart.version.to_s\n+        'version' => '123'\n       )\n     end\n ","old_code":"describe CommunicartMailerHelper do\n  describe '#approval_action_url' do\n    it \"returns a URL\" do\n      approval = FactoryGirl.create(:approval, :with_cart, :with_user)\n      token = approval.create_api_token!\n\n      url = helper.approval_action_url(approval)\n      uri = Addressable::URI.parse(url)\n      expect(uri.query_values).to eq(\n        'approver_action' => 'approve',\n        'cart_id' => approval.cart_id.to_s,\n        'cch' => token.access_token,\n        'version' => approval.proposal.version.to_s\n      )\n    end\n\n    it \"links to the cart if the approver has delegates\" do\n      approver = FactoryGirl.create(:user, :with_delegate)\n      approval = FactoryGirl.create(:approval, :with_cart, user: approver)\n      approval.create_api_token!\n\n      url = helper.approval_action_url(approval)\n      uri = Addressable::URI.parse(url)\n      expect(uri.path).to eq('\/approval_response')\n      cart = approval.cart\n      expect(uri.query_values).to eq(\n        'approver_action' => 'approve',\n        'cart_id' => cart.id.to_s,\n        'version' => cart.version.to_s\n      )\n    end\n\n    it \"throws an error if there's no token\" do\n      approval = FactoryGirl.build(:approval)\n      expect(approval.api_token).to eq(nil)\n\n      expect {\n        helper.approval_action_url(approval)\n      }.to raise_error\n    end\n  end\nend\n","lang_cluster":"Ruby","length":42,"code_uid":"973f5ad7854e485f84ed61f7be47a336"}
{"diff_hunk":"@@ -117,6 +117,19 @@ module Faker\n         regexify(bothify(fetch(key)))\n       end\n \n+      def mercosur_license_plate(legacy_state_abreviation = NOT_GIVEN, state_abreviation: '')\n+        key = 'vehicle.mercosur_license_plate'\n+        if legacy_state_abreviation != NOT_GIVEN\n+          warn_with_uplevel \"Passing `state_abreviation` with the 1st argument of `Vehicle.mercosur_license_plate` is deprecated. Use keyword argument like `Vehicle.mercosur_license_plate(state_abreviation: ...)` instead.\", uplevel: 1\n+          state_abreviation = legacy_state_abreviation\n+        end\n+        \n+        return regexify(bothify(fetch(key))) if state_abreviation.empty?\n+\n+        key = key + '.by_state.' + state_abreviation\n+        regexify(bothify(fetch(key)))\n+      end\n+\n       def singapore_license_plate\n         key = 'vehicle.license_plate'\n         plate_number = regexify(bothify(fetch(key)))","old_code":"# frozen_string_literal: true\n\nmodule Faker\n  class Vehicle < Base\n    flexible :vehicle\n\n    MILEAGE_MIN = 10_000\n    MILEAGE_MAX = 90_000\n    VIN_LETTERS = 'ABCDEFGHJKLMNPRSTUVWXYZ'\n    VIN_MAP = '0123456789X'\n    VIN_WEIGHTS = '8765432X098765432'\n    VIN_REGEX = \/^[A-Z0-9]{3}[A-Z0-9]{5}[A-Z0-9]{1}[A-Z0-9]{1}[A-Z0-0]{1}[A-Z0-9]{1}\\d{5}$\/\n    SG_CHECKSUM_WEIGHTS = [3, 14, 2, 12, 2, 11, 1].freeze\n    SG_CHECKSUM_CHARS = 'AYUSPLJGDBZXTRMKHEC'\n\n    class << self\n      def vin\n        regexify(VIN_REGEX)\n      end\n\n      def manufacture\n        fetch('vehicle.manufacture')\n      end\n\n      def make\n        fetch('vehicle.makes')\n      end\n\n      def model(legacy_make_of_model = NOT_GIVEN, make_of_model: '')\n        if legacy_make_of_model != NOT_GIVEN\n          warn_with_uplevel 'Passing `make_of_model` with the 1st argument of `Vehicle.model` is deprecated. Use keyword argument like `Vehicle.model(make_of_model: ...)` instead.', uplevel: 1\n          make_of_model = legacy_make_of_model\n        end\n\n        return fetch(\"vehicle.models_by_make.#{make}\") if make_of_model.empty?\n\n        fetch(\"vehicle.models_by_make.#{make_of_model}\")\n      end\n\n      def make_and_model\n        m = make\n\n        \"#{m} #{model(make_of_model: m)}\"\n      end\n\n      def style\n        fetch('vehicle.styles')\n      end\n\n      def color\n        fetch('vehicle.colors')\n      end\n\n      def transmission\n        fetch('vehicle.transmissions')\n      end\n\n      def drive_type\n        fetch('vehicle.drive_types')\n      end\n\n      def fuel_type\n        fetch('vehicle.fuel_types')\n      end\n\n      def car_type\n        fetch('vehicle.car_types')\n      end\n\n      def engine\n        \"#{sample(fetch_all('vehicle.doors'))} #{fetch('vehicle.cylinder_engine')}\"\n      end\n\n      alias engine_size engine\n\n      def car_options\n        Array.new(rand(5...10)) { fetch('vehicle.car_options') }\n      end\n\n      def standard_specs\n        Array.new(rand(5...10)) { fetch('vehicle.standard_specs') }\n      end\n\n      def doors\n        sample(fetch_all('vehicle.doors'))\n      end\n      alias door_count doors\n\n      def year\n        Faker::Time.backward(days: rand_in_range(365, 5475), period: :all, format: '%Y').to_i\n      end\n\n      def mileage(legacy_min = NOT_GIVEN, legacy_max = NOT_GIVEN, min: MILEAGE_MIN, max: MILEAGE_MAX)\n        if legacy_min != NOT_GIVEN\n          warn_with_uplevel 'Passing `min` with the 1st argument of `Vehicle.mileage` is deprecated. Use keyword argument like `Vehicle.mileage(min: ...)` instead.', uplevel: 1\n          min = legacy_min\n        end\n        if legacy_max != NOT_GIVEN\n          warn_with_uplevel 'Passing `max` with the 2nd argument of `Vehicle.mileage` is deprecated. Use keyword argument like `Vehicle.mileage(max: ...)` instead.', uplevel: 1\n          max = legacy_max\n        end\n\n        rand_in_range(min, max)\n      end\n\n      alias kilometrage mileage\n\n      def license_plate(legacy_state_abreviation = NOT_GIVEN, state_abreviation: '')\n        if legacy_state_abreviation != NOT_GIVEN\n          warn_with_uplevel 'Passing `state_abreviation` with the 1st argument of `Vehicle.license_plate` is deprecated. Use keyword argument like `Vehicle.license_plate(state_abreviation: ...)` instead.', uplevel: 1\n          state_abreviation = legacy_state_abreviation\n        end\n\n        return regexify(bothify(fetch('vehicle.license_plate'))) if state_abreviation.empty?\n\n        key = 'vehicle.license_plate_by_state.' + state_abreviation\n        regexify(bothify(fetch(key)))\n      end\n\n      def singapore_license_plate\n        key = 'vehicle.license_plate'\n        plate_number = regexify(bothify(fetch(key)))\n        \"#{plate_number}#{singapore_checksum(plate_number)}\"\n      end\n\n      private\n\n      def first_eight(number)\n        return number[0...8] unless number.nil?\n\n        regexify(VIN_REGEX)\n      end\n      alias last_eight first_eight\n\n      def calculate_vin_check_digit(vin)\n        sum = 0\n\n        vin.each_char.with_index do |c, i|\n          n = vin_char_to_number(c).to_i\n          weight = VIN_WEIGHTS[i].to_i\n          sum += weight * n\n        end\n\n        mod = sum % 11\n        mod == 10 ? 'X' : mod\n      end\n\n      def vin_char_to_number(char)\n        index = VIN_LETTERS.split('').index(char)\n\n        return char.to_i if index.nil?\n\n        VIN_MAP[index]\n      end\n\n      def singapore_checksum(plate_number)\n        padded_alphabets = format('%3s', plate_number[\/^[A-Z]+\/]).tr(' ', '-').split('')\n        padded_digits = format('%04d', plate_number[\/\\d+\/]).split('').map(&:to_i)\n        sum = [*padded_alphabets, *padded_digits].each_with_index.reduce(0) do |memo, (char, i)|\n          value = char.is_a?(Integer) ? char : char.ord - 64\n          memo + (SG_CHECKSUM_WEIGHTS[i] * value)\n        end\n\n        SG_CHECKSUM_CHARS.split('')[sum % 19]\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":168,"code_uid":"733ae42867ec4c1da7b208bd7aa7dfe0"}
{"diff_hunk":"@@ -141,6 +141,16 @@ exit \/B %errorlevel%\n             # therefore, this command will always exit 0 if either service is installed\n             on host, Command.new(\"sc query puppet || sc query pe-puppet\", [], { :cmdexe => true })\n \n+            # (PA-514) value for PUPPET_AGENT_STARTUP_MODE should be present in\n+            # registry and honored after install\/upgrade.\n+            reg_query_command = %Q(reg query \"HKLM\\\\SOFTWARE\\\\Wow6432Node\\\\Puppet Labs\\\\PuppetInstaller\" \/v \"RememberedPuppetAgentStartupMode\" | findstr #{msi_opts['PUPPET_AGENT_STARTUP_MODE']})\n+            on host, Command.new(reg_query_command, [], { :cmdexe => true })\n+\n+            start_mode = msi_opts['PUPPET_AGENT_STARTUP_MODE'] == \"Automatic\" ? \"Auto\" : msi_opts['PUPPET_AGENT_STARTUP_MODE']\n+            service_query_command = %Q('WMIC SERVICE where (Name like \"%Puppet\" AND StartMode=\"#{start_mode}\") | findstr Puppet')\n+\n+            on host, Command.new(service_query_command, [], { :cmdexe => true })\n+\n             # emit the misc\/versions.txt file which contains component versions for\n             # puppet, facter, hiera, pxp-agent, packaging and vendored Ruby\n             [","old_code":"module Beaker\n  module DSL\n    module InstallUtils\n      #\n      # This module contains methods useful for Windows installs\n      #\n      module WindowsUtils\n        # Given a host, returns it's system TEMP path\n        #\n        # @param [Host] host An object implementing {Beaker::Hosts}'s interface.\n        #\n        # @return [String] system temp path\n        def get_system_temp_path(host)\n          host.system_temp_path\n        end\n        alias_method :get_temp_path, :get_system_temp_path\n\n        # Generates commands to be inserted into a Windows batch file to launch an MSI install\n        # @param [String] msi_path The path of the MSI - can be a local Windows style file path like\n        #                   c:\\temp\\puppet.msi OR a url like https:\/\/download.com\/puppet.msi or file:\/\/c:\\temp\\puppet.msi\n        # @param  [Hash{String=>String}] msi_opts MSI installer options\n        #                   See https:\/\/docs.puppetlabs.com\/guides\/install_puppet\/install_windows.html#msi-properties\n        # @param [String] log_path The path to write the MSI log - must be a local Windows style file path\n        #\n        # @api private\n        def msi_install_script(msi_path, msi_opts, log_path)\n          # msiexec requires backslashes in file paths launched under cmd.exe start \/w\n          url_pattern = \/^(https?|file):\\\/\\\/\/\n          msi_path = msi_path.gsub(\/\\\/\/, \"\\\\\") if msi_path !~ url_pattern\n\n          msi_params = msi_opts.map{|k, v| \"#{k}=#{v}\"}.join(' ')\n\n          # msiexec requires quotes around paths with backslashes - c:\\ or file:\/\/c:\\\n          # not strictly needed for http:\/\/ but it simplifies this code\n          batch_contents = <<-BATCH\nstart \/w msiexec.exe \/i \\\"#{msi_path}\\\" \/qn \/L*V #{log_path} #{msi_params}\nexit \/B %errorlevel%\n          BATCH\n        end\n\n        # Given a host, path to MSI and MSI options, will create a batch file\n        #   on the host, returning the path to the randomized batch file and\n        #   the randomized log file\n        #\n        # @param [Host] host An object implementing {Beaker::Hosts}'s interface.\n        # @param [String] msi_path The path of the MSI - can be a local Windows\n        #   style file path like c:\\temp\\puppet.msi OR a url like\n        #   https:\/\/download.com\/puppet.msi or file:\/\/c:\\temp\\puppet.msi\n        # @param  [Hash{String=>String}] msi_opts MSI installer options\n        #   See https:\/\/docs.puppetlabs.com\/guides\/install_puppet\/install_windows.html#msi-properties\n        #\n        # @api private\n        # @return [String, String] path to the batch file, patch to the log file\n        def create_install_msi_batch_on(host, msi_path, msi_opts)\n          timestamp = Time.new.strftime('%Y-%m-%d_%H.%M.%S')\n          tmp_path = host.system_temp_path\n          tmp_path.gsub!('\/', '\\\\')\n\n          batch_name = \"install-puppet-msi-#{timestamp}.bat\"\n          batch_path = \"#{tmp_path}#{host.scp_separator}#{batch_name}\"\n          log_path = \"#{tmp_path}\\\\install-puppet-#{timestamp}.log\"\n\n          Tempfile.open(batch_name) do |tmp_file|\n            batch_contents = msi_install_script(msi_path, msi_opts, log_path)\n\n            File.open(tmp_file.path, 'w') { |file| file.puts(batch_contents) }\n            host.do_scp_to(tmp_file.path, batch_path, {})\n          end\n\n          return batch_path, log_path\n        end\n\n        # Given hosts construct a PATH that includes puppetbindir, facterbindir and hierabindir\n        # @param [Host, Array<Host>, String, Symbol] hosts    One or more hosts to act upon,\n        #                            or a role (String or Symbol) that identifies one or more hosts.\n        # @param [String] msi_path The path of the MSI - can be a local Windows style file path like\n        #                   c:\\temp\\puppet.msi OR a url like https:\/\/download.com\/puppet.msi or file:\/\/c:\\temp\\puppet.msi\n        # @param  [Hash{String=>String}] msi_opts MSI installer options\n        #                   See https:\/\/docs.puppetlabs.com\/guides\/install_puppet\/install_windows.html#msi-properties\n        # @option msi_opts [String] INSTALLIDIR Where Puppet and its dependencies should be installed.\n        #                  (Defaults vary based on operating system and intaller architecture)\n        #                  Requires Puppet 2.7.12 \/ PE 2.5.0\n        # @option msi_opts [String] PUPPET_MASTER_SERVER The hostname where the puppet master server can be reached.\n        #                  (Defaults to puppet)\n        #                  Requires Puppet 2.7.12 \/ PE 2.5.0\n        # @option msi_opts [String] PUPPET_CA_SERVER The hostname where the CA puppet master server can be reached, if you are using multiple masters and only one of them is acting as the CA.\n        #                  (Defaults the value of PUPPET_MASTER_SERVER)\n        #                  Requires Puppet 2.7.12 \/ PE 2.5.0\n        # @option msi_opts [String] PUPPET_AGENT_CERTNAME The node\u2019s certificate name, and the name it uses when requesting catalogs. This will set a value for\n        #                  (Defaults to the node's fqdn as discovered by facter fqdn)\n        #                  Requires Puppet 2.7.12 \/ PE 2.5.0\n        # @option msi_opts [String] PUPPET_AGENT_ENVIRONMENT The node\u2019s environment.\n        #                  (Defaults to production)\n        #                  Requires Puppet 3.3.1 \/ PE 3.1.0\n        # @option msi_opts [String] PUPPET_AGENT_STARTUP_MODE Whether the puppet agent service should run (or be allowed to run)\n        #                  (Defaults to Manual - valid values are Automatic, Manual or Disabled)\n        #                  Requires Puppet 3.4.0 \/ PE 3.2.0\n        # @option msi_opts [String] PUPPET_AGENT_ACCOUNT_USER Whether the puppet agent service should run (or be allowed to run)\n        #                  (Defaults to LocalSystem)\n        #                  Requires Puppet 3.4.0 \/ PE 3.2.0\n        # @option msi_opts [String] PUPPET_AGENT_ACCOUNT_PASSWORD The password to use for puppet agent\u2019s user account\n        #                  (No default)\n        #                  Requires Puppet 3.4.0 \/ PE 3.2.0\n        # @option msi_opts [String] PUPPET_AGENT_ACCOUNT_DOMAIN The domain of puppet agent\u2019s user account.\n        #                  (Defaults to .)\n        #                  Requires Puppet 3.4.0 \/ PE 3.2.0\n        # @option opts [Boolean] :debug output the MSI installation log when set to true\n        #                 otherwise do not output log (false; default behavior)\n        #\n        # @example\n        #  install_msi_on(hosts, 'c:\\puppet.msi', {:debug => true})\n        #\n        # @api private\n        def install_msi_on(hosts, msi_path, msi_opts = {}, opts = {})\n          block_on hosts do | host |\n            msi_opts['PUPPET_AGENT_STARTUP_MODE'] ||= 'Manual'\n            batch_path, log_file = create_install_msi_batch_on(host, msi_path, msi_opts)\n\n            # begin \/ rescue here so that we can reuse existing error msg propagation\n            begin\n              # 1641 = ERROR_SUCCESS_REBOOT_INITIATED\n              # 3010 = ERROR_SUCCESS_REBOOT_REQUIRED\n              on host, Command.new(\"\\\"#{batch_path}\\\"\", [], { :cmdexe => true }), :acceptable_exit_codes => [0, 1641, 3010]\n            rescue\n              on host, Command.new(\"type \\\"#{log_file}\\\"\", [], { :cmdexe => true })\n              raise\n            end\n\n            if opts[:debug]\n              on host, Command.new(\"type \\\"#{log_file}\\\"\", [], { :cmdexe => true })\n            end\n\n            if !host.is_cygwin?\n              # HACK: for some reason, post install we need to refresh the connection to make puppet available for execution\n              host.close\n            end\n\n            # verify service status post install\n            # if puppet service exists, then pe-puppet is not queried\n            # if puppet service does not exist, pe-puppet is queried and that exit code is used\n            # therefore, this command will always exit 0 if either service is installed\n            on host, Command.new(\"sc query puppet || sc query pe-puppet\", [], { :cmdexe => true })\n\n            # emit the misc\/versions.txt file which contains component versions for\n            # puppet, facter, hiera, pxp-agent, packaging and vendored Ruby\n            [\n              \"\\\\\\\"%ProgramFiles%\\\\Puppet Labs\\\\puppet\\\\misc\\\\versions.txt\\\\\\\"\",\n              \"\\\\\\\"%ProgramFiles(x86)%\\\\Puppet Labs\\\\puppet\\\\misc\\\\versions.txt\\\\\\\"\"\n            ].each do |path|\n              on host, Command.new(\"\\\"if exist #{path} type #{path}\\\"\", [], { :cmdexe => true })\n            end\n          end\n        end\n\n        # Installs a specified msi path on given hosts\n        # @param [Host, Array<Host>, String, Symbol] hosts    One or more hosts to act upon,\n        #                            or a role (String or Symbol) that identifies one or more hosts.\n        # @param [String] msi_path The path of the MSI - can be a local Windows style file path like\n        #                   c:\\temp\\foo.msi OR a url like https:\/\/download.com\/foo.msi or file:\/\/c:\\temp\\foo.msi\n        # @param  [Hash{String=>String}] msi_opts MSI installer options\n        # @option opts [Boolean] :debug output the MSI installation log when set to true\n        #                 otherwise do not output log (false; default behavior)\n        #\n        # @example\n        #  generic_install_msi_on(hosts, 'https:\/\/releases.hashicorp.com\/vagrant\/1.8.4\/vagrant_1.8.4.msi', {}, {:debug => true})\n        #\n        # @api private\n        def generic_install_msi_on(hosts, msi_path, msi_opts = {}, opts = {})\n          block_on hosts do | host |\n            batch_path, log_file = create_install_msi_batch_on(host, msi_path, msi_opts)\n\n            # begin \/ rescue here so that we can reuse existing error msg propagation\n            begin\n              # 1641 = ERROR_SUCCESS_REBOOT_INITIATED\n              # 3010 = ERROR_SUCCESS_REBOOT_REQUIRED\n              on host, Command.new(\"\\\"#{batch_path}\\\"\", [], { :cmdexe => true }), :acceptable_exit_codes => [0, 1641, 3010]\n            rescue\n              on host, Command.new(\"type \\\"#{log_file}\\\"\", [], { :cmdexe => true })\n              raise\n            end\n\n            if opts[:debug]\n              on host, Command.new(\"type \\\"#{log_file}\\\"\", [], { :cmdexe => true })\n            end\n\n            if !host.is_cygwin?\n              # HACK: for some reason, post install we need to refresh the connection to make puppet available for execution\n              host.close\n            end\n\n          end\n        end\n\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":197,"code_uid":"577f59a4afec463bac5f5d1bcd34da07"}
{"diff_hunk":"@@ -48,7 +48,15 @@ class ApplicationController < ActionController::Base\n   private\n \n   def current_user\n-    @current_user ||= User.find_or_create_by(email_address: session[:user]['email']) if session[:user] && session[:user]['email']\n+    @current_user ||= find_current_user\n+  end\n+\n+  def find_current_user\n+    if ENV['FORCE_USER_ID'] && !Rails.env.production?\n+      User.find ENV['FORCE_USER_ID']\n+    else\n+      User.find_or_create_by(email_address: session[:user]['email']) if session[:user] && session[:user]['email']\n+    end\n   end\n \n   def sign_in(user)","old_code":"class ApplicationController < ActionController::Base\n  include Pundit    # For authorization checks\n  include ReturnToHelper\n  include MarkdownHelper\n\n  helper ValueHelper\n  add_template_helper ClientHelper\n\n  protect_from_forgery with: :exception\n  helper_method :current_user, :signed_in?, :return_to\n\n  before_action :disable_peek_by_default\n\n\n  protected\n\n  # We are overriding this method to account for ExceptionPolicies\n  def authorize(record, query=nil, user=nil)\n    user ||= @current_user\n    policy = ::PolicyFinder.policy_for(user, record)\n\n    # use the action as a default permission\n    query ||= (\"can_\" + params[:action].to_s + \"!\").to_sym\n    unless policy.public_send(query)\n      # the method might raise its own exception, or it might return a\n      # boolean. Both systems are accommodated\n      # will need to replace this when a new version of pundit arrives\n      ex = NotAuthorizedError.new(\"not allowed to #{q} this #{record}\")\n      ex.query, ex.record, ex.policy = q, record, pol\n      raise ex\n    end\n  end\n\n  # Override Pundit to account for proposal gymnastics\n  def policy(record)\n    obj = ::PolicyFinder.authorizing_object(record)\n    super(obj)\n  end\n\n  def admin?\n    signed_in? && current_user.admin?\n  end\n\n  def peek_enabled?\n    Rails.env.development? || self.admin?\n  end\n\n  private\n\n  def current_user\n    @current_user ||= User.find_or_create_by(email_address: session[:user]['email']) if session[:user] && session[:user]['email']\n  end\n\n  def sign_in(user)\n    session[:user] ||= {}\n    session[:user]['email'] = user.email_address\n    @current_user = user\n  end\n\n  def sign_out\n    reset_session\n    @current_user = nil\n  end\n\n  def signed_in?\n    !!current_user\n  end\n\n  def authenticate_user!\n    unless signed_in?\n      flash[:error] = 'You need to sign in for access to this page.'\n      redirect_to root_url(return_to: self.make_return_to(\"Previous\", request.fullpath))\n    end\n  end\n\n  def disable_peek_by_default\n    if cookies[:peek].nil?\n      cookies[:peek] = false\n    end\n  end\nend\n","lang_cluster":"Ruby","length":81,"code_uid":"60eb9be1f6dd46be90837036dc0d2a4e"}
{"diff_hunk":"@@ -29,7 +29,7 @@ module Ncr\n \n       super\n \n-      if @model_changing && !@model_instance.emergency # skip approvals if emergency\n+      unless skip_state_update?\n         @model_instance.setup_approvals_and_observers(@approver_email)\n         @model_instance.email_approvers\n       end","old_code":"module Ncr\n  class WorkOrdersController < UseCaseController\n    # arbitrary number...number of upload fields that \"ought to be enough for anybody\"\n    MAX_UPLOADS_ON_NEW = 10\n\n    def new\n      @approver_email = self.suggested_approver_email\n      super\n    end\n\n    def create\n      @approver_email = params[:approver_email]\n      super\n    end\n\n    def edit\n      if self.proposal.approved?\n        flash[:warning] = \"You are about to modify a fully approved request. Changes will be logged and sent to approvers but this request will not require re-approval.\"\n      end\n      first_approver = self.proposal.approvers.first\n      @approver_email = first_approver.try(:email_address)\n\n      super\n    end\n\n    def update\n      @approver_email = params[:approver_email]\n      @model_instance.modifier = current_user\n\n      super\n\n      if @model_changing && !@model_instance.emergency # skip approvals if emergency\n        @model_instance.setup_approvals_and_observers(@approver_email)\n        @model_instance.email_approvers\n      end\n    end\n\n    def attribute_changes?\n      super || @model_instance.approver_changed?(@approver_email)\n    end\n\n    protected\n\n    def model_class\n      Ncr::WorkOrder\n    end\n\n    def suggested_approver_email\n      last_proposal = current_user.last_requested_proposal\n      last_proposal.try(:approvers).try(:first).try(:email_address) || ''\n    end\n\n    def permitted_params\n      fields = Ncr::WorkOrder.relevant_fields(\n        params[:ncr_work_order][:expense_type])\n      if @model_instance\n        fields.delete(:emergency) # emergency field cannot be edited\n      end\n      params.require(:ncr_work_order).permit(:project_title, *fields)\n    end\n\n    def errors\n      results = super\n      if @approver_email.blank? && !@model_instance.approver_email_frozen?\n        results += [\"Approver email is required\"]\n      end\n      results\n    end\n\n    # @pre: @approver_email is set\n    def add_approvals\n      super\n      if self.errors.empty?\n        @model_instance.setup_approvals_and_observers(@approver_email)\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":78,"code_uid":"8dd193e678f84009ac37a2d96c5f4d83"}
{"diff_hunk":"@@ -2,11 +2,17 @@ class AttachmentDecorator < Draper::Decorator\n   include Rails.application.routes.url_helpers\n   include ActionView::Helpers::AssetTagHelper\n   include ActionView::Helpers::UrlHelper\n+  default_url_options[:host] = ::Rails.application.routes.default_url_options[:host]\n+\n   delegate_all\n \n   def file_preview\n     if file.content_type =~ \/\\Aimage\/\n-      image_tag(file.url, alt: \"\", class: \"image-with-border\")\n+      image_tag(\n+        object.url,\n+        alt: \"\",\n+        class: \"image-with-border\"\n+      )\n     else\n       \"<br><table class='button'><tr><td>#{link_text}<\/td><\/tr><\/table>\"\n     end","old_code":"class AttachmentDecorator < Draper::Decorator\n  include Rails.application.routes.url_helpers\n  include ActionView::Helpers::AssetTagHelper\n  include ActionView::Helpers::UrlHelper\n  delegate_all\n\n  def file_preview\n    if file.content_type =~ \/\\Aimage\/\n      image_tag(file.url, alt: \"\", class: \"image-with-border\")\n    else\n      \"<br><table class='button'><tr><td>#{link_text}<\/td><\/tr><\/table>\"\n    end\n  end\n\n  private\n\n  def link_text\n    I18n.t(\n      \"mailer.attachment_mailer.new_attachment_notification.attachment_cta\",\n      attachment_name: file.original_filename\n    )\n  end\nend\n","lang_cluster":"Ruby","length":23,"code_uid":"fc6b983d25e04ca6a047091ce7805d75"}
{"diff_hunk":"@@ -1,2 +1,3 @@\n-class PagesController < ApplicationController\n-end\n+class PagesController < HighVoltage::PagesController\n+  layout false\n+end","old_code":"class PagesController < ApplicationController\nend\n","lang_cluster":"Ruby","length":2,"code_uid":"d58aa8fe4b6249aca75e917ddca9c25b"}
{"diff_hunk":"@@ -71,4 +71,17 @@ ActiveAdmin.register Proposal do\n     flash[:alert] = \"Completed!\"\n     redirect_to admin_proposal_path(resource)\n   end\n+\n+  csv do\n+    proposal_attributes = %w(id status created_at updated_at client_data_type public_id visit_id)\n+    proposal_attributes.each do |proposal_attr|\n+      column(proposal_attr.to_sym) { |proposal| proposal.attributes[proposal_attr] }\n+    end\n+    column(:requester) { |proposal| proposal.requester.display_name }\n+    client_data_attributes = %w(expense_type vendor not_to_exceed building_number emergency rwa_number work_order_code project_title description direct_pay cl_number function_code soc_code ncr_organization_id)\n+    client_data_attributes.each do |data_attr|\n+      column(data_attr.to_sym) { |proposal| proposal.client_data.attributes[data_attr] }\n+    end\n+    column(:approving_offical_name) { |proposal| User.find(proposal.client_data.approving_official_id).display_name }\n+  end\n end","old_code":"ActiveAdmin.register Proposal do\n  actions :index, :show\n\n  permit_params :status\n\n  filter :client_data_type\n  filter :status\n  filter :created_at\n  filter :updated_at\n\n  index do\n    column :id\n    column :status\n    column :name\n    column :public_id\n    column :requester\n    actions\n  end\n\n  # \/:id page\n  show do\n    attributes_table do\n      row :id\n      row :public_id\n      row :name\n      row :status\n      row :requester\n      row :created_at\n      row :updated_at\n    end\n\n    panel \"Steps\" do\n      table_for proposal.individual_steps do |tbl|\n        tbl.column(\"Position\") { |step| link_to step.position, admin_step_path(step) }\n        tbl.column(\"User\") { |step| step.user }\n        tbl.column(\"Status\") { |step| step.status }\n        tbl.column(\"Created\") { |step| step.created_at }\n        tbl.column(\"Updated\") { |step| step.updated_at }\n        tbl.column(\"Completer\") { |step| step.completer }\n        tbl.column(\"Completed\") { |step| step.completed_at }\n      end\n    end\n  end\n\n  action_item :reindex, only: [:show] do\n    link_to \"Re-index\", reindex_admin_proposal_path(proposal), \"data-method\" => :post, title: \"Re-index this proposal\"\n  end\n\n  action_item :fully_complete, only: [:show] do\n    link_to \"Complete\", fully_complete_admin_proposal_path(proposal), \"data-method\" => :post, title: \"Fully complete this proposal\"\n  end\n\n  action_item :fully_complete_no_email, only: [:show] do\n    link_to \"Complete without notifications\", fully_complete_no_email_admin_proposal_path(proposal), \"data-method\" => :post, title: \"Fully complete this proposal without sending notifications to affected subscribers\"\n  end\n\n  member_action :reindex, method: :post do\n    resource.delay.reindex\n    flash[:alert] = \"Re-index scheduled!\"\n    redirect_to admin_proposal_path(resource)\n  end\n\n  member_action :fully_complete, method: :post do\n    resource.fully_complete!(current_user)\n    flash[:alert] = \"Completed!\"\n    redirect_to admin_proposal_path(resource)\n  end\n\n  member_action :fully_complete_no_email, method: :post do\n    resource.fully_complete!(current_user, true)\n    flash[:alert] = \"Completed!\"\n    redirect_to admin_proposal_path(resource)\n  end\nend\n","lang_cluster":"Ruby","length":74,"code_uid":"afaf4c7544384cde96a24525126b82d0"}
{"diff_hunk":"@@ -1,8 +1,13 @@\n class Analytics\n   include AnalyticsHelper\n \n-  SAMPLER = \"sampler\"\n-  SUBSCRIBER = \"subscriber\"\n+  SAMPLER = \"sampler\".freeze\n+  SUBSCRIBER = \"subscriber\".freeze\n+  TRACKERS = {\n+    \"Video\" => VideoTracker,\n+    \"Exercise\" => ExerciseTracker,\n+    \"Trail\" => TrailTracker,\n+  }.freeze\n \n   class_attribute :backend\n   self.backend = AnalyticsRuby","old_code":"class Analytics\n  include AnalyticsHelper\n\n  SAMPLER = \"sampler\"\n  SUBSCRIBER = \"subscriber\"\n\n  class_attribute :backend\n  self.backend = AnalyticsRuby\n\n  def initialize(user)\n    @user = user\n  end\n\n  def track_updated\n    backend.identify(user_id: user.id, traits: identify_hash(user))\n  end\n\n  def track_video_finished(name:, watchable_name:)\n    track(\"Finished video\", name: name, watchable_name: watchable_name)\n  end\n\n  def track_video_started(name:, watchable_name:)\n    track(\"Started video\", name: name, watchable_name: watchable_name)\n    track_touched_video(name: name, watchable_name: watchable_name)\n  end\n\n  def track_searched(query:, results_count:)\n    track(\"Searched\", query: query, results_count: results_count)\n  end\n\n  def track_collaborated(repository_name:)\n    track(\"Created Collaboration\", repository_name: repository_name)\n  end\n\n  def track_accessed_forum\n    track(\"Logged into Forum\")\n  end\n\n  def track_cancelled(reason:)\n    track(\"Cancelled\", reason: reason)\n  end\n\n  def track_flashcard_attempted(deck:, title:)\n    track(\"Flashcard Attempted\", deck: deck, title: title)\n  end\n\n  def track_downloaded(name:, watchable_name:, download_type:)\n    track(\n      \"Downloaded Video\",\n      name: name,\n      watchable_name: watchable_name,\n      download_type: download_type,\n    )\n    track_touched_video(name: name, watchable_name: watchable_name)\n  end\n\n  def track_replied_to_beta_offer(name:, accepted:)\n    track(\n      \"Replied to beta offer\",\n      name: name,\n      accepted: accepted,\n    )\n  end\n\n  def track_authed_to_access(video_name:, watchable_name:)\n    track(\n      \"Authed to Access\",\n      video_name: video_name,\n      watchable_name: watchable_name,\n    )\n  end\n\n  private\n\n  attr_reader :user\n\n  def user_type(user)\n    if user.subscriber?\n      SUBSCRIBER\n    else\n      SAMPLER\n    end\n  end\n\n  def track_touched_video(name:, watchable_name:)\n    track(\"Touched Video\", name: name, watchable_name: watchable_name)\n  end\n\n  def track(event, properties = {})\n    backend.track(\n      event: event,\n      user_id: user.id,\n      properties: properties.merge(\n        email: user.email,\n        user_type: user_type(user),\n      ),\n    )\n  end\nend\n","lang_cluster":"Ruby","length":99,"code_uid":"a21f403723714587bc2ef23b443a7133"}
{"diff_hunk":"@@ -72,9 +72,13 @@ module Beaker\n     def cmd_line host, cmd = @command, env = @environment, pc = @prepend_cmds\n       env_string = host.environment_string( env )\n       prepend_commands = host.prepend_commands( cmd, pc, :cmd_exe => @cmdexe )\n+      if host[:platform] =~ \/cisco_nexus\/ && host[:user] != 'root'\n+          append_command = '\"'\n+        cmd = cmd.gsub('\"') { '\\\\\"' }\n+      end\n \n       # This will cause things like `puppet -t -v agent` which is maybe bad.\n-      cmd_line_array = [env_string, prepend_commands, cmd, options_string, args_string]\n+      cmd_line_array = [env_string, prepend_commands, cmd, options_string, args_string, append_command]\n       cmd_line_array.compact.reject( &:empty? ).join( ' ' )\n     end\n ","old_code":"module Beaker\n  # An object that represents a \"command\" on a remote host. Is responsible\n  # for munging the environment correctly. Probably poorly named.\n  #\n  # @api public\n  class Command\n\n    # A string representing the (possibly) incomplete command\n    attr_accessor :command\n\n    # A hash key-values where the keys are environment variables to be set\n    attr_accessor :environment\n\n    # A hash of options. Keys with values of nil are considered flags\n    attr_accessor :options\n\n    # An array of additional arguments to be supplied to the command\n    attr_accessor :args\n\n    # @param [String] command The program to call, either an absolute path\n    #                         or one in the PATH (can be overridden)\n    # @param [Array]  args    These are addition arguments to the command\n    # @param [Hash]   options These are addition options to the command. They\n    #                         will be added in \"--key=value\" after the command\n    #                         but before the arguments. There is a special key,\n    #                         'ENV', that won't be used as a command option,\n    #                         but instead can be used to set any default\n    #                         environment variables\n    #\n    # @example Recommended usage programmatically:\n    #     Command.new('git add', files, :patch => true, 'ENV' => {'PATH' => '\/opt\/csw\/bin'})\n    #\n    # @example My favorite example of a signature that we must maintain\n    #     Command.new('puppet', :resource, 'scheduled_task', name,\n    #                 [ 'ensure=present',\n    #                   'command=c:\\\\\\\\windows\\\\\\\\system32\\\\\\\\notepad2.exe',\n    #                   \"arguments=args-#{name}\" ] )\n    #\n    # @note For backwards compatability we must support any number of strings\n    #       or symbols (or arrays of strings an symbols) and essentially\n    #       ensure they are in a flattened array, coerced to strings, and\n    #       call #join(' ') on it.  We have options for the command line\n    #       invocation that must be turned into '--key=value' and similarly\n    #       joined as well as a hash of environment key value pairs, and\n    #       finally we need a hash of options to control the default envs that\n    #       are included.\n    def initialize command, args = [], options = {}\n      @command = command\n      @options = options\n      @args    = args\n      @environment = {}\n      @cmdexe = @options.delete(:cmdexe) || false\n      @prepend_cmds = @options.delete(:prepend_cmds) || nil\n\n      # this is deprecated and will not allow you to use a command line\n      # option of `--environment`, please use ENV instead.\n      [:ENV, :environment, 'environment', 'ENV'].each do |k|\n         if @options[k].is_a?(Hash)\n           @environment = @environment.merge(@options.delete(k))\n         end\n      end\n\n    end\n\n    # @param [Host]   host An object that implements {Beaker::Host}'s\n    #                      interface.\n    # @param [String] cmd  An command to call.\n    # @param [Hash]   env  An optional hash of environment variables to be used\n    # @param [String] pc   An optional list of commands to prepend \n    #\n    # @return [String] This returns the fully formed command line invocation.\n    def cmd_line host, cmd = @command, env = @environment, pc = @prepend_cmds\n      env_string = host.environment_string( env )\n      prepend_commands = host.prepend_commands( cmd, pc, :cmd_exe => @cmdexe )\n\n      # This will cause things like `puppet -t -v agent` which is maybe bad.\n      cmd_line_array = [env_string, prepend_commands, cmd, options_string, args_string]\n      cmd_line_array.compact.reject( &:empty? ).join( ' ' )\n    end\n\n    # @param [Hash] opts These are the options that the command takes\n    #\n    # @return [String] String of the options and flags for command.\n    #\n    # @note Why no. Not the least bit Unixy, why do you ask?\n    def options_string opts = @options\n      flags = []\n      options = opts.dup\n      options.each_key do |key|\n        if options[key] == nil\n          flags << key\n          options.delete(key)\n        end\n      end\n\n      short_flags, long_flags = flags.partition {|flag| flag.to_s.length == 1 }\n      parsed_short_flags = short_flags.map {|f| \"-#{f}\" }\n      parsed_long_flags = long_flags.map {|f| \"--#{f}\" }\n\n      short_opts, long_opts = {}, {}\n      options.each_key do |key|\n        if key.to_s.length == 1\n          short_opts[key] = options[key]\n        else\n          long_opts[key] = options[key]\n        end\n      end\n      parsed_short_opts = short_opts.map {|k,v| \"-#{k}=#{v}\" }\n      parsed_long_opts = long_opts.map {|k,v| \"--#{k}=#{v}\" }\n\n      return (parsed_short_flags +\n              parsed_long_flags +\n              parsed_short_opts + parsed_long_opts).join(' ')\n    end\n\n    # @param [Array] args An array of arguments to the command.\n    #\n    # @return [String] String of the arguments for command.\n    def args_string args = @args\n      args.flatten.compact.join(' ')\n    end\n\n\n\n  end\n\n  class PuppetCommand < Command\n    def initialize *args\n      command = \"puppet #{args.shift}\"\n      opts = args.last.is_a?(Hash) ? args.pop : Hash.new\n      opts['ENV'] ||= Hash.new\n      opts[:cmdexe] = true\n      super( command, args, opts )\n    end\n  end\n\n  class HostCommand < Command\n    def cmd_line host\n      eval \"\\\"#{@command}\\\"\"\n    end\n  end\n\n  class SedCommand < Command\n\n    # sets up a SedCommand for a particular platform\n    #\n    # the purpose is to abstract away platform-dependent details of the sed command\n    #\n    # @param [String] platform The host platform string\n    # @param [String] expression The sed expression\n    # @param [String] filename The file to apply the sed expression to\n    # @param [Hash{Symbol=>String}] opts Additional options\n    # @option opts [String] :temp_file The temp file to use for in-place substitution\n    #         (only applies to solaris hosts, they don't provide the -i option)\n    #\n    # @return a new {SedCommand} object\n    def initialize platform, expression, filename, opts = {}\n      command = \"sed -i -e \\\"#{expression}\\\" #{filename}\"\n      if platform =~ \/solaris|aix|osx|openbsd\/\n        command.slice! '-i '\n        temp_file = opts[:temp_file] ? opts[:temp_file] : \"#{filename}.tmp\"\n        command << \" > #{temp_file} && mv #{temp_file} #{filename} && rm -f #{temp_file}\"\n      end\n      args = []\n      opts['ENV'] ||= Hash.new\n      super( command, args, opts )\n    end\n  end\nend\n","lang_cluster":"Ruby","length":169,"code_uid":"089f5b1b4ef6472f8dd50101a02e4dfd"}
{"diff_hunk":"@@ -6,6 +6,7 @@ module Travis\n \n         def setup\n           super\n+          cmd \"export PATH=\/usr\/local\/ghc\/#{ghc_version}\/bin\/:$PATH\"\n           cmd 'cabal update', fold: 'cabal', retry: true\n         end\n ","old_code":"module Travis\n  module Build\n    class Script\n      class Haskell < Script\n        DEFAULTS = {}\n\n        def setup\n          super\n          cmd 'cabal update', fold: 'cabal', retry: true\n        end\n\n        def announce\n          super\n          cmd 'ghc --version'\n          cmd 'cabal --version'\n        end\n\n        def install\n          cmd 'cabal install --only-dependencies --enable-tests', fold: 'install', retry: true\n        end\n\n        def script\n          cmd 'cabal configure --enable-tests && cabal build && cabal test'\n        end\n      end\n    end\n  end\nend\n","lang_cluster":"Ruby","length":28,"code_uid":"e35f7e54780549f6ad5584ea7add7928"}
{"diff_hunk":"@@ -32,5 +32,24 @@ module Bolt\n       fs = ::WinRM::FS::FileManager.new(@connection)\n       fs.upload(source, destination)\n     end\n+\n+    def make_tempdir\n+      execute(<<-EOS).stdout.chomp\n+$parent = [System.IO.Path]::GetTempPath()\n+$name = [System.IO.Path]::GetRandomFileName()\n+$path = Join-Path $parent $name\n+New-Item -ItemType Directory -Path $path | Out-Null\n+$path\n+EOS\n+    end\n+\n+    def run_script(script)\n+      dir = make_tempdir\n+      remote_path = \"#{dir}\\\\#{File.basename(script, File.extname(script))}.ps1\"\n+      copy(script, remote_path)\n+      args = '-NoProfile -NonInteractive -NoLogo -ExecutionPolicy Bypass'\n+      execute(\"powershell.exe #{args} -File '#{remote_path}'\")\n+      execute(\"Remove-Item -force -recurse '#{dir}'\")\n+    end\n   end\n end","old_code":"require 'winrm'\nrequire 'winrm-fs'\n\nmodule Bolt\n  class WinRM < Node\n    def initialize(endpoint, user, password, shell = :powershell)\n      @endpoint = endpoint\n      @user = user\n      @password = password\n      @shell = shell\n      @connection = ::WinRM::Connection.new(endpoint: @endpoint,\n                                            user: @user,\n                                            password: @password)\n    end\n\n    def connect\n      @session = @connection.shell(@shell)\n    end\n\n    def disconnect\n      @session.close if @session\n    end\n\n    def execute(command)\n      @session.run(command) do |stdout, stderr|\n        print stdout\n        print stderr\n      end\n    end\n\n    def copy(source, destination)\n      fs = ::WinRM::FS::FileManager.new(@connection)\n      fs.upload(source, destination)\n    end\n  end\nend\n","lang_cluster":"Ruby","length":36,"code_uid":"75c8d803ec734c0584e1c4da0907ec41"}
{"diff_hunk":"@@ -1,6 +1,11 @@\n require File.expand_path('..\/boot', __FILE__)\n \n-require 'rails\/all'\n+# Pick the frameworks you want:\n+require 'active_record\/railtie'\n+require 'action_controller\/railtie'\n+require 'action_mailer\/railtie'\n+require 'sprockets\/railtie'\n+# require \"rails\/test_unit\/railtie\"\n \n # If you have a Gemfile, require the default gems, the ones in the\n # current environment and also include :assets gems if in development","old_code":"require File.expand_path('..\/boot', __FILE__)\n\nrequire 'rails\/all'\n\n# If you have a Gemfile, require the default gems, the ones in the\n# current environment and also include :assets gems if in development\n# or test environments.\nBundler.require(:default, Rails.env)\n\nmodule Workshops\n  class Application < Rails::Application\n    # Settings in config\/environments\/* take precedence over those specified here.\n    # Application configuration should go into files in config\/initializers\n    # -- all .rb files in that directory are automatically loaded.\n\n    # Custom directories with classes and modules you want to be autoloadable.\n    # config.autoload_paths += %W(#{config.root}\/extras)\n    config.autoload_paths += [\n      \"#{config.root}\/lib\",\n      \"#{config.root}\/app\/serializers\",\n      \"#{config.root}\/app\/jobs\",\n      \"#{config.root}\/app\/modules\"\n    ]\n\n    config.paths['app\/views'] << \"#{config.root}\/app\/modules\/teams\/views\"\n\n    # Set Time.zone default to the specified zone and make Active Record auto-convert to this zone.\n    # Run \"rake -D time\" for a list of tasks for finding time zone names. Default is UTC.\n    # config.time_zone = 'Central Time (US & Canada)'\n\n    # The default locale is :en and all translations from config\/locales\/*.rb,yml are auto loaded.\n    # config.i18n.load_path += Dir[Rails.root.join('my', 'locales', '*.{rb,yml}').to_s]\n    # config.i18n.default_locale = :de\n    config.i18n.enforce_available_locales = true\n  end\nend\n\nBLACKLIST_TOPICS = [\"this week in open source\", \"thoughtbot\", \"hoptoad\", \"airbrake\"]\n","lang_cluster":"Ruby","length":38,"code_uid":"83ec5374e90e4cf2bdb6890b382529b7"}
{"diff_hunk":"@@ -35,12 +35,29 @@ describe AuthHashService, '#find_or_create_user_from_auth_hash' do\n   end\n \n   context \"when a new account is created\" do\n-    it \"notififes analytics of account_created\" do\n-      tracker = stub_analytics_tracker\n+    context \"and successfully saves\" do\n+      it \"notififes analytics of account_created\" do\n+        tracker = stub_analytics_tracker\n \n-      AuthHashService.new(auth_hash).find_or_create_user_from_auth_hash\n+        AuthHashService.new(auth_hash).\n+          find_or_create_user_from_auth_hash\n+\n+        expect(tracker).to have_received(:track_account_created)\n+      end\n+    end\n+\n+    context \"and fails to save\" do\n+      it \"does not notify analytics of account_created\" do\n+        tracker = stub_analytics_tracker\n+        invalid_auth_hash = auth_hash.merge(\n+          \"info\" => { \"name\" => nil, \"nickname\" => nil },\n+        )\n+\n+        AuthHashService.new(invalid_auth_hash).\n+          find_or_create_user_from_auth_hash\n \n-      expect(tracker).to have_received(:track_account_created)\n+        expect(tracker).not_to have_received(:track_account_created)\n+      end\n     end\n   end\n ","old_code":"require \"rails_helper\"\n\ndescribe AuthHashService, '#find_or_create_user_from_auth_hash' do\n  it 'creates a user using nickname as a name when name is blank in auth_hash' do\n    hash = auth_hash('info' => {'name' => nil,\n                                'email' => 'user@example.com',\n                                'nickname' => 'thoughtbot'})\n    user = AuthHashService.new(hash).find_or_create_user_from_auth_hash\n\n    expect(user).to be_persisted\n    expect(user.name).to eq 'thoughtbot'\n    expect(user.github_username).to eq 'thoughtbot'\n  end\n\n  it 'creates a new user when no matching user' do\n    stub_team_member false\n\n    user = AuthHashService.new(auth_hash).find_or_create_user_from_auth_hash\n\n    expect(user).to be_persisted\n    expect(user.name).to eq 'Test User'\n    expect(user.email).to eq 'user@example.com'\n    expect(user.github_username).to eq 'thoughtbot'\n    expect(user.auth_provider).to eq 'github'\n    expect(user.auth_uid).to eq 1\n    expect(user).not_to be_admin\n  end\n\n  it 'creates a new admin when no matching user from our organization' do\n    stub_team_member true\n\n    user = AuthHashService.new(auth_hash).find_or_create_user_from_auth_hash\n\n    expect(user).to be_admin\n  end\n\n  context \"when a new account is created\" do\n    it \"notififes analytics of account_created\" do\n      tracker = stub_analytics_tracker\n\n      AuthHashService.new(auth_hash).find_or_create_user_from_auth_hash\n\n      expect(tracker).to have_received(:track_account_created)\n    end\n  end\n\n  context 'with an existing user' do\n    it \"finds the user by email\" do\n      existing_user = create(:user, email: auth_hash[\"info\"][\"email\"])\n\n      expect(existing_user).to eq AuthHashService.new(auth_hash).\n        find_or_create_user_from_auth_hash\n      expect(existing_user.reload.auth_provider).to eq auth_hash[\"provider\"]\n      expect(existing_user.auth_uid).to eq auth_hash[\"uid\"]\n    end\n\n    it \"finds the user by github username\" do\n      github_username = \"a_github_username\"\n      existing_user = create(:user, github_username: github_username)\n      options = { \"info\" => { \"nickname\" => github_username } }\n\n      expect(existing_user).to eq AuthHashService.new(auth_hash(options)).\n        find_or_create_user_from_auth_hash\n      expect(existing_user.reload.auth_provider).to eq auth_hash[\"provider\"]\n      expect(existing_user.auth_uid).to eq auth_hash[\"uid\"]\n    end\n\n    it \"finds the user by auth_provider and auth_uid\" do\n      existing_user = create(:user, auth_provider: 'github', auth_uid: 1)\n\n      expect(existing_user).to eq AuthHashService.new(auth_hash).\n        find_or_create_user_from_auth_hash\n    end\n\n    it \"does not track account_created\" do\n      tracker = stub_analytics_tracker\n      create_user_with_github_auth\n\n      AuthHashService.new(auth_hash).find_or_create_user_from_auth_hash\n\n      expect(tracker).not_to have_received(:track_account_created)\n    end\n  end\n\n  def stub_team_member(return_value)\n    client = double(\"github_client\")\n    allow(client).to receive(:team_member?).\n      with(AuthHashService::THOUGHTBOT_GITHUB_TEAM_ID, 'thoughtbot').\n      and_return(return_value)\n    allow(Octokit::Client).to receive(:new).\n      with(access_token: GITHUB_ACCESS_TOKEN).\n      and_return(client)\n  end\n\n  def auth_hash(options = {})\n    {\n      'provider' => 'github',\n      'uid' => 1,\n      'info' => {\n        'email' => 'user@example.com',\n        'name' => 'Test User',\n        'nickname' => 'thoughtbot',\n      }\n    }.merge(options)\n  end\n\n  def stub_analytics_tracker\n    double(\"Analytics\").tap do |tracker|\n      allow(tracker).to receive(:track_account_created)\n      allow(tracker).to receive(:track_updated)\n      allow(Analytics).to receive(:new).and_return(tracker)\n    end\n  end\n\n  def create_user_with_github_auth\n    create(:user, auth_provider: \"github\", auth_uid: auth_hash[\"uid\"])\n  end\nend\n","lang_cluster":"Ruby","length":118,"code_uid":"b2b29f050b6241c6930213eac7f3ba92"}
